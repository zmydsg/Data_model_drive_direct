
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.12033862620592117 
model_pd.l_d.mean(): -24.47382926940918 
model_pd.lagr.mean(): -24.353490829467773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0805], device='cuda:0')), ('power', tensor([-24.5543], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.12033862620592117
epoch£º0	 i:1 	 global-step:1	 l-p:0.15623173117637634
epoch£º0	 i:2 	 global-step:2	 l-p:0.15599572658538818
epoch£º0	 i:3 	 global-step:3	 l-p:0.13998126983642578
epoch£º0	 i:4 	 global-step:4	 l-p:-0.8872278928756714
epoch£º0	 i:5 	 global-step:5	 l-p:0.11750897765159607
epoch£º0	 i:6 	 global-step:6	 l-p:0.13981249928474426
epoch£º0	 i:7 	 global-step:7	 l-p:0.11974195390939713
epoch£º0	 i:8 	 global-step:8	 l-p:0.03877580538392067
epoch£º0	 i:9 	 global-step:9	 l-p:0.11709310114383698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2256, 3.2260, 3.2256],
        [3.2256, 3.4164, 3.3781],
        [3.2256, 3.2803, 3.2454],
        [3.2256, 4.1137, 4.7313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.10776599496603012 
model_pd.l_d.mean(): -23.923336029052734 
model_pd.lagr.mean(): -23.815570831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2747], device='cuda:0')), ('power', tensor([-23.6487], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.10776599496603012
epoch£º1	 i:1 	 global-step:21	 l-p:0.1131468340754509
epoch£º1	 i:2 	 global-step:22	 l-p:0.10735069215297699
epoch£º1	 i:3 	 global-step:23	 l-p:0.09871793538331985
epoch£º1	 i:4 	 global-step:24	 l-p:0.1287594437599182
epoch£º1	 i:5 	 global-step:25	 l-p:0.11391725391149521
epoch£º1	 i:6 	 global-step:26	 l-p:0.12586139142513275
epoch£º1	 i:7 	 global-step:27	 l-p:0.1297922283411026
epoch£º1	 i:8 	 global-step:28	 l-p:0.1225266084074974
epoch£º1	 i:9 	 global-step:29	 l-p:0.09531988948583603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9208, 3.4956, 3.7844],
        [2.9208, 2.9750, 2.9420],
        [2.9208, 3.3585, 3.5046],
        [2.9208, 2.9663, 2.9367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.1364690661430359 
model_pd.l_d.mean(): -24.47289276123047 
model_pd.lagr.mean(): -24.336423873901367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1897], device='cuda:0')), ('power', tensor([-24.2832], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.1364690661430359
epoch£º2	 i:1 	 global-step:41	 l-p:0.1863512396812439
epoch£º2	 i:2 	 global-step:42	 l-p:0.13883858919143677
epoch£º2	 i:3 	 global-step:43	 l-p:0.12673039734363556
epoch£º2	 i:4 	 global-step:44	 l-p:0.13562938570976257
epoch£º2	 i:5 	 global-step:45	 l-p:0.21877653896808624
epoch£º2	 i:6 	 global-step:46	 l-p:0.11812800914049149
epoch£º2	 i:7 	 global-step:47	 l-p:0.2314431518316269
epoch£º2	 i:8 	 global-step:48	 l-p:0.1351613849401474
epoch£º2	 i:9 	 global-step:49	 l-p:0.1271619349718094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8749, 3.3876, 3.6167],
        [2.8749, 2.8999, 2.8809],
        [2.8749, 2.9266, 2.8948],
        [2.8749, 3.6070, 4.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.13393445312976837 
model_pd.l_d.mean(): -23.677711486816406 
model_pd.lagr.mean(): -23.543777465820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0696], device='cuda:0')), ('power', tensor([-23.6081], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.13393445312976837
epoch£º3	 i:1 	 global-step:61	 l-p:0.15742972493171692
epoch£º3	 i:2 	 global-step:62	 l-p:0.11244471371173859
epoch£º3	 i:3 	 global-step:63	 l-p:0.12329031527042389
epoch£º3	 i:4 	 global-step:64	 l-p:0.14087189733982086
epoch£º3	 i:5 	 global-step:65	 l-p:0.11345462501049042
epoch£º3	 i:6 	 global-step:66	 l-p:0.11638368666172028
epoch£º3	 i:7 	 global-step:67	 l-p:0.12700121104717255
epoch£º3	 i:8 	 global-step:68	 l-p:0.12586621940135956
epoch£º3	 i:9 	 global-step:69	 l-p:0.13128750026226044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9794, 3.3690, 3.4682],
        [2.9794, 3.4537, 3.6308],
        [2.9794, 3.7974, 4.3829],
        [2.9794, 3.0279, 2.9969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.1179668977856636 
model_pd.l_d.mean(): -23.86979103088379 
model_pd.lagr.mean(): -23.75182342529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1574], device='cuda:0')), ('power', tensor([-23.7124], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.1179668977856636
epoch£º4	 i:1 	 global-step:81	 l-p:0.12162087857723236
epoch£º4	 i:2 	 global-step:82	 l-p:0.12294978648424149
epoch£º4	 i:3 	 global-step:83	 l-p:0.11910766363143921
epoch£º4	 i:4 	 global-step:84	 l-p:0.1431090086698532
epoch£º4	 i:5 	 global-step:85	 l-p:0.12753446400165558
epoch£º4	 i:6 	 global-step:86	 l-p:0.1403995156288147
epoch£º4	 i:7 	 global-step:87	 l-p:0.12259811162948608
epoch£º4	 i:8 	 global-step:88	 l-p:0.11314066499471664
epoch£º4	 i:9 	 global-step:89	 l-p:0.15153400599956512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9345, 2.9346, 2.9345],
        [2.9345, 2.9380, 2.9347],
        [2.9345, 2.9881, 2.9556],
        [2.9345, 3.0113, 2.9725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.15390753746032715 
model_pd.l_d.mean(): -24.3869571685791 
model_pd.lagr.mean(): -24.233049392700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1706], device='cuda:0')), ('power', tensor([-24.2164], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.15390753746032715
epoch£º5	 i:1 	 global-step:101	 l-p:0.11670949310064316
epoch£º5	 i:2 	 global-step:102	 l-p:0.11712371557950974
epoch£º5	 i:3 	 global-step:103	 l-p:0.12532100081443787
epoch£º5	 i:4 	 global-step:104	 l-p:0.15207991003990173
epoch£º5	 i:5 	 global-step:105	 l-p:0.1284780353307724
epoch£º5	 i:6 	 global-step:106	 l-p:0.11871394515037537
epoch£º5	 i:7 	 global-step:107	 l-p:0.12135492265224457
epoch£º5	 i:8 	 global-step:108	 l-p:0.12177775800228119
epoch£º5	 i:9 	 global-step:109	 l-p:0.1166517585515976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9792, 2.9945, 2.9819],
        [2.9792, 2.9792, 2.9792],
        [2.9792, 3.7402, 4.2534],
        [2.9792, 2.9819, 2.9794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.12990251183509827 
model_pd.l_d.mean(): -24.6672420501709 
model_pd.lagr.mean(): -24.53734016418457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2477], device='cuda:0')), ('power', tensor([-24.4195], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.12990251183509827
epoch£º6	 i:1 	 global-step:121	 l-p:0.11832030117511749
epoch£º6	 i:2 	 global-step:122	 l-p:0.14484575390815735
epoch£º6	 i:3 	 global-step:123	 l-p:0.11133626103401184
epoch£º6	 i:4 	 global-step:124	 l-p:0.1696237474679947
epoch£º6	 i:5 	 global-step:125	 l-p:0.12318462878465652
epoch£º6	 i:6 	 global-step:126	 l-p:0.12460335344076157
epoch£º6	 i:7 	 global-step:127	 l-p:0.1434207558631897
epoch£º6	 i:8 	 global-step:128	 l-p:0.11829575896263123
epoch£º6	 i:9 	 global-step:129	 l-p:0.12945161759853363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0274, 3.3849, 3.4576],
        [3.0274, 3.0620, 3.0376],
        [3.0274, 3.0275, 3.0274],
        [3.0274, 3.0391, 3.0292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12662453949451447 
model_pd.l_d.mean(): -24.579347610473633 
model_pd.lagr.mean(): -24.452722549438477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2524], device='cuda:0')), ('power', tensor([-24.3270], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12662453949451447
epoch£º7	 i:1 	 global-step:141	 l-p:0.1374436467885971
epoch£º7	 i:2 	 global-step:142	 l-p:0.12060954421758652
epoch£º7	 i:3 	 global-step:143	 l-p:0.1206703931093216
epoch£º7	 i:4 	 global-step:144	 l-p:0.12406408041715622
epoch£º7	 i:5 	 global-step:145	 l-p:0.13588589429855347
epoch£º7	 i:6 	 global-step:146	 l-p:0.1710631102323532
epoch£º7	 i:7 	 global-step:147	 l-p:0.14493328332901
epoch£º7	 i:8 	 global-step:148	 l-p:0.11248449981212616
epoch£º7	 i:9 	 global-step:149	 l-p:0.11096993088722229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9928, 2.9928, 2.9928],
        [2.9928, 2.9928, 2.9928],
        [2.9928, 3.4413, 3.5989],
        [2.9928, 3.0733, 3.0341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13157469034194946 
model_pd.l_d.mean(): -23.752622604370117 
model_pd.lagr.mean(): -23.621047973632812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1528], device='cuda:0')), ('power', tensor([-23.5999], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13157469034194946
epoch£º8	 i:1 	 global-step:161	 l-p:0.1220419630408287
epoch£º8	 i:2 	 global-step:162	 l-p:0.12351909279823303
epoch£º8	 i:3 	 global-step:163	 l-p:0.12446314841508865
epoch£º8	 i:4 	 global-step:164	 l-p:0.0888684093952179
epoch£º8	 i:5 	 global-step:165	 l-p:0.12393789738416672
epoch£º8	 i:6 	 global-step:166	 l-p:0.10921338200569153
epoch£º8	 i:7 	 global-step:167	 l-p:0.1744295060634613
epoch£º8	 i:8 	 global-step:168	 l-p:0.12596097588539124
epoch£º8	 i:9 	 global-step:169	 l-p:0.12539008259773254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9972, 2.9982, 2.9972],
        [2.9972, 2.9982, 2.9972],
        [2.9972, 3.0766, 3.0378],
        [2.9972, 3.0083, 2.9988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.15287259221076965 
model_pd.l_d.mean(): -23.232685089111328 
model_pd.lagr.mean(): -23.07981300354004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1244], device='cuda:0')), ('power', tensor([-23.1083], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.15287259221076965
epoch£º9	 i:1 	 global-step:181	 l-p:0.12551631033420563
epoch£º9	 i:2 	 global-step:182	 l-p:0.10874101519584656
epoch£º9	 i:3 	 global-step:183	 l-p:0.13128477334976196
epoch£º9	 i:4 	 global-step:184	 l-p:0.11302938312292099
epoch£º9	 i:5 	 global-step:185	 l-p:0.11586610227823257
epoch£º9	 i:6 	 global-step:186	 l-p:0.12223751842975616
epoch£º9	 i:7 	 global-step:187	 l-p:0.12883159518241882
epoch£º9	 i:8 	 global-step:188	 l-p:0.13497954607009888
epoch£º9	 i:9 	 global-step:189	 l-p:0.12585894763469696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8590, 3.4821, 3.8534],
        [2.8590, 3.2840, 3.4384],
        [2.8590, 2.8615, 2.8591],
        [2.8590, 2.8590, 2.8590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.1417713314294815 
model_pd.l_d.mean(): -24.468774795532227 
model_pd.lagr.mean(): -24.327003479003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1494], device='cuda:0')), ('power', tensor([-24.3194], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.1417713314294815
epoch£º10	 i:1 	 global-step:201	 l-p:0.27795320749282837
epoch£º10	 i:2 	 global-step:202	 l-p:0.2713924050331116
epoch£º10	 i:3 	 global-step:203	 l-p:0.12900996208190918
epoch£º10	 i:4 	 global-step:204	 l-p:0.13719329237937927
epoch£º10	 i:5 	 global-step:205	 l-p:0.1340588927268982
epoch£º10	 i:6 	 global-step:206	 l-p:-0.19827458262443542
epoch£º10	 i:7 	 global-step:207	 l-p:0.128431037068367
epoch£º10	 i:8 	 global-step:208	 l-p:0.12818261981010437
epoch£º10	 i:9 	 global-step:209	 l-p:0.131196990609169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9005, 3.1688, 3.1924],
        [2.9005, 3.5784, 4.0108],
        [2.9005, 3.2684, 3.3674],
        [2.9005, 2.9124, 2.9024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.12280435115098953 
model_pd.l_d.mean(): -24.40215492248535 
model_pd.lagr.mean(): -24.27935028076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1618], device='cuda:0')), ('power', tensor([-24.2403], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.12280435115098953
epoch£º11	 i:1 	 global-step:221	 l-p:0.12713170051574707
epoch£º11	 i:2 	 global-step:222	 l-p:0.16267280280590057
epoch£º11	 i:3 	 global-step:223	 l-p:0.14571841061115265
epoch£º11	 i:4 	 global-step:224	 l-p:0.1261797547340393
epoch£º11	 i:5 	 global-step:225	 l-p:0.12158329039812088
epoch£º11	 i:6 	 global-step:226	 l-p:0.12595513463020325
epoch£º11	 i:7 	 global-step:227	 l-p:0.12492869794368744
epoch£º11	 i:8 	 global-step:228	 l-p:0.2059941291809082
epoch£º11	 i:9 	 global-step:229	 l-p:0.12344181537628174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9478, 3.1523, 3.1387],
        [2.9478, 2.9663, 2.9517],
        [2.9478, 2.9635, 2.9508],
        [2.9478, 3.0340, 2.9958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.1563117355108261 
model_pd.l_d.mean(): -24.006811141967773 
model_pd.lagr.mean(): -23.850500106811523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1227], device='cuda:0')), ('power', tensor([-23.8841], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.1563117355108261
epoch£º12	 i:1 	 global-step:241	 l-p:0.12310846149921417
epoch£º12	 i:2 	 global-step:242	 l-p:0.12110182642936707
epoch£º12	 i:3 	 global-step:243	 l-p:0.11956682056188583
epoch£º12	 i:4 	 global-step:244	 l-p:0.12897734344005585
epoch£º12	 i:5 	 global-step:245	 l-p:0.12799592316150665
epoch£º12	 i:6 	 global-step:246	 l-p:0.12067283689975739
epoch£º12	 i:7 	 global-step:247	 l-p:0.10315204411745071
epoch£º12	 i:8 	 global-step:248	 l-p:0.1312023252248764
epoch£º12	 i:9 	 global-step:249	 l-p:0.19106435775756836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9148, 3.4976, 3.8158],
        [2.9148, 2.9148, 2.9148],
        [2.9148, 2.9165, 2.9149],
        [2.9148, 3.1142, 3.1005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.14103834331035614 
model_pd.l_d.mean(): -23.718503952026367 
model_pd.lagr.mean(): -23.577465057373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1180], device='cuda:0')), ('power', tensor([-23.6005], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.14103834331035614
epoch£º13	 i:1 	 global-step:261	 l-p:0.13420352339744568
epoch£º13	 i:2 	 global-step:262	 l-p:0.1168648898601532
epoch£º13	 i:3 	 global-step:263	 l-p:0.120892234146595
epoch£º13	 i:4 	 global-step:264	 l-p:0.12489595264196396
epoch£º13	 i:5 	 global-step:265	 l-p:0.12159951776266098
epoch£º13	 i:6 	 global-step:266	 l-p:0.1918051540851593
epoch£º13	 i:7 	 global-step:267	 l-p:1.0637760162353516
epoch£º13	 i:8 	 global-step:268	 l-p:0.12438526004552841
epoch£º13	 i:9 	 global-step:269	 l-p:0.1548273265361786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9322, 2.9322, 2.9322],
        [2.9322, 3.3924, 3.5776],
        [2.9322, 2.9493, 2.9357],
        [2.9322, 3.3582, 3.5101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.11936375498771667 
model_pd.l_d.mean(): -23.689416885375977 
model_pd.lagr.mean(): -23.570053100585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1388], device='cuda:0')), ('power', tensor([-23.5507], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.11936375498771667
epoch£º14	 i:1 	 global-step:281	 l-p:0.11038560420274734
epoch£º14	 i:2 	 global-step:282	 l-p:0.11817453056573868
epoch£º14	 i:3 	 global-step:283	 l-p:0.13677123188972473
epoch£º14	 i:4 	 global-step:284	 l-p:0.12743033468723297
epoch£º14	 i:5 	 global-step:285	 l-p:0.10242354869842529
epoch£º14	 i:6 	 global-step:286	 l-p:0.13154512643814087
epoch£º14	 i:7 	 global-step:287	 l-p:0.13079676032066345
epoch£º14	 i:8 	 global-step:288	 l-p:0.14637435972690582
epoch£º14	 i:9 	 global-step:289	 l-p:0.12189865857362747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0010, 3.0171, 3.0041],
        [3.0010, 3.0029, 3.0011],
        [3.0010, 3.0912, 3.0527],
        [3.0010, 3.0064, 3.0015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.12690649926662445 
model_pd.l_d.mean(): -24.621623992919922 
model_pd.lagr.mean(): -24.49471664428711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2454], device='cuda:0')), ('power', tensor([-24.3762], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.12690649926662445
epoch£º15	 i:1 	 global-step:301	 l-p:0.11763694137334824
epoch£º15	 i:2 	 global-step:302	 l-p:0.13112850487232208
epoch£º15	 i:3 	 global-step:303	 l-p:0.1214975118637085
epoch£º15	 i:4 	 global-step:304	 l-p:0.12885168194770813
epoch£º15	 i:5 	 global-step:305	 l-p:0.1250174343585968
epoch£º15	 i:6 	 global-step:306	 l-p:0.25387272238731384
epoch£º15	 i:7 	 global-step:307	 l-p:0.4735668897628784
epoch£º15	 i:8 	 global-step:308	 l-p:0.10564835369586945
epoch£º15	 i:9 	 global-step:309	 l-p:0.1276254802942276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9427, 2.9437, 2.9428],
        [2.9427, 2.9556, 2.9450],
        [2.9427, 2.9429, 2.9428],
        [2.9427, 3.1216, 3.1002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.11556152254343033 
model_pd.l_d.mean(): -23.902481079101562 
model_pd.lagr.mean(): -23.78691864013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1697], device='cuda:0')), ('power', tensor([-23.7327], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.11556152254343033
epoch£º16	 i:1 	 global-step:321	 l-p:0.1354447901248932
epoch£º16	 i:2 	 global-step:322	 l-p:0.11594164371490479
epoch£º16	 i:3 	 global-step:323	 l-p:0.13988491892814636
epoch£º16	 i:4 	 global-step:324	 l-p:0.13526783883571625
epoch£º16	 i:5 	 global-step:325	 l-p:0.1366182118654251
epoch£º16	 i:6 	 global-step:326	 l-p:0.11316756904125214
epoch£º16	 i:7 	 global-step:327	 l-p:0.1419343501329422
epoch£º16	 i:8 	 global-step:328	 l-p:0.12258408218622208
epoch£º16	 i:9 	 global-step:329	 l-p:0.13692550361156464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0113, 3.4904, 3.6881],
        [3.0113, 3.1705, 3.1406],
        [3.0113, 3.0114, 3.0113],
        [3.0113, 3.4840, 3.6753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.14987999200820923 
model_pd.l_d.mean(): -24.433488845825195 
model_pd.lagr.mean(): -24.28360939025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2195], device='cuda:0')), ('power', tensor([-24.2140], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.14987999200820923
epoch£º17	 i:1 	 global-step:341	 l-p:0.12653081119060516
epoch£º17	 i:2 	 global-step:342	 l-p:0.1132335290312767
epoch£º17	 i:3 	 global-step:343	 l-p:0.16084223985671997
epoch£º17	 i:4 	 global-step:344	 l-p:0.12554991245269775
epoch£º17	 i:5 	 global-step:345	 l-p:0.1202460452914238
epoch£º17	 i:6 	 global-step:346	 l-p:0.1299247145652771
epoch£º17	 i:7 	 global-step:347	 l-p:0.09848647564649582
epoch£º17	 i:8 	 global-step:348	 l-p:0.1197749674320221
epoch£º17	 i:9 	 global-step:349	 l-p:0.11750995367765427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9300, 2.9537, 2.9361],
        [2.9300, 2.9302, 2.9300],
        [2.9300, 2.9854, 2.9543],
        [2.9300, 2.9361, 2.9307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14307357370853424 
model_pd.l_d.mean(): -24.36539077758789 
model_pd.lagr.mean(): -24.22231674194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1637], device='cuda:0')), ('power', tensor([-24.2017], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14307357370853424
epoch£º18	 i:1 	 global-step:361	 l-p:0.11653202027082443
epoch£º18	 i:2 	 global-step:362	 l-p:0.13438019156455994
epoch£º18	 i:3 	 global-step:363	 l-p:0.13054803013801575
epoch£º18	 i:4 	 global-step:364	 l-p:0.18547150492668152
epoch£º18	 i:5 	 global-step:365	 l-p:0.12787362933158875
epoch£º18	 i:6 	 global-step:366	 l-p:0.1285320520401001
epoch£º18	 i:7 	 global-step:367	 l-p:-0.2818983495235443
epoch£º18	 i:8 	 global-step:368	 l-p:0.1958192139863968
epoch£º18	 i:9 	 global-step:369	 l-p:0.11609324812889099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9407, 3.0260, 2.9899],
        [2.9407, 2.9551, 2.9434],
        [2.9407, 2.9407, 2.9407],
        [2.9407, 3.0424, 3.0059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.11729912459850311 
model_pd.l_d.mean(): -24.64566421508789 
model_pd.lagr.mean(): -24.528364181518555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2265], device='cuda:0')), ('power', tensor([-24.4192], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.11729912459850311
epoch£º19	 i:1 	 global-step:381	 l-p:0.1262056827545166
epoch£º19	 i:2 	 global-step:382	 l-p:0.12346972525119781
epoch£º19	 i:3 	 global-step:383	 l-p:0.11278003454208374
epoch£º19	 i:4 	 global-step:384	 l-p:0.14407213032245636
epoch£º19	 i:5 	 global-step:385	 l-p:0.12392108887434006
epoch£º19	 i:6 	 global-step:386	 l-p:0.13920722901821136
epoch£º19	 i:7 	 global-step:387	 l-p:0.1262422353029251
epoch£º19	 i:8 	 global-step:388	 l-p:0.12353639304637909
epoch£º19	 i:9 	 global-step:389	 l-p:0.11911079287528992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0376, 3.0389, 3.0377],
        [3.0376, 3.2503, 3.2417],
        [3.0376, 3.0582, 3.0424],
        [3.0376, 3.0674, 3.0463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.128296360373497 
model_pd.l_d.mean(): -24.605546951293945 
model_pd.lagr.mean(): -24.477251052856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2572], device='cuda:0')), ('power', tensor([-24.3483], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.128296360373497
epoch£º20	 i:1 	 global-step:401	 l-p:0.1041003093123436
epoch£º20	 i:2 	 global-step:402	 l-p:0.14255744218826294
epoch£º20	 i:3 	 global-step:403	 l-p:0.16295693814754486
epoch£º20	 i:4 	 global-step:404	 l-p:0.11994275450706482
epoch£º20	 i:5 	 global-step:405	 l-p:0.1310141533613205
epoch£º20	 i:6 	 global-step:406	 l-p:0.12068392336368561
epoch£º20	 i:7 	 global-step:407	 l-p:0.13087691366672516
epoch£º20	 i:8 	 global-step:408	 l-p:0.10940740257501602
epoch£º20	 i:9 	 global-step:409	 l-p:0.1246616542339325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0364, 3.7686, 4.2559],
        [3.0364, 3.0662, 3.0451],
        [3.0364, 3.0364, 3.0364],
        [3.0364, 3.0852, 3.0559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13975796103477478 
model_pd.l_d.mean(): -24.517566680908203 
model_pd.lagr.mean(): -24.377809524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2387], device='cuda:0')), ('power', tensor([-24.2788], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.13975796103477478
epoch£º21	 i:1 	 global-step:421	 l-p:0.1305776685476303
epoch£º21	 i:2 	 global-step:422	 l-p:0.1567506492137909
epoch£º21	 i:3 	 global-step:423	 l-p:0.12489833682775497
epoch£º21	 i:4 	 global-step:424	 l-p:0.12295713275671005
epoch£º21	 i:5 	 global-step:425	 l-p:0.13267959654331207
epoch£º21	 i:6 	 global-step:426	 l-p:0.10930099338293076
epoch£º21	 i:7 	 global-step:427	 l-p:0.26562899351119995
epoch£º21	 i:8 	 global-step:428	 l-p:0.13384941220283508
epoch£º21	 i:9 	 global-step:429	 l-p:0.11670801043510437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8592, 2.9044, 2.8775],
        [2.8592, 2.9935, 2.9645],
        [2.8592, 2.8602, 2.8592],
        [2.8592, 2.8732, 2.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13467782735824585 
model_pd.l_d.mean(): -24.633989334106445 
model_pd.lagr.mean(): -24.499311447143555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1765], device='cuda:0')), ('power', tensor([-24.4575], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13467782735824585
epoch£º22	 i:1 	 global-step:441	 l-p:0.35029760003089905
epoch£º22	 i:2 	 global-step:442	 l-p:0.12440413981676102
epoch£º22	 i:3 	 global-step:443	 l-p:0.1215953454375267
epoch£º22	 i:4 	 global-step:444	 l-p:0.11990214884281158
epoch£º22	 i:5 	 global-step:445	 l-p:0.13978587090969086
epoch£º22	 i:6 	 global-step:446	 l-p:0.11654531210660934
epoch£º22	 i:7 	 global-step:447	 l-p:0.1337696760892868
epoch£º22	 i:8 	 global-step:448	 l-p:0.16347874701023102
epoch£º22	 i:9 	 global-step:449	 l-p:0.135588601231575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0562, 3.7872, 4.2710],
        [3.0562, 3.3304, 3.3576],
        [3.0562, 3.0565, 3.0562],
        [3.0562, 3.5246, 3.7129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.15203216671943665 
model_pd.l_d.mean(): -24.350074768066406 
model_pd.lagr.mean(): -24.198041915893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2216], device='cuda:0')), ('power', tensor([-24.1285], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.15203216671943665
epoch£º23	 i:1 	 global-step:461	 l-p:0.11735938489437103
epoch£º23	 i:2 	 global-step:462	 l-p:0.12082429975271225
epoch£º23	 i:3 	 global-step:463	 l-p:0.11810995638370514
epoch£º23	 i:4 	 global-step:464	 l-p:0.08387698978185654
epoch£º23	 i:5 	 global-step:465	 l-p:0.12370302528142929
epoch£º23	 i:6 	 global-step:466	 l-p:0.13255642354488373
epoch£º23	 i:7 	 global-step:467	 l-p:0.1395840346813202
epoch£º23	 i:8 	 global-step:468	 l-p:0.10313732177019119
epoch£º23	 i:9 	 global-step:469	 l-p:0.11679337918758392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9603, 3.1786, 3.1795],
        [2.9603, 3.0967, 3.0660],
        [2.9603, 2.9979, 2.9736],
        [2.9603, 2.9603, 2.9603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.12279924005270004 
model_pd.l_d.mean(): -24.87738609313965 
model_pd.lagr.mean(): -24.754587173461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2720], device='cuda:0')), ('power', tensor([-24.6054], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.12279924005270004
epoch£º24	 i:1 	 global-step:481	 l-p:0.1187129020690918
epoch£º24	 i:2 	 global-step:482	 l-p:-0.020899228751659393
epoch£º24	 i:3 	 global-step:483	 l-p:0.12346751987934113
epoch£º24	 i:4 	 global-step:484	 l-p:0.2989986538887024
epoch£º24	 i:5 	 global-step:485	 l-p:0.23732323944568634
epoch£º24	 i:6 	 global-step:486	 l-p:0.129246324300766
epoch£º24	 i:7 	 global-step:487	 l-p:0.1239209696650505
epoch£º24	 i:8 	 global-step:488	 l-p:0.1291358917951584
epoch£º24	 i:9 	 global-step:489	 l-p:0.13085173070430756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9063, 2.9063, 2.9063],
        [2.9063, 2.9063, 2.9063],
        [2.9063, 2.9063, 2.9063],
        [2.9063, 3.4503, 3.7413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.152187779545784 
model_pd.l_d.mean(): -23.528156280517578 
model_pd.lagr.mean(): -23.37596893310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0510], device='cuda:0')), ('power', tensor([-23.4771], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.152187779545784
epoch£º25	 i:1 	 global-step:501	 l-p:0.12046433985233307
epoch£º25	 i:2 	 global-step:502	 l-p:0.129293292760849
epoch£º25	 i:3 	 global-step:503	 l-p:0.14473290741443634
epoch£º25	 i:4 	 global-step:504	 l-p:0.13739819824695587
epoch£º25	 i:5 	 global-step:505	 l-p:0.13324981927871704
epoch£º25	 i:6 	 global-step:506	 l-p:0.12369439750909805
epoch£º25	 i:7 	 global-step:507	 l-p:0.1549694538116455
epoch£º25	 i:8 	 global-step:508	 l-p:0.08770196884870529
epoch£º25	 i:9 	 global-step:509	 l-p:0.48637649416923523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9170, 3.0045, 2.9702],
        [2.9170, 3.2026, 3.2500],
        [2.9170, 2.9436, 2.9248],
        [2.9170, 3.1619, 3.1819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.1303320825099945 
model_pd.l_d.mean(): -24.670934677124023 
model_pd.lagr.mean(): -24.54060173034668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2055], device='cuda:0')), ('power', tensor([-24.4654], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.1303320825099945
epoch£º26	 i:1 	 global-step:521	 l-p:0.15270815789699554
epoch£º26	 i:2 	 global-step:522	 l-p:0.16262592375278473
epoch£º26	 i:3 	 global-step:523	 l-p:0.1254439502954483
epoch£º26	 i:4 	 global-step:524	 l-p:0.09349606186151505
epoch£º26	 i:5 	 global-step:525	 l-p:0.09768602252006531
epoch£º26	 i:6 	 global-step:526	 l-p:0.11310288310050964
epoch£º26	 i:7 	 global-step:527	 l-p:0.12524284422397614
epoch£º26	 i:8 	 global-step:528	 l-p:0.10310810804367065
epoch£º26	 i:9 	 global-step:529	 l-p:0.11992134153842926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2036, 3.2636, 3.2303],
        [3.2036, 3.3791, 3.3520],
        [3.2036, 3.2036, 3.2036],
        [3.2036, 3.2048, 3.2036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.12188389152288437 
model_pd.l_d.mean(): -23.543325424194336 
model_pd.lagr.mean(): -23.42144203186035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2250], device='cuda:0')), ('power', tensor([-23.3183], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.12188389152288437
epoch£º27	 i:1 	 global-step:541	 l-p:0.11012132465839386
epoch£º27	 i:2 	 global-step:542	 l-p:0.12006454169750214
epoch£º27	 i:3 	 global-step:543	 l-p:0.11957333236932755
epoch£º27	 i:4 	 global-step:544	 l-p:0.11059179902076721
epoch£º27	 i:5 	 global-step:545	 l-p:0.16153563559055328
epoch£º27	 i:6 	 global-step:546	 l-p:0.12854699790477753
epoch£º27	 i:7 	 global-step:547	 l-p:0.12890148162841797
epoch£º27	 i:8 	 global-step:548	 l-p:0.13725492358207703
epoch£º27	 i:9 	 global-step:549	 l-p:0.12163377553224564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8057, 2.8912, 2.8592],
        [2.8057, 3.0718, 3.1152],
        [2.8057, 2.8258, 2.8109],
        [2.8057, 3.4107, 3.7926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.14044612646102905 
model_pd.l_d.mean(): -24.327695846557617 
model_pd.lagr.mean(): -24.1872501373291 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0969], device='cuda:0')), ('power', tensor([-24.2308], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.14044612646102905
epoch£º28	 i:1 	 global-step:561	 l-p:0.13287131488323212
epoch£º28	 i:2 	 global-step:562	 l-p:0.1058332622051239
epoch£º28	 i:3 	 global-step:563	 l-p:0.12348195910453796
epoch£º28	 i:4 	 global-step:564	 l-p:0.14924556016921997
epoch£º28	 i:5 	 global-step:565	 l-p:0.07956024259328842
epoch£º28	 i:6 	 global-step:566	 l-p:0.13248030841350555
epoch£º28	 i:7 	 global-step:567	 l-p:0.12305065989494324
epoch£º28	 i:8 	 global-step:568	 l-p:0.13575823605060577
epoch£º28	 i:9 	 global-step:569	 l-p:0.12487442791461945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9677, 2.9677, 2.9677],
        [2.9677, 3.0593, 3.0246],
        [2.9677, 3.2699, 3.3272],
        [2.9677, 2.9696, 2.9678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12438827008008957 
model_pd.l_d.mean(): -24.222305297851562 
model_pd.lagr.mean(): -24.097917556762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1759], device='cuda:0')), ('power', tensor([-24.0464], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.12438827008008957
epoch£º29	 i:1 	 global-step:581	 l-p:0.21102149784564972
epoch£º29	 i:2 	 global-step:582	 l-p:0.12353599071502686
epoch£º29	 i:3 	 global-step:583	 l-p:0.12793925404548645
epoch£º29	 i:4 	 global-step:584	 l-p:0.125247523188591
epoch£º29	 i:5 	 global-step:585	 l-p:0.11973991245031357
epoch£º29	 i:6 	 global-step:586	 l-p:0.13149712979793549
epoch£º29	 i:7 	 global-step:587	 l-p:0.11312513053417206
epoch£º29	 i:8 	 global-step:588	 l-p:0.15056979656219482
epoch£º29	 i:9 	 global-step:589	 l-p:0.11365431547164917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9741, 2.9748, 2.9741],
        [2.9741, 3.6209, 4.0240],
        [2.9741, 3.0729, 3.0385],
        [2.9741, 3.5588, 3.8892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.12082117795944214 
model_pd.l_d.mean(): -23.81825828552246 
model_pd.lagr.mean(): -23.697437286376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1081], device='cuda:0')), ('power', tensor([-23.7102], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.12082117795944214
epoch£º30	 i:1 	 global-step:601	 l-p:0.12565109133720398
epoch£º30	 i:2 	 global-step:602	 l-p:0.16256149113178253
epoch£º30	 i:3 	 global-step:603	 l-p:0.12768127024173737
epoch£º30	 i:4 	 global-step:604	 l-p:0.11769962310791016
epoch£º30	 i:5 	 global-step:605	 l-p:0.12675145268440247
epoch£º30	 i:6 	 global-step:606	 l-p:0.11391477286815643
epoch£º30	 i:7 	 global-step:607	 l-p:0.125
epoch£º30	 i:8 	 global-step:608	 l-p:0.16687794029712677
epoch£º30	 i:9 	 global-step:609	 l-p:0.1552104502916336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9548, 2.9548, 2.9548],
        [2.9548, 2.9584, 2.9551],
        [2.9548, 3.6292, 4.0704],
        [2.9548, 2.9795, 2.9618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.21108311414718628 
model_pd.l_d.mean(): -23.419620513916016 
model_pd.lagr.mean(): -23.208538055419922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0602], device='cuda:0')), ('power', tensor([-23.3594], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.21108311414718628
epoch£º31	 i:1 	 global-step:621	 l-p:0.11612085998058319
epoch£º31	 i:2 	 global-step:622	 l-p:0.10369281470775604
epoch£º31	 i:3 	 global-step:623	 l-p:0.11449535936117172
epoch£º31	 i:4 	 global-step:624	 l-p:0.11182096600532532
epoch£º31	 i:5 	 global-step:625	 l-p:0.12066642194986343
epoch£º31	 i:6 	 global-step:626	 l-p:0.1351182460784912
epoch£º31	 i:7 	 global-step:627	 l-p:0.14539149403572083
epoch£º31	 i:8 	 global-step:628	 l-p:0.12210622429847717
epoch£º31	 i:9 	 global-step:629	 l-p:0.1275656819343567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9342, 3.6431, 4.1323],
        [2.9342, 3.3845, 3.5786],
        [2.9342, 2.9590, 2.9413],
        [2.9342, 2.9348, 2.9342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.1246531680226326 
model_pd.l_d.mean(): -24.362937927246094 
model_pd.lagr.mean(): -24.238285064697266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1860], device='cuda:0')), ('power', tensor([-24.1769], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.1246531680226326
epoch£º32	 i:1 	 global-step:641	 l-p:0.33485472202301025
epoch£º32	 i:2 	 global-step:642	 l-p:0.15194661915302277
epoch£º32	 i:3 	 global-step:643	 l-p:0.06221291422843933
epoch£º32	 i:4 	 global-step:644	 l-p:0.13472291827201843
epoch£º32	 i:5 	 global-step:645	 l-p:0.11904135346412659
epoch£º32	 i:6 	 global-step:646	 l-p:0.12298883497714996
epoch£º32	 i:7 	 global-step:647	 l-p:0.14570598304271698
epoch£º32	 i:8 	 global-step:648	 l-p:1.2154521942138672
epoch£º32	 i:9 	 global-step:649	 l-p:0.13221274316310883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8895, 2.8896, 2.8895],
        [2.8895, 2.9951, 2.9637],
        [2.8895, 2.8897, 2.8895],
        [2.8895, 2.9952, 2.9638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.14015020430088043 
model_pd.l_d.mean(): -24.56951904296875 
model_pd.lagr.mean(): -24.42936897277832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1710], device='cuda:0')), ('power', tensor([-24.3986], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.14015020430088043
epoch£º33	 i:1 	 global-step:661	 l-p:0.12513704597949982
epoch£º33	 i:2 	 global-step:662	 l-p:0.11949083209037781
epoch£º33	 i:3 	 global-step:663	 l-p:0.25317901372909546
epoch£º33	 i:4 	 global-step:664	 l-p:0.051623132079839706
epoch£º33	 i:5 	 global-step:665	 l-p:0.12352189421653748
epoch£º33	 i:6 	 global-step:666	 l-p:0.12894418835639954
epoch£º33	 i:7 	 global-step:667	 l-p:0.11112508177757263
epoch£º33	 i:8 	 global-step:668	 l-p:0.13831856846809387
epoch£º33	 i:9 	 global-step:669	 l-p:0.12432248890399933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9878, 3.2715, 3.3170],
        [2.9878, 2.9879, 2.9878],
        [2.9878, 3.1157, 3.0855],
        [2.9878, 3.0120, 2.9946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.17166858911514282 
model_pd.l_d.mean(): -24.573392868041992 
model_pd.lagr.mean(): -24.401723861694336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2193], device='cuda:0')), ('power', tensor([-24.3541], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.17166858911514282
epoch£º34	 i:1 	 global-step:681	 l-p:0.10424785315990448
epoch£º34	 i:2 	 global-step:682	 l-p:0.13574357330799103
epoch£º34	 i:3 	 global-step:683	 l-p:0.13302581012248993
epoch£º34	 i:4 	 global-step:684	 l-p:0.1318093240261078
epoch£º34	 i:5 	 global-step:685	 l-p:0.12094545364379883
epoch£º34	 i:6 	 global-step:686	 l-p:0.1245659589767456
epoch£º34	 i:7 	 global-step:687	 l-p:0.09512334316968918
epoch£º34	 i:8 	 global-step:688	 l-p:0.12092681974172592
epoch£º34	 i:9 	 global-step:689	 l-p:0.11693818122148514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9914, 3.2006, 3.2006],
        [2.9914, 2.9914, 2.9914],
        [2.9914, 3.0491, 3.0190],
        [2.9914, 3.6543, 4.0779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.12794466316699982 
model_pd.l_d.mean(): -24.320758819580078 
model_pd.lagr.mean(): -24.192813873291016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1763], device='cuda:0')), ('power', tensor([-24.1444], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.12794466316699982
epoch£º35	 i:1 	 global-step:701	 l-p:0.14931578934192657
epoch£º35	 i:2 	 global-step:702	 l-p:0.1296904981136322
epoch£º35	 i:3 	 global-step:703	 l-p:0.1277688592672348
epoch£º35	 i:4 	 global-step:704	 l-p:0.12301895022392273
epoch£º35	 i:5 	 global-step:705	 l-p:0.12387529015541077
epoch£º35	 i:6 	 global-step:706	 l-p:0.12679074704647064
epoch£º35	 i:7 	 global-step:707	 l-p:0.13742214441299438
epoch£º35	 i:8 	 global-step:708	 l-p:0.05956057086586952
epoch£º35	 i:9 	 global-step:709	 l-p:0.12808650732040405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7988, 2.7988, 2.7988],
        [2.7988, 2.7988, 2.7988],
        [2.7988, 3.0689, 3.1211],
        [2.7988, 2.8162, 2.8031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.12952083349227905 
model_pd.l_d.mean(): -23.906330108642578 
model_pd.lagr.mean(): -23.776809692382812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0662], device='cuda:0')), ('power', tensor([-23.8402], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.12952083349227905
epoch£º36	 i:1 	 global-step:721	 l-p:0.12708725035190582
epoch£º36	 i:2 	 global-step:722	 l-p:0.09230313450098038
epoch£º36	 i:3 	 global-step:723	 l-p:0.13209040462970734
epoch£º36	 i:4 	 global-step:724	 l-p:0.11937002092599869
epoch£º36	 i:5 	 global-step:725	 l-p:0.133168563246727
epoch£º36	 i:6 	 global-step:726	 l-p:0.15902908146381378
epoch£º36	 i:7 	 global-step:727	 l-p:0.14357388019561768
epoch£º36	 i:8 	 global-step:728	 l-p:0.12451571226119995
epoch£º36	 i:9 	 global-step:729	 l-p:0.11837296932935715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0535, 3.0561, 3.0537],
        [3.0535, 3.7127, 4.1222],
        [3.0535, 3.2404, 3.2278],
        [3.0535, 3.2909, 3.3037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.13758856058120728 
model_pd.l_d.mean(): -23.65366554260254 
model_pd.lagr.mean(): -23.516077041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1377], device='cuda:0')), ('power', tensor([-23.5160], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.13758856058120728
epoch£º37	 i:1 	 global-step:741	 l-p:0.11593116819858551
epoch£º37	 i:2 	 global-step:742	 l-p:0.12038608640432358
epoch£º37	 i:3 	 global-step:743	 l-p:0.12280005216598511
epoch£º37	 i:4 	 global-step:744	 l-p:0.12967504560947418
epoch£º37	 i:5 	 global-step:745	 l-p:0.10974370688199997
epoch£º37	 i:6 	 global-step:746	 l-p:0.1165899857878685
epoch£º37	 i:7 	 global-step:747	 l-p:-0.2513039708137512
epoch£º37	 i:8 	 global-step:748	 l-p:0.26167353987693787
epoch£º37	 i:9 	 global-step:749	 l-p:0.13284359872341156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9112, 2.9558, 2.9302],
        [2.9112, 2.9372, 2.9192],
        [2.9112, 2.9210, 2.9129],
        [2.9112, 2.9360, 2.9186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.13127626478672028 
model_pd.l_d.mean(): -24.321744918823242 
model_pd.lagr.mean(): -24.190467834472656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1395], device='cuda:0')), ('power', tensor([-24.1823], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.13127626478672028
epoch£º38	 i:1 	 global-step:761	 l-p:0.2892830967903137
epoch£º38	 i:2 	 global-step:762	 l-p:0.13294172286987305
epoch£º38	 i:3 	 global-step:763	 l-p:0.5448488593101501
epoch£º38	 i:4 	 global-step:764	 l-p:0.11332124471664429
epoch£º38	 i:5 	 global-step:765	 l-p:0.1352272778749466
epoch£º38	 i:6 	 global-step:766	 l-p:0.11019296199083328
epoch£º38	 i:7 	 global-step:767	 l-p:0.1272750198841095
epoch£º38	 i:8 	 global-step:768	 l-p:0.14516635239124298
epoch£º38	 i:9 	 global-step:769	 l-p:0.12706579267978668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9818, 2.9818, 2.9818],
        [2.9818, 3.4987, 3.7609],
        [2.9818, 2.9818, 2.9817],
        [2.9818, 2.9826, 2.9818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.12517312169075012 
model_pd.l_d.mean(): -24.772436141967773 
model_pd.lagr.mean(): -24.647262573242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2550], device='cuda:0')), ('power', tensor([-24.5175], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.12517312169075012
epoch£º39	 i:1 	 global-step:781	 l-p:0.11805738508701324
epoch£º39	 i:2 	 global-step:782	 l-p:0.12203399091959
epoch£º39	 i:3 	 global-step:783	 l-p:0.1348397582769394
epoch£º39	 i:4 	 global-step:784	 l-p:-563.5287475585938
epoch£º39	 i:5 	 global-step:785	 l-p:0.127888023853302
epoch£º39	 i:6 	 global-step:786	 l-p:-0.03420955687761307
epoch£º39	 i:7 	 global-step:787	 l-p:0.12625570595264435
epoch£º39	 i:8 	 global-step:788	 l-p:0.08308829367160797
epoch£º39	 i:9 	 global-step:789	 l-p:0.1248580813407898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9485, 2.9485, 2.9485],
        [2.9485, 2.9556, 2.9495],
        [2.9485, 2.9939, 2.9679],
        [2.9485, 2.9513, 2.9487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.13539279997348785 
model_pd.l_d.mean(): -24.694971084594727 
model_pd.lagr.mean(): -24.55957794189453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2202], device='cuda:0')), ('power', tensor([-24.4747], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.13539279997348785
epoch£º40	 i:1 	 global-step:801	 l-p:0.24340881407260895
epoch£º40	 i:2 	 global-step:802	 l-p:0.12118065357208252
epoch£º40	 i:3 	 global-step:803	 l-p:0.14151416718959808
epoch£º40	 i:4 	 global-step:804	 l-p:0.13657620549201965
epoch£º40	 i:5 	 global-step:805	 l-p:0.12085970491170883
epoch£º40	 i:6 	 global-step:806	 l-p:0.09609842300415039
epoch£º40	 i:7 	 global-step:807	 l-p:0.11968532204627991
epoch£º40	 i:8 	 global-step:808	 l-p:0.12925492227077484
epoch£º40	 i:9 	 global-step:809	 l-p:0.10018233954906464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0560, 3.0571, 3.0561],
        [3.0560, 3.0562, 3.0560],
        [3.0560, 3.1369, 3.1034],
        [3.0560, 3.1056, 3.0777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10605788975954056 
model_pd.l_d.mean(): -24.620223999023438 
model_pd.lagr.mean(): -24.5141658782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2630], device='cuda:0')), ('power', tensor([-24.3572], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10605788975954056
epoch£º41	 i:1 	 global-step:821	 l-p:0.14551346004009247
epoch£º41	 i:2 	 global-step:822	 l-p:0.15013593435287476
epoch£º41	 i:3 	 global-step:823	 l-p:0.11596597731113434
epoch£º41	 i:4 	 global-step:824	 l-p:0.12240879982709885
epoch£º41	 i:5 	 global-step:825	 l-p:0.12733928859233856
epoch£º41	 i:6 	 global-step:826	 l-p:0.11628379672765732
epoch£º41	 i:7 	 global-step:827	 l-p:0.3536117374897003
epoch£º41	 i:8 	 global-step:828	 l-p:0.12281028181314468
epoch£º41	 i:9 	 global-step:829	 l-p:0.12627652287483215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9164, 3.0344, 3.0064],
        [2.9164, 2.9164, 2.9164],
        [2.9164, 3.5627, 3.9860],
        [2.9164, 2.9223, 2.9171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.1308511197566986 
model_pd.l_d.mean(): -23.897945404052734 
model_pd.lagr.mean(): -23.767093658447266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1219], device='cuda:0')), ('power', tensor([-23.7760], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.1308511197566986
epoch£º42	 i:1 	 global-step:841	 l-p:0.02639157697558403
epoch£º42	 i:2 	 global-step:842	 l-p:0.11898055672645569
epoch£º42	 i:3 	 global-step:843	 l-p:0.2431458830833435
epoch£º42	 i:4 	 global-step:844	 l-p:0.17082160711288452
epoch£º42	 i:5 	 global-step:845	 l-p:0.13607706129550934
epoch£º42	 i:6 	 global-step:846	 l-p:0.11771582067012787
epoch£º42	 i:7 	 global-step:847	 l-p:0.12049318104982376
epoch£º42	 i:8 	 global-step:848	 l-p:0.1292138248682022
epoch£º42	 i:9 	 global-step:849	 l-p:0.1302509605884552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[2.9742, 3.5678, 3.9191],
        [2.9742, 3.2261, 3.2561],
        [2.9742, 3.1077, 3.0817],
        [2.9742, 3.5362, 3.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.15409469604492188 
model_pd.l_d.mean(): -24.327896118164062 
model_pd.lagr.mean(): -24.17380142211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1702], device='cuda:0')), ('power', tensor([-24.1577], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.15409469604492188
epoch£º43	 i:1 	 global-step:861	 l-p:0.13073976337909698
epoch£º43	 i:2 	 global-step:862	 l-p:0.18701200187206268
epoch£º43	 i:3 	 global-step:863	 l-p:0.15396557748317719
epoch£º43	 i:4 	 global-step:864	 l-p:0.11735154688358307
epoch£º43	 i:5 	 global-step:865	 l-p:0.1153608113527298
epoch£º43	 i:6 	 global-step:866	 l-p:0.09755735844373703
epoch£º43	 i:7 	 global-step:867	 l-p:0.11123302578926086
epoch£º43	 i:8 	 global-step:868	 l-p:0.1270366758108139
epoch£º43	 i:9 	 global-step:869	 l-p:0.125404492020607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0165, 3.0695, 3.0412],
        [3.0165, 3.0193, 3.0168],
        [3.0165, 3.0174, 3.0166],
        [3.0165, 3.0166, 3.0165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.11346249282360077 
model_pd.l_d.mean(): -24.60232162475586 
model_pd.lagr.mean(): -24.488859176635742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2443], device='cuda:0')), ('power', tensor([-24.3580], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.11346249282360077
epoch£º44	 i:1 	 global-step:881	 l-p:0.1299351155757904
epoch£º44	 i:2 	 global-step:882	 l-p:0.12316732853651047
epoch£º44	 i:3 	 global-step:883	 l-p:-1.9286589622497559
epoch£º44	 i:4 	 global-step:884	 l-p:0.15026502311229706
epoch£º44	 i:5 	 global-step:885	 l-p:0.041910961270332336
epoch£º44	 i:6 	 global-step:886	 l-p:0.11989422887563705
epoch£º44	 i:7 	 global-step:887	 l-p:0.12328772991895676
epoch£º44	 i:8 	 global-step:888	 l-p:0.12733514606952667
epoch£º44	 i:9 	 global-step:889	 l-p:0.1475016176700592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9214, 2.9214, 2.9214],
        [2.9214, 2.9218, 2.9214],
        [2.9214, 3.0163, 2.9858],
        [2.9214, 2.9214, 2.9214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13517262041568756 
model_pd.l_d.mean(): -24.83953094482422 
model_pd.lagr.mean(): -24.70435905456543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2353], device='cuda:0')), ('power', tensor([-24.6042], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13517262041568756
epoch£º45	 i:1 	 global-step:901	 l-p:0.1320086270570755
epoch£º45	 i:2 	 global-step:902	 l-p:0.14448873698711395
epoch£º45	 i:3 	 global-step:903	 l-p:0.1281701624393463
epoch£º45	 i:4 	 global-step:904	 l-p:0.29508793354034424
epoch£º45	 i:5 	 global-step:905	 l-p:0.11697553098201752
epoch£º45	 i:6 	 global-step:906	 l-p:0.30102095007896423
epoch£º45	 i:7 	 global-step:907	 l-p:0.1503828763961792
epoch£º45	 i:8 	 global-step:908	 l-p:0.11036533117294312
epoch£º45	 i:9 	 global-step:909	 l-p:0.12628121674060822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0967, 3.0984, 3.0968],
        [3.0967, 3.1077, 3.0987],
        [3.0967, 3.0967, 3.0967],
        [3.0967, 3.1074, 3.0986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11612698435783386 
model_pd.l_d.mean(): -24.218006134033203 
model_pd.lagr.mean(): -24.101879119873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2205], device='cuda:0')), ('power', tensor([-23.9975], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11612698435783386
epoch£º46	 i:1 	 global-step:921	 l-p:0.1191265657544136
epoch£º46	 i:2 	 global-step:922	 l-p:0.11911426484584808
epoch£º46	 i:3 	 global-step:923	 l-p:0.12368592619895935
epoch£º46	 i:4 	 global-step:924	 l-p:0.11489996314048767
epoch£º46	 i:5 	 global-step:925	 l-p:0.1574191302061081
epoch£º46	 i:6 	 global-step:926	 l-p:0.12172212451696396
epoch£º46	 i:7 	 global-step:927	 l-p:0.12205703556537628
epoch£º46	 i:8 	 global-step:928	 l-p:0.12521520256996155
epoch£º46	 i:9 	 global-step:929	 l-p:0.11722194403409958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8918, 2.9124, 2.8976],
        [2.8918, 2.9322, 2.9087],
        [2.8918, 2.8924, 2.8918],
        [2.8918, 3.0319, 3.0117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.13154135644435883 
model_pd.l_d.mean(): -23.652252197265625 
model_pd.lagr.mean(): -23.520709991455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1064], device='cuda:0')), ('power', tensor([-23.5459], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.13154135644435883
epoch£º47	 i:1 	 global-step:941	 l-p:0.14049571752548218
epoch£º47	 i:2 	 global-step:942	 l-p:0.12910842895507812
epoch£º47	 i:3 	 global-step:943	 l-p:0.13868364691734314
epoch£º47	 i:4 	 global-step:944	 l-p:0.09963802248239517
epoch£º47	 i:5 	 global-step:945	 l-p:0.13494794070720673
epoch£º47	 i:6 	 global-step:946	 l-p:0.1341131031513214
epoch£º47	 i:7 	 global-step:947	 l-p:0.1420905590057373
epoch£º47	 i:8 	 global-step:948	 l-p:0.09323420375585556
epoch£º47	 i:9 	 global-step:949	 l-p:0.13525176048278809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8862, 3.0251, 3.0051],
        [2.8862, 2.9569, 2.9276],
        [2.8862, 2.8862, 2.8862],
        [2.8862, 2.9779, 2.9485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.1274261325597763 
model_pd.l_d.mean(): -24.840547561645508 
model_pd.lagr.mean(): -24.71312141418457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2145], device='cuda:0')), ('power', tensor([-24.6261], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.1274261325597763
epoch£º48	 i:1 	 global-step:961	 l-p:0.11234819144010544
epoch£º48	 i:2 	 global-step:962	 l-p:0.6564671993255615
epoch£º48	 i:3 	 global-step:963	 l-p:0.15164977312088013
epoch£º48	 i:4 	 global-step:964	 l-p:0.12503352761268616
epoch£º48	 i:5 	 global-step:965	 l-p:0.12650160491466522
epoch£º48	 i:6 	 global-step:966	 l-p:0.12483363598585129
epoch£º48	 i:7 	 global-step:967	 l-p:0.11934874951839447
epoch£º48	 i:8 	 global-step:968	 l-p:0.14234711229801178
epoch£º48	 i:9 	 global-step:969	 l-p:0.11560632288455963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0388, 3.0520, 3.0415],
        [3.0388, 3.0393, 3.0388],
        [3.0388, 3.1039, 3.0734],
        [3.0388, 3.0411, 3.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12284976989030838 
model_pd.l_d.mean(): -24.826427459716797 
model_pd.lagr.mean(): -24.703577041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2892], device='cuda:0')), ('power', tensor([-24.5372], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12284976989030838
epoch£º49	 i:1 	 global-step:981	 l-p:0.11166881024837494
epoch£º49	 i:2 	 global-step:982	 l-p:0.13146308064460754
epoch£º49	 i:3 	 global-step:983	 l-p:0.12093238532543182
epoch£º49	 i:4 	 global-step:984	 l-p:0.05600648745894432
epoch£º49	 i:5 	 global-step:985	 l-p:0.1355326622724533
epoch£º49	 i:6 	 global-step:986	 l-p:0.08891280740499496
epoch£º49	 i:7 	 global-step:987	 l-p:0.00825454667210579
epoch£º49	 i:8 	 global-step:988	 l-p:0.1331195831298828
epoch£º49	 i:9 	 global-step:989	 l-p:0.1329648792743683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9718, 3.3547, 3.4930],
        [2.9718, 2.9718, 2.9718],
        [2.9718, 3.1061, 3.0823],
        [2.9718, 2.9724, 2.9718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.16047996282577515 
model_pd.l_d.mean(): -24.431407928466797 
model_pd.lagr.mean(): -24.27092742919922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1830], device='cuda:0')), ('power', tensor([-24.2484], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.16047996282577515
epoch£º50	 i:1 	 global-step:1001	 l-p:0.1193765252828598
epoch£º50	 i:2 	 global-step:1002	 l-p:0.10418026894330978
epoch£º50	 i:3 	 global-step:1003	 l-p:0.12392127513885498
epoch£º50	 i:4 	 global-step:1004	 l-p:0.1365177184343338
epoch£º50	 i:5 	 global-step:1005	 l-p:0.135729119181633
epoch£º50	 i:6 	 global-step:1006	 l-p:0.12250922620296478
epoch£º50	 i:7 	 global-step:1007	 l-p:0.12202468514442444
epoch£º50	 i:8 	 global-step:1008	 l-p:0.1175924763083458
epoch£º50	 i:9 	 global-step:1009	 l-p:0.1161777600646019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0283, 3.0283, 3.0283],
        [3.0283, 3.2954, 3.3357],
        [3.0283, 3.6580, 4.0477],
        [3.0283, 3.0283, 3.0283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.14154985547065735 
model_pd.l_d.mean(): -23.747682571411133 
model_pd.lagr.mean(): -23.60613250732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1486], device='cuda:0')), ('power', tensor([-23.5991], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.14154985547065735
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10051967203617096
epoch£º51	 i:2 	 global-step:1022	 l-p:0.16419778764247894
epoch£º51	 i:3 	 global-step:1023	 l-p:0.1281811147928238
epoch£º51	 i:4 	 global-step:1024	 l-p:0.11606453359127045
epoch£º51	 i:5 	 global-step:1025	 l-p:0.1208881139755249
epoch£º51	 i:6 	 global-step:1026	 l-p:-0.1062091812491417
epoch£º51	 i:7 	 global-step:1027	 l-p:0.28287118673324585
epoch£º51	 i:8 	 global-step:1028	 l-p:0.13058163225650787
epoch£º51	 i:9 	 global-step:1029	 l-p:0.12933209538459778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9282, 3.1855, 3.2272],
        [2.9282, 3.1074, 3.1029],
        [2.9282, 2.9462, 2.9329],
        [2.9282, 2.9282, 2.9282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.11968500167131424 
model_pd.l_d.mean(): -24.690135955810547 
model_pd.lagr.mean(): -24.570451736450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2236], device='cuda:0')), ('power', tensor([-24.4665], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.11968500167131424
epoch£º52	 i:1 	 global-step:1041	 l-p:0.13374002277851105
epoch£º52	 i:2 	 global-step:1042	 l-p:-0.012885562144219875
epoch£º52	 i:3 	 global-step:1043	 l-p:0.14196954667568207
epoch£º52	 i:4 	 global-step:1044	 l-p:0.11782265454530716
epoch£º52	 i:5 	 global-step:1045	 l-p:0.16432513296604156
epoch£º52	 i:6 	 global-step:1046	 l-p:0.07715165615081787
epoch£º52	 i:7 	 global-step:1047	 l-p:0.13460224866867065
epoch£º52	 i:8 	 global-step:1048	 l-p:0.12188974767923355
epoch£º52	 i:9 	 global-step:1049	 l-p:0.12642741203308105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0063, 3.0093, 3.0066],
        [3.0063, 3.0063, 3.0063],
        [3.0063, 3.0065, 3.0063],
        [3.0063, 3.0065, 3.0063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.12711037695407867 
model_pd.l_d.mean(): -23.967838287353516 
model_pd.lagr.mean(): -23.840728759765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1776], device='cuda:0')), ('power', tensor([-23.7902], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.12711037695407867
epoch£º53	 i:1 	 global-step:1061	 l-p:0.14850619435310364
epoch£º53	 i:2 	 global-step:1062	 l-p:0.12798576056957245
epoch£º53	 i:3 	 global-step:1063	 l-p:0.11835581809282303
epoch£º53	 i:4 	 global-step:1064	 l-p:0.10150260478258133
epoch£º53	 i:5 	 global-step:1065	 l-p:0.1301981657743454
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12129757553339005
epoch£º53	 i:7 	 global-step:1067	 l-p:0.10931076854467392
epoch£º53	 i:8 	 global-step:1068	 l-p:0.1145692989230156
epoch£º53	 i:9 	 global-step:1069	 l-p:0.11403568089008331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9894, 2.9894, 2.9894],
        [2.9894, 3.2940, 3.3665],
        [2.9894, 3.3185, 3.4107],
        [2.9894, 3.0282, 3.0051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1316750943660736 
model_pd.l_d.mean(): -24.711862564086914 
model_pd.lagr.mean(): -24.58018684387207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2415], device='cuda:0')), ('power', tensor([-24.4704], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1316750943660736
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12989510595798492
epoch£º54	 i:2 	 global-step:1082	 l-p:0.11067942529916763
epoch£º54	 i:3 	 global-step:1083	 l-p:0.2300567626953125
epoch£º54	 i:4 	 global-step:1084	 l-p:0.11718246340751648
epoch£º54	 i:5 	 global-step:1085	 l-p:0.11403247714042664
epoch£º54	 i:6 	 global-step:1086	 l-p:0.13762255012989044
epoch£º54	 i:7 	 global-step:1087	 l-p:0.13262850046157837
epoch£º54	 i:8 	 global-step:1088	 l-p:0.1685725748538971
epoch£º54	 i:9 	 global-step:1089	 l-p:0.13167491555213928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[2.7959, 3.0851, 3.1640],
        [2.7959, 3.2211, 3.4257],
        [2.7959, 2.9557, 2.9498],
        [2.7959, 3.0891, 3.1713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.1572108268737793 
model_pd.l_d.mean(): -24.42010498046875 
model_pd.lagr.mean(): -24.262893676757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0944], device='cuda:0')), ('power', tensor([-24.3257], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.1572108268737793
epoch£º55	 i:1 	 global-step:1101	 l-p:0.12540122866630554
epoch£º55	 i:2 	 global-step:1102	 l-p:0.1194329634308815
epoch£º55	 i:3 	 global-step:1103	 l-p:0.12084421515464783
epoch£º55	 i:4 	 global-step:1104	 l-p:0.05567875877022743
epoch£º55	 i:5 	 global-step:1105	 l-p:0.1760692149400711
epoch£º55	 i:6 	 global-step:1106	 l-p:0.15037746727466583
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12990707159042358
epoch£º55	 i:8 	 global-step:1108	 l-p:0.11895041167736053
epoch£º55	 i:9 	 global-step:1109	 l-p:0.10497928410768509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1490, 3.8088, 4.2159],
        [3.1490, 3.1490, 3.1490],
        [3.1490, 3.2525, 3.2197],
        [3.1490, 3.1800, 3.1595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.15173666179180145 
model_pd.l_d.mean(): -24.349294662475586 
model_pd.lagr.mean(): -24.19755744934082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2458], device='cuda:0')), ('power', tensor([-24.1035], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.15173666179180145
epoch£º56	 i:1 	 global-step:1121	 l-p:0.09301929175853729
epoch£º56	 i:2 	 global-step:1122	 l-p:0.11307864636182785
epoch£º56	 i:3 	 global-step:1123	 l-p:0.1201118528842926
epoch£º56	 i:4 	 global-step:1124	 l-p:0.1201557144522667
epoch£º56	 i:5 	 global-step:1125	 l-p:0.1167997419834137
epoch£º56	 i:6 	 global-step:1126	 l-p:0.12484044581651688
epoch£º56	 i:7 	 global-step:1127	 l-p:0.19034284353256226
epoch£º56	 i:8 	 global-step:1128	 l-p:0.1301678568124771
epoch£º56	 i:9 	 global-step:1129	 l-p:0.12507952749729156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8905, 3.3148, 3.5072],
        [2.8905, 3.3707, 3.6204],
        [2.8905, 2.8905, 2.8905],
        [2.8905, 2.9125, 2.8972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.1361640989780426 
model_pd.l_d.mean(): -24.55730438232422 
model_pd.lagr.mean(): -24.421140670776367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1569], device='cuda:0')), ('power', tensor([-24.4004], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.1361640989780426
epoch£º57	 i:1 	 global-step:1141	 l-p:0.09792720526456833
epoch£º57	 i:2 	 global-step:1142	 l-p:0.13077683746814728
epoch£º57	 i:3 	 global-step:1143	 l-p:0.13992971181869507
epoch£º57	 i:4 	 global-step:1144	 l-p:0.08071331679821014
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14239296317100525
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14012004435062408
epoch£º57	 i:7 	 global-step:1147	 l-p:0.13274706900119781
epoch£º57	 i:8 	 global-step:1148	 l-p:0.10609076917171478
epoch£º57	 i:9 	 global-step:1149	 l-p:0.14181779325008392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9059, 3.0208, 2.9962],
        [2.9059, 2.9834, 2.9547],
        [2.9059, 3.4509, 3.7694],
        [2.9059, 2.9059, 2.9059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.14222776889801025 
model_pd.l_d.mean(): -24.73885154724121 
model_pd.lagr.mean(): -24.59662437438965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2030], device='cuda:0')), ('power', tensor([-24.5359], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.14222776889801025
epoch£º58	 i:1 	 global-step:1161	 l-p:0.11599797755479813
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12923121452331543
epoch£º58	 i:3 	 global-step:1163	 l-p:0.12451957166194916
epoch£º58	 i:4 	 global-step:1164	 l-p:0.13307300209999084
epoch£º58	 i:5 	 global-step:1165	 l-p:0.16786342859268188
epoch£º58	 i:6 	 global-step:1166	 l-p:0.2982364892959595
epoch£º58	 i:7 	 global-step:1167	 l-p:0.11410500109195709
epoch£º58	 i:8 	 global-step:1168	 l-p:0.12024112790822983
epoch£º58	 i:9 	 global-step:1169	 l-p:0.12889328598976135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0687, 3.2140, 3.1924],
        [3.0687, 3.0694, 3.0687],
        [3.0687, 3.0687, 3.0687],
        [3.0687, 3.1342, 3.1043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.11652688682079315 
model_pd.l_d.mean(): -23.869665145874023 
model_pd.lagr.mean(): -23.753137588500977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1951], device='cuda:0')), ('power', tensor([-23.6746], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.11652688682079315
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14244699478149414
epoch£º59	 i:2 	 global-step:1182	 l-p:0.12006553262472153
epoch£º59	 i:3 	 global-step:1183	 l-p:0.1366651952266693
epoch£º59	 i:4 	 global-step:1184	 l-p:0.10999385267496109
epoch£º59	 i:5 	 global-step:1185	 l-p:0.16133376955986023
epoch£º59	 i:6 	 global-step:1186	 l-p:0.11235588788986206
epoch£º59	 i:7 	 global-step:1187	 l-p:0.1288195699453354
epoch£º59	 i:8 	 global-step:1188	 l-p:0.14050541818141937
epoch£º59	 i:9 	 global-step:1189	 l-p:0.11311211436986923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9193, 2.9541, 2.9331],
        [2.9193, 2.9193, 2.9193],
        [2.9193, 2.9198, 2.9193],
        [2.9193, 3.0446, 3.0226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.12392690032720566 
model_pd.l_d.mean(): -23.483911514282227 
model_pd.lagr.mean(): -23.3599853515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0353], device='cuda:0')), ('power', tensor([-23.4486], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.12392690032720566
epoch£º60	 i:1 	 global-step:1201	 l-p:0.12637601792812347
epoch£º60	 i:2 	 global-step:1202	 l-p:0.15001468360424042
epoch£º60	 i:3 	 global-step:1203	 l-p:0.07830503582954407
epoch£º60	 i:4 	 global-step:1204	 l-p:0.1353919804096222
epoch£º60	 i:5 	 global-step:1205	 l-p:0.09083495289087296
epoch£º60	 i:6 	 global-step:1206	 l-p:0.13037841022014618
epoch£º60	 i:7 	 global-step:1207	 l-p:0.12926055490970612
epoch£º60	 i:8 	 global-step:1208	 l-p:0.1118217259645462
epoch£º60	 i:9 	 global-step:1209	 l-p:0.05981874465942383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9015, 2.9015, 2.9015],
        [2.9015, 3.1516, 3.1948],
        [2.9015, 2.9034, 2.9017],
        [2.9015, 3.1919, 3.2653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.11566255986690521 
model_pd.l_d.mean(): -24.03676414489746 
model_pd.lagr.mean(): -23.92110252380371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1265], device='cuda:0')), ('power', tensor([-23.9103], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.11566255986690521
epoch£º61	 i:1 	 global-step:1221	 l-p:0.12332257628440857
epoch£º61	 i:2 	 global-step:1222	 l-p:0.13035988807678223
epoch£º61	 i:3 	 global-step:1223	 l-p:0.13341930508613586
epoch£º61	 i:4 	 global-step:1224	 l-p:0.136079341173172
epoch£º61	 i:5 	 global-step:1225	 l-p:0.051357533782720566
epoch£º61	 i:6 	 global-step:1226	 l-p:0.12237723916769028
epoch£º61	 i:7 	 global-step:1227	 l-p:0.08268887549638748
epoch£º61	 i:8 	 global-step:1228	 l-p:0.07492170482873917
epoch£º61	 i:9 	 global-step:1229	 l-p:0.14232690632343292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9704, 2.9804, 2.9723],
        [2.9704, 3.3800, 3.5514],
        [2.9704, 3.0163, 2.9915],
        [2.9704, 3.0489, 3.0196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.1343449354171753 
model_pd.l_d.mean(): -24.361364364624023 
model_pd.lagr.mean(): -24.227020263671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1618], device='cuda:0')), ('power', tensor([-24.1996], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.1343449354171753
epoch£º62	 i:1 	 global-step:1241	 l-p:0.11780818551778793
epoch£º62	 i:2 	 global-step:1242	 l-p:0.1313435137271881
epoch£º62	 i:3 	 global-step:1243	 l-p:0.13656698167324066
epoch£º62	 i:4 	 global-step:1244	 l-p:0.10828624665737152
epoch£º62	 i:5 	 global-step:1245	 l-p:0.12497212737798691
epoch£º62	 i:6 	 global-step:1246	 l-p:0.13064417243003845
epoch£º62	 i:7 	 global-step:1247	 l-p:0.11713626235723495
epoch£º62	 i:8 	 global-step:1248	 l-p:0.14935699105262756
epoch£º62	 i:9 	 global-step:1249	 l-p:0.13020797073841095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0456, 3.0464, 3.0456],
        [3.0456, 3.6742, 4.0680],
        [3.0456, 3.6767, 4.0733],
        [3.0456, 3.0456, 3.0456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.10351535677909851 
model_pd.l_d.mean(): -24.405418395996094 
model_pd.lagr.mean(): -24.301902770996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2134], device='cuda:0')), ('power', tensor([-24.1920], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.10351535677909851
epoch£º63	 i:1 	 global-step:1261	 l-p:0.12775105237960815
epoch£º63	 i:2 	 global-step:1262	 l-p:0.12643663585186005
epoch£º63	 i:3 	 global-step:1263	 l-p:0.12265273928642273
epoch£º63	 i:4 	 global-step:1264	 l-p:0.18028628826141357
epoch£º63	 i:5 	 global-step:1265	 l-p:0.19840840995311737
epoch£º63	 i:6 	 global-step:1266	 l-p:0.11753389239311218
epoch£º63	 i:7 	 global-step:1267	 l-p:0.14801423251628876
epoch£º63	 i:8 	 global-step:1268	 l-p:0.16705001890659332
epoch£º63	 i:9 	 global-step:1269	 l-p:0.13599038124084473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7223, 2.7226, 2.7223],
        [2.7223, 2.7223, 2.7223],
        [2.7223, 2.7286, 2.7233],
        [2.7223, 2.7223, 2.7223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.8207859396934509 
model_pd.l_d.mean(): -24.318328857421875 
model_pd.lagr.mean(): -23.497543334960938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0548], device='cuda:0')), ('power', tensor([-24.2636], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.8207859396934509
epoch£º64	 i:1 	 global-step:1281	 l-p:0.12551438808441162
epoch£º64	 i:2 	 global-step:1282	 l-p:0.0955868810415268
epoch£º64	 i:3 	 global-step:1283	 l-p:0.12257787585258484
epoch£º64	 i:4 	 global-step:1284	 l-p:0.13413667678833008
epoch£º64	 i:5 	 global-step:1285	 l-p:0.11829381436109543
epoch£º64	 i:6 	 global-step:1286	 l-p:0.1253681182861328
epoch£º64	 i:7 	 global-step:1287	 l-p:0.15088361501693726
epoch£º64	 i:8 	 global-step:1288	 l-p:0.12742212414741516
epoch£º64	 i:9 	 global-step:1289	 l-p:0.1040160059928894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8455, 2.8456, 2.8455],
        [2.8455, 2.8455, 2.8455],
        [2.8455, 3.2715, 3.4770],
        [2.8455, 2.8801, 2.8597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.10220563411712646 
model_pd.l_d.mean(): -24.422119140625 
model_pd.lagr.mean(): -24.319913864135742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1154], device='cuda:0')), ('power', tensor([-24.3067], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.10220563411712646
epoch£º65	 i:1 	 global-step:1301	 l-p:0.1382584422826767
epoch£º65	 i:2 	 global-step:1302	 l-p:0.08979927748441696
epoch£º65	 i:3 	 global-step:1303	 l-p:0.14515815675258636
epoch£º65	 i:4 	 global-step:1304	 l-p:0.13287337124347687
epoch£º65	 i:5 	 global-step:1305	 l-p:0.12427212297916412
epoch£º65	 i:6 	 global-step:1306	 l-p:0.15135227143764496
epoch£º65	 i:7 	 global-step:1307	 l-p:0.17420008778572083
epoch£º65	 i:8 	 global-step:1308	 l-p:0.12204433232545853
epoch£º65	 i:9 	 global-step:1309	 l-p:0.10977262258529663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0912, 3.0912, 3.0912],
        [3.0912, 3.5780, 3.8121],
        [3.0912, 3.3321, 3.3575],
        [3.0912, 3.0995, 3.0926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.15134452283382416 
model_pd.l_d.mean(): -24.589914321899414 
model_pd.lagr.mean(): -24.438570022583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2649], device='cuda:0')), ('power', tensor([-24.3251], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.15134452283382416
epoch£º66	 i:1 	 global-step:1321	 l-p:0.1175977885723114
epoch£º66	 i:2 	 global-step:1322	 l-p:0.1289534717798233
epoch£º66	 i:3 	 global-step:1323	 l-p:0.1176384910941124
epoch£º66	 i:4 	 global-step:1324	 l-p:0.1180526539683342
epoch£º66	 i:5 	 global-step:1325	 l-p:0.09931319952011108
epoch£º66	 i:6 	 global-step:1326	 l-p:0.12106934934854507
epoch£º66	 i:7 	 global-step:1327	 l-p:0.15795601904392242
epoch£º66	 i:8 	 global-step:1328	 l-p:0.10998120903968811
epoch£º66	 i:9 	 global-step:1329	 l-p:0.13763104379177094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9473, 2.9474, 2.9473],
        [2.9473, 2.9473, 2.9473],
        [2.9473, 3.3590, 3.5382],
        [2.9473, 3.2605, 3.3511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.12228348106145859 
model_pd.l_d.mean(): -24.38375473022461 
model_pd.lagr.mean(): -24.261470794677734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1526], device='cuda:0')), ('power', tensor([-24.2312], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.12228348106145859
epoch£º67	 i:1 	 global-step:1341	 l-p:0.13330204784870148
epoch£º67	 i:2 	 global-step:1342	 l-p:0.13690923154354095
epoch£º67	 i:3 	 global-step:1343	 l-p:0.12439823150634766
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13691295683383942
epoch£º67	 i:5 	 global-step:1345	 l-p:0.12473181635141373
epoch£º67	 i:6 	 global-step:1346	 l-p:0.10875082015991211
epoch£º67	 i:7 	 global-step:1347	 l-p:0.15539465844631195
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14418773353099823
epoch£º67	 i:9 	 global-step:1349	 l-p:-0.3879651427268982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7636, 3.2547, 3.5426],
        [2.7636, 3.2804, 3.5970],
        [2.7636, 2.7655, 2.7637],
        [2.7636, 3.1580, 3.3447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.21442332863807678 
model_pd.l_d.mean(): -23.941598892211914 
model_pd.lagr.mean(): -23.727174758911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0110], device='cuda:0')), ('power', tensor([-23.9306], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.21442332863807678
epoch£º68	 i:1 	 global-step:1361	 l-p:0.0741901770234108
epoch£º68	 i:2 	 global-step:1362	 l-p:0.1372322142124176
epoch£º68	 i:3 	 global-step:1363	 l-p:0.12467140704393387
epoch£º68	 i:4 	 global-step:1364	 l-p:0.10637214779853821
epoch£º68	 i:5 	 global-step:1365	 l-p:0.11625168472528458
epoch£º68	 i:6 	 global-step:1366	 l-p:0.12232230603694916
epoch£º68	 i:7 	 global-step:1367	 l-p:0.12131286412477493
epoch£º68	 i:8 	 global-step:1368	 l-p:0.1301904320716858
epoch£º68	 i:9 	 global-step:1369	 l-p:0.1706250160932541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0529, 3.1121, 3.0841],
        [3.0529, 3.5483, 3.7984],
        [3.0529, 3.1357, 3.1058],
        [3.0529, 3.0529, 3.0529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.10062293708324432 
model_pd.l_d.mean(): -23.712142944335938 
model_pd.lagr.mean(): -23.611520767211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1277], device='cuda:0')), ('power', tensor([-23.5845], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.10062293708324432
epoch£º69	 i:1 	 global-step:1381	 l-p:0.1303262859582901
epoch£º69	 i:2 	 global-step:1382	 l-p:0.12275934219360352
epoch£º69	 i:3 	 global-step:1383	 l-p:0.11495287716388702
epoch£º69	 i:4 	 global-step:1384	 l-p:0.13761115074157715
epoch£º69	 i:5 	 global-step:1385	 l-p:0.15050305426120758
epoch£º69	 i:6 	 global-step:1386	 l-p:0.15531228482723236
epoch£º69	 i:7 	 global-step:1387	 l-p:0.11213760077953339
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12287736684083939
epoch£º69	 i:9 	 global-step:1389	 l-p:0.125407874584198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0455, 3.6835, 4.0923],
        [3.0455, 3.0536, 3.0469],
        [3.0455, 3.0455, 3.0455],
        [3.0455, 3.6371, 3.9929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.21601849794387817 
model_pd.l_d.mean(): -23.42571449279785 
model_pd.lagr.mean(): -23.20969581604004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0974], device='cuda:0')), ('power', tensor([-23.3283], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.21601849794387817
epoch£º70	 i:1 	 global-step:1401	 l-p:0.13381485641002655
epoch£º70	 i:2 	 global-step:1402	 l-p:0.16637000441551208
epoch£º70	 i:3 	 global-step:1403	 l-p:0.1131429523229599
epoch£º70	 i:4 	 global-step:1404	 l-p:0.13497181236743927
epoch£º70	 i:5 	 global-step:1405	 l-p:0.11665024608373642
epoch£º70	 i:6 	 global-step:1406	 l-p:0.1303705871105194
epoch£º70	 i:7 	 global-step:1407	 l-p:0.14095446467399597
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12728005647659302
epoch£º70	 i:9 	 global-step:1409	 l-p:0.13503026962280273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6611, 2.7071, 2.6849],
        [2.6611, 2.6615, 2.6611],
        [2.6611, 2.6611, 2.6611],
        [2.6611, 2.8496, 2.8733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.10824292153120041 
model_pd.l_d.mean(): -24.596532821655273 
model_pd.lagr.mean(): -24.488290786743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0436], device='cuda:0')), ('power', tensor([-24.5529], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.10824292153120041
epoch£º71	 i:1 	 global-step:1421	 l-p:0.1137404590845108
epoch£º71	 i:2 	 global-step:1422	 l-p:0.14871706068515778
epoch£º71	 i:3 	 global-step:1423	 l-p:0.13960272073745728
epoch£º71	 i:4 	 global-step:1424	 l-p:0.12988917529582977
epoch£º71	 i:5 	 global-step:1425	 l-p:0.1113116592168808
epoch£º71	 i:6 	 global-step:1426	 l-p:0.14586934447288513
epoch£º71	 i:7 	 global-step:1427	 l-p:0.15857325494289398
epoch£º71	 i:8 	 global-step:1428	 l-p:0.1271318942308426
epoch£º71	 i:9 	 global-step:1429	 l-p:0.13428446650505066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9176, 2.9176, 2.9176],
        [2.9176, 2.9176, 2.9176],
        [2.9176, 2.9186, 2.9176],
        [2.9176, 2.9176, 2.9176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.02969868667423725 
model_pd.l_d.mean(): -24.05025863647461 
model_pd.lagr.mean(): -24.020559310913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1502], device='cuda:0')), ('power', tensor([-23.9001], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.02969868667423725
epoch£º72	 i:1 	 global-step:1441	 l-p:0.12385297566652298
epoch£º72	 i:2 	 global-step:1442	 l-p:0.12803895771503448
epoch£º72	 i:3 	 global-step:1443	 l-p:-0.25914543867111206
epoch£º72	 i:4 	 global-step:1444	 l-p:0.1113542690873146
epoch£º72	 i:5 	 global-step:1445	 l-p:0.12329127639532089
epoch£º72	 i:6 	 global-step:1446	 l-p:0.12086352705955505
epoch£º72	 i:7 	 global-step:1447	 l-p:0.12484697252511978
epoch£º72	 i:8 	 global-step:1448	 l-p:0.17300400137901306
epoch£º72	 i:9 	 global-step:1449	 l-p:0.12262048572301865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9777, 2.9777, 2.9777],
        [2.9777, 3.0270, 3.0018],
        [2.9777, 3.3854, 3.5605],
        [2.9777, 3.0506, 3.0223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.12585949897766113 
model_pd.l_d.mean(): -24.10529136657715 
model_pd.lagr.mean(): -23.97943115234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1868], device='cuda:0')), ('power', tensor([-23.9185], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.12585949897766113
epoch£º73	 i:1 	 global-step:1461	 l-p:0.05425276607275009
epoch£º73	 i:2 	 global-step:1462	 l-p:-0.7568035125732422
epoch£º73	 i:3 	 global-step:1463	 l-p:0.13084350526332855
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12553565204143524
epoch£º73	 i:5 	 global-step:1465	 l-p:0.1545306295156479
epoch£º73	 i:6 	 global-step:1466	 l-p:0.11451151221990585
epoch£º73	 i:7 	 global-step:1467	 l-p:0.11490186303853989
epoch£º73	 i:8 	 global-step:1468	 l-p:0.12844626605510712
epoch£º73	 i:9 	 global-step:1469	 l-p:0.11667929589748383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0249, 3.0296, 3.0255],
        [3.0249, 3.1607, 3.1410],
        [3.0249, 3.3251, 3.4014],
        [3.0249, 3.0997, 3.0708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.27842360734939575 
model_pd.l_d.mean(): -24.267925262451172 
model_pd.lagr.mean(): -23.989501953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1694], device='cuda:0')), ('power', tensor([-24.0985], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.27842360734939575
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11979833245277405
epoch£º74	 i:2 	 global-step:1482	 l-p:0.11191213130950928
epoch£º74	 i:3 	 global-step:1483	 l-p:0.12954449653625488
epoch£º74	 i:4 	 global-step:1484	 l-p:0.13386651873588562
epoch£º74	 i:5 	 global-step:1485	 l-p:0.11448274552822113
epoch£º74	 i:6 	 global-step:1486	 l-p:0.13198743760585785
epoch£º74	 i:7 	 global-step:1487	 l-p:0.14187273383140564
epoch£º74	 i:8 	 global-step:1488	 l-p:0.12393534183502197
epoch£º74	 i:9 	 global-step:1489	 l-p:0.11437764763832092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7775, 2.8723, 2.8497],
        [2.7775, 2.8315, 2.8071],
        [2.7775, 2.7775, 2.7775],
        [2.7775, 3.0479, 3.1240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.1088850349187851 
model_pd.l_d.mean(): -24.47740364074707 
model_pd.lagr.mean(): -24.368518829345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0886], device='cuda:0')), ('power', tensor([-24.3888], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.1088850349187851
epoch£º75	 i:1 	 global-step:1501	 l-p:0.14121879637241364
epoch£º75	 i:2 	 global-step:1502	 l-p:0.16485726833343506
epoch£º75	 i:3 	 global-step:1503	 l-p:0.1330868899822235
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13340651988983154
epoch£º75	 i:5 	 global-step:1505	 l-p:0.12537997961044312
epoch£º75	 i:6 	 global-step:1506	 l-p:0.12037277221679688
epoch£º75	 i:7 	 global-step:1507	 l-p:0.10498229414224625
epoch£º75	 i:8 	 global-step:1508	 l-p:2.096421241760254
epoch£º75	 i:9 	 global-step:1509	 l-p:0.1261695772409439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8470, 2.8470, 2.8470],
        [2.8470, 2.8470, 2.8470],
        [2.8470, 3.1144, 3.1822],
        [2.8470, 2.9114, 2.8853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.1831645965576172 
model_pd.l_d.mean(): -24.12023162841797 
model_pd.lagr.mean(): -23.93706703186035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0759], device='cuda:0')), ('power', tensor([-24.0444], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.1831645965576172
epoch£º76	 i:1 	 global-step:1521	 l-p:0.12003087997436523
epoch£º76	 i:2 	 global-step:1522	 l-p:0.16856008768081665
epoch£º76	 i:3 	 global-step:1523	 l-p:0.12506714463233948
epoch£º76	 i:4 	 global-step:1524	 l-p:0.09739766269922256
epoch£º76	 i:5 	 global-step:1525	 l-p:0.1457255482673645
epoch£º76	 i:6 	 global-step:1526	 l-p:0.10763164609670639
epoch£º76	 i:7 	 global-step:1527	 l-p:0.12361444532871246
epoch£º76	 i:8 	 global-step:1528	 l-p:0.10590799897909164
epoch£º76	 i:9 	 global-step:1529	 l-p:0.10377078503370285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1624, 3.1672, 3.1630],
        [3.1624, 3.1625, 3.1624],
        [3.1624, 3.1751, 3.1651],
        [3.1624, 3.1958, 3.1748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.13442547619342804 
model_pd.l_d.mean(): -24.59550666809082 
model_pd.lagr.mean(): -24.46108055114746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2929], device='cuda:0')), ('power', tensor([-24.3026], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.13442547619342804
epoch£º77	 i:1 	 global-step:1541	 l-p:0.10548295080661774
epoch£º77	 i:2 	 global-step:1542	 l-p:0.1331736296415329
epoch£º77	 i:3 	 global-step:1543	 l-p:0.12777811288833618
epoch£º77	 i:4 	 global-step:1544	 l-p:0.1284528374671936
epoch£º77	 i:5 	 global-step:1545	 l-p:0.15326008200645447
epoch£º77	 i:6 	 global-step:1546	 l-p:0.14467790722846985
epoch£º77	 i:7 	 global-step:1547	 l-p:0.1261034607887268
epoch£º77	 i:8 	 global-step:1548	 l-p:0.11657582223415375
epoch£º77	 i:9 	 global-step:1549	 l-p:0.13234098255634308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6943, 2.8727, 2.8902],
        [2.6943, 2.9836, 3.0852],
        [2.6943, 2.7130, 2.6996],
        [2.6943, 2.7024, 2.6957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.10606187582015991 
model_pd.l_d.mean(): -24.379934310913086 
model_pd.lagr.mean(): -24.27387237548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0230], device='cuda:0')), ('power', tensor([-24.3570], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.10606187582015991
epoch£º78	 i:1 	 global-step:1561	 l-p:0.15114200115203857
epoch£º78	 i:2 	 global-step:1562	 l-p:0.13461653888225555
epoch£º78	 i:3 	 global-step:1563	 l-p:0.14280588924884796
epoch£º78	 i:4 	 global-step:1564	 l-p:0.125982403755188
epoch£º78	 i:5 	 global-step:1565	 l-p:0.15153390169143677
epoch£º78	 i:6 	 global-step:1566	 l-p:0.12180539965629578
epoch£º78	 i:7 	 global-step:1567	 l-p:0.15775251388549805
epoch£º78	 i:8 	 global-step:1568	 l-p:0.12710274755954742
epoch£º78	 i:9 	 global-step:1569	 l-p:0.12598983943462372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0437, 3.3052, 3.3535],
        [3.0437, 3.0437, 3.0437],
        [3.0437, 3.0440, 3.0437],
        [3.0437, 3.0446, 3.0437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.13145366311073303 
model_pd.l_d.mean(): -24.610074996948242 
model_pd.lagr.mean(): -24.478620529174805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2338], device='cuda:0')), ('power', tensor([-24.3763], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.13145366311073303
epoch£º79	 i:1 	 global-step:1581	 l-p:0.1311844438314438
epoch£º79	 i:2 	 global-step:1582	 l-p:0.1089065745472908
epoch£º79	 i:3 	 global-step:1583	 l-p:0.11406217515468597
epoch£º79	 i:4 	 global-step:1584	 l-p:0.13893991708755493
epoch£º79	 i:5 	 global-step:1585	 l-p:-0.19870616495609283
epoch£º79	 i:6 	 global-step:1586	 l-p:0.10611497610807419
epoch£º79	 i:7 	 global-step:1587	 l-p:0.05076868087053299
epoch£º79	 i:8 	 global-step:1588	 l-p:0.14572618901729584
epoch£º79	 i:9 	 global-step:1589	 l-p:0.0788201093673706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9891, 3.5849, 3.9597],
        [2.9891, 2.9926, 2.9895],
        [2.9891, 2.9891, 2.9891],
        [2.9891, 3.3808, 3.5432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.12522423267364502 
model_pd.l_d.mean(): -23.97532844543457 
model_pd.lagr.mean(): -23.8501033782959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1565], device='cuda:0')), ('power', tensor([-23.8189], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.12522423267364502
epoch£º80	 i:1 	 global-step:1601	 l-p:0.6505019068717957
epoch£º80	 i:2 	 global-step:1602	 l-p:0.1510310173034668
epoch£º80	 i:3 	 global-step:1603	 l-p:0.13098083436489105
epoch£º80	 i:4 	 global-step:1604	 l-p:0.11108259856700897
epoch£º80	 i:5 	 global-step:1605	 l-p:0.11595553904771805
epoch£º80	 i:6 	 global-step:1606	 l-p:0.10099967569112778
epoch£º80	 i:7 	 global-step:1607	 l-p:0.11373548209667206
epoch£º80	 i:8 	 global-step:1608	 l-p:0.10976605862379074
epoch£º80	 i:9 	 global-step:1609	 l-p:0.12481605261564255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0323, 3.0335, 3.0324],
        [3.0323, 3.0323, 3.0323],
        [3.0323, 3.4350, 3.6032],
        [3.0323, 3.0504, 3.0372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.32919400930404663 
model_pd.l_d.mean(): -24.50673484802246 
model_pd.lagr.mean(): -24.177541732788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2106], device='cuda:0')), ('power', tensor([-24.2961], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.32919400930404663
epoch£º81	 i:1 	 global-step:1621	 l-p:0.12565255165100098
epoch£º81	 i:2 	 global-step:1622	 l-p:0.10986259579658508
epoch£º81	 i:3 	 global-step:1623	 l-p:0.08823146671056747
epoch£º81	 i:4 	 global-step:1624	 l-p:0.1259230375289917
epoch£º81	 i:5 	 global-step:1625	 l-p:0.13769949972629547
epoch£º81	 i:6 	 global-step:1626	 l-p:0.14107872545719147
epoch£º81	 i:7 	 global-step:1627	 l-p:0.5565313100814819
epoch£º81	 i:8 	 global-step:1628	 l-p:0.14247137308120728
epoch£º81	 i:9 	 global-step:1629	 l-p:0.13723790645599365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8504, 2.8639, 2.8535],
        [2.8504, 3.0778, 3.1178],
        [2.8504, 2.9032, 2.8782],
        [2.8504, 3.2553, 3.4483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.12431537359952927 
model_pd.l_d.mean(): -24.557010650634766 
model_pd.lagr.mean(): -24.432695388793945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1278], device='cuda:0')), ('power', tensor([-24.4292], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.12431537359952927
epoch£º82	 i:1 	 global-step:1641	 l-p:0.1735205054283142
epoch£º82	 i:2 	 global-step:1642	 l-p:0.12707187235355377
epoch£º82	 i:3 	 global-step:1643	 l-p:0.1412539780139923
epoch£º82	 i:4 	 global-step:1644	 l-p:0.09599386155605316
epoch£º82	 i:5 	 global-step:1645	 l-p:0.14684559404850006
epoch£º82	 i:6 	 global-step:1646	 l-p:0.10313379019498825
epoch£º82	 i:7 	 global-step:1647	 l-p:0.12819665670394897
epoch£º82	 i:8 	 global-step:1648	 l-p:0.11693324893712997
epoch£º82	 i:9 	 global-step:1649	 l-p:1.6214842796325684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9991, 3.3626, 3.4993],
        [2.9991, 3.0682, 3.0401],
        [2.9991, 2.9992, 2.9991],
        [2.9991, 3.1749, 3.1747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.12201377004384995 
model_pd.l_d.mean(): -24.052127838134766 
model_pd.lagr.mean(): -23.93011474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1481], device='cuda:0')), ('power', tensor([-23.9041], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.12201377004384995
epoch£º83	 i:1 	 global-step:1661	 l-p:0.1333654522895813
epoch£º83	 i:2 	 global-step:1662	 l-p:0.1211455762386322
epoch£º83	 i:3 	 global-step:1663	 l-p:0.19257110357284546
epoch£º83	 i:4 	 global-step:1664	 l-p:0.1241924986243248
epoch£º83	 i:5 	 global-step:1665	 l-p:0.1272580325603485
epoch£º83	 i:6 	 global-step:1666	 l-p:0.11747100204229355
epoch£º83	 i:7 	 global-step:1667	 l-p:0.12243019789457321
epoch£º83	 i:8 	 global-step:1668	 l-p:0.07773353159427643
epoch£º83	 i:9 	 global-step:1669	 l-p:0.12336579710245132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8916, 2.8916, 2.8916],
        [2.8916, 2.8928, 2.8917],
        [2.8916, 2.9062, 2.8950],
        [2.8916, 2.9516, 2.9252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1435999870300293 
model_pd.l_d.mean(): -24.560340881347656 
model_pd.lagr.mean(): -24.41674041748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1427], device='cuda:0')), ('power', tensor([-24.4176], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.1435999870300293
epoch£º84	 i:1 	 global-step:1681	 l-p:0.13152369856834412
epoch£º84	 i:2 	 global-step:1682	 l-p:0.11897353082895279
epoch£º84	 i:3 	 global-step:1683	 l-p:0.1216980367898941
epoch£º84	 i:4 	 global-step:1684	 l-p:0.11786564439535141
epoch£º84	 i:5 	 global-step:1685	 l-p:0.13718318939208984
epoch£º84	 i:6 	 global-step:1686	 l-p:0.12951968610286713
epoch£º84	 i:7 	 global-step:1687	 l-p:0.13444140553474426
epoch£º84	 i:8 	 global-step:1688	 l-p:0.10893067717552185
epoch£º84	 i:9 	 global-step:1689	 l-p:0.6044582724571228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[2.8173, 3.0601, 3.1154],
        [2.8173, 2.9531, 2.9432],
        [2.8173, 3.1052, 3.1963],
        [2.8173, 2.9027, 2.8780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.1402651071548462 
model_pd.l_d.mean(): -24.42180633544922 
model_pd.lagr.mean(): -24.28154182434082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0820], device='cuda:0')), ('power', tensor([-24.3398], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.1402651071548462
epoch£º85	 i:1 	 global-step:1701	 l-p:0.1683049350976944
epoch£º85	 i:2 	 global-step:1702	 l-p:0.1324644684791565
epoch£º85	 i:3 	 global-step:1703	 l-p:0.09770805388689041
epoch£º85	 i:4 	 global-step:1704	 l-p:0.12532231211662292
epoch£º85	 i:5 	 global-step:1705	 l-p:0.13281920552253723
epoch£º85	 i:6 	 global-step:1706	 l-p:0.14860320091247559
epoch£º85	 i:7 	 global-step:1707	 l-p:-0.6309431791305542
epoch£º85	 i:8 	 global-step:1708	 l-p:0.1267315000295639
epoch£º85	 i:9 	 global-step:1709	 l-p:-0.023008909076452255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0273, 3.3663, 3.4801],
        [3.0273, 3.0290, 3.0275],
        [3.0273, 3.0273, 3.0273],
        [3.0273, 3.1253, 3.0982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.13620567321777344 
model_pd.l_d.mean(): -23.09429359436035 
model_pd.lagr.mean(): -22.958087921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0004], device='cuda:0')), ('power', tensor([-23.0939], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.13620567321777344
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11045461148023605
epoch£º86	 i:2 	 global-step:1722	 l-p:0.10901492834091187
epoch£º86	 i:3 	 global-step:1723	 l-p:0.12953947484493256
epoch£º86	 i:4 	 global-step:1724	 l-p:0.14645680785179138
epoch£º86	 i:5 	 global-step:1725	 l-p:0.19189707934856415
epoch£º86	 i:6 	 global-step:1726	 l-p:0.11333808302879333
epoch£º86	 i:7 	 global-step:1727	 l-p:0.12698888778686523
epoch£º86	 i:8 	 global-step:1728	 l-p:0.11994768679141998
epoch£º86	 i:9 	 global-step:1729	 l-p:0.12789258360862732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9705, 2.9911, 2.9762],
        [2.9705, 2.9711, 2.9705],
        [2.9705, 2.9705, 2.9705],
        [2.9705, 3.1432, 3.1437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): -0.23210711777210236 
model_pd.l_d.mean(): -24.21832847595215 
model_pd.lagr.mean(): -24.450435638427734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1184], device='cuda:0')), ('power', tensor([-24.0999], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:-0.23210711777210236
epoch£º87	 i:1 	 global-step:1741	 l-p:0.12203110754489899
epoch£º87	 i:2 	 global-step:1742	 l-p:0.12997736036777496
epoch£º87	 i:3 	 global-step:1743	 l-p:0.12754544615745544
epoch£º87	 i:4 	 global-step:1744	 l-p:0.13069649040699005
epoch£º87	 i:5 	 global-step:1745	 l-p:0.14661630988121033
epoch£º87	 i:6 	 global-step:1746	 l-p:0.12581001222133636
epoch£º87	 i:7 	 global-step:1747	 l-p:0.10784104466438293
epoch£º87	 i:8 	 global-step:1748	 l-p:-0.11339744180440903
epoch£º87	 i:9 	 global-step:1749	 l-p:0.13259294629096985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7928, 2.7928, 2.7928],
        [2.7928, 2.8604, 2.8343],
        [2.7928, 2.8087, 2.7961],
        [2.7928, 2.7931, 2.7928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.14525197446346283 
model_pd.l_d.mean(): -24.7810115814209 
model_pd.lagr.mean(): -24.635759353637695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1407], device='cuda:0')), ('power', tensor([-24.6404], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.14525197446346283
epoch£º88	 i:1 	 global-step:1761	 l-p:0.22579091787338257
epoch£º88	 i:2 	 global-step:1762	 l-p:0.1103978157043457
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13409322500228882
epoch£º88	 i:4 	 global-step:1764	 l-p:0.14491795003414154
epoch£º88	 i:5 	 global-step:1765	 l-p:0.1091725155711174
epoch£º88	 i:6 	 global-step:1766	 l-p:0.12608741223812103
epoch£º88	 i:7 	 global-step:1767	 l-p:0.12424925714731216
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11753533780574799
epoch£º88	 i:9 	 global-step:1769	 l-p:0.12476911395788193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9460, 3.3288, 3.4919],
        [2.9460, 2.9497, 2.9463],
        [2.9460, 2.9513, 2.9466],
        [2.9460, 2.9816, 2.9598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.04816742613911629 
model_pd.l_d.mean(): -24.078632354736328 
model_pd.lagr.mean(): -24.03046417236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0823], device='cuda:0')), ('power', tensor([-23.9963], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.04816742613911629
epoch£º89	 i:1 	 global-step:1781	 l-p:0.14060761034488678
epoch£º89	 i:2 	 global-step:1782	 l-p:0.12656207382678986
epoch£º89	 i:3 	 global-step:1783	 l-p:0.12014883011579514
epoch£º89	 i:4 	 global-step:1784	 l-p:0.033455055207014084
epoch£º89	 i:5 	 global-step:1785	 l-p:0.12703333795070648
epoch£º89	 i:6 	 global-step:1786	 l-p:0.1115855798125267
epoch£º89	 i:7 	 global-step:1787	 l-p:0.12862494587898254
epoch£º89	 i:8 	 global-step:1788	 l-p:0.17142711579799652
epoch£º89	 i:9 	 global-step:1789	 l-p:0.11885791271924973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0460, 3.5001, 3.7204],
        [3.0460, 3.0460, 3.0460],
        [3.0460, 3.0460, 3.0460],
        [3.0460, 3.0529, 3.0470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.12576013803482056 
model_pd.l_d.mean(): -24.130338668823242 
model_pd.lagr.mean(): -24.00457763671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1930], device='cuda:0')), ('power', tensor([-23.9373], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.12576013803482056
epoch£º90	 i:1 	 global-step:1801	 l-p:-0.572770357131958
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12422323226928711
epoch£º90	 i:3 	 global-step:1803	 l-p:0.1903674155473709
epoch£º90	 i:4 	 global-step:1804	 l-p:0.1278599202632904
epoch£º90	 i:5 	 global-step:1805	 l-p:0.11263053864240646
epoch£º90	 i:6 	 global-step:1806	 l-p:0.13012520968914032
epoch£º90	 i:7 	 global-step:1807	 l-p:0.13720610737800598
epoch£º90	 i:8 	 global-step:1808	 l-p:0.163014218211174
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13342629373073578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8422, 2.8422, 2.8422],
        [2.8422, 2.8422, 2.8422],
        [2.8422, 2.8426, 2.8422],
        [2.8422, 3.0964, 3.1603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.12078000605106354 
model_pd.l_d.mean(): -24.137584686279297 
model_pd.lagr.mean(): -24.016803741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1249], device='cuda:0')), ('power', tensor([-24.0127], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.12078000605106354
epoch£º91	 i:1 	 global-step:1821	 l-p:0.13878025114536285
epoch£º91	 i:2 	 global-step:1822	 l-p:0.09217780828475952
epoch£º91	 i:3 	 global-step:1823	 l-p:0.12898659706115723
epoch£º91	 i:4 	 global-step:1824	 l-p:0.14114420115947723
epoch£º91	 i:5 	 global-step:1825	 l-p:0.11639320850372314
epoch£º91	 i:6 	 global-step:1826	 l-p:0.13709783554077148
epoch£º91	 i:7 	 global-step:1827	 l-p:0.155890092253685
epoch£º91	 i:8 	 global-step:1828	 l-p:0.1673881858587265
epoch£º91	 i:9 	 global-step:1829	 l-p:0.17810684442520142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9189, 2.9189, 2.9189],
        [2.9189, 3.3065, 3.4788],
        [2.9189, 2.9479, 2.9282],
        [2.9189, 2.9276, 2.9200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.15111583471298218 
model_pd.l_d.mean(): -24.208189010620117 
model_pd.lagr.mean(): -24.05707359313965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0823], device='cuda:0')), ('power', tensor([-24.1259], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.15111583471298218
epoch£º92	 i:1 	 global-step:1841	 l-p:0.12263250350952148
epoch£º92	 i:2 	 global-step:1842	 l-p:0.12498646229505539
epoch£º92	 i:3 	 global-step:1843	 l-p:0.11391428858041763
epoch£º92	 i:4 	 global-step:1844	 l-p:0.14776702225208282
epoch£º92	 i:5 	 global-step:1845	 l-p:0.12253868579864502
epoch£º92	 i:6 	 global-step:1846	 l-p:0.12502209842205048
epoch£º92	 i:7 	 global-step:1847	 l-p:0.0891583263874054
epoch£º92	 i:8 	 global-step:1848	 l-p:0.06244291737675667
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12514148652553558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9083, 2.9083, 2.9083],
        [2.9083, 3.0041, 2.9790],
        [2.9083, 3.0163, 2.9941],
        [2.9083, 2.9730, 2.9452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.1375969797372818 
model_pd.l_d.mean(): -24.275304794311523 
model_pd.lagr.mean(): -24.13770866394043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1280], device='cuda:0')), ('power', tensor([-24.1473], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.1375969797372818
epoch£º93	 i:1 	 global-step:1861	 l-p:0.14070028066635132
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12613588571548462
epoch£º93	 i:3 	 global-step:1863	 l-p:0.12012346088886261
epoch£º93	 i:4 	 global-step:1864	 l-p:0.34209144115448
epoch£º93	 i:5 	 global-step:1865	 l-p:0.13097165524959564
epoch£º93	 i:6 	 global-step:1866	 l-p:0.1373828649520874
epoch£º93	 i:7 	 global-step:1867	 l-p:0.12844571471214294
epoch£º93	 i:8 	 global-step:1868	 l-p:0.11359817534685135
epoch£º93	 i:9 	 global-step:1869	 l-p:0.0942864716053009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8674, 3.0179, 3.0135],
        [2.8674, 2.8674, 2.8675],
        [2.8674, 2.8716, 2.8674],
        [2.8674, 2.8682, 2.8674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.12972401082515717 
model_pd.l_d.mean(): -24.809707641601562 
model_pd.lagr.mean(): -24.679983139038086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1800], device='cuda:0')), ('power', tensor([-24.6297], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.12972401082515717
epoch£º94	 i:1 	 global-step:1881	 l-p:0.15099824965000153
epoch£º94	 i:2 	 global-step:1882	 l-p:0.15866442024707794
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12988591194152832
epoch£º94	 i:4 	 global-step:1884	 l-p:0.10670610517263412
epoch£º94	 i:5 	 global-step:1885	 l-p:0.0535866916179657
epoch£º94	 i:6 	 global-step:1886	 l-p:0.1278839260339737
epoch£º94	 i:7 	 global-step:1887	 l-p:0.43857046961784363
epoch£º94	 i:8 	 global-step:1888	 l-p:0.11483994871377945
epoch£º94	 i:9 	 global-step:1889	 l-p:0.21031305193901062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1285, 3.1285, 3.1285],
        [3.1285, 3.1285, 3.1285],
        [3.1285, 3.1286, 3.1285],
        [3.1285, 3.1307, 3.1286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.14488917589187622 
model_pd.l_d.mean(): -24.043872833251953 
model_pd.lagr.mean(): -23.898983001708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1911], device='cuda:0')), ('power', tensor([-23.8527], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.14488917589187622
epoch£º95	 i:1 	 global-step:1901	 l-p:0.10944116115570068
epoch£º95	 i:2 	 global-step:1902	 l-p:0.10990776121616364
epoch£º95	 i:3 	 global-step:1903	 l-p:0.09978591650724411
epoch£º95	 i:4 	 global-step:1904	 l-p:0.11711394041776657
epoch£º95	 i:5 	 global-step:1905	 l-p:0.11855139583349228
epoch£º95	 i:6 	 global-step:1906	 l-p:0.14104972779750824
epoch£º95	 i:7 	 global-step:1907	 l-p:0.14939168095588684
epoch£º95	 i:8 	 global-step:1908	 l-p:0.12831436097621918
epoch£º95	 i:9 	 global-step:1909	 l-p:0.12329885363578796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0090, 3.4759, 3.7161],
        [3.0090, 3.0090, 3.0090],
        [3.0090, 3.2843, 3.3521],
        [3.0090, 3.0090, 3.0090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.23356521129608154 
model_pd.l_d.mean(): -23.598560333251953 
model_pd.lagr.mean(): -23.3649959564209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0387], device='cuda:0')), ('power', tensor([-23.5598], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.23356521129608154
epoch£º96	 i:1 	 global-step:1921	 l-p:0.11295019090175629
epoch£º96	 i:2 	 global-step:1922	 l-p:0.04368458315730095
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13503804802894592
epoch£º96	 i:4 	 global-step:1924	 l-p:0.13292521238327026
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13269159197807312
epoch£º96	 i:6 	 global-step:1926	 l-p:0.13992570340633392
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12959611415863037
epoch£º96	 i:8 	 global-step:1928	 l-p:0.12466148287057877
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12712129950523376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7654, 2.8693, 2.8493],
        [2.7654, 3.0065, 3.0692],
        [2.7654, 2.7654, 2.7654],
        [2.7654, 3.1943, 3.4282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.13275931775569916 
model_pd.l_d.mean(): -24.6270809173584 
model_pd.lagr.mean(): -24.494321823120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0960], device='cuda:0')), ('power', tensor([-24.5311], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.13275931775569916
epoch£º97	 i:1 	 global-step:1941	 l-p:0.13949277997016907
epoch£º97	 i:2 	 global-step:1942	 l-p:-0.21328367292881012
epoch£º97	 i:3 	 global-step:1943	 l-p:0.15616485476493835
epoch£º97	 i:4 	 global-step:1944	 l-p:0.1293240636587143
epoch£º97	 i:5 	 global-step:1945	 l-p:0.201007679104805
epoch£º97	 i:6 	 global-step:1946	 l-p:0.09690936654806137
epoch£º97	 i:7 	 global-step:1947	 l-p:0.11834076792001724
epoch£º97	 i:8 	 global-step:1948	 l-p:0.1258651465177536
epoch£º97	 i:9 	 global-step:1949	 l-p:0.1378673017024994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9551, 3.0549, 3.0292],
        [2.9551, 2.9551, 2.9551],
        [2.9551, 2.9589, 2.9549],
        [2.9551, 3.1513, 3.1683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.140202596783638 
model_pd.l_d.mean(): -24.560239791870117 
model_pd.lagr.mean(): -24.42003631591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1680], device='cuda:0')), ('power', tensor([-24.3923], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.140202596783638
epoch£º98	 i:1 	 global-step:1961	 l-p:-0.19098380208015442
epoch£º98	 i:2 	 global-step:1962	 l-p:0.11745037883520126
epoch£º98	 i:3 	 global-step:1963	 l-p:0.15573616325855255
epoch£º98	 i:4 	 global-step:1964	 l-p:0.11782374978065491
epoch£º98	 i:5 	 global-step:1965	 l-p:0.12045465409755707
epoch£º98	 i:6 	 global-step:1966	 l-p:0.10147546231746674
epoch£º98	 i:7 	 global-step:1967	 l-p:0.10243991762399673
epoch£º98	 i:8 	 global-step:1968	 l-p:0.14244197309017181
epoch£º98	 i:9 	 global-step:1969	 l-p:0.12452130019664764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1213, 3.1213, 3.1213],
        [3.1213, 3.1213, 3.1213],
        [3.1213, 3.2718, 3.2574],
        [3.1213, 3.1213, 3.1213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.11655909568071365 
model_pd.l_d.mean(): -24.154645919799805 
model_pd.lagr.mean(): -24.0380859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2483], device='cuda:0')), ('power', tensor([-23.9064], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.11655909568071365
epoch£º99	 i:1 	 global-step:1981	 l-p:0.13249380886554718
epoch£º99	 i:2 	 global-step:1982	 l-p:0.1310889720916748
epoch£º99	 i:3 	 global-step:1983	 l-p:0.09976333379745483
epoch£º99	 i:4 	 global-step:1984	 l-p:0.1761995404958725
epoch£º99	 i:5 	 global-step:1985	 l-p:0.1225319430232048
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12428396940231323
epoch£º99	 i:7 	 global-step:1987	 l-p:0.15056467056274414
epoch£º99	 i:8 	 global-step:1988	 l-p:1.1624289751052856
epoch£º99	 i:9 	 global-step:1989	 l-p:0.12634938955307007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8292, 2.8291, 2.8292],
        [2.8292, 2.9507, 2.9350],
        [2.8292, 2.8358, 2.8274],
        [2.8292, 2.8292, 2.8292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.1313135325908661 
model_pd.l_d.mean(): -24.82989501953125 
model_pd.lagr.mean(): -24.69858169555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1613], device='cuda:0')), ('power', tensor([-24.6686], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.1313135325908661
epoch£º100	 i:1 	 global-step:2001	 l-p:0.12025757879018784
epoch£º100	 i:2 	 global-step:2002	 l-p:0.11586225032806396
epoch£º100	 i:3 	 global-step:2003	 l-p:0.5767765045166016
epoch£º100	 i:4 	 global-step:2004	 l-p:0.15169082581996918
epoch£º100	 i:5 	 global-step:2005	 l-p:0.1358022689819336
epoch£º100	 i:6 	 global-step:2006	 l-p:0.12912414968013763
epoch£º100	 i:7 	 global-step:2007	 l-p:0.10376700013875961
epoch£º100	 i:8 	 global-step:2008	 l-p:0.15132074058055878
epoch£º100	 i:9 	 global-step:2009	 l-p:0.12237747758626938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0211, 3.0277, 3.0209],
        [3.0211, 3.0211, 3.0211],
        [3.0211, 3.0211, 3.0211],
        [3.0211, 3.0211, 3.0211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.5725218653678894 
model_pd.l_d.mean(): -24.47247314453125 
model_pd.lagr.mean(): -23.899951934814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2046], device='cuda:0')), ('power', tensor([-24.2679], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.5725218653678894
epoch£º101	 i:1 	 global-step:2021	 l-p:0.12453411519527435
epoch£º101	 i:2 	 global-step:2022	 l-p:0.14072875678539276
epoch£º101	 i:3 	 global-step:2023	 l-p:0.12189289182424545
epoch£º101	 i:4 	 global-step:2024	 l-p:0.12234549224376678
epoch£º101	 i:5 	 global-step:2025	 l-p:0.11591698974370956
epoch£º101	 i:6 	 global-step:2026	 l-p:0.10503879934549332
epoch£º101	 i:7 	 global-step:2027	 l-p:-0.07095926254987717
epoch£º101	 i:8 	 global-step:2028	 l-p:0.11428844183683395
epoch£º101	 i:9 	 global-step:2029	 l-p:0.12157237529754639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[2.9343, 3.1484, 3.1791],
        [2.9343, 3.1505, 3.1825],
        [2.9343, 3.3788, 3.6079],
        [2.9343, 2.9587, 2.9379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.12556953728199005 
model_pd.l_d.mean(): -24.47217559814453 
model_pd.lagr.mean(): -24.34660530090332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1405], device='cuda:0')), ('power', tensor([-24.3317], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.12556953728199005
epoch£º102	 i:1 	 global-step:2041	 l-p:0.10189453512430191
epoch£º102	 i:2 	 global-step:2042	 l-p:0.1442943662405014
epoch£º102	 i:3 	 global-step:2043	 l-p:0.12063118815422058
epoch£º102	 i:4 	 global-step:2044	 l-p:0.1315937340259552
epoch£º102	 i:5 	 global-step:2045	 l-p:0.14398740231990814
epoch£º102	 i:6 	 global-step:2046	 l-p:0.12764839828014374
epoch£º102	 i:7 	 global-step:2047	 l-p:0.11898167431354523
epoch£º102	 i:8 	 global-step:2048	 l-p:0.12601712346076965
epoch£º102	 i:9 	 global-step:2049	 l-p:0.09663642942905426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7490, 2.7515, 2.7440],
        [2.7490, 2.8042, 2.7734],
        [2.7490, 2.7508, 2.7442],
        [2.7490, 2.7479, 2.7487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.2422114461660385 
model_pd.l_d.mean(): -24.384445190429688 
model_pd.lagr.mean(): -24.14223289489746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0315], device='cuda:0')), ('power', tensor([-24.3530], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.2422114461660385
epoch£º103	 i:1 	 global-step:2061	 l-p:0.11941511183977127
epoch£º103	 i:2 	 global-step:2062	 l-p:0.12980607151985168
epoch£º103	 i:3 	 global-step:2063	 l-p:0.1315738409757614
epoch£º103	 i:4 	 global-step:2064	 l-p:0.12646600604057312
epoch£º103	 i:5 	 global-step:2065	 l-p:0.1421627551317215
epoch£º103	 i:6 	 global-step:2066	 l-p:0.133334681391716
epoch£º103	 i:7 	 global-step:2067	 l-p:0.17826497554779053
epoch£º103	 i:8 	 global-step:2068	 l-p:0.11830219626426697
epoch£º103	 i:9 	 global-step:2069	 l-p:0.03685905039310455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0513, 3.0513, 3.0513],
        [3.0513, 3.0739, 3.0544],
        [3.0513, 3.0562, 3.0505],
        [3.0513, 3.3854, 3.4998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.13336877524852753 
model_pd.l_d.mean(): -24.78611183166504 
model_pd.lagr.mean(): -24.652742385864258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2682], device='cuda:0')), ('power', tensor([-24.5180], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.13336877524852753
epoch£º104	 i:1 	 global-step:2081	 l-p:0.17521968483924866
epoch£º104	 i:2 	 global-step:2082	 l-p:0.11880739033222198
epoch£º104	 i:3 	 global-step:2083	 l-p:0.11715692281723022
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12476308643817902
epoch£º104	 i:5 	 global-step:2085	 l-p:0.113998182117939
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12074804306030273
epoch£º104	 i:7 	 global-step:2087	 l-p:0.09904757887125015
epoch£º104	 i:8 	 global-step:2088	 l-p:0.12270082533359528
epoch£º104	 i:9 	 global-step:2089	 l-p:-0.11546758562326431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0210, 3.0210, 3.0210],
        [3.0210, 3.0210, 3.0210],
        [3.0210, 3.0575, 3.0308],
        [3.0210, 3.1731, 3.1635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.12071382999420166 
model_pd.l_d.mean(): -24.58979606628418 
model_pd.lagr.mean(): -24.46908187866211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2126], device='cuda:0')), ('power', tensor([-24.3772], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.12071382999420166
epoch£º105	 i:1 	 global-step:2101	 l-p:0.12819160521030426
epoch£º105	 i:2 	 global-step:2102	 l-p:0.1149488016963005
epoch£º105	 i:3 	 global-step:2103	 l-p:0.12768834829330444
epoch£º105	 i:4 	 global-step:2104	 l-p:0.010650052689015865
epoch£º105	 i:5 	 global-step:2105	 l-p:0.19169814884662628
epoch£º105	 i:6 	 global-step:2106	 l-p:0.1321144998073578
epoch£º105	 i:7 	 global-step:2107	 l-p:0.13038717210292816
epoch£º105	 i:8 	 global-step:2108	 l-p:0.12467404454946518
epoch£º105	 i:9 	 global-step:2109	 l-p:0.10657777637243271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8375, 2.8371, 2.8375],
        [2.8375, 3.0228, 3.0414],
        [2.8375, 2.8361, 2.8369],
        [2.8375, 3.1088, 3.1913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.15079347789287567 
model_pd.l_d.mean(): -23.710302352905273 
model_pd.lagr.mean(): -23.55950927734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0072], device='cuda:0')), ('power', tensor([-23.7031], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.15079347789287567
epoch£º106	 i:1 	 global-step:2121	 l-p:0.15532490611076355
epoch£º106	 i:2 	 global-step:2122	 l-p:0.1275123804807663
epoch£º106	 i:3 	 global-step:2123	 l-p:0.12814918160438538
epoch£º106	 i:4 	 global-step:2124	 l-p:0.11779329180717468
epoch£º106	 i:5 	 global-step:2125	 l-p:0.11994241178035736
epoch£º106	 i:6 	 global-step:2126	 l-p:0.6412324905395508
epoch£º106	 i:7 	 global-step:2127	 l-p:0.15847668051719666
epoch£º106	 i:8 	 global-step:2128	 l-p:0.10382858663797379
epoch£º106	 i:9 	 global-step:2129	 l-p:0.1262369006872177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2039, 3.2039, 3.2039],
        [3.2039, 3.2040, 3.2039],
        [3.2039, 3.6346, 3.8206],
        [3.2039, 3.5369, 3.6374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.09142681956291199 
model_pd.l_d.mean(): -24.108356475830078 
model_pd.lagr.mean(): -24.016929626464844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2385], device='cuda:0')), ('power', tensor([-23.8698], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.09142681956291199
epoch£º107	 i:1 	 global-step:2141	 l-p:0.13073357939720154
epoch£º107	 i:2 	 global-step:2142	 l-p:0.1220584288239479
epoch£º107	 i:3 	 global-step:2143	 l-p:0.12774620950222015
epoch£º107	 i:4 	 global-step:2144	 l-p:0.11489228159189224
epoch£º107	 i:5 	 global-step:2145	 l-p:0.12529778480529785
epoch£º107	 i:6 	 global-step:2146	 l-p:0.12623868882656097
epoch£º107	 i:7 	 global-step:2147	 l-p:0.09383906424045563
epoch£º107	 i:8 	 global-step:2148	 l-p:0.17540563642978668
epoch£º107	 i:9 	 global-step:2149	 l-p:0.1297067254781723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6804, 2.6785, 2.6800],
        [2.6804, 2.6804, 2.6804],
        [2.6804, 2.6763, 2.6714],
        [2.6804, 2.6804, 2.6804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.11983586847782135 
model_pd.l_d.mean(): -23.891490936279297 
model_pd.lagr.mean(): -23.77165412902832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0186], device='cuda:0')), ('power', tensor([-23.9101], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.11983586847782135
epoch£º108	 i:1 	 global-step:2161	 l-p:0.14115875959396362
epoch£º108	 i:2 	 global-step:2162	 l-p:0.1425553858280182
epoch£º108	 i:3 	 global-step:2163	 l-p:0.12689752876758575
epoch£º108	 i:4 	 global-step:2164	 l-p:0.19226230680942535
epoch£º108	 i:5 	 global-step:2165	 l-p:0.12869521975517273
epoch£º108	 i:6 	 global-step:2166	 l-p:0.13959001004695892
epoch£º108	 i:7 	 global-step:2167	 l-p:0.250613808631897
epoch£º108	 i:8 	 global-step:2168	 l-p:0.1169828400015831
epoch£º108	 i:9 	 global-step:2169	 l-p:0.8219897150993347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9226, 2.9248, 2.9182],
        [2.9226, 2.9224, 2.9226],
        [2.9226, 3.3676, 3.5996],
        [2.9226, 2.9226, 2.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.1599685102701187 
model_pd.l_d.mean(): -24.452436447143555 
model_pd.lagr.mean(): -24.29246711730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1358], device='cuda:0')), ('power', tensor([-24.3167], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.1599685102701187
epoch£º109	 i:1 	 global-step:2181	 l-p:0.3833715319633484
epoch£º109	 i:2 	 global-step:2182	 l-p:0.15778516232967377
epoch£º109	 i:3 	 global-step:2183	 l-p:0.11439558118581772
epoch£º109	 i:4 	 global-step:2184	 l-p:0.059302918612957
epoch£º109	 i:5 	 global-step:2185	 l-p:0.10237035155296326
epoch£º109	 i:6 	 global-step:2186	 l-p:0.19699981808662415
epoch£º109	 i:7 	 global-step:2187	 l-p:0.10395696759223938
epoch£º109	 i:8 	 global-step:2188	 l-p:0.10920972377061844
epoch£º109	 i:9 	 global-step:2189	 l-p:0.10565376281738281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.7785, 3.7786, 3.7785],
        [3.7785, 3.7785, 3.7785],
        [3.7785, 3.7787, 3.7785],
        [3.7785, 3.7841, 3.7791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.10262748599052429 
model_pd.l_d.mean(): -24.54203987121582 
model_pd.lagr.mean(): -24.43941307067871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5600], device='cuda:0')), ('power', tensor([-23.9820], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.10262748599052429
epoch£º110	 i:1 	 global-step:2201	 l-p:0.10382281243801117
epoch£º110	 i:2 	 global-step:2202	 l-p:0.11597473174333572
epoch£º110	 i:3 	 global-step:2203	 l-p:0.08675588667392731
epoch£º110	 i:4 	 global-step:2204	 l-p:0.10194068402051926
epoch£º110	 i:5 	 global-step:2205	 l-p:0.07425175607204437
epoch£º110	 i:6 	 global-step:2206	 l-p:0.11037357151508331
epoch£º110	 i:7 	 global-step:2207	 l-p:0.1035022884607315
epoch£º110	 i:8 	 global-step:2208	 l-p:0.12893128395080566
epoch£º110	 i:9 	 global-step:2209	 l-p:-0.1782887578010559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9234, 2.9228, 2.9233],
        [2.9234, 3.1164, 3.1347],
        [2.9234, 3.0499, 3.0323],
        [2.9234, 3.1979, 3.2755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.1323074847459793 
model_pd.l_d.mean(): -24.475862503051758 
model_pd.lagr.mean(): -24.343555450439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1341], device='cuda:0')), ('power', tensor([-24.3417], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.1323074847459793
epoch£º111	 i:1 	 global-step:2221	 l-p:0.6362295150756836
epoch£º111	 i:2 	 global-step:2222	 l-p:0.13607828319072723
epoch£º111	 i:3 	 global-step:2223	 l-p:0.141987606883049
epoch£º111	 i:4 	 global-step:2224	 l-p:0.5655505061149597
epoch£º111	 i:5 	 global-step:2225	 l-p:0.11607488989830017
epoch£º111	 i:6 	 global-step:2226	 l-p:0.06114958971738815
epoch£º111	 i:7 	 global-step:2227	 l-p:0.18844085931777954
epoch£º111	 i:8 	 global-step:2228	 l-p:0.09040800482034683
epoch£º111	 i:9 	 global-step:2229	 l-p:0.14108844101428986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8896, 2.8892, 2.8859],
        [2.8896, 2.8890, 2.8860],
        [2.8896, 2.8896, 2.8896],
        [2.8896, 2.8879, 2.8887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.2230028361082077 
model_pd.l_d.mean(): -24.732940673828125 
model_pd.lagr.mean(): -24.509937286376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1689], device='cuda:0')), ('power', tensor([-24.5641], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.2230028361082077
epoch£º112	 i:1 	 global-step:2241	 l-p:0.1214761734008789
epoch£º112	 i:2 	 global-step:2242	 l-p:0.13045893609523773
epoch£º112	 i:3 	 global-step:2243	 l-p:0.2449241578578949
epoch£º112	 i:4 	 global-step:2244	 l-p:0.11717764288187027
epoch£º112	 i:5 	 global-step:2245	 l-p:0.12249574065208435
epoch£º112	 i:6 	 global-step:2246	 l-p:0.13077297806739807
epoch£º112	 i:7 	 global-step:2247	 l-p:0.12318442761898041
epoch£º112	 i:8 	 global-step:2248	 l-p:0.11709308624267578
epoch£º112	 i:9 	 global-step:2249	 l-p:0.09242980927228928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1370, 3.1370, 3.1370],
        [3.1370, 3.3771, 3.4139],
        [3.1370, 3.1422, 3.1355],
        [3.1370, 3.1370, 3.1370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.16014550626277924 
model_pd.l_d.mean(): -24.177494049072266 
model_pd.lagr.mean(): -24.017349243164062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2171], device='cuda:0')), ('power', tensor([-23.9604], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.16014550626277924
epoch£º113	 i:1 	 global-step:2261	 l-p:0.15239393711090088
epoch£º113	 i:2 	 global-step:2262	 l-p:0.1510716825723648
epoch£º113	 i:3 	 global-step:2263	 l-p:0.12618336081504822
epoch£º113	 i:4 	 global-step:2264	 l-p:0.11027537286281586
epoch£º113	 i:5 	 global-step:2265	 l-p:0.1334773600101471
epoch£º113	 i:6 	 global-step:2266	 l-p:0.12020029127597809
epoch£º113	 i:7 	 global-step:2267	 l-p:0.1172502264380455
epoch£º113	 i:8 	 global-step:2268	 l-p:0.16495680809020996
epoch£º113	 i:9 	 global-step:2269	 l-p:0.12610946595668793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9211, 2.9211, 2.9211],
        [2.9211, 2.9209, 2.9211],
        [2.9211, 2.9210, 2.9211],
        [2.9211, 3.2008, 3.2833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.14208924770355225 
model_pd.l_d.mean(): -24.22909927368164 
model_pd.lagr.mean(): -24.08700942993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0882], device='cuda:0')), ('power', tensor([-24.1409], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.14208924770355225
epoch£º114	 i:1 	 global-step:2281	 l-p:0.10723374783992767
epoch£º114	 i:2 	 global-step:2282	 l-p:0.10866924375295639
epoch£º114	 i:3 	 global-step:2283	 l-p:0.12712469696998596
epoch£º114	 i:4 	 global-step:2284	 l-p:0.14037244021892548
epoch£º114	 i:5 	 global-step:2285	 l-p:0.1281757354736328
epoch£º114	 i:6 	 global-step:2286	 l-p:0.11202535033226013
epoch£º114	 i:7 	 global-step:2287	 l-p:0.1508083939552307
epoch£º114	 i:8 	 global-step:2288	 l-p:0.12820708751678467
epoch£º114	 i:9 	 global-step:2289	 l-p:0.25251200795173645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8604, 2.8604, 2.8604],
        [2.8604, 2.8604, 2.8604],
        [2.8604, 2.8604, 2.8604],
        [2.8604, 2.8760, 2.8524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.0980570837855339 
model_pd.l_d.mean(): -24.584142684936523 
model_pd.lagr.mean(): -24.486085891723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1237], device='cuda:0')), ('power', tensor([-24.4604], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.0980570837855339
epoch£º115	 i:1 	 global-step:2301	 l-p:0.14270734786987305
epoch£º115	 i:2 	 global-step:2302	 l-p:0.10616065561771393
epoch£º115	 i:3 	 global-step:2303	 l-p:0.12122846394777298
epoch£º115	 i:4 	 global-step:2304	 l-p:0.12570500373840332
epoch£º115	 i:5 	 global-step:2305	 l-p:0.1379256248474121
epoch£º115	 i:6 	 global-step:2306	 l-p:0.11989643424749374
epoch£º115	 i:7 	 global-step:2307	 l-p:0.18645921349525452
epoch£º115	 i:8 	 global-step:2308	 l-p:0.15089930593967438
epoch£º115	 i:9 	 global-step:2309	 l-p:0.12740372121334076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9511, 2.9512, 2.9455],
        [2.9511, 2.9487, 2.9488],
        [2.9511, 2.9511, 2.9511],
        [2.9511, 2.9723, 2.9467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.09974732995033264 
model_pd.l_d.mean(): -24.31393051147461 
model_pd.lagr.mean(): -24.214183807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1281], device='cuda:0')), ('power', tensor([-24.1858], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.09974732995033264
epoch£º116	 i:1 	 global-step:2321	 l-p:0.11666341871023178
epoch£º116	 i:2 	 global-step:2322	 l-p:0.12404773384332657
epoch£º116	 i:3 	 global-step:2323	 l-p:0.1400410681962967
epoch£º116	 i:4 	 global-step:2324	 l-p:0.13321514427661896
epoch£º116	 i:5 	 global-step:2325	 l-p:0.13169053196907043
epoch£º116	 i:6 	 global-step:2326	 l-p:0.13045455515384674
epoch£º116	 i:7 	 global-step:2327	 l-p:0.12495312094688416
epoch£º116	 i:8 	 global-step:2328	 l-p:0.1662786900997162
epoch£º116	 i:9 	 global-step:2329	 l-p:0.08886021375656128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7450, 2.7447, 2.7450],
        [2.7450, 2.7378, 2.7386],
        [2.7450, 2.7450, 2.7450],
        [2.7450, 2.7450, 2.7450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.10842125862836838 
model_pd.l_d.mean(): -24.896278381347656 
model_pd.lagr.mean(): -24.787857055664062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1346], device='cuda:0')), ('power', tensor([-24.7617], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.10842125862836838
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13064120709896088
epoch£º117	 i:2 	 global-step:2342	 l-p:0.1309620589017868
epoch£º117	 i:3 	 global-step:2343	 l-p:0.2077333778142929
epoch£º117	 i:4 	 global-step:2344	 l-p:0.13326334953308105
epoch£º117	 i:5 	 global-step:2345	 l-p:0.34216824173927307
epoch£º117	 i:6 	 global-step:2346	 l-p:0.144057497382164
epoch£º117	 i:7 	 global-step:2347	 l-p:-3.3389220237731934
epoch£º117	 i:8 	 global-step:2348	 l-p:0.1268601417541504
epoch£º117	 i:9 	 global-step:2349	 l-p:0.11649370193481445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0481, 3.5117, 3.7479],
        [3.0481, 3.1071, 3.0714],
        [3.0481, 3.0463, 3.0471],
        [3.0481, 3.0481, 3.0481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.12673130631446838 
model_pd.l_d.mean(): -24.2885684967041 
model_pd.lagr.mean(): -24.161836624145508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1645], device='cuda:0')), ('power', tensor([-24.1241], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.12673130631446838
epoch£º118	 i:1 	 global-step:2361	 l-p:0.11485843360424042
epoch£º118	 i:2 	 global-step:2362	 l-p:0.11741328239440918
epoch£º118	 i:3 	 global-step:2363	 l-p:0.12565526366233826
epoch£º118	 i:4 	 global-step:2364	 l-p:0.1778719276189804
epoch£º118	 i:5 	 global-step:2365	 l-p:0.12139654159545898
epoch£º118	 i:6 	 global-step:2366	 l-p:0.43886107206344604
epoch£º118	 i:7 	 global-step:2367	 l-p:0.12069802731275558
epoch£º118	 i:8 	 global-step:2368	 l-p:0.1257646679878235
epoch£º118	 i:9 	 global-step:2369	 l-p:0.12217998504638672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0243, 3.0447, 3.0188],
        [3.0243, 3.0230, 3.0240],
        [3.0243, 3.1598, 3.1419],
        [3.0243, 3.2170, 3.2300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.1237393170595169 
model_pd.l_d.mean(): -24.785253524780273 
model_pd.lagr.mean(): -24.661514282226562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2419], device='cuda:0')), ('power', tensor([-24.5433], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.1237393170595169
epoch£º119	 i:1 	 global-step:2381	 l-p:0.14713597297668457
epoch£º119	 i:2 	 global-step:2382	 l-p:0.11866646260023117
epoch£º119	 i:3 	 global-step:2383	 l-p:-0.14038421213626862
epoch£º119	 i:4 	 global-step:2384	 l-p:0.15696941316127777
epoch£º119	 i:5 	 global-step:2385	 l-p:0.16813762485980988
epoch£º119	 i:6 	 global-step:2386	 l-p:0.11496739834547043
epoch£º119	 i:7 	 global-step:2387	 l-p:0.1289423406124115
epoch£º119	 i:8 	 global-step:2388	 l-p:0.12209831178188324
epoch£º119	 i:9 	 global-step:2389	 l-p:0.11672881990671158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8973, 2.8973, 2.8973],
        [2.8973, 2.8972, 2.8973],
        [2.8973, 2.8970, 2.8973],
        [2.8973, 2.9628, 2.9256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.11292287707328796 
model_pd.l_d.mean(): -24.60563087463379 
model_pd.lagr.mean(): -24.492708206176758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1372], device='cuda:0')), ('power', tensor([-24.4685], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.11292287707328796
epoch£º120	 i:1 	 global-step:2401	 l-p:0.16484865546226501
epoch£º120	 i:2 	 global-step:2402	 l-p:0.1628587692975998
epoch£º120	 i:3 	 global-step:2403	 l-p:0.11803033202886581
epoch£º120	 i:4 	 global-step:2404	 l-p:0.11603853106498718
epoch£º120	 i:5 	 global-step:2405	 l-p:0.1092166006565094
epoch£º120	 i:6 	 global-step:2406	 l-p:0.04542278125882149
epoch£º120	 i:7 	 global-step:2407	 l-p:0.26373058557510376
epoch£º120	 i:8 	 global-step:2408	 l-p:0.11908292770385742
epoch£º120	 i:9 	 global-step:2409	 l-p:0.1234048530459404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1245, 3.1486, 3.1213],
        [3.1245, 3.1224, 3.1229],
        [3.1245, 3.1245, 3.1245],
        [3.1245, 3.1244, 3.1245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.11577286571264267 
model_pd.l_d.mean(): -24.22751235961914 
model_pd.lagr.mean(): -24.111740112304688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2558], device='cuda:0')), ('power', tensor([-23.9717], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.11577286571264267
epoch£º121	 i:1 	 global-step:2421	 l-p:0.1310129165649414
epoch£º121	 i:2 	 global-step:2422	 l-p:0.08616260439157486
epoch£º121	 i:3 	 global-step:2423	 l-p:-0.004166986793279648
epoch£º121	 i:4 	 global-step:2424	 l-p:0.1296319216489792
epoch£º121	 i:5 	 global-step:2425	 l-p:0.1367175281047821
epoch£º121	 i:6 	 global-step:2426	 l-p:0.12693816423416138
epoch£º121	 i:7 	 global-step:2427	 l-p:0.1151931956410408
epoch£º121	 i:8 	 global-step:2428	 l-p:0.12746410071849823
epoch£º121	 i:9 	 global-step:2429	 l-p:0.07643244415521622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7625, 2.7560, 2.7599],
        [2.7625, 2.7595, 2.7619],
        [2.7625, 2.7625, 2.7625],
        [2.7625, 2.7603, 2.7622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.11627782881259918 
model_pd.l_d.mean(): -23.792261123657227 
model_pd.lagr.mean(): -23.675983428955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0052], device='cuda:0')), ('power', tensor([-23.7870], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.11627782881259918
epoch£º122	 i:1 	 global-step:2441	 l-p:0.15520834922790527
epoch£º122	 i:2 	 global-step:2442	 l-p:0.09125662595033646
epoch£º122	 i:3 	 global-step:2443	 l-p:0.12461366504430771
epoch£º122	 i:4 	 global-step:2444	 l-p:0.12987986207008362
epoch£º122	 i:5 	 global-step:2445	 l-p:1.1098790168762207
epoch£º122	 i:6 	 global-step:2446	 l-p:0.12796007096767426
epoch£º122	 i:7 	 global-step:2447	 l-p:0.12411878257989883
epoch£º122	 i:8 	 global-step:2448	 l-p:0.19444066286087036
epoch£º122	 i:9 	 global-step:2449	 l-p:0.15337707102298737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8563, 2.8561, 2.8563],
        [2.8563, 2.8478, 2.8494],
        [2.8563, 2.8544, 2.8560],
        [2.8563, 2.8563, 2.8563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.14530083537101746 
model_pd.l_d.mean(): -24.714012145996094 
model_pd.lagr.mean(): -24.56871223449707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1372], device='cuda:0')), ('power', tensor([-24.5768], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.14530083537101746
epoch£º123	 i:1 	 global-step:2461	 l-p:0.15740233659744263
epoch£º123	 i:2 	 global-step:2462	 l-p:0.13091838359832764
epoch£º123	 i:3 	 global-step:2463	 l-p:-0.08577462285757065
epoch£º123	 i:4 	 global-step:2464	 l-p:0.11664222180843353
epoch£º123	 i:5 	 global-step:2465	 l-p:0.12509585916996002
epoch£º123	 i:6 	 global-step:2466	 l-p:0.11934489011764526
epoch£º123	 i:7 	 global-step:2467	 l-p:0.03387317433953285
epoch£º123	 i:8 	 global-step:2468	 l-p:0.12828350067138672
epoch£º123	 i:9 	 global-step:2469	 l-p:0.17971491813659668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1666, 3.1665, 3.1666],
        [3.1666, 3.4128, 3.4529],
        [3.1666, 3.1638, 3.1639],
        [3.1666, 3.1644, 3.1622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.12296859174966812 
model_pd.l_d.mean(): -24.464012145996094 
model_pd.lagr.mean(): -24.34104347229004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2600], device='cuda:0')), ('power', tensor([-24.2040], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.12296859174966812
epoch£º124	 i:1 	 global-step:2481	 l-p:0.12717792391777039
epoch£º124	 i:2 	 global-step:2482	 l-p:0.11307328194379807
epoch£º124	 i:3 	 global-step:2483	 l-p:0.1219966933131218
epoch£º124	 i:4 	 global-step:2484	 l-p:-1.3612301349639893
epoch£º124	 i:5 	 global-step:2485	 l-p:0.12474764883518219
epoch£º124	 i:6 	 global-step:2486	 l-p:0.12432359158992767
epoch£º124	 i:7 	 global-step:2487	 l-p:0.04684397950768471
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11672757565975189
epoch£º124	 i:9 	 global-step:2489	 l-p:0.07942914962768555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9653, 2.9620, 2.9644],
        [2.9653, 3.4043, 3.6254],
        [2.9653, 2.9613, 2.9641],
        [2.9653, 3.0271, 2.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.11800563335418701 
model_pd.l_d.mean(): -24.017629623413086 
model_pd.lagr.mean(): -23.89962387084961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1348], device='cuda:0')), ('power', tensor([-23.8829], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.11800563335418701
epoch£º125	 i:1 	 global-step:2501	 l-p:0.12529917061328888
epoch£º125	 i:2 	 global-step:2502	 l-p:0.1637994349002838
epoch£º125	 i:3 	 global-step:2503	 l-p:0.13820531964302063
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11033329367637634
epoch£º125	 i:5 	 global-step:2505	 l-p:0.13017691671848297
epoch£º125	 i:6 	 global-step:2506	 l-p:0.10827747732400894
epoch£º125	 i:7 	 global-step:2507	 l-p:0.10005594044923782
epoch£º125	 i:8 	 global-step:2508	 l-p:0.15223965048789978
epoch£º125	 i:9 	 global-step:2509	 l-p:0.12430011481046677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0188, 3.2083, 3.2190],
        [3.0188, 3.1031, 3.0650],
        [3.0188, 3.0188, 3.0188],
        [3.0188, 3.0171, 3.0185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.11494922637939453 
model_pd.l_d.mean(): -24.768108367919922 
model_pd.lagr.mean(): -24.653160095214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2455], device='cuda:0')), ('power', tensor([-24.5226], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.11494922637939453
epoch£º126	 i:1 	 global-step:2521	 l-p:0.12739773094654083
epoch£º126	 i:2 	 global-step:2522	 l-p:0.08743611723184586
epoch£º126	 i:3 	 global-step:2523	 l-p:0.16880615055561066
epoch£º126	 i:4 	 global-step:2524	 l-p:0.13826993107795715
epoch£º126	 i:5 	 global-step:2525	 l-p:0.0792098194360733
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11635906994342804
epoch£º126	 i:7 	 global-step:2527	 l-p:0.11614975333213806
epoch£º126	 i:8 	 global-step:2528	 l-p:0.14944440126419067
epoch£º126	 i:9 	 global-step:2529	 l-p:0.09685595333576202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[3.0189, 3.0143, 3.0027],
        [3.0189, 3.0115, 3.0075],
        [3.0189, 3.0273, 2.9995],
        [3.0189, 3.2574, 3.3015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.1230824813246727 
model_pd.l_d.mean(): -24.323833465576172 
model_pd.lagr.mean(): -24.20075035095215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1781], device='cuda:0')), ('power', tensor([-24.1458], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.1230824813246727
epoch£º127	 i:1 	 global-step:2541	 l-p:0.13863855600357056
epoch£º127	 i:2 	 global-step:2542	 l-p:0.10656873136758804
epoch£º127	 i:3 	 global-step:2543	 l-p:0.13389335572719574
epoch£º127	 i:4 	 global-step:2544	 l-p:0.13432149589061737
epoch£º127	 i:5 	 global-step:2545	 l-p:0.1493118703365326
epoch£º127	 i:6 	 global-step:2546	 l-p:0.13765227794647217
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11512209475040436
epoch£º127	 i:8 	 global-step:2548	 l-p:0.1374971717596054
epoch£º127	 i:9 	 global-step:2549	 l-p:0.139518141746521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7687, 2.8025, 2.7544],
        [2.7687, 2.7524, 2.7351],
        [2.7687, 2.7675, 2.7686],
        [2.7687, 2.7684, 2.7687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.1112368106842041 
model_pd.l_d.mean(): -24.326433181762695 
model_pd.lagr.mean(): -24.21519660949707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0246], device='cuda:0')), ('power', tensor([-24.3018], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.1112368106842041
epoch£º128	 i:1 	 global-step:2561	 l-p:0.11703658849000931
epoch£º128	 i:2 	 global-step:2562	 l-p:0.13439981639385223
epoch£º128	 i:3 	 global-step:2563	 l-p:0.12552428245544434
epoch£º128	 i:4 	 global-step:2564	 l-p:0.13025222718715668
epoch£º128	 i:5 	 global-step:2565	 l-p:0.11422309279441833
epoch£º128	 i:6 	 global-step:2566	 l-p:0.20229710638523102
epoch£º128	 i:7 	 global-step:2567	 l-p:0.01909163035452366
epoch£º128	 i:8 	 global-step:2568	 l-p:0.14722207188606262
epoch£º128	 i:9 	 global-step:2569	 l-p:0.134673610329628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9320, 2.9316, 2.9320],
        [2.9320, 2.9262, 2.9302],
        [2.9320, 2.9320, 2.9320],
        [2.9320, 2.9208, 2.9084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.13220874965190887 
model_pd.l_d.mean(): -24.427471160888672 
model_pd.lagr.mean(): -24.295263290405273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1057], device='cuda:0')), ('power', tensor([-24.3218], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.13220874965190887
epoch£º129	 i:1 	 global-step:2581	 l-p:0.21000507473945618
epoch£º129	 i:2 	 global-step:2582	 l-p:0.12043483555316925
epoch£º129	 i:3 	 global-step:2583	 l-p:0.13650672137737274
epoch£º129	 i:4 	 global-step:2584	 l-p:0.037497639656066895
epoch£º129	 i:5 	 global-step:2585	 l-p:0.11718900501728058
epoch£º129	 i:6 	 global-step:2586	 l-p:0.1204495057463646
epoch£º129	 i:7 	 global-step:2587	 l-p:0.12391936033964157
epoch£º129	 i:8 	 global-step:2588	 l-p:0.11671721935272217
epoch£º129	 i:9 	 global-step:2589	 l-p:0.14067690074443817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8967, 2.8823, 2.8696],
        [2.8967, 2.8967, 2.8967],
        [2.8967, 2.8967, 2.8967],
        [2.8967, 2.8962, 2.8967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.14597532153129578 
model_pd.l_d.mean(): -23.87898826599121 
model_pd.lagr.mean(): -23.733013153076172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0714], device='cuda:0')), ('power', tensor([-23.8076], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.14597532153129578
epoch£º130	 i:1 	 global-step:2601	 l-p:0.12507767975330353
epoch£º130	 i:2 	 global-step:2602	 l-p:0.12067539244890213
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13631796836853027
epoch£º130	 i:4 	 global-step:2604	 l-p:0.011623906902968884
epoch£º130	 i:5 	 global-step:2605	 l-p:-0.10244985669851303
epoch£º130	 i:6 	 global-step:2606	 l-p:0.10272510349750519
epoch£º130	 i:7 	 global-step:2607	 l-p:0.14079847931861877
epoch£º130	 i:8 	 global-step:2608	 l-p:0.21298757195472717
epoch£º130	 i:9 	 global-step:2609	 l-p:0.14701710641384125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7716, 2.7716, 2.7716],
        [2.7716, 2.7515, 2.7548],
        [2.7716, 2.7715, 2.7716],
        [2.7716, 2.7716, 2.7716]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.1406187266111374 
model_pd.l_d.mean(): -24.34691047668457 
model_pd.lagr.mean(): -24.20629119873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0347], device='cuda:0')), ('power', tensor([-24.3123], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.1406187266111374
epoch£º131	 i:1 	 global-step:2621	 l-p:0.1285475343465805
epoch£º131	 i:2 	 global-step:2622	 l-p:0.1332838535308838
epoch£º131	 i:3 	 global-step:2623	 l-p:0.11401508003473282
epoch£º131	 i:4 	 global-step:2624	 l-p:0.1719406545162201
epoch£º131	 i:5 	 global-step:2625	 l-p:0.12399947643280029
epoch£º131	 i:6 	 global-step:2626	 l-p:0.2006487399339676
epoch£º131	 i:7 	 global-step:2627	 l-p:0.11254295706748962
epoch£º131	 i:8 	 global-step:2628	 l-p:0.14454297721385956
epoch£º131	 i:9 	 global-step:2629	 l-p:0.12631915509700775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0482, 3.0481, 3.0482],
        [3.0482, 3.3943, 3.5209],
        [3.0482, 3.0482, 3.0482],
        [3.0482, 3.1071, 3.0595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.12734240293502808 
model_pd.l_d.mean(): -24.032527923583984 
model_pd.lagr.mean(): -23.90518569946289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1330], device='cuda:0')), ('power', tensor([-23.8996], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.12734240293502808
epoch£º132	 i:1 	 global-step:2641	 l-p:0.12961360812187195
epoch£º132	 i:2 	 global-step:2642	 l-p:0.1847628355026245
epoch£º132	 i:3 	 global-step:2643	 l-p:0.1154453456401825
epoch£º132	 i:4 	 global-step:2644	 l-p:0.13180549442768097
epoch£º132	 i:5 	 global-step:2645	 l-p:0.15465527772903442
epoch£º132	 i:6 	 global-step:2646	 l-p:0.11066219210624695
epoch£º132	 i:7 	 global-step:2647	 l-p:0.10844186693429947
epoch£º132	 i:8 	 global-step:2648	 l-p:0.18434275686740875
epoch£º132	 i:9 	 global-step:2649	 l-p:0.11493401229381561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1472, 3.1472, 3.1472],
        [3.1472, 3.1472, 3.1472],
        [3.1472, 3.1780, 3.1334],
        [3.1472, 3.1384, 3.1286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.11420553177595139 
model_pd.l_d.mean(): -23.798885345458984 
model_pd.lagr.mean(): -23.68467903137207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1765], device='cuda:0')), ('power', tensor([-23.6224], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.11420553177595139
epoch£º133	 i:1 	 global-step:2661	 l-p:0.11099360883235931
epoch£º133	 i:2 	 global-step:2662	 l-p:0.1271091252565384
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13524065911769867
epoch£º133	 i:4 	 global-step:2664	 l-p:0.10919985175132751
epoch£º133	 i:5 	 global-step:2665	 l-p:0.11402830481529236
epoch£º133	 i:6 	 global-step:2666	 l-p:0.10358016937971115
epoch£º133	 i:7 	 global-step:2667	 l-p:0.469082772731781
epoch£º133	 i:8 	 global-step:2668	 l-p:0.1397995948791504
epoch£º133	 i:9 	 global-step:2669	 l-p:0.0965406596660614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7570, 2.7393, 2.6963],
        [2.7570, 2.7324, 2.7367],
        [2.7570, 2.7489, 2.7549],
        [2.7570, 2.7554, 2.7568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13669142127037048 
model_pd.l_d.mean(): -24.67084503173828 
model_pd.lagr.mean(): -24.53415298461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0823], device='cuda:0')), ('power', tensor([-24.5886], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13669142127037048
epoch£º134	 i:1 	 global-step:2681	 l-p:0.1849828064441681
epoch£º134	 i:2 	 global-step:2682	 l-p:0.1163971871137619
epoch£º134	 i:3 	 global-step:2683	 l-p:0.09809651225805283
epoch£º134	 i:4 	 global-step:2684	 l-p:0.13978494703769684
epoch£º134	 i:5 	 global-step:2685	 l-p:0.12608395516872406
epoch£º134	 i:6 	 global-step:2686	 l-p:0.018945841118693352
epoch£º134	 i:7 	 global-step:2687	 l-p:0.15830464661121368
epoch£º134	 i:8 	 global-step:2688	 l-p:0.1419324427843094
epoch£º134	 i:9 	 global-step:2689	 l-p:0.10645551234483719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9671, 2.9671, 2.9671],
        [2.9671, 3.2871, 3.3973],
        [2.9671, 2.9476, 2.9381],
        [2.9671, 3.1994, 3.2402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.1365683525800705 
model_pd.l_d.mean(): -23.866931915283203 
model_pd.lagr.mean(): -23.730363845825195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0626], device='cuda:0')), ('power', tensor([-23.8043], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.1365683525800705
epoch£º135	 i:1 	 global-step:2701	 l-p:0.13411499559879303
epoch£º135	 i:2 	 global-step:2702	 l-p:0.12851570546627045
epoch£º135	 i:3 	 global-step:2703	 l-p:0.1267472356557846
epoch£º135	 i:4 	 global-step:2704	 l-p:0.09810812771320343
epoch£º135	 i:5 	 global-step:2705	 l-p:0.14712193608283997
epoch£º135	 i:6 	 global-step:2706	 l-p:0.21291160583496094
epoch£º135	 i:7 	 global-step:2707	 l-p:0.06599622219800949
epoch£º135	 i:8 	 global-step:2708	 l-p:0.07400738447904587
epoch£º135	 i:9 	 global-step:2709	 l-p:0.12319465726613998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9548, 2.9548, 2.9548],
        [2.9548, 2.9334, 2.9195],
        [2.9548, 2.9544, 2.9548],
        [2.9548, 2.9354, 2.9140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.12466177344322205 
model_pd.l_d.mean(): -24.944223403930664 
model_pd.lagr.mean(): -24.819561004638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2371], device='cuda:0')), ('power', tensor([-24.7071], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.12466177344322205
epoch£º136	 i:1 	 global-step:2721	 l-p:0.1267216056585312
epoch£º136	 i:2 	 global-step:2722	 l-p:0.17913344502449036
epoch£º136	 i:3 	 global-step:2723	 l-p:0.10663461685180664
epoch£º136	 i:4 	 global-step:2724	 l-p:0.13789618015289307
epoch£º136	 i:5 	 global-step:2725	 l-p:0.1406993865966797
epoch£º136	 i:6 	 global-step:2726	 l-p:0.1410456746816635
epoch£º136	 i:7 	 global-step:2727	 l-p:0.10409053415060043
epoch£º136	 i:8 	 global-step:2728	 l-p:0.12661680579185486
epoch£º136	 i:9 	 global-step:2729	 l-p:0.042878858745098114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9009, 2.9009, 2.9009],
        [2.9009, 2.8925, 2.8986],
        [2.9009, 2.9009, 2.9009],
        [2.9009, 3.1429, 3.1948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.1332898736000061 
model_pd.l_d.mean(): -24.803421020507812 
model_pd.lagr.mean(): -24.67013168334961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1706], device='cuda:0')), ('power', tensor([-24.6328], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.1332898736000061
epoch£º137	 i:1 	 global-step:2741	 l-p:0.1241370141506195
epoch£º137	 i:2 	 global-step:2742	 l-p:0.1779380440711975
epoch£º137	 i:3 	 global-step:2743	 l-p:0.1654837727546692
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11204449087381363
epoch£º137	 i:5 	 global-step:2745	 l-p:0.05220349133014679
epoch£º137	 i:6 	 global-step:2746	 l-p:0.10844682157039642
epoch£º137	 i:7 	 global-step:2747	 l-p:0.14714835584163666
epoch£º137	 i:8 	 global-step:2748	 l-p:0.10012371838092804
epoch£º137	 i:9 	 global-step:2749	 l-p:0.08469411730766296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0889, 3.0717, 3.0731],
        [3.0889, 3.0889, 3.0889],
        [3.0889, 3.0889, 3.0889],
        [3.0889, 3.0889, 3.0889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): -0.01181200984865427 
model_pd.l_d.mean(): -24.75971221923828 
model_pd.lagr.mean(): -24.77152442932129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2610], device='cuda:0')), ('power', tensor([-24.4987], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:-0.01181200984865427
epoch£º138	 i:1 	 global-step:2761	 l-p:0.398381769657135
epoch£º138	 i:2 	 global-step:2762	 l-p:0.1141752302646637
epoch£º138	 i:3 	 global-step:2763	 l-p:0.12365501374006271
epoch£º138	 i:4 	 global-step:2764	 l-p:0.11510414630174637
epoch£º138	 i:5 	 global-step:2765	 l-p:0.1099492534995079
epoch£º138	 i:6 	 global-step:2766	 l-p:0.11364366859197617
epoch£º138	 i:7 	 global-step:2767	 l-p:0.11758099496364594
epoch£º138	 i:8 	 global-step:2768	 l-p:0.12889216840267181
epoch£º138	 i:9 	 global-step:2769	 l-p:0.12218980491161346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9849, 3.2461, 3.3064],
        [2.9849, 3.3370, 3.4715],
        [2.9849, 2.9849, 2.9849],
        [2.9849, 2.9834, 2.9847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.1515519618988037 
model_pd.l_d.mean(): -23.87333106994629 
model_pd.lagr.mean(): -23.721778869628906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0957], device='cuda:0')), ('power', tensor([-23.7776], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.1515519618988037
epoch£º139	 i:1 	 global-step:2781	 l-p:0.12693963944911957
epoch£º139	 i:2 	 global-step:2782	 l-p:0.11948195844888687
epoch£º139	 i:3 	 global-step:2783	 l-p:0.0740848258137703
epoch£º139	 i:4 	 global-step:2784	 l-p:0.23169967532157898
epoch£º139	 i:5 	 global-step:2785	 l-p:0.08113289624452591
epoch£º139	 i:6 	 global-step:2786	 l-p:0.17374937236309052
epoch£º139	 i:7 	 global-step:2787	 l-p:0.09387660771608353
epoch£º139	 i:8 	 global-step:2788	 l-p:0.1317969709634781
epoch£º139	 i:9 	 global-step:2789	 l-p:-0.08160347491502762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8026, 3.0311, 3.0793],
        [2.8026, 2.7847, 2.7944],
        [2.8026, 2.8600, 2.8040],
        [2.8026, 3.0395, 3.0941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.13965439796447754 
model_pd.l_d.mean(): -24.535175323486328 
model_pd.lagr.mean(): -24.39552116394043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0579], device='cuda:0')), ('power', tensor([-24.4773], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.13965439796447754
epoch£º140	 i:1 	 global-step:2801	 l-p:0.10269025713205338
epoch£º140	 i:2 	 global-step:2802	 l-p:-1.7234740257263184
epoch£º140	 i:3 	 global-step:2803	 l-p:0.1365264505147934
epoch£º140	 i:4 	 global-step:2804	 l-p:0.12564630806446075
epoch£º140	 i:5 	 global-step:2805	 l-p:0.13056530058383942
epoch£º140	 i:6 	 global-step:2806	 l-p:0.14311188459396362
epoch£º140	 i:7 	 global-step:2807	 l-p:0.1147516667842865
epoch£º140	 i:8 	 global-step:2808	 l-p:-0.8214741349220276
epoch£º140	 i:9 	 global-step:2809	 l-p:0.11061783879995346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9778, 2.9777, 2.9778],
        [2.9778, 2.9778, 2.9778],
        [2.9778, 2.9778, 2.9778],
        [2.9778, 2.9511, 2.9417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.09789885580539703 
model_pd.l_d.mean(): -24.547954559326172 
model_pd.lagr.mean(): -24.450056076049805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1611], device='cuda:0')), ('power', tensor([-24.3868], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.09789885580539703
epoch£º141	 i:1 	 global-step:2821	 l-p:0.12355893850326538
epoch£º141	 i:2 	 global-step:2822	 l-p:0.1261839121580124
epoch£º141	 i:3 	 global-step:2823	 l-p:0.13092166185379028
epoch£º141	 i:4 	 global-step:2824	 l-p:0.13217227160930634
epoch£º141	 i:5 	 global-step:2825	 l-p:0.10607898235321045
epoch£º141	 i:6 	 global-step:2826	 l-p:-0.14517824351787567
epoch£º141	 i:7 	 global-step:2827	 l-p:0.14390312135219574
epoch£º141	 i:8 	 global-step:2828	 l-p:0.23096589744091034
epoch£º141	 i:9 	 global-step:2829	 l-p:0.11846528947353363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8578, 2.8246, 2.8233],
        [2.8578, 2.8222, 2.8004],
        [2.8578, 2.8219, 2.8053],
        [2.8578, 2.8574, 2.8578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.13724081218242645 
model_pd.l_d.mean(): -24.60365104675293 
model_pd.lagr.mean(): -24.46640968322754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1082], device='cuda:0')), ('power', tensor([-24.4955], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.13724081218242645
epoch£º142	 i:1 	 global-step:2841	 l-p:0.12977707386016846
epoch£º142	 i:2 	 global-step:2842	 l-p:0.18046140670776367
epoch£º142	 i:3 	 global-step:2843	 l-p:0.12287528812885284
epoch£º142	 i:4 	 global-step:2844	 l-p:0.11743539571762085
epoch£º142	 i:5 	 global-step:2845	 l-p:0.06498630344867706
epoch£º142	 i:6 	 global-step:2846	 l-p:0.1727389097213745
epoch£º142	 i:7 	 global-step:2847	 l-p:-0.06879780441522598
epoch£º142	 i:8 	 global-step:2848	 l-p:0.09199199825525284
epoch£º142	 i:9 	 global-step:2849	 l-p:0.12754714488983154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1062, 3.1062, 3.1062],
        [3.1062, 3.1062, 3.1062],
        [3.1062, 3.1062, 3.1062],
        [3.1062, 3.2646, 3.2467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.06870982050895691 
model_pd.l_d.mean(): -23.976823806762695 
model_pd.lagr.mean(): -23.908113479614258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1625], device='cuda:0')), ('power', tensor([-23.8143], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.06870982050895691
epoch£º143	 i:1 	 global-step:2861	 l-p:0.12511181831359863
epoch£º143	 i:2 	 global-step:2862	 l-p:0.12906424701213837
epoch£º143	 i:3 	 global-step:2863	 l-p:0.11805658787488937
epoch£º143	 i:4 	 global-step:2864	 l-p:0.12592662870883942
epoch£º143	 i:5 	 global-step:2865	 l-p:0.01469430886209011
epoch£º143	 i:6 	 global-step:2866	 l-p:0.13402268290519714
epoch£º143	 i:7 	 global-step:2867	 l-p:0.12221477180719376
epoch£º143	 i:8 	 global-step:2868	 l-p:0.11458728462457657
epoch£º143	 i:9 	 global-step:2869	 l-p:0.1284405142068863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8741, 3.1968, 3.3127],
        [2.8741, 2.9258, 2.8634],
        [2.8741, 2.8393, 2.8394],
        [2.8741, 2.8695, 2.8734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.15889476239681244 
model_pd.l_d.mean(): -24.286352157592773 
model_pd.lagr.mean(): -24.127456665039062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1166], device='cuda:0')), ('power', tensor([-24.1697], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.15889476239681244
epoch£º144	 i:1 	 global-step:2881	 l-p:0.11403106153011322
epoch£º144	 i:2 	 global-step:2882	 l-p:0.1300911158323288
epoch£º144	 i:3 	 global-step:2883	 l-p:0.12851546704769135
epoch£º144	 i:4 	 global-step:2884	 l-p:0.013509501703083515
epoch£º144	 i:5 	 global-step:2885	 l-p:0.2002955824136734
epoch£º144	 i:6 	 global-step:2886	 l-p:0.13622473180294037
epoch£º144	 i:7 	 global-step:2887	 l-p:0.0874686911702156
epoch£º144	 i:8 	 global-step:2888	 l-p:0.11691518872976303
epoch£º144	 i:9 	 global-step:2889	 l-p:0.13093623518943787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9353, 3.0368, 2.9928],
        [2.9353, 3.0744, 3.0507],
        [2.9353, 2.9290, 2.9341],
        [2.9353, 3.1930, 3.2514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.13624899089336395 
model_pd.l_d.mean(): -24.72509002685547 
model_pd.lagr.mean(): -24.58884048461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1603], device='cuda:0')), ('power', tensor([-24.5648], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.13624899089336395
epoch£º145	 i:1 	 global-step:2901	 l-p:0.12814198434352875
epoch£º145	 i:2 	 global-step:2902	 l-p:0.13297809660434723
epoch£º145	 i:3 	 global-step:2903	 l-p:0.11121920496225357
epoch£º145	 i:4 	 global-step:2904	 l-p:0.12379410117864609
epoch£º145	 i:5 	 global-step:2905	 l-p:0.04648234695196152
epoch£º145	 i:6 	 global-step:2906	 l-p:0.39355430006980896
epoch£º145	 i:7 	 global-step:2907	 l-p:0.11711284518241882
epoch£º145	 i:8 	 global-step:2908	 l-p:0.1320810168981552
epoch£º145	 i:9 	 global-step:2909	 l-p:0.12134047597646713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0596, 3.0596, 3.0596],
        [3.0596, 3.0428, 3.0518],
        [3.0596, 3.0418, 2.9955],
        [3.0596, 3.0352, 3.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.11519675701856613 
model_pd.l_d.mean(): -24.30136489868164 
model_pd.lagr.mean(): -24.186168670654297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1770], device='cuda:0')), ('power', tensor([-24.1244], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.11519675701856613
epoch£º146	 i:1 	 global-step:2921	 l-p:0.08493812382221222
epoch£º146	 i:2 	 global-step:2922	 l-p:0.11131606996059418
epoch£º146	 i:3 	 global-step:2923	 l-p:0.16813185811042786
epoch£º146	 i:4 	 global-step:2924	 l-p:0.1316174566745758
epoch£º146	 i:5 	 global-step:2925	 l-p:0.1132761538028717
epoch£º146	 i:6 	 global-step:2926	 l-p:0.12466464936733246
epoch£º146	 i:7 	 global-step:2927	 l-p:0.14156530797481537
epoch£º146	 i:8 	 global-step:2928	 l-p:0.4556712210178375
epoch£º146	 i:9 	 global-step:2929	 l-p:0.1010289192199707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9603, 2.9218, 2.8967],
        [2.9603, 2.9602, 2.9603],
        [2.9603, 2.9358, 2.9459],
        [2.9603, 2.9301, 2.8822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.1367398500442505 
model_pd.l_d.mean(): -24.854909896850586 
model_pd.lagr.mean(): -24.718170166015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2111], device='cuda:0')), ('power', tensor([-24.6438], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.1367398500442505
epoch£º147	 i:1 	 global-step:2941	 l-p:0.052897971123456955
epoch£º147	 i:2 	 global-step:2942	 l-p:0.16130931675434113
epoch£º147	 i:3 	 global-step:2943	 l-p:0.1146664246916771
epoch£º147	 i:4 	 global-step:2944	 l-p:0.12429038435220718
epoch£º147	 i:5 	 global-step:2945	 l-p:0.15306268632411957
epoch£º147	 i:6 	 global-step:2946	 l-p:0.0924084335565567
epoch£º147	 i:7 	 global-step:2947	 l-p:0.140655517578125
epoch£º147	 i:8 	 global-step:2948	 l-p:0.11565278470516205
epoch£º147	 i:9 	 global-step:2949	 l-p:0.11259318143129349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8630, 2.8477, 2.8581],
        [2.8630, 2.8549, 2.8615],
        [2.8630, 3.0952, 3.1373],
        [2.8630, 2.8630, 2.8630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12936395406723022 
model_pd.l_d.mean(): -24.764013290405273 
model_pd.lagr.mean(): -24.6346492767334 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1402], device='cuda:0')), ('power', tensor([-24.6238], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12936395406723022
epoch£º148	 i:1 	 global-step:2961	 l-p:0.15998254716396332
epoch£º148	 i:2 	 global-step:2962	 l-p:0.29256296157836914
epoch£º148	 i:3 	 global-step:2963	 l-p:0.05643639340996742
epoch£º148	 i:4 	 global-step:2964	 l-p:0.1479855626821518
epoch£º148	 i:5 	 global-step:2965	 l-p:0.10681412369012833
epoch£º148	 i:6 	 global-step:2966	 l-p:0.12292172759771347
epoch£º148	 i:7 	 global-step:2967	 l-p:0.1209302693605423
epoch£º148	 i:8 	 global-step:2968	 l-p:0.15872351825237274
epoch£º148	 i:9 	 global-step:2969	 l-p:0.08178157359361649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8968, 2.8627, 2.8722],
        [2.8968, 2.8757, 2.8877],
        [2.8968, 3.1283, 3.1669],
        [2.8968, 2.8971, 2.8197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.10940954089164734 
model_pd.l_d.mean(): -23.90270233154297 
model_pd.lagr.mean(): -23.793292999267578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0196], device='cuda:0')), ('power', tensor([-23.8831], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.10940954089164734
epoch£º149	 i:1 	 global-step:2981	 l-p:0.14252357184886932
epoch£º149	 i:2 	 global-step:2982	 l-p:0.12968693673610687
epoch£º149	 i:3 	 global-step:2983	 l-p:0.12230241298675537
epoch£º149	 i:4 	 global-step:2984	 l-p:0.13886316120624542
epoch£º149	 i:5 	 global-step:2985	 l-p:0.12895242869853973
epoch£º149	 i:6 	 global-step:2986	 l-p:0.10896720737218857
epoch£º149	 i:7 	 global-step:2987	 l-p:0.09188346564769745
epoch£º149	 i:8 	 global-step:2988	 l-p:-0.03302506357431412
epoch£º149	 i:9 	 global-step:2989	 l-p:0.12027756124734879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8317, 2.7828, 2.7226],
        [2.8317, 2.8071, 2.7270],
        [2.8317, 2.7820, 2.7232],
        [2.8317, 2.8295, 2.8316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.12249040603637695 
model_pd.l_d.mean(): -23.881912231445312 
model_pd.lagr.mean(): -23.759422302246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0184], device='cuda:0')), ('power', tensor([-23.8635], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.12249040603637695
epoch£º150	 i:1 	 global-step:3001	 l-p:0.1370074301958084
epoch£º150	 i:2 	 global-step:3002	 l-p:0.12818774580955505
epoch£º150	 i:3 	 global-step:3003	 l-p:0.08767984062433243
epoch£º150	 i:4 	 global-step:3004	 l-p:0.1271640807390213
epoch£º150	 i:5 	 global-step:3005	 l-p:0.11934028565883636
epoch£º150	 i:6 	 global-step:3006	 l-p:0.15325620770454407
epoch£º150	 i:7 	 global-step:3007	 l-p:0.11819653958082199
epoch£º150	 i:8 	 global-step:3008	 l-p:0.1317756623029709
epoch£º150	 i:9 	 global-step:3009	 l-p:0.11359766870737076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1862, 3.1908, 3.1238],
        [3.1862, 3.1862, 3.1862],
        [3.1862, 3.3605, 3.3434],
        [3.1862, 3.2306, 3.1605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.11945350468158722 
model_pd.l_d.mean(): -24.77791404724121 
model_pd.lagr.mean(): -24.65846061706543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3014], device='cuda:0')), ('power', tensor([-24.4765], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.11945350468158722
epoch£º151	 i:1 	 global-step:3021	 l-p:0.0832579955458641
epoch£º151	 i:2 	 global-step:3022	 l-p:0.1281438171863556
epoch£º151	 i:3 	 global-step:3023	 l-p:0.1107277050614357
epoch£º151	 i:4 	 global-step:3024	 l-p:0.12036246806383133
epoch£º151	 i:5 	 global-step:3025	 l-p:0.12463359534740448
epoch£º151	 i:6 	 global-step:3026	 l-p:1.752580165863037
epoch£º151	 i:7 	 global-step:3027	 l-p:0.10182151198387146
epoch£º151	 i:8 	 global-step:3028	 l-p:0.13723531365394592
epoch£º151	 i:9 	 global-step:3029	 l-p:0.1266392469406128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[2.7347, 2.6712, 2.6637],
        [2.7347, 2.8530, 2.8207],
        [2.7347, 2.6663, 2.6438],
        [2.7347, 2.6667, 2.6462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): -0.01408944558352232 
model_pd.l_d.mean(): -24.437515258789062 
model_pd.lagr.mean(): -24.45160484313965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0184], device='cuda:0')), ('power', tensor([-24.4559], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:-0.01408944558352232
epoch£º152	 i:1 	 global-step:3041	 l-p:0.1410701870918274
epoch£º152	 i:2 	 global-step:3042	 l-p:0.12336201220750809
epoch£º152	 i:3 	 global-step:3043	 l-p:-0.12329841405153275
epoch£º152	 i:4 	 global-step:3044	 l-p:0.14477333426475525
epoch£º152	 i:5 	 global-step:3045	 l-p:0.13739265501499176
epoch£º152	 i:6 	 global-step:3046	 l-p:0.14809662103652954
epoch£º152	 i:7 	 global-step:3047	 l-p:0.13177023828029633
epoch£º152	 i:8 	 global-step:3048	 l-p:0.1429632157087326
epoch£º152	 i:9 	 global-step:3049	 l-p:0.11511582136154175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7660, 2.7615, 2.7655],
        [2.7660, 2.7659, 2.7660],
        [2.7660, 2.7551, 2.7638],
        [2.7660, 2.7660, 2.7660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.134586364030838 
model_pd.l_d.mean(): -24.244848251342773 
model_pd.lagr.mean(): -24.110261917114258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0515], device='cuda:0')), ('power', tensor([-24.1933], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.134586364030838
epoch£º153	 i:1 	 global-step:3061	 l-p:0.1153097003698349
epoch£º153	 i:2 	 global-step:3062	 l-p:0.16387507319450378
epoch£º153	 i:3 	 global-step:3063	 l-p:0.10733284801244736
epoch£º153	 i:4 	 global-step:3064	 l-p:0.14869576692581177
epoch£º153	 i:5 	 global-step:3065	 l-p:0.16823802888393402
epoch£º153	 i:6 	 global-step:3066	 l-p:0.1827574074268341
epoch£º153	 i:7 	 global-step:3067	 l-p:0.11238142102956772
epoch£º153	 i:8 	 global-step:3068	 l-p:0.10033737123012543
epoch£º153	 i:9 	 global-step:3069	 l-p:0.11891036480665207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0408, 3.0992, 3.0252],
        [3.0408, 2.9906, 2.9730],
        [3.0408, 3.0213, 3.0334],
        [3.0408, 3.0360, 3.0402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.13865119218826294 
model_pd.l_d.mean(): -23.917932510375977 
model_pd.lagr.mean(): -23.779281616210938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1059], device='cuda:0')), ('power', tensor([-23.8121], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.13865119218826294
epoch£º154	 i:1 	 global-step:3081	 l-p:0.10925734043121338
epoch£º154	 i:2 	 global-step:3082	 l-p:0.13039644062519073
epoch£º154	 i:3 	 global-step:3083	 l-p:0.11866553872823715
epoch£º154	 i:4 	 global-step:3084	 l-p:0.1168886348605156
epoch£º154	 i:5 	 global-step:3085	 l-p:0.11715467274188995
epoch£º154	 i:6 	 global-step:3086	 l-p:0.12310200184583664
epoch£º154	 i:7 	 global-step:3087	 l-p:0.10149899125099182
epoch£º154	 i:8 	 global-step:3088	 l-p:0.12204595655202866
epoch£º154	 i:9 	 global-step:3089	 l-p:0.17837680876255035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6764, 2.6522, 2.5518],
        [2.6764, 2.6291, 2.5267],
        [2.6764, 2.6390, 2.6577],
        [2.6764, 2.5911, 2.5203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.14385484158992767 
model_pd.l_d.mean(): -24.521392822265625 
model_pd.lagr.mean(): -24.377538681030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0020], device='cuda:0')), ('power', tensor([-24.5234], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.14385484158992767
epoch£º155	 i:1 	 global-step:3101	 l-p:0.19708451628684998
epoch£º155	 i:2 	 global-step:3102	 l-p:0.05935566872358322
epoch£º155	 i:3 	 global-step:3103	 l-p:0.12993071973323822
epoch£º155	 i:4 	 global-step:3104	 l-p:0.056635838001966476
epoch£º155	 i:5 	 global-step:3105	 l-p:0.1424623429775238
epoch£º155	 i:6 	 global-step:3106	 l-p:0.10723357647657394
epoch£º155	 i:7 	 global-step:3107	 l-p:0.13136352598667145
epoch£º155	 i:8 	 global-step:3108	 l-p:0.1253877580165863
epoch£º155	 i:9 	 global-step:3109	 l-p:0.2729342579841614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[2.7111, 2.8724, 2.8634],
        [2.7111, 2.8821, 2.8798],
        [2.7111, 2.6318, 2.6174],
        [2.7111, 2.8710, 2.8611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.08024441450834274 
model_pd.l_d.mean(): -24.513517379760742 
model_pd.lagr.mean(): -24.433273315429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0057], device='cuda:0')), ('power', tensor([-24.5079], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.08024441450834274
epoch£º156	 i:1 	 global-step:3121	 l-p:0.14789487421512604
epoch£º156	 i:2 	 global-step:3122	 l-p:0.153840571641922
epoch£º156	 i:3 	 global-step:3123	 l-p:0.14433078467845917
epoch£º156	 i:4 	 global-step:3124	 l-p:0.12184718996286392
epoch£º156	 i:5 	 global-step:3125	 l-p:0.16476371884346008
epoch£º156	 i:6 	 global-step:3126	 l-p:0.11904283612966537
epoch£º156	 i:7 	 global-step:3127	 l-p:0.11889186501502991
epoch£º156	 i:8 	 global-step:3128	 l-p:0.07867692410945892
epoch£º156	 i:9 	 global-step:3129	 l-p:0.13320820033550262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[3.1049, 3.0661, 3.0763],
        [3.1049, 3.1183, 3.0312],
        [3.1049, 3.2826, 3.2653],
        [3.1049, 3.0547, 3.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.08857687562704086 
model_pd.l_d.mean(): -24.538291931152344 
model_pd.lagr.mean(): -24.44971466064453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1862], device='cuda:0')), ('power', tensor([-24.3521], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.08857687562704086
epoch£º157	 i:1 	 global-step:3141	 l-p:0.1406543105840683
epoch£º157	 i:2 	 global-step:3142	 l-p:0.12416204065084457
epoch£º157	 i:3 	 global-step:3143	 l-p:0.10055607557296753
epoch£º157	 i:4 	 global-step:3144	 l-p:0.12212194502353668
epoch£º157	 i:5 	 global-step:3145	 l-p:0.26078978180885315
epoch£º157	 i:6 	 global-step:3146	 l-p:0.09060203284025192
epoch£º157	 i:7 	 global-step:3147	 l-p:0.12895630300045013
epoch£º157	 i:8 	 global-step:3148	 l-p:0.14042255282402039
epoch£º157	 i:9 	 global-step:3149	 l-p:0.15286698937416077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9132, 2.9132, 2.9132],
        [2.9132, 2.9032, 2.9114],
        [2.9132, 2.9132, 2.9132],
        [2.9132, 2.8670, 2.7757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.1279575675725937 
model_pd.l_d.mean(): -24.331178665161133 
model_pd.lagr.mean(): -24.20322036743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1178], device='cuda:0')), ('power', tensor([-24.2134], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.1279575675725937
epoch£º158	 i:1 	 global-step:3161	 l-p:0.1641852855682373
epoch£º158	 i:2 	 global-step:3162	 l-p:0.10093958675861359
epoch£º158	 i:3 	 global-step:3163	 l-p:0.23226800560951233
epoch£º158	 i:4 	 global-step:3164	 l-p:0.1344330757856369
epoch£º158	 i:5 	 global-step:3165	 l-p:0.1345306932926178
epoch£º158	 i:6 	 global-step:3166	 l-p:0.5307491421699524
epoch£º158	 i:7 	 global-step:3167	 l-p:0.12475999444723129
epoch£º158	 i:8 	 global-step:3168	 l-p:0.12065142393112183
epoch£º158	 i:9 	 global-step:3169	 l-p:0.11794469505548477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9977, 2.9334, 2.9175],
        [2.9977, 2.9527, 2.9654],
        [2.9977, 2.9391, 2.9375],
        [2.9977, 2.9669, 2.9829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.13651779294013977 
model_pd.l_d.mean(): -24.70578384399414 
model_pd.lagr.mean(): -24.569265365600586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1770], device='cuda:0')), ('power', tensor([-24.5288], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.13651779294013977
epoch£º159	 i:1 	 global-step:3181	 l-p:0.14645332098007202
epoch£º159	 i:2 	 global-step:3182	 l-p:0.05612783879041672
epoch£º159	 i:3 	 global-step:3183	 l-p:0.13720549643039703
epoch£º159	 i:4 	 global-step:3184	 l-p:0.11142995953559875
epoch£º159	 i:5 	 global-step:3185	 l-p:0.11947204172611237
epoch£º159	 i:6 	 global-step:3186	 l-p:0.13098695874214172
epoch£º159	 i:7 	 global-step:3187	 l-p:0.13253439962863922
epoch£º159	 i:8 	 global-step:3188	 l-p:0.1284918636083603
epoch£º159	 i:9 	 global-step:3189	 l-p:0.16423213481903076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7814, 2.7303, 2.7481],
        [2.7814, 2.7919, 2.6933],
        [2.7814, 2.7814, 2.7814],
        [2.7814, 2.7804, 2.7814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.134868785738945 
model_pd.l_d.mean(): -24.975706100463867 
model_pd.lagr.mean(): -24.840837478637695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1303], device='cuda:0')), ('power', tensor([-24.8454], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.134868785738945
epoch£º160	 i:1 	 global-step:3201	 l-p:0.13352440297603607
epoch£º160	 i:2 	 global-step:3202	 l-p:0.17171603441238403
epoch£º160	 i:3 	 global-step:3203	 l-p:0.11612047255039215
epoch£º160	 i:4 	 global-step:3204	 l-p:0.17898882925510406
epoch£º160	 i:5 	 global-step:3205	 l-p:0.11274875700473785
epoch£º160	 i:6 	 global-step:3206	 l-p:0.09106575697660446
epoch£º160	 i:7 	 global-step:3207	 l-p:0.22923927009105682
epoch£º160	 i:8 	 global-step:3208	 l-p:0.11035284399986267
epoch£º160	 i:9 	 global-step:3209	 l-p:0.13351766765117645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[3.3720, 3.3283, 3.3085],
        [3.3720, 3.3361, 3.3425],
        [3.3720, 3.4618, 3.3910],
        [3.3720, 3.4972, 3.4409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.11227075010538101 
model_pd.l_d.mean(): -24.781850814819336 
model_pd.lagr.mean(): -24.669580459594727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3967], device='cuda:0')), ('power', tensor([-24.3852], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.11227075010538101
epoch£º161	 i:1 	 global-step:3221	 l-p:0.11385175585746765
epoch£º161	 i:2 	 global-step:3222	 l-p:0.10692756623029709
epoch£º161	 i:3 	 global-step:3223	 l-p:0.11836515367031097
epoch£º161	 i:4 	 global-step:3224	 l-p:0.08874407410621643
epoch£º161	 i:5 	 global-step:3225	 l-p:0.11460261046886444
epoch£º161	 i:6 	 global-step:3226	 l-p:0.16706088185310364
epoch£º161	 i:7 	 global-step:3227	 l-p:0.1526588350534439
epoch£º161	 i:8 	 global-step:3228	 l-p:0.08084035664796829
epoch£º161	 i:9 	 global-step:3229	 l-p:0.1300649642944336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8313, 2.7861, 2.6738],
        [2.8313, 2.8303, 2.8313],
        [2.8313, 2.7638, 2.7722],
        [2.8313, 2.8251, 2.7176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.13826802372932434 
model_pd.l_d.mean(): -24.65967559814453 
model_pd.lagr.mean(): -24.521408081054688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0744], device='cuda:0')), ('power', tensor([-24.5852], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.13826802372932434
epoch£º162	 i:1 	 global-step:3241	 l-p:-0.07713072746992111
epoch£º162	 i:2 	 global-step:3242	 l-p:0.11302939802408218
epoch£º162	 i:3 	 global-step:3243	 l-p:0.1312323659658432
epoch£º162	 i:4 	 global-step:3244	 l-p:0.15315058827400208
epoch£º162	 i:5 	 global-step:3245	 l-p:0.07338922470808029
epoch£º162	 i:6 	 global-step:3246	 l-p:0.1390918791294098
epoch£º162	 i:7 	 global-step:3247	 l-p:-0.4963296353816986
epoch£º162	 i:8 	 global-step:3248	 l-p:0.16965201497077942
epoch£º162	 i:9 	 global-step:3249	 l-p:-3.7051024436950684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9040, 2.9965, 2.9298],
        [2.9040, 2.9040, 2.9040],
        [2.9040, 2.8239, 2.7418],
        [2.9040, 2.9040, 2.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.11583515256643295 
model_pd.l_d.mean(): -23.830018997192383 
model_pd.lagr.mean(): -23.714183807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0056], device='cuda:0')), ('power', tensor([-23.8244], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.11583515256643295
epoch£º163	 i:1 	 global-step:3261	 l-p:0.10122308135032654
epoch£º163	 i:2 	 global-step:3262	 l-p:0.13302503526210785
epoch£º163	 i:3 	 global-step:3263	 l-p:0.14004161953926086
epoch£º163	 i:4 	 global-step:3264	 l-p:0.11484657973051071
epoch£º163	 i:5 	 global-step:3265	 l-p:0.1251072734594345
epoch£º163	 i:6 	 global-step:3266	 l-p:0.12372893840074539
epoch£º163	 i:7 	 global-step:3267	 l-p:0.1188008189201355
epoch£º163	 i:8 	 global-step:3268	 l-p:0.11131396889686584
epoch£º163	 i:9 	 global-step:3269	 l-p:0.1372183859348297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6541, 2.5407, 2.4919],
        [2.6541, 2.6469, 2.6532],
        [2.6541, 2.6524, 2.6540],
        [2.6541, 2.6228, 2.6429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1482713520526886 
model_pd.l_d.mean(): -24.64077377319336 
model_pd.lagr.mean(): -24.492502212524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0091], device='cuda:0')), ('power', tensor([-24.6499], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.1482713520526886
epoch£º164	 i:1 	 global-step:3281	 l-p:0.12878358364105225
epoch£º164	 i:2 	 global-step:3282	 l-p:0.11311551928520203
epoch£º164	 i:3 	 global-step:3283	 l-p:0.05895378813147545
epoch£º164	 i:4 	 global-step:3284	 l-p:0.1811046153306961
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12833093106746674
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12873601913452148
epoch£º164	 i:7 	 global-step:3287	 l-p:0.13126730918884277
epoch£º164	 i:8 	 global-step:3288	 l-p:0.12554879486560822
epoch£º164	 i:9 	 global-step:3289	 l-p:0.5323666930198669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8531, 2.8531, 2.8531],
        [2.8531, 2.8811, 2.7812],
        [2.8531, 2.8048, 2.6863],
        [2.8531, 2.7583, 2.7216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.06423085927963257 
model_pd.l_d.mean(): -24.62221336364746 
model_pd.lagr.mean(): -24.5579833984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0943], device='cuda:0')), ('power', tensor([-24.5280], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.06423085927963257
epoch£º165	 i:1 	 global-step:3301	 l-p:0.09759540110826492
epoch£º165	 i:2 	 global-step:3302	 l-p:0.14552953839302063
epoch£º165	 i:3 	 global-step:3303	 l-p:0.12611204385757446
epoch£º165	 i:4 	 global-step:3304	 l-p:0.1510360687971115
epoch£º165	 i:5 	 global-step:3305	 l-p:0.12567785382270813
epoch£º165	 i:6 	 global-step:3306	 l-p:0.1112549751996994
epoch£º165	 i:7 	 global-step:3307	 l-p:0.013436433859169483
epoch£º165	 i:8 	 global-step:3308	 l-p:0.1452048271894455
epoch£º165	 i:9 	 global-step:3309	 l-p:0.12113334983587265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9052, 2.9020, 2.9050],
        [2.9052, 2.8421, 2.8565],
        [2.9052, 2.8375, 2.8486],
        [2.9052, 2.9051, 2.9052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.07597732543945312 
model_pd.l_d.mean(): -24.05609130859375 
model_pd.lagr.mean(): -23.980113983154297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0485], device='cuda:0')), ('power', tensor([-24.0076], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.07597732543945312
epoch£º166	 i:1 	 global-step:3321	 l-p:0.14397208392620087
epoch£º166	 i:2 	 global-step:3322	 l-p:0.12624932825565338
epoch£º166	 i:3 	 global-step:3323	 l-p:0.09494413435459137
epoch£º166	 i:4 	 global-step:3324	 l-p:0.1078002080321312
epoch£º166	 i:5 	 global-step:3325	 l-p:0.1538773775100708
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12341659516096115
epoch£º166	 i:7 	 global-step:3327	 l-p:0.14295706152915955
epoch£º166	 i:8 	 global-step:3328	 l-p:0.12229923158884048
epoch£º166	 i:9 	 global-step:3329	 l-p:0.19513998925685883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9883, 2.9883, 2.9883],
        [2.9883, 2.9025, 2.8807],
        [2.9883, 3.2358, 3.2604],
        [2.9883, 2.9710, 2.9841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.1236976608633995 
model_pd.l_d.mean(): -24.267284393310547 
model_pd.lagr.mean(): -24.143587112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1212], device='cuda:0')), ('power', tensor([-24.1460], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.1236976608633995
epoch£º167	 i:1 	 global-step:3341	 l-p:0.10345016419887543
epoch£º167	 i:2 	 global-step:3342	 l-p:0.16021691262722015
epoch£º167	 i:3 	 global-step:3343	 l-p:0.11844679713249207
epoch£º167	 i:4 	 global-step:3344	 l-p:1.118798017501831
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12352897226810455
epoch£º167	 i:6 	 global-step:3346	 l-p:0.13245916366577148
epoch£º167	 i:7 	 global-step:3347	 l-p:0.123306043446064
epoch£º167	 i:8 	 global-step:3348	 l-p:0.12913063168525696
epoch£º167	 i:9 	 global-step:3349	 l-p:0.10752847790718079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9125, 2.9111, 2.9124],
        [2.9125, 3.0731, 3.0398],
        [2.9125, 2.9125, 2.9125],
        [2.9125, 2.9124, 2.9125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.3808140158653259 
model_pd.l_d.mean(): -23.917970657348633 
model_pd.lagr.mean(): -23.53715705871582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0304], device='cuda:0')), ('power', tensor([-23.8875], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.3808140158653259
epoch£º168	 i:1 	 global-step:3361	 l-p:0.12620986998081207
epoch£º168	 i:2 	 global-step:3362	 l-p:0.13443714380264282
epoch£º168	 i:3 	 global-step:3363	 l-p:0.1435967981815338
epoch£º168	 i:4 	 global-step:3364	 l-p:0.034950140863657
epoch£º168	 i:5 	 global-step:3365	 l-p:0.04204834997653961
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11613406985998154
epoch£º168	 i:7 	 global-step:3367	 l-p:0.21858979761600494
epoch£º168	 i:8 	 global-step:3368	 l-p:0.12096293270587921
epoch£º168	 i:9 	 global-step:3369	 l-p:0.1154378205537796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0339, 3.0334, 3.0339],
        [3.0339, 3.0062, 3.0242],
        [3.0339, 2.9473, 2.9307],
        [3.0339, 2.9787, 2.9973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.1083081066608429 
model_pd.l_d.mean(): -24.835140228271484 
model_pd.lagr.mean(): -24.726831436157227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2313], device='cuda:0')), ('power', tensor([-24.6038], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.1083081066608429
epoch£º169	 i:1 	 global-step:3381	 l-p:0.12176214158535004
epoch£º169	 i:2 	 global-step:3382	 l-p:0.14297018945217133
epoch£º169	 i:3 	 global-step:3383	 l-p:0.12397152930498123
epoch£º169	 i:4 	 global-step:3384	 l-p:0.10503721237182617
epoch£º169	 i:5 	 global-step:3385	 l-p:0.1435963660478592
epoch£º169	 i:6 	 global-step:3386	 l-p:0.1778971254825592
epoch£º169	 i:7 	 global-step:3387	 l-p:0.2571926414966583
epoch£º169	 i:8 	 global-step:3388	 l-p:0.12880612909793854
epoch£º169	 i:9 	 global-step:3389	 l-p:0.1378549337387085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8827, 2.7711, 2.7245],
        [2.8827, 2.8806, 2.8826],
        [2.8827, 2.8467, 2.8684],
        [2.8827, 2.8812, 2.8827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.1343236267566681 
model_pd.l_d.mean(): -23.99386215209961 
model_pd.lagr.mean(): -23.859539031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0122], device='cuda:0')), ('power', tensor([-23.9816], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.1343236267566681
epoch£º170	 i:1 	 global-step:3401	 l-p:0.2841567099094391
epoch£º170	 i:2 	 global-step:3402	 l-p:0.12975181639194489
epoch£º170	 i:3 	 global-step:3403	 l-p:0.13558954000473022
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12239989638328552
epoch£º170	 i:5 	 global-step:3405	 l-p:0.13525065779685974
epoch£º170	 i:6 	 global-step:3406	 l-p:0.0739469826221466
epoch£º170	 i:7 	 global-step:3407	 l-p:0.4906587600708008
epoch£º170	 i:8 	 global-step:3408	 l-p:0.20065154135227203
epoch£º170	 i:9 	 global-step:3409	 l-p:0.08098377287387848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8872, 2.8046, 2.6702],
        [2.8872, 2.8839, 2.8869],
        [2.8872, 2.8484, 2.8712],
        [2.8872, 2.8871, 2.8872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11107080429792404 
model_pd.l_d.mean(): -23.93233871459961 
model_pd.lagr.mean(): -23.82126808166504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0033], device='cuda:0')), ('power', tensor([-23.9290], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11107080429792404
epoch£º171	 i:1 	 global-step:3421	 l-p:0.10978914052248001
epoch£º171	 i:2 	 global-step:3422	 l-p:0.13023683428764343
epoch£º171	 i:3 	 global-step:3423	 l-p:0.13462553918361664
epoch£º171	 i:4 	 global-step:3424	 l-p:-0.03475258871912956
epoch£º171	 i:5 	 global-step:3425	 l-p:0.03365074470639229
epoch£º171	 i:6 	 global-step:3426	 l-p:0.11548833549022675
epoch£º171	 i:7 	 global-step:3427	 l-p:0.12892040610313416
epoch£º171	 i:8 	 global-step:3428	 l-p:0.13311725854873657
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15235751867294312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0070, 3.0055, 3.0070],
        [3.0070, 2.9946, 3.0049],
        [3.0070, 2.8996, 2.8183],
        [3.0070, 3.0070, 3.0070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.13210418820381165 
model_pd.l_d.mean(): -24.866540908813477 
model_pd.lagr.mean(): -24.73443603515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2199], device='cuda:0')), ('power', tensor([-24.6466], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.13210418820381165
epoch£º172	 i:1 	 global-step:3441	 l-p:0.1503499150276184
epoch£º172	 i:2 	 global-step:3442	 l-p:0.10528135299682617
epoch£º172	 i:3 	 global-step:3443	 l-p:0.12193432450294495
epoch£º172	 i:4 	 global-step:3444	 l-p:0.14104680716991425
epoch£º172	 i:5 	 global-step:3445	 l-p:0.12174508720636368
epoch£º172	 i:6 	 global-step:3446	 l-p:0.17022216320037842
epoch£º172	 i:7 	 global-step:3447	 l-p:0.13052289187908173
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12313482165336609
epoch£º172	 i:9 	 global-step:3449	 l-p:0.11535276472568512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[2.8096, 2.9157, 2.8433],
        [2.8096, 2.7127, 2.7219],
        [2.8096, 2.7514, 2.6066],
        [2.8096, 2.7070, 2.7103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.10335061699151993 
model_pd.l_d.mean(): -24.31899070739746 
model_pd.lagr.mean(): -24.215641021728516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0107], device='cuda:0')), ('power', tensor([-24.3297], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.10335061699151993
epoch£º173	 i:1 	 global-step:3461	 l-p:-0.0781683400273323
epoch£º173	 i:2 	 global-step:3462	 l-p:0.13115623593330383
epoch£º173	 i:3 	 global-step:3463	 l-p:0.12915034592151642
epoch£º173	 i:4 	 global-step:3464	 l-p:0.13101345300674438
epoch£º173	 i:5 	 global-step:3465	 l-p:0.25967779755592346
epoch£º173	 i:6 	 global-step:3466	 l-p:0.10482394695281982
epoch£º173	 i:7 	 global-step:3467	 l-p:0.13363614678382874
epoch£º173	 i:8 	 global-step:3468	 l-p:0.12853048741817474
epoch£º173	 i:9 	 global-step:3469	 l-p:-0.08897784352302551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7214, 2.7012, 2.5650],
        [2.7214, 2.6908, 2.7123],
        [2.7214, 2.6243, 2.6414],
        [2.7214, 2.5934, 2.5763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.14435093104839325 
model_pd.l_d.mean(): -24.676101684570312 
model_pd.lagr.mean(): -24.53175163269043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0198], device='cuda:0')), ('power', tensor([-24.6563], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.14435093104839325
epoch£º174	 i:1 	 global-step:3481	 l-p:0.04487508535385132
epoch£º174	 i:2 	 global-step:3482	 l-p:0.1362822949886322
epoch£º174	 i:3 	 global-step:3483	 l-p:0.11785922944545746
epoch£º174	 i:4 	 global-step:3484	 l-p:0.13376395404338837
epoch£º174	 i:5 	 global-step:3485	 l-p:0.20432829856872559
epoch£º174	 i:6 	 global-step:3486	 l-p:0.1295541375875473
epoch£º174	 i:7 	 global-step:3487	 l-p:0.12295346707105637
epoch£º174	 i:8 	 global-step:3488	 l-p:0.3132866621017456
epoch£º174	 i:9 	 global-step:3489	 l-p:0.08241908997297287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0231, 3.0171, 3.0225],
        [3.0231, 3.0019, 3.0179],
        [3.0231, 2.9577, 2.9810],
        [3.0231, 2.9389, 2.9523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.11709926277399063 
model_pd.l_d.mean(): -24.832368850708008 
model_pd.lagr.mean(): -24.715269088745117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2351], device='cuda:0')), ('power', tensor([-24.5972], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.11709926277399063
epoch£º175	 i:1 	 global-step:3501	 l-p:0.11775538325309753
epoch£º175	 i:2 	 global-step:3502	 l-p:0.11392110586166382
epoch£º175	 i:3 	 global-step:3503	 l-p:0.14184823632240295
epoch£º175	 i:4 	 global-step:3504	 l-p:0.13533614575862885
epoch£º175	 i:5 	 global-step:3505	 l-p:0.13766655325889587
epoch£º175	 i:6 	 global-step:3506	 l-p:0.23686887323856354
epoch£º175	 i:7 	 global-step:3507	 l-p:0.1169927641749382
epoch£º175	 i:8 	 global-step:3508	 l-p:0.10525781661272049
epoch£º175	 i:9 	 global-step:3509	 l-p:0.0861872062087059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9143, 2.9143, 2.9143],
        [2.9143, 2.8772, 2.9010],
        [2.9143, 2.8233, 2.6694],
        [2.9143, 2.7787, 2.7162]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.08671616017818451 
model_pd.l_d.mean(): -24.244342803955078 
model_pd.lagr.mean(): -24.15762710571289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0667], device='cuda:0')), ('power', tensor([-24.1777], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.08671616017818451
epoch£º176	 i:1 	 global-step:3521	 l-p:0.12551963329315186
epoch£º176	 i:2 	 global-step:3522	 l-p:0.12299209088087082
epoch£º176	 i:3 	 global-step:3523	 l-p:0.12370552122592926
epoch£º176	 i:4 	 global-step:3524	 l-p:0.11936460435390472
epoch£º176	 i:5 	 global-step:3525	 l-p:0.13235794007778168
epoch£º176	 i:6 	 global-step:3526	 l-p:0.1143554151058197
epoch£º176	 i:7 	 global-step:3527	 l-p:0.23812198638916016
epoch£º176	 i:8 	 global-step:3528	 l-p:0.11816413700580597
epoch£º176	 i:9 	 global-step:3529	 l-p:0.14086787402629852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8783, 3.0129, 2.9462],
        [2.8783, 2.8761, 2.8782],
        [2.8783, 2.7653, 2.7654],
        [2.8783, 2.8773, 2.8783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.11363948881626129 
model_pd.l_d.mean(): -24.463035583496094 
model_pd.lagr.mean(): -24.349395751953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0765], device='cuda:0')), ('power', tensor([-24.3865], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.11363948881626129
epoch£º177	 i:1 	 global-step:3541	 l-p:0.07010315358638763
epoch£º177	 i:2 	 global-step:3542	 l-p:0.11786968261003494
epoch£º177	 i:3 	 global-step:3543	 l-p:0.1572374850511551
epoch£º177	 i:4 	 global-step:3544	 l-p:0.11510593444108963
epoch£º177	 i:5 	 global-step:3545	 l-p:0.4819183945655823
epoch£º177	 i:6 	 global-step:3546	 l-p:0.12299052625894547
epoch£º177	 i:7 	 global-step:3547	 l-p:0.1773814707994461
epoch£º177	 i:8 	 global-step:3548	 l-p:0.12206511199474335
epoch£º177	 i:9 	 global-step:3549	 l-p:0.08101843297481537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0900, 3.0893, 3.0899],
        [3.0900, 3.0899, 3.0900],
        [3.0900, 3.0900, 3.0900],
        [3.0900, 3.0054, 3.0205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.1240483745932579 
model_pd.l_d.mean(): -23.789426803588867 
model_pd.lagr.mean(): -23.66537857055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1124], device='cuda:0')), ('power', tensor([-23.6770], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.1240483745932579
epoch£º178	 i:1 	 global-step:3561	 l-p:0.1219443678855896
epoch£º178	 i:2 	 global-step:3562	 l-p:0.12342899292707443
epoch£º178	 i:3 	 global-step:3563	 l-p:0.12493965774774551
epoch£º178	 i:4 	 global-step:3564	 l-p:0.1297292560338974
epoch£º178	 i:5 	 global-step:3565	 l-p:0.130167618393898
epoch£º178	 i:6 	 global-step:3566	 l-p:0.015695694833993912
epoch£º178	 i:7 	 global-step:3567	 l-p:0.1771574169397354
epoch£º178	 i:8 	 global-step:3568	 l-p:0.055238183587789536
epoch£º178	 i:9 	 global-step:3569	 l-p:0.1631794571876526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6908, 2.6867, 2.6905],
        [2.6908, 2.6403, 2.6702],
        [2.6908, 2.6424, 2.6718],
        [2.6908, 2.6890, 2.6907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.13635730743408203 
model_pd.l_d.mean(): -24.857027053833008 
model_pd.lagr.mean(): -24.72066879272461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0311], device='cuda:0')), ('power', tensor([-24.8259], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.13635730743408203
epoch£º179	 i:1 	 global-step:3581	 l-p:0.13178718090057373
epoch£º179	 i:2 	 global-step:3582	 l-p:0.248618021607399
epoch£º179	 i:3 	 global-step:3583	 l-p:0.09278879314661026
epoch£º179	 i:4 	 global-step:3584	 l-p:0.1559649109840393
epoch£º179	 i:5 	 global-step:3585	 l-p:0.13142366707324982
epoch£º179	 i:6 	 global-step:3586	 l-p:0.07437147945165634
epoch£º179	 i:7 	 global-step:3587	 l-p:0.10633882880210876
epoch£º179	 i:8 	 global-step:3588	 l-p:0.12239814549684525
epoch£º179	 i:9 	 global-step:3589	 l-p:0.11389989405870438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2555, 3.2530, 3.2554],
        [3.2555, 3.2537, 3.2554],
        [3.2555, 3.1420, 3.0800],
        [3.2555, 3.2555, 3.2555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.11676329374313354 
model_pd.l_d.mean(): -24.860698699951172 
model_pd.lagr.mean(): -24.743934631347656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3348], device='cuda:0')), ('power', tensor([-24.5259], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.11676329374313354
epoch£º180	 i:1 	 global-step:3601	 l-p:0.12413382530212402
epoch£º180	 i:2 	 global-step:3602	 l-p:0.06953377276659012
epoch£º180	 i:3 	 global-step:3603	 l-p:0.14152513444423676
epoch£º180	 i:4 	 global-step:3604	 l-p:0.3500920534133911
epoch£º180	 i:5 	 global-step:3605	 l-p:0.11853227019309998
epoch£º180	 i:6 	 global-step:3606	 l-p:0.15151053667068481
epoch£º180	 i:7 	 global-step:3607	 l-p:0.14023789763450623
epoch£º180	 i:8 	 global-step:3608	 l-p:0.10375972837209702
epoch£º180	 i:9 	 global-step:3609	 l-p:0.1387796401977539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8467, 2.8272, 2.8428],
        [2.8467, 2.7410, 2.7592],
        [2.8467, 2.6917, 2.5414],
        [2.8467, 2.7625, 2.5918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.1342158317565918 
model_pd.l_d.mean(): -24.781068801879883 
model_pd.lagr.mean(): -24.646852493286133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1026], device='cuda:0')), ('power', tensor([-24.6784], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.1342158317565918
epoch£º181	 i:1 	 global-step:3621	 l-p:0.13089774549007416
epoch£º181	 i:2 	 global-step:3622	 l-p:0.1443014144897461
epoch£º181	 i:3 	 global-step:3623	 l-p:0.14893656969070435
epoch£º181	 i:4 	 global-step:3624	 l-p:0.14518223702907562
epoch£º181	 i:5 	 global-step:3625	 l-p:0.059190548956394196
epoch£º181	 i:6 	 global-step:3626	 l-p:0.13243503868579865
epoch£º181	 i:7 	 global-step:3627	 l-p:0.11862266063690186
epoch£º181	 i:8 	 global-step:3628	 l-p:0.131130188703537
epoch£º181	 i:9 	 global-step:3629	 l-p:0.07994002848863602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9535, 2.9304, 2.9482],
        [2.9535, 2.8697, 2.8968],
        [2.9535, 2.8219, 2.8067],
        [2.9535, 2.9535, 2.9535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.11511529982089996 
model_pd.l_d.mean(): -24.678512573242188 
model_pd.lagr.mean(): -24.563396453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1675], device='cuda:0')), ('power', tensor([-24.5110], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.11511529982089996
epoch£º182	 i:1 	 global-step:3641	 l-p:0.15506114065647125
epoch£º182	 i:2 	 global-step:3642	 l-p:0.12828929722309113
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12574589252471924
epoch£º182	 i:4 	 global-step:3644	 l-p:0.11708993464708328
epoch£º182	 i:5 	 global-step:3645	 l-p:0.12463662773370743
epoch£º182	 i:6 	 global-step:3646	 l-p:0.09025514125823975
epoch£º182	 i:7 	 global-step:3647	 l-p:0.12270662933588028
epoch£º182	 i:8 	 global-step:3648	 l-p:-0.0005427884752862155
epoch£º182	 i:9 	 global-step:3649	 l-p:0.08400289714336395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0151, 2.8636, 2.7421],
        [3.0151, 2.8802, 2.8566],
        [3.0151, 2.9931, 3.0103],
        [3.0151, 2.9756, 2.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.14720003306865692 
model_pd.l_d.mean(): -24.70747184753418 
model_pd.lagr.mean(): -24.560272216796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1780], device='cuda:0')), ('power', tensor([-24.5295], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.14720003306865692
epoch£º183	 i:1 	 global-step:3661	 l-p:0.08262838423252106
epoch£º183	 i:2 	 global-step:3662	 l-p:0.12637768685817719
epoch£º183	 i:3 	 global-step:3663	 l-p:0.14367146790027618
epoch£º183	 i:4 	 global-step:3664	 l-p:0.12312464416027069
epoch£º183	 i:5 	 global-step:3665	 l-p:0.11204249411821365
epoch£º183	 i:6 	 global-step:3666	 l-p:0.12427793443202972
epoch£º183	 i:7 	 global-step:3667	 l-p:0.12911710143089294
epoch£º183	 i:8 	 global-step:3668	 l-p:0.3584815263748169
epoch£º183	 i:9 	 global-step:3669	 l-p:0.10922166705131531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6750, 2.5544, 2.5777],
        [2.6750, 2.5767, 2.3897],
        [2.6750, 2.5996, 2.6354],
        [2.6750, 2.6750, 2.6750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.18023860454559326 
model_pd.l_d.mean(): -24.135475158691406 
model_pd.lagr.mean(): -23.955236434936523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0685], device='cuda:0')), ('power', tensor([-24.2040], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.18023860454559326
epoch£º184	 i:1 	 global-step:3681	 l-p:0.12403278797864914
epoch£º184	 i:2 	 global-step:3682	 l-p:0.12895552814006805
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12387115508317947
epoch£º184	 i:4 	 global-step:3684	 l-p:0.12913131713867188
epoch£º184	 i:5 	 global-step:3685	 l-p:0.034269388765096664
epoch£º184	 i:6 	 global-step:3686	 l-p:0.1322452872991562
epoch£º184	 i:7 	 global-step:3687	 l-p:0.0053053926676511765
epoch£º184	 i:8 	 global-step:3688	 l-p:-0.0073457146063447
epoch£º184	 i:9 	 global-step:3689	 l-p:0.13508285582065582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8847, 2.7390, 2.7259],
        [2.8847, 2.8847, 2.8847],
        [2.8847, 2.8434, 2.8709],
        [2.8847, 2.8156, 2.6357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.09797350317239761 
model_pd.l_d.mean(): -24.286352157592773 
model_pd.lagr.mean(): -24.188379287719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0321], device='cuda:0')), ('power', tensor([-24.2542], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.09797350317239761
epoch£º185	 i:1 	 global-step:3701	 l-p:0.1991407424211502
epoch£º185	 i:2 	 global-step:3702	 l-p:0.08609519898891449
epoch£º185	 i:3 	 global-step:3703	 l-p:0.15131258964538574
epoch£º185	 i:4 	 global-step:3704	 l-p:0.10943295061588287
epoch£º185	 i:5 	 global-step:3705	 l-p:0.10326481610536575
epoch£º185	 i:6 	 global-step:3706	 l-p:0.10961315035820007
epoch£º185	 i:7 	 global-step:3707	 l-p:0.11491422355175018
epoch£º185	 i:8 	 global-step:3708	 l-p:0.1201486587524414
epoch£º185	 i:9 	 global-step:3709	 l-p:0.12308333814144135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2496, 3.2496, 3.2496],
        [3.2496, 3.1294, 2.9871],
        [3.2496, 3.1206, 2.9919],
        [3.2496, 3.1313, 3.1200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.13123716413974762 
model_pd.l_d.mean(): -24.566205978393555 
model_pd.lagr.mean(): -24.434968948364258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2451], device='cuda:0')), ('power', tensor([-24.3212], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.13123716413974762
epoch£º186	 i:1 	 global-step:3721	 l-p:0.12929721176624298
epoch£º186	 i:2 	 global-step:3722	 l-p:0.17474216222763062
epoch£º186	 i:3 	 global-step:3723	 l-p:0.14761869609355927
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10407847911119461
epoch£º186	 i:5 	 global-step:3725	 l-p:0.09996484220027924
epoch£º186	 i:6 	 global-step:3726	 l-p:0.1409251093864441
epoch£º186	 i:7 	 global-step:3727	 l-p:0.14375554025173187
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12910814583301544
epoch£º186	 i:9 	 global-step:3729	 l-p:0.18573181331157684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6468, 2.6466, 2.6468],
        [2.6468, 2.4549, 2.4071],
        [2.6468, 2.4286, 2.2963],
        [2.6468, 2.6468, 2.6468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.10941636562347412 
model_pd.l_d.mean(): -24.349597930908203 
model_pd.lagr.mean(): -24.24018096923828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0769], device='cuda:0')), ('power', tensor([-24.4265], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.10941636562347412
epoch£º187	 i:1 	 global-step:3741	 l-p:0.12755180895328522
epoch£º187	 i:2 	 global-step:3742	 l-p:0.119475819170475
epoch£º187	 i:3 	 global-step:3743	 l-p:0.1334017515182495
epoch£º187	 i:4 	 global-step:3744	 l-p:0.12965412437915802
epoch£º187	 i:5 	 global-step:3745	 l-p:0.1050313413143158
epoch£º187	 i:6 	 global-step:3746	 l-p:0.07470236718654633
epoch£º187	 i:7 	 global-step:3747	 l-p:0.14043013751506805
epoch£º187	 i:8 	 global-step:3748	 l-p:6.972807884216309
epoch£º187	 i:9 	 global-step:3749	 l-p:0.11325199156999588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7332, 2.6265, 2.4259],
        [2.7332, 2.5991, 2.6186],
        [2.7332, 2.7331, 2.7332],
        [2.7332, 2.6221, 2.4205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.09120692312717438 
model_pd.l_d.mean(): -24.90301513671875 
model_pd.lagr.mean(): -24.81180763244629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0581], device='cuda:0')), ('power', tensor([-24.8449], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.09120692312717438
epoch£º188	 i:1 	 global-step:3761	 l-p:0.07468719035387039
epoch£º188	 i:2 	 global-step:3762	 l-p:0.14391006529331207
epoch£º188	 i:3 	 global-step:3763	 l-p:0.11383235454559326
epoch£º188	 i:4 	 global-step:3764	 l-p:0.18521423637866974
epoch£º188	 i:5 	 global-step:3765	 l-p:0.09972154349088669
epoch£º188	 i:6 	 global-step:3766	 l-p:0.29565665125846863
epoch£º188	 i:7 	 global-step:3767	 l-p:0.20998181402683258
epoch£º188	 i:8 	 global-step:3768	 l-p:0.12027932703495026
epoch£º188	 i:9 	 global-step:3769	 l-p:0.10780127346515656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2855, 3.4312, 3.3430],
        [3.2855, 3.4400, 3.3571],
        [3.2855, 3.2854, 3.2855],
        [3.2855, 3.2855, 3.2855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.1272873729467392 
model_pd.l_d.mean(): -24.333219528198242 
model_pd.lagr.mean(): -24.2059326171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2528], device='cuda:0')), ('power', tensor([-24.0804], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.1272873729467392
epoch£º189	 i:1 	 global-step:3781	 l-p:0.10084043443202972
epoch£º189	 i:2 	 global-step:3782	 l-p:0.1716669797897339
epoch£º189	 i:3 	 global-step:3783	 l-p:0.11343888938426971
epoch£º189	 i:4 	 global-step:3784	 l-p:0.12227354943752289
epoch£º189	 i:5 	 global-step:3785	 l-p:0.12129537016153336
epoch£º189	 i:6 	 global-step:3786	 l-p:0.08455366641283035
epoch£º189	 i:7 	 global-step:3787	 l-p:0.11788463592529297
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11374162137508392
epoch£º189	 i:9 	 global-step:3789	 l-p:0.1138928011059761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5769, 2.5769, 2.5770],
        [2.5769, 2.5769, 2.5769],
        [2.5769, 2.5769, 2.5769],
        [2.5769, 2.5283, 2.5610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.11743149161338806 
model_pd.l_d.mean(): -24.734432220458984 
model_pd.lagr.mean(): -24.617000579833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0509], device='cuda:0')), ('power', tensor([-24.7854], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.11743149161338806
epoch£º190	 i:1 	 global-step:3801	 l-p:0.142618790268898
epoch£º190	 i:2 	 global-step:3802	 l-p:0.1254473775625229
epoch£º190	 i:3 	 global-step:3803	 l-p:0.14944502711296082
epoch£º190	 i:4 	 global-step:3804	 l-p:0.017520971596240997
epoch£º190	 i:5 	 global-step:3805	 l-p:0.09318598359823227
epoch£º190	 i:6 	 global-step:3806	 l-p:0.006771030370146036
epoch£º190	 i:7 	 global-step:3807	 l-p:0.06943152099847794
epoch£º190	 i:8 	 global-step:3808	 l-p:0.1242004781961441
epoch£º190	 i:9 	 global-step:3809	 l-p:0.22540681064128876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2925, 3.4044, 3.2926],
        [3.2925, 3.2002, 3.2255],
        [3.2925, 3.2923, 3.2925],
        [3.2925, 3.2925, 3.2925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.12390151619911194 
model_pd.l_d.mean(): -23.96771240234375 
model_pd.lagr.mean(): -23.84381103515625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1496], device='cuda:0')), ('power', tensor([-23.8181], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.12390151619911194
epoch£º191	 i:1 	 global-step:3821	 l-p:0.11072328686714172
epoch£º191	 i:2 	 global-step:3822	 l-p:0.10914929956197739
epoch£º191	 i:3 	 global-step:3823	 l-p:0.155020609498024
epoch£º191	 i:4 	 global-step:3824	 l-p:0.10415490716695786
epoch£º191	 i:5 	 global-step:3825	 l-p:0.1875768005847931
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12340056896209717
epoch£º191	 i:7 	 global-step:3827	 l-p:0.13451328873634338
epoch£º191	 i:8 	 global-step:3828	 l-p:0.1063028872013092
epoch£º191	 i:9 	 global-step:3829	 l-p:0.12344861775636673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2075, 3.1587, 3.1886],
        [3.2075, 3.1734, 3.1977],
        [3.2075, 3.2448, 3.0935],
        [3.2075, 3.1455, 3.1781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.15749748051166534 
model_pd.l_d.mean(): -24.58342933654785 
model_pd.lagr.mean(): -24.425931930541992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2289], device='cuda:0')), ('power', tensor([-24.3545], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.15749748051166534
epoch£º192	 i:1 	 global-step:3841	 l-p:0.10264850407838821
epoch£º192	 i:2 	 global-step:3842	 l-p:0.14011813700199127
epoch£º192	 i:3 	 global-step:3843	 l-p:-0.9578726887702942
epoch£º192	 i:4 	 global-step:3844	 l-p:0.1309833824634552
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13147775828838348
epoch£º192	 i:6 	 global-step:3846	 l-p:0.1492232382297516
epoch£º192	 i:7 	 global-step:3847	 l-p:0.18186259269714355
epoch£º192	 i:8 	 global-step:3848	 l-p:0.07793281972408295
epoch£º192	 i:9 	 global-step:3849	 l-p:0.43068429827690125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0452, 2.9992, 2.8094],
        [3.0452, 2.9964, 3.0276],
        [3.0452, 3.0361, 3.0442],
        [3.0452, 3.0446, 3.0452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.18194861710071564 
model_pd.l_d.mean(): -24.782428741455078 
model_pd.lagr.mean(): -24.600481033325195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1955], device='cuda:0')), ('power', tensor([-24.5869], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.18194861710071564
epoch£º193	 i:1 	 global-step:3861	 l-p:0.11753491312265396
epoch£º193	 i:2 	 global-step:3862	 l-p:0.14176827669143677
epoch£º193	 i:3 	 global-step:3863	 l-p:0.11550235748291016
epoch£º193	 i:4 	 global-step:3864	 l-p:0.14454804360866547
epoch£º193	 i:5 	 global-step:3865	 l-p:0.09383530169725418
epoch£º193	 i:6 	 global-step:3866	 l-p:0.10606881976127625
epoch£º193	 i:7 	 global-step:3867	 l-p:0.1218988224864006
epoch£º193	 i:8 	 global-step:3868	 l-p:0.1206011101603508
epoch£º193	 i:9 	 global-step:3869	 l-p:0.13153910636901855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8896, 2.8896, 2.8896],
        [2.8896, 2.7082, 2.4932],
        [2.8896, 2.6858, 2.6119],
        [2.8896, 2.8044, 2.8435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.13115356862545013 
model_pd.l_d.mean(): -24.98630142211914 
model_pd.lagr.mean(): -24.855148315429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1561], device='cuda:0')), ('power', tensor([-24.8302], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.13115356862545013
epoch£º194	 i:1 	 global-step:3881	 l-p:0.11336170136928558
epoch£º194	 i:2 	 global-step:3882	 l-p:0.13318346440792084
epoch£º194	 i:3 	 global-step:3883	 l-p:0.125413715839386
epoch£º194	 i:4 	 global-step:3884	 l-p:0.1538291722536087
epoch£º194	 i:5 	 global-step:3885	 l-p:0.12387194484472275
epoch£º194	 i:6 	 global-step:3886	 l-p:0.14664757251739502
epoch£º194	 i:7 	 global-step:3887	 l-p:0.1335446685552597
epoch£º194	 i:8 	 global-step:3888	 l-p:0.14836570620536804
epoch£º194	 i:9 	 global-step:3889	 l-p:0.10290294140577316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7352, 2.5935, 2.6224],
        [2.7352, 2.5047, 2.2858],
        [2.7352, 2.7232, 2.7338],
        [2.7352, 2.4907, 2.2970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.10726812481880188 
model_pd.l_d.mean(): -24.78244972229004 
model_pd.lagr.mean(): -24.675182342529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0258], device='cuda:0')), ('power', tensor([-24.7566], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.10726812481880188
epoch£º195	 i:1 	 global-step:3901	 l-p:-0.34092631936073303
epoch£º195	 i:2 	 global-step:3902	 l-p:0.1388247311115265
epoch£º195	 i:3 	 global-step:3903	 l-p:0.11640964448451996
epoch£º195	 i:4 	 global-step:3904	 l-p:0.11852370947599411
epoch£º195	 i:5 	 global-step:3905	 l-p:0.12944887578487396
epoch£º195	 i:6 	 global-step:3906	 l-p:0.12165700644254684
epoch£º195	 i:7 	 global-step:3907	 l-p:0.10546711832284927
epoch£º195	 i:8 	 global-step:3908	 l-p:0.12702831625938416
epoch£º195	 i:9 	 global-step:3909	 l-p:-0.3418399393558502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7653, 2.7653, 2.7653],
        [2.7653, 2.7373, 2.7594],
        [2.7653, 2.7639, 2.7653],
        [2.7653, 2.7435, 2.7615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.44269004464149475 
model_pd.l_d.mean(): -24.41751480102539 
model_pd.lagr.mean(): -23.974824905395508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0474], device='cuda:0')), ('power', tensor([-24.4649], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.44269004464149475
epoch£º196	 i:1 	 global-step:3921	 l-p:0.10088479518890381
epoch£º196	 i:2 	 global-step:3922	 l-p:0.08471328765153885
epoch£º196	 i:3 	 global-step:3923	 l-p:0.1347251832485199
epoch£º196	 i:4 	 global-step:3924	 l-p:0.16371673345565796
epoch£º196	 i:5 	 global-step:3925	 l-p:0.13058757781982422
epoch£º196	 i:6 	 global-step:3926	 l-p:0.10813803225755692
epoch£º196	 i:7 	 global-step:3927	 l-p:0.08249480277299881
epoch£º196	 i:8 	 global-step:3928	 l-p:0.12541528046131134
epoch£º196	 i:9 	 global-step:3929	 l-p:0.0921100303530693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9260, 2.9236, 2.9259],
        [2.9260, 2.7046, 2.4991],
        [2.9260, 2.9260, 2.9260],
        [2.9260, 2.8721, 2.9070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.12583008408546448 
model_pd.l_d.mean(): -24.177196502685547 
model_pd.lagr.mean(): -24.051366806030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0281], device='cuda:0')), ('power', tensor([-24.1491], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.12583008408546448
epoch£º197	 i:1 	 global-step:3941	 l-p:0.11987674236297607
epoch£º197	 i:2 	 global-step:3942	 l-p:0.35388967394828796
epoch£º197	 i:3 	 global-step:3943	 l-p:0.11334222555160522
epoch£º197	 i:4 	 global-step:3944	 l-p:0.13509555160999298
epoch£º197	 i:5 	 global-step:3945	 l-p:0.12269561737775803
epoch£º197	 i:6 	 global-step:3946	 l-p:0.10749665647745132
epoch£º197	 i:7 	 global-step:3947	 l-p:0.11296459287405014
epoch£º197	 i:8 	 global-step:3948	 l-p:0.19267702102661133
epoch£º197	 i:9 	 global-step:3949	 l-p:0.10976387560367584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6328, 2.5551, 2.5992],
        [2.6328, 2.6320, 2.6328],
        [2.6328, 2.6328, 2.6328],
        [2.6328, 2.6267, 2.6323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.1313188672065735 
model_pd.l_d.mean(): -24.884239196777344 
model_pd.lagr.mean(): -24.752920150756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0007], device='cuda:0')), ('power', tensor([-24.8849], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.1313188672065735
epoch£º198	 i:1 	 global-step:3961	 l-p:0.12355665862560272
epoch£º198	 i:2 	 global-step:3962	 l-p:0.1463758498430252
epoch£º198	 i:3 	 global-step:3963	 l-p:0.10403815656900406
epoch£º198	 i:4 	 global-step:3964	 l-p:0.13945285975933075
epoch£º198	 i:5 	 global-step:3965	 l-p:0.11975755542516708
epoch£º198	 i:6 	 global-step:3966	 l-p:0.13206109404563904
epoch£º198	 i:7 	 global-step:3967	 l-p:0.12606213986873627
epoch£º198	 i:8 	 global-step:3968	 l-p:0.21630440652370453
epoch£º198	 i:9 	 global-step:3969	 l-p:0.10996322333812714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8046, 2.7986, 2.8042],
        [2.8046, 2.7564, 2.7900],
        [2.8046, 2.8046, 2.8046],
        [2.8046, 2.6011, 2.5847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.13168171048164368 
model_pd.l_d.mean(): -24.20577621459961 
model_pd.lagr.mean(): -24.074094772338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0034], device='cuda:0')), ('power', tensor([-24.2092], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.13168171048164368
epoch£º199	 i:1 	 global-step:3981	 l-p:-0.02221176028251648
epoch£º199	 i:2 	 global-step:3982	 l-p:0.03794408589601517
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12427711486816406
epoch£º199	 i:4 	 global-step:3984	 l-p:0.08626704663038254
epoch£º199	 i:5 	 global-step:3985	 l-p:0.1312546730041504
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11883869767189026
epoch£º199	 i:7 	 global-step:3987	 l-p:0.12646210193634033
epoch£º199	 i:8 	 global-step:3988	 l-p:0.015085391700267792
epoch£º199	 i:9 	 global-step:3989	 l-p:0.10546109825372696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1241, 3.1241, 3.1241],
        [3.1241, 3.1210, 3.1240],
        [3.1241, 3.0735, 3.1072],
        [3.1241, 2.9013, 2.7195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.1155211329460144 
model_pd.l_d.mean(): -24.785865783691406 
model_pd.lagr.mean(): -24.670345306396484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2255], device='cuda:0')), ('power', tensor([-24.5604], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.1155211329460144
epoch£º200	 i:1 	 global-step:4001	 l-p:0.12259679287672043
epoch£º200	 i:2 	 global-step:4002	 l-p:0.11453870683908463
epoch£º200	 i:3 	 global-step:4003	 l-p:0.12533259391784668
epoch£º200	 i:4 	 global-step:4004	 l-p:0.15037333965301514
epoch£º200	 i:5 	 global-step:4005	 l-p:0.13402383029460907
epoch£º200	 i:6 	 global-step:4006	 l-p:0.055106136947870255
epoch£º200	 i:7 	 global-step:4007	 l-p:0.11221679300069809
epoch£º200	 i:8 	 global-step:4008	 l-p:0.12506942451000214
epoch£º200	 i:9 	 global-step:4009	 l-p:0.16933591663837433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7061, 2.7052, 2.7060],
        [2.7061, 2.4328, 2.1863],
        [2.7061, 2.7060, 2.7061],
        [2.7061, 2.5050, 2.5075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.13716678321361542 
model_pd.l_d.mean(): -24.022144317626953 
model_pd.lagr.mean(): -23.884977340698242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1311], device='cuda:0')), ('power', tensor([-24.1532], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.13716678321361542
epoch£º201	 i:1 	 global-step:4021	 l-p:0.06785771995782852
epoch£º201	 i:2 	 global-step:4022	 l-p:0.10718289762735367
epoch£º201	 i:3 	 global-step:4023	 l-p:0.1456211358308792
epoch£º201	 i:4 	 global-step:4024	 l-p:0.13033495843410492
epoch£º201	 i:5 	 global-step:4025	 l-p:0.12489283084869385
epoch£º201	 i:6 	 global-step:4026	 l-p:0.15033835172653198
epoch£º201	 i:7 	 global-step:4027	 l-p:0.07127267867326736
epoch£º201	 i:8 	 global-step:4028	 l-p:0.13544180989265442
epoch£º201	 i:9 	 global-step:4029	 l-p:0.13050924241542816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8659, 2.8639, 2.8658],
        [2.8659, 2.8651, 2.8659],
        [2.8659, 2.6795, 2.6857],
        [2.8659, 2.8074, 2.8456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.10900309681892395 
model_pd.l_d.mean(): -24.631175994873047 
model_pd.lagr.mean(): -24.522172927856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0847], device='cuda:0')), ('power', tensor([-24.5465], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.10900309681892395
epoch£º202	 i:1 	 global-step:4041	 l-p:0.026291364803910255
epoch£º202	 i:2 	 global-step:4042	 l-p:0.1121848002076149
epoch£º202	 i:3 	 global-step:4043	 l-p:0.12635910511016846
epoch£º202	 i:4 	 global-step:4044	 l-p:0.10590405017137527
epoch£º202	 i:5 	 global-step:4045	 l-p:0.1252785176038742
epoch£º202	 i:6 	 global-step:4046	 l-p:0.1993691623210907
epoch£º202	 i:7 	 global-step:4047	 l-p:0.1468552201986313
epoch£º202	 i:8 	 global-step:4048	 l-p:-0.05679258331656456
epoch£º202	 i:9 	 global-step:4049	 l-p:0.12347003072500229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8344, 2.5988, 2.3395],
        [2.8344, 2.6367, 2.6358],
        [2.8344, 2.6776, 2.7094],
        [2.8344, 2.8324, 2.8344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.12201759964227676 
model_pd.l_d.mean(): -24.155704498291016 
model_pd.lagr.mean(): -24.033687591552734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0133], device='cuda:0')), ('power', tensor([-24.1424], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.12201759964227676
epoch£º203	 i:1 	 global-step:4061	 l-p:0.13468556106090546
epoch£º203	 i:2 	 global-step:4062	 l-p:0.12342803925275803
epoch£º203	 i:3 	 global-step:4063	 l-p:0.1164809986948967
epoch£º203	 i:4 	 global-step:4064	 l-p:0.14193636178970337
epoch£º203	 i:5 	 global-step:4065	 l-p:0.5709214806556702
epoch£º203	 i:6 	 global-step:4066	 l-p:0.07590131461620331
epoch£º203	 i:7 	 global-step:4067	 l-p:0.10863950848579407
epoch£º203	 i:8 	 global-step:4068	 l-p:0.12705974280834198
epoch£º203	 i:9 	 global-step:4069	 l-p:0.1353309452533722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6382, 2.3502, 2.0886],
        [2.6382, 2.6382, 2.6382],
        [2.6382, 2.3482, 2.0885],
        [2.6382, 2.4107, 2.3969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.09529569745063782 
model_pd.l_d.mean(): -24.838970184326172 
model_pd.lagr.mean(): -24.743675231933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0567], device='cuda:0')), ('power', tensor([-24.8956], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.09529569745063782
epoch£º204	 i:1 	 global-step:4081	 l-p:0.1468888372182846
epoch£º204	 i:2 	 global-step:4082	 l-p:0.14114727079868317
epoch£º204	 i:3 	 global-step:4083	 l-p:-0.04745027422904968
epoch£º204	 i:4 	 global-step:4084	 l-p:0.13949988782405853
epoch£º204	 i:5 	 global-step:4085	 l-p:0.12747593224048615
epoch£º204	 i:6 	 global-step:4086	 l-p:0.11727108806371689
epoch£º204	 i:7 	 global-step:4087	 l-p:0.06923352181911469
epoch£º204	 i:8 	 global-step:4088	 l-p:0.13607607781887054
epoch£º204	 i:9 	 global-step:4089	 l-p:0.12025677412748337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7552, 2.5758, 2.6007],
        [2.7552, 2.7552, 2.7552],
        [2.7552, 2.7536, 2.7551],
        [2.7552, 2.7552, 2.7552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.11124931275844574 
model_pd.l_d.mean(): -24.479249954223633 
model_pd.lagr.mean(): -24.368000030517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0186], device='cuda:0')), ('power', tensor([-24.4978], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.11124931275844574
epoch£º205	 i:1 	 global-step:4101	 l-p:0.13305936753749847
epoch£º205	 i:2 	 global-step:4102	 l-p:0.10024518519639969
epoch£º205	 i:3 	 global-step:4103	 l-p:0.12132423371076584
epoch£º205	 i:4 	 global-step:4104	 l-p:0.1248200461268425
epoch£º205	 i:5 	 global-step:4105	 l-p:0.09862644225358963
epoch£º205	 i:6 	 global-step:4106	 l-p:0.12661725282669067
epoch£º205	 i:7 	 global-step:4107	 l-p:0.13409438729286194
epoch£º205	 i:8 	 global-step:4108	 l-p:0.13167399168014526
epoch£º205	 i:9 	 global-step:4109	 l-p:0.1843055933713913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7762, 2.6451, 2.6922],
        [2.7762, 2.6536, 2.3901],
        [2.7762, 2.5143, 2.2427],
        [2.7762, 2.7762, 2.7762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.11064876616001129 
model_pd.l_d.mean(): -24.753726959228516 
model_pd.lagr.mean(): -24.643077850341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0118], device='cuda:0')), ('power', tensor([-24.7656], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.11064876616001129
epoch£º206	 i:1 	 global-step:4121	 l-p:0.11260076612234116
epoch£º206	 i:2 	 global-step:4122	 l-p:0.04474514722824097
epoch£º206	 i:3 	 global-step:4123	 l-p:-0.11940500885248184
epoch£º206	 i:4 	 global-step:4124	 l-p:0.10058385878801346
epoch£º206	 i:5 	 global-step:4125	 l-p:0.16977842152118683
epoch£º206	 i:6 	 global-step:4126	 l-p:0.12436531484127045
epoch£º206	 i:7 	 global-step:4127	 l-p:0.12398520857095718
epoch£º206	 i:8 	 global-step:4128	 l-p:0.1226172223687172
epoch£º206	 i:9 	 global-step:4129	 l-p:0.10371015965938568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9932, 2.9028, 2.9489],
        [2.9932, 2.9932, 2.9932],
        [2.9932, 2.7254, 2.5501],
        [2.9932, 2.7471, 2.6625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.11774679273366928 
model_pd.l_d.mean(): -24.53158187866211 
model_pd.lagr.mean(): -24.413835525512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1412], device='cuda:0')), ('power', tensor([-24.3904], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.11774679273366928
epoch£º207	 i:1 	 global-step:4141	 l-p:0.13476262986660004
epoch£º207	 i:2 	 global-step:4142	 l-p:0.12037918716669083
epoch£º207	 i:3 	 global-step:4143	 l-p:0.058623041957616806
epoch£º207	 i:4 	 global-step:4144	 l-p:0.11311565339565277
epoch£º207	 i:5 	 global-step:4145	 l-p:0.1149464026093483
epoch£º207	 i:6 	 global-step:4146	 l-p:0.03711744025349617
epoch£º207	 i:7 	 global-step:4147	 l-p:0.1306280642747879
epoch£º207	 i:8 	 global-step:4148	 l-p:0.22006218135356903
epoch£º207	 i:9 	 global-step:4149	 l-p:0.13290061056613922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6435, 2.6422, 2.6434],
        [2.6435, 2.3372, 2.0653],
        [2.6435, 2.3806, 2.3330],
        [2.6435, 2.6435, 2.6435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.132976695895195 
model_pd.l_d.mean(): -25.044063568115234 
model_pd.lagr.mean(): -24.911087036132812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0389], device='cuda:0')), ('power', tensor([-25.0051], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.132976695895195
epoch£º208	 i:1 	 global-step:4161	 l-p:0.1305026262998581
epoch£º208	 i:2 	 global-step:4162	 l-p:0.0459434911608696
epoch£º208	 i:3 	 global-step:4163	 l-p:0.12539702653884888
epoch£º208	 i:4 	 global-step:4164	 l-p:0.09823598712682724
epoch£º208	 i:5 	 global-step:4165	 l-p:0.1263638585805893
epoch£º208	 i:6 	 global-step:4166	 l-p:0.14160002768039703
epoch£º208	 i:7 	 global-step:4167	 l-p:0.12880069017410278
epoch£º208	 i:8 	 global-step:4168	 l-p:0.14726920425891876
epoch£º208	 i:9 	 global-step:4169	 l-p:0.14853474497795105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[2.6468, 2.3188, 2.0931],
        [2.6468, 2.4872, 2.1995],
        [2.6468, 2.4708, 2.5094],
        [2.6468, 2.3264, 2.1568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.14389802515506744 
model_pd.l_d.mean(): -24.8110294342041 
model_pd.lagr.mean(): -24.667131423950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1079], device='cuda:0')), ('power', tensor([-24.9189], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.14389802515506744
epoch£º209	 i:1 	 global-step:4181	 l-p:0.1409849226474762
epoch£º209	 i:2 	 global-step:4182	 l-p:0.13580313324928284
epoch£º209	 i:3 	 global-step:4183	 l-p:0.13033199310302734
epoch£º209	 i:4 	 global-step:4184	 l-p:0.1519661843776703
epoch£º209	 i:5 	 global-step:4185	 l-p:0.1298934519290924
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13539808988571167
epoch£º209	 i:7 	 global-step:4187	 l-p:0.15768879652023315
epoch£º209	 i:8 	 global-step:4188	 l-p:0.12599405646324158
epoch£º209	 i:9 	 global-step:4189	 l-p:0.1280231773853302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6251, 2.6251, 2.6251],
        [2.6251, 2.6251, 2.6251],
        [2.6251, 2.3399, 2.2726],
        [2.6251, 2.6251, 2.6251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.14586900174617767 
model_pd.l_d.mean(): -24.969806671142578 
model_pd.lagr.mean(): -24.823938369750977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0228], device='cuda:0')), ('power', tensor([-24.9926], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.14586900174617767
epoch£º210	 i:1 	 global-step:4201	 l-p:0.13257265090942383
epoch£º210	 i:2 	 global-step:4202	 l-p:0.13441133499145508
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12241750210523605
epoch£º210	 i:4 	 global-step:4204	 l-p:0.15443606674671173
epoch£º210	 i:5 	 global-step:4205	 l-p:0.19401343166828156
epoch£º210	 i:6 	 global-step:4206	 l-p:0.14252163469791412
epoch£º210	 i:7 	 global-step:4207	 l-p:0.11654522269964218
epoch£º210	 i:8 	 global-step:4208	 l-p:0.15872682631015778
epoch£º210	 i:9 	 global-step:4209	 l-p:0.16356158256530762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0494, 2.7759, 2.5366],
        [3.0494, 3.0493, 3.0494],
        [3.0494, 2.8083, 2.7551],
        [3.0494, 3.0494, 3.0494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.24383439123630524 
model_pd.l_d.mean(): -24.420215606689453 
model_pd.lagr.mean(): -24.176382064819336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1428], device='cuda:0')), ('power', tensor([-24.2774], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.24383439123630524
epoch£º211	 i:1 	 global-step:4221	 l-p:0.12128697335720062
epoch£º211	 i:2 	 global-step:4222	 l-p:0.10766827315092087
epoch£º211	 i:3 	 global-step:4223	 l-p:-0.053954627364873886
epoch£º211	 i:4 	 global-step:4224	 l-p:0.0989595279097557
epoch£º211	 i:5 	 global-step:4225	 l-p:0.1030520424246788
epoch£º211	 i:6 	 global-step:4226	 l-p:0.11570332944393158
epoch£º211	 i:7 	 global-step:4227	 l-p:0.04752427339553833
epoch£º211	 i:8 	 global-step:4228	 l-p:0.1221935972571373
epoch£º211	 i:9 	 global-step:4229	 l-p:0.14195598661899567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1554, 2.9182, 2.6573],
        [3.1554, 3.1532, 3.1553],
        [3.1554, 3.1554, 3.1554],
        [3.1554, 2.9099, 2.6558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.11921917647123337 
model_pd.l_d.mean(): -24.9617977142334 
model_pd.lagr.mean(): -24.842578887939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2870], device='cuda:0')), ('power', tensor([-24.6748], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.11921917647123337
epoch£º212	 i:1 	 global-step:4241	 l-p:0.15327057242393494
epoch£º212	 i:2 	 global-step:4242	 l-p:0.09924235194921494
epoch£º212	 i:3 	 global-step:4243	 l-p:0.08467031270265579
epoch£º212	 i:4 	 global-step:4244	 l-p:0.12945404648780823
epoch£º212	 i:5 	 global-step:4245	 l-p:0.12350581586360931
epoch£º212	 i:6 	 global-step:4246	 l-p:0.07796686887741089
epoch£º212	 i:7 	 global-step:4247	 l-p:0.17585191130638123
epoch£º212	 i:8 	 global-step:4248	 l-p:0.145046666264534
epoch£º212	 i:9 	 global-step:4249	 l-p:0.27324607968330383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9220, 2.9118, 2.9210],
        [2.9220, 2.9220, 2.9220],
        [2.9220, 2.6469, 2.5587],
        [2.9220, 2.8611, 2.9020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.11041329801082611 
model_pd.l_d.mean(): -24.40281105041504 
model_pd.lagr.mean(): -24.29239845275879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0153], device='cuda:0')), ('power', tensor([-24.3875], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.11041329801082611
epoch£º213	 i:1 	 global-step:4261	 l-p:0.11289264261722565
epoch£º213	 i:2 	 global-step:4262	 l-p:-0.712794303894043
epoch£º213	 i:3 	 global-step:4263	 l-p:0.1372377574443817
epoch£º213	 i:4 	 global-step:4264	 l-p:0.11230970174074173
epoch£º213	 i:5 	 global-step:4265	 l-p:0.11796583980321884
epoch£º213	 i:6 	 global-step:4266	 l-p:0.17943242192268372
epoch£º213	 i:7 	 global-step:4267	 l-p:-0.009900481440126896
epoch£º213	 i:8 	 global-step:4268	 l-p:0.13965868949890137
epoch£º213	 i:9 	 global-step:4269	 l-p:0.13126634061336517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8398, 2.8374, 2.8397],
        [2.8398, 2.7097, 2.7619],
        [2.8398, 2.5592, 2.4805],
        [2.8398, 2.8398, 2.8398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.11178159713745117 
model_pd.l_d.mean(): -24.90164566040039 
model_pd.lagr.mean(): -24.78986358642578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0895], device='cuda:0')), ('power', tensor([-24.8121], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.11178159713745117
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12516890466213226
epoch£º214	 i:2 	 global-step:4282	 l-p:0.1280050426721573
epoch£º214	 i:3 	 global-step:4283	 l-p:0.13951973617076874
epoch£º214	 i:4 	 global-step:4284	 l-p:0.1205005869269371
epoch£º214	 i:5 	 global-step:4285	 l-p:0.1401284784078598
epoch£º214	 i:6 	 global-step:4286	 l-p:0.13858342170715332
epoch£º214	 i:7 	 global-step:4287	 l-p:0.160161092877388
epoch£º214	 i:8 	 global-step:4288	 l-p:0.15153838694095612
epoch£º214	 i:9 	 global-step:4289	 l-p:0.14125217497348785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5660, 2.2580, 2.1810],
        [2.5660, 2.5618, 2.5658],
        [2.5660, 2.4881, 2.5374],
        [2.5660, 2.5657, 2.5660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.13639071583747864 
model_pd.l_d.mean(): -24.93185806274414 
model_pd.lagr.mean(): -24.795467376708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0749], device='cuda:0')), ('power', tensor([-25.0068], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.13639071583747864
epoch£º215	 i:1 	 global-step:4301	 l-p:0.13906028866767883
epoch£º215	 i:2 	 global-step:4302	 l-p:0.13612522184848785
epoch£º215	 i:3 	 global-step:4303	 l-p:-0.6637750267982483
epoch£º215	 i:4 	 global-step:4304	 l-p:0.13899587094783783
epoch£º215	 i:5 	 global-step:4305	 l-p:0.13372543454170227
epoch£º215	 i:6 	 global-step:4306	 l-p:-0.022405661642551422
epoch£º215	 i:7 	 global-step:4307	 l-p:0.11075905710458755
epoch£º215	 i:8 	 global-step:4308	 l-p:0.11228742450475693
epoch£º215	 i:9 	 global-step:4309	 l-p:0.1079416275024414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7620, 2.7529, 2.7612],
        [2.7620, 2.4214, 2.1900],
        [2.7620, 2.6740, 2.7254],
        [2.7620, 2.7620, 2.7620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11996030807495117 
model_pd.l_d.mean(): -24.283424377441406 
model_pd.lagr.mean(): -24.163463592529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0163], device='cuda:0')), ('power', tensor([-24.2997], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11996030807495117
epoch£º216	 i:1 	 global-step:4321	 l-p:0.11548084020614624
epoch£º216	 i:2 	 global-step:4322	 l-p:0.1348695456981659
epoch£º216	 i:3 	 global-step:4323	 l-p:0.14766323566436768
epoch£º216	 i:4 	 global-step:4324	 l-p:0.10423921048641205
epoch£º216	 i:5 	 global-step:4325	 l-p:0.14788851141929626
epoch£º216	 i:6 	 global-step:4326	 l-p:0.13031697273254395
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12455381453037262
epoch£º216	 i:8 	 global-step:4328	 l-p:0.13460685312747955
epoch£º216	 i:9 	 global-step:4329	 l-p:0.158054381608963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5788, 2.5108, 2.5568],
        [2.5788, 2.2087, 1.9218],
        [2.5788, 2.2420, 2.1246],
        [2.5788, 2.5788, 2.5788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.14565835893154144 
model_pd.l_d.mean(): -24.98798179626465 
model_pd.lagr.mean(): -24.842323303222656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0372], device='cuda:0')), ('power', tensor([-25.0251], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.14565835893154144
epoch£º217	 i:1 	 global-step:4341	 l-p:0.14767976105213165
epoch£º217	 i:2 	 global-step:4342	 l-p:0.14050258696079254
epoch£º217	 i:3 	 global-step:4343	 l-p:0.10416751354932785
epoch£º217	 i:4 	 global-step:4344	 l-p:0.15244179964065552
epoch£º217	 i:5 	 global-step:4345	 l-p:0.10680224001407623
epoch£º217	 i:6 	 global-step:4346	 l-p:0.09509959071874619
epoch£º217	 i:7 	 global-step:4347	 l-p:0.13297738134860992
epoch£º217	 i:8 	 global-step:4348	 l-p:0.07202598452568054
epoch£º217	 i:9 	 global-step:4349	 l-p:0.09312263131141663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9098, 2.8825, 2.9051],
        [2.9098, 2.9098, 2.9098],
        [2.9098, 2.9084, 2.9098],
        [2.9098, 2.9098, 2.9098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.1394115686416626 
model_pd.l_d.mean(): -24.90945053100586 
model_pd.lagr.mean(): -24.770038604736328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1417], device='cuda:0')), ('power', tensor([-24.7677], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.1394115686416626
epoch£º218	 i:1 	 global-step:4361	 l-p:0.31830650568008423
epoch£º218	 i:2 	 global-step:4362	 l-p:0.15179601311683655
epoch£º218	 i:3 	 global-step:4363	 l-p:0.12493396550416946
epoch£º218	 i:4 	 global-step:4364	 l-p:0.1991451531648636
epoch£º218	 i:5 	 global-step:4365	 l-p:0.05974388122558594
epoch£º218	 i:6 	 global-step:4366	 l-p:0.12217414379119873
epoch£º218	 i:7 	 global-step:4367	 l-p:0.13207125663757324
epoch£º218	 i:8 	 global-step:4368	 l-p:0.12504954636096954
epoch£º218	 i:9 	 global-step:4369	 l-p:0.11917722225189209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2927, 3.2561, 3.2843],
        [3.2927, 3.2281, 3.2699],
        [3.2927, 3.0379, 2.9521],
        [3.2927, 3.3204, 3.0973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.10450002551078796 
model_pd.l_d.mean(): -24.07918357849121 
model_pd.lagr.mean(): -23.97468376159668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2017], device='cuda:0')), ('power', tensor([-23.8774], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.10450002551078796
epoch£º219	 i:1 	 global-step:4381	 l-p:0.08100993931293488
epoch£º219	 i:2 	 global-step:4382	 l-p:0.12057925760746002
epoch£º219	 i:3 	 global-step:4383	 l-p:0.13605105876922607
epoch£º219	 i:4 	 global-step:4384	 l-p:0.19984173774719238
epoch£º219	 i:5 	 global-step:4385	 l-p:-0.04453645274043083
epoch£º219	 i:6 	 global-step:4386	 l-p:0.07992902398109436
epoch£º219	 i:7 	 global-step:4387	 l-p:0.13495485484600067
epoch£º219	 i:8 	 global-step:4388	 l-p:0.11462117731571198
epoch£º219	 i:9 	 global-step:4389	 l-p:0.13787899911403656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0162, 3.0127, 3.0160],
        [3.0162, 2.8220, 2.8503],
        [3.0162, 3.0162, 3.0162],
        [3.0162, 2.9413, 2.9883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.10947497934103012 
model_pd.l_d.mean(): -24.763832092285156 
model_pd.lagr.mean(): -24.65435791015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1597], device='cuda:0')), ('power', tensor([-24.6041], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.10947497934103012
epoch£º220	 i:1 	 global-step:4401	 l-p:0.24805156886577606
epoch£º220	 i:2 	 global-step:4402	 l-p:0.14213769137859344
epoch£º220	 i:3 	 global-step:4403	 l-p:0.12713979184627533
epoch£º220	 i:4 	 global-step:4404	 l-p:0.12457168102264404
epoch£º220	 i:5 	 global-step:4405	 l-p:0.14725252985954285
epoch£º220	 i:6 	 global-step:4406	 l-p:0.1237095445394516
epoch£º220	 i:7 	 global-step:4407	 l-p:0.11192440986633301
epoch£º220	 i:8 	 global-step:4408	 l-p:0.12047065794467926
epoch£º220	 i:9 	 global-step:4409	 l-p:0.13853615522384644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6976, 2.3368, 2.0363],
        [2.6976, 2.3310, 2.1164],
        [2.6976, 2.4157, 2.3849],
        [2.6976, 2.6953, 2.6975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.11273414641618729 
model_pd.l_d.mean(): -24.79096031188965 
model_pd.lagr.mean(): -24.678226470947266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0556], device='cuda:0')), ('power', tensor([-24.8466], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.11273414641618729
epoch£º221	 i:1 	 global-step:4421	 l-p:0.13580289483070374
epoch£º221	 i:2 	 global-step:4422	 l-p:0.1164080947637558
epoch£º221	 i:3 	 global-step:4423	 l-p:0.129398375749588
epoch£º221	 i:4 	 global-step:4424	 l-p:0.14502248167991638
epoch£º221	 i:5 	 global-step:4425	 l-p:0.16872325539588928
epoch£º221	 i:6 	 global-step:4426	 l-p:0.10529901087284088
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13101693987846375
epoch£º221	 i:8 	 global-step:4428	 l-p:0.13702701032161713
epoch£º221	 i:9 	 global-step:4429	 l-p:0.13259559869766235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[2.6064, 2.4059, 2.0747],
        [2.6064, 2.2384, 1.9143],
        [2.6064, 2.3870, 2.0522],
        [2.6064, 2.2297, 1.9158]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.1105426698923111 
model_pd.l_d.mean(): -24.512056350708008 
model_pd.lagr.mean(): -24.401514053344727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1783], device='cuda:0')), ('power', tensor([-24.6904], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.1105426698923111
epoch£º222	 i:1 	 global-step:4441	 l-p:0.1576218605041504
epoch£º222	 i:2 	 global-step:4442	 l-p:0.12809249758720398
epoch£º222	 i:3 	 global-step:4443	 l-p:0.1358909159898758
epoch£º222	 i:4 	 global-step:4444	 l-p:0.10520707815885544
epoch£º222	 i:5 	 global-step:4445	 l-p:0.13928893208503723
epoch£º222	 i:6 	 global-step:4446	 l-p:0.1502751111984253
epoch£º222	 i:7 	 global-step:4447	 l-p:0.13478392362594604
epoch£º222	 i:8 	 global-step:4448	 l-p:0.15412800014019012
epoch£º222	 i:9 	 global-step:4449	 l-p:0.1394706517457962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5702, 2.5641, 2.5698],
        [2.5702, 2.2633, 2.2246],
        [2.5702, 2.2009, 2.0557],
        [2.5702, 2.4844, 2.5384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.16962599754333496 
model_pd.l_d.mean(): -25.059253692626953 
model_pd.lagr.mean(): -24.88962745666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0756], device='cuda:0')), ('power', tensor([-25.1349], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.16962599754333496
epoch£º223	 i:1 	 global-step:4461	 l-p:0.12674713134765625
epoch£º223	 i:2 	 global-step:4462	 l-p:0.13453438878059387
epoch£º223	 i:3 	 global-step:4463	 l-p:0.14577797055244446
epoch£º223	 i:4 	 global-step:4464	 l-p:0.11652001738548279
epoch£º223	 i:5 	 global-step:4465	 l-p:0.15355320274829865
epoch£º223	 i:6 	 global-step:4466	 l-p:0.09592805057764053
epoch£º223	 i:7 	 global-step:4467	 l-p:0.13159015774726868
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13839252293109894
epoch£º223	 i:9 	 global-step:4469	 l-p:0.13557426631450653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7861, 2.7860, 2.7861],
        [2.7861, 2.7861, 2.7861],
        [2.7861, 2.6358, 2.6951],
        [2.7861, 2.7748, 2.7851]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12184255570173264 
model_pd.l_d.mean(): -24.89511489868164 
model_pd.lagr.mean(): -24.773271560668945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1040], device='cuda:0')), ('power', tensor([-24.7911], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12184255570173264
epoch£º224	 i:1 	 global-step:4481	 l-p:0.12285568565130234
epoch£º224	 i:2 	 global-step:4482	 l-p:0.1373991221189499
epoch£º224	 i:3 	 global-step:4483	 l-p:0.12462564557790756
epoch£º224	 i:4 	 global-step:4484	 l-p:0.12932507693767548
epoch£º224	 i:5 	 global-step:4485	 l-p:0.13180804252624512
epoch£º224	 i:6 	 global-step:4486	 l-p:0.1511698067188263
epoch£º224	 i:7 	 global-step:4487	 l-p:0.14225392043590546
epoch£º224	 i:8 	 global-step:4488	 l-p:0.07242819666862488
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12700532376766205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7333, 2.7333, 2.7333],
        [2.7333, 2.7333, 2.7333],
        [2.7333, 2.7333, 2.7333],
        [2.7333, 2.4195, 2.3629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.12271295487880707 
model_pd.l_d.mean(): -24.582910537719727 
model_pd.lagr.mean(): -24.46019744873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0119], device='cuda:0')), ('power', tensor([-24.5948], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.12271295487880707
epoch£º225	 i:1 	 global-step:4501	 l-p:0.1137862578034401
epoch£º225	 i:2 	 global-step:4502	 l-p:0.12061609327793121
epoch£º225	 i:3 	 global-step:4503	 l-p:0.1434771716594696
epoch£º225	 i:4 	 global-step:4504	 l-p:0.09381675720214844
epoch£º225	 i:5 	 global-step:4505	 l-p:0.14445750415325165
epoch£º225	 i:6 	 global-step:4506	 l-p:0.10912033170461655
epoch£º225	 i:7 	 global-step:4507	 l-p:0.11753880977630615
epoch£º225	 i:8 	 global-step:4508	 l-p:0.12347745895385742
epoch£º225	 i:9 	 global-step:4509	 l-p:0.18433482944965363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7977, 2.4606, 2.3602],
        [2.7977, 2.4383, 2.1014],
        [2.7977, 2.5616, 2.5869],
        [2.7977, 2.7963, 2.7977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.11694566160440445 
model_pd.l_d.mean(): -24.958988189697266 
model_pd.lagr.mean(): -24.842042922973633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0475], device='cuda:0')), ('power', tensor([-24.9115], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.11694566160440445
epoch£º226	 i:1 	 global-step:4521	 l-p:0.14320509135723114
epoch£º226	 i:2 	 global-step:4522	 l-p:-0.357888787984848
epoch£º226	 i:3 	 global-step:4523	 l-p:0.11411228030920029
epoch£º226	 i:4 	 global-step:4524	 l-p:0.13514497876167297
epoch£º226	 i:5 	 global-step:4525	 l-p:0.11528389900922775
epoch£º226	 i:6 	 global-step:4526	 l-p:0.0997505709528923
epoch£º226	 i:7 	 global-step:4527	 l-p:0.22484038770198822
epoch£º226	 i:8 	 global-step:4528	 l-p:0.13068987429141998
epoch£º226	 i:9 	 global-step:4529	 l-p:0.10262766480445862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9059, 2.6107, 2.5669],
        [2.9059, 2.8875, 2.9036],
        [2.9059, 2.7735, 2.8342],
        [2.9059, 2.9036, 2.9058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.1317244917154312 
model_pd.l_d.mean(): -24.683481216430664 
model_pd.lagr.mean(): -24.551755905151367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0641], device='cuda:0')), ('power', tensor([-24.6193], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.1317244917154312
epoch£º227	 i:1 	 global-step:4541	 l-p:0.10880532115697861
epoch£º227	 i:2 	 global-step:4542	 l-p:0.12314900010824203
epoch£º227	 i:3 	 global-step:4543	 l-p:0.12622764706611633
epoch£º227	 i:4 	 global-step:4544	 l-p:0.06450002640485764
epoch£º227	 i:5 	 global-step:4545	 l-p:0.1300826221704483
epoch£º227	 i:6 	 global-step:4546	 l-p:0.06846468150615692
epoch£º227	 i:7 	 global-step:4547	 l-p:0.3512738049030304
epoch£º227	 i:8 	 global-step:4548	 l-p:0.10699289292097092
epoch£º227	 i:9 	 global-step:4549	 l-p:0.14963872730731964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9114, 2.9113, 2.9114],
        [2.9114, 2.9110, 2.9114],
        [2.9114, 2.9071, 2.9112],
        [2.9114, 2.7551, 2.8141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.1343401074409485 
model_pd.l_d.mean(): -24.899188995361328 
model_pd.lagr.mean(): -24.764848709106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0914], device='cuda:0')), ('power', tensor([-24.8078], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.1343401074409485
epoch£º228	 i:1 	 global-step:4561	 l-p:0.1192520335316658
epoch£º228	 i:2 	 global-step:4562	 l-p:0.12946997582912445
epoch£º228	 i:3 	 global-step:4563	 l-p:0.10855202376842499
epoch£º228	 i:4 	 global-step:4564	 l-p:0.13496574759483337
epoch£º228	 i:5 	 global-step:4565	 l-p:0.09788653999567032
epoch£º228	 i:6 	 global-step:4566	 l-p:0.1439458578824997
epoch£º228	 i:7 	 global-step:4567	 l-p:0.13680589199066162
epoch£º228	 i:8 	 global-step:4568	 l-p:0.1519913375377655
epoch£º228	 i:9 	 global-step:4569	 l-p:0.1357593536376953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6183, 2.6140, 2.6181],
        [2.6183, 2.2186, 1.8664],
        [2.6183, 2.5106, 2.5723],
        [2.6183, 2.5365, 2.5902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.1393764615058899 
model_pd.l_d.mean(): -24.10763931274414 
model_pd.lagr.mean(): -23.968263626098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1724], device='cuda:0')), ('power', tensor([-24.2800], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.1393764615058899
epoch£º229	 i:1 	 global-step:4581	 l-p:0.1355089396238327
epoch£º229	 i:2 	 global-step:4582	 l-p:0.14054125547409058
epoch£º229	 i:3 	 global-step:4583	 l-p:0.15795718133449554
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11484792083501816
epoch£º229	 i:5 	 global-step:4585	 l-p:0.1222195029258728
epoch£º229	 i:6 	 global-step:4586	 l-p:0.14726215600967407
epoch£º229	 i:7 	 global-step:4587	 l-p:0.12480289489030838
epoch£º229	 i:8 	 global-step:4588	 l-p:0.11422597616910934
epoch£º229	 i:9 	 global-step:4589	 l-p:0.09659348428249359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[2.6394, 2.5037, 2.5701],
        [2.6394, 2.3742, 2.0098],
        [2.6394, 2.4749, 2.5407],
        [2.6394, 2.2491, 2.1010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.11512263119220734 
model_pd.l_d.mean(): -24.297021865844727 
model_pd.lagr.mean(): -24.181900024414062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1339], device='cuda:0')), ('power', tensor([-24.4309], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.11512263119220734
epoch£º230	 i:1 	 global-step:4601	 l-p:0.13665376603603363
epoch£º230	 i:2 	 global-step:4602	 l-p:0.11489122360944748
epoch£º230	 i:3 	 global-step:4603	 l-p:0.13208262622356415
epoch£º230	 i:4 	 global-step:4604	 l-p:0.12316320091485977
epoch£º230	 i:5 	 global-step:4605	 l-p:0.11681012064218521
epoch£º230	 i:6 	 global-step:4606	 l-p:0.14284084737300873
epoch£º230	 i:7 	 global-step:4607	 l-p:0.13588060438632965
epoch£º230	 i:8 	 global-step:4608	 l-p:0.14047394692897797
epoch£º230	 i:9 	 global-step:4609	 l-p:0.1397830694913864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6644, 2.6644, 2.6644],
        [2.6644, 2.6593, 2.6641],
        [2.6644, 2.3853, 2.0149],
        [2.6644, 2.4277, 2.4704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.11565390974283218 
model_pd.l_d.mean(): -24.78421974182129 
model_pd.lagr.mean(): -24.66856575012207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0960], device='cuda:0')), ('power', tensor([-24.8803], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.11565390974283218
epoch£º231	 i:1 	 global-step:4621	 l-p:0.16321106255054474
epoch£º231	 i:2 	 global-step:4622	 l-p:0.12487388402223587
epoch£º231	 i:3 	 global-step:4623	 l-p:0.11311730742454529
epoch£º231	 i:4 	 global-step:4624	 l-p:0.13966448605060577
epoch£º231	 i:5 	 global-step:4625	 l-p:0.1568399965763092
epoch£º231	 i:6 	 global-step:4626	 l-p:0.11173097044229507
epoch£º231	 i:7 	 global-step:4627	 l-p:0.12607966363430023
epoch£º231	 i:8 	 global-step:4628	 l-p:0.1487838327884674
epoch£º231	 i:9 	 global-step:4629	 l-p:0.13576176762580872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7238, 2.7222, 2.7237],
        [2.7238, 2.5473, 2.6122],
        [2.7238, 2.2939, 1.9723],
        [2.7238, 2.6929, 2.7187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.13075265288352966 
model_pd.l_d.mean(): -25.0001277923584 
model_pd.lagr.mean(): -24.869375228881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0025], device='cuda:0')), ('power', tensor([-25.0027], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.13075265288352966
epoch£º232	 i:1 	 global-step:4641	 l-p:0.01118175033479929
epoch£º232	 i:2 	 global-step:4642	 l-p:0.14319507777690887
epoch£º232	 i:3 	 global-step:4643	 l-p:0.1332072764635086
epoch£º232	 i:4 	 global-step:4644	 l-p:0.13793927431106567
epoch£º232	 i:5 	 global-step:4645	 l-p:0.1202431246638298
epoch£º232	 i:6 	 global-step:4646	 l-p:0.13753806054592133
epoch£º232	 i:7 	 global-step:4647	 l-p:0.11908457428216934
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11862851679325104
epoch£º232	 i:9 	 global-step:4649	 l-p:0.13121597468852997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6772, 2.4646, 2.5229],
        [2.6772, 2.5434, 2.6117],
        [2.6772, 2.2361, 1.9793],
        [2.6772, 2.3425, 1.9589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.11727231740951538 
model_pd.l_d.mean(): -24.770166397094727 
model_pd.lagr.mean(): -24.652894973754883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1284], device='cuda:0')), ('power', tensor([-24.8985], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.11727231740951538
epoch£º233	 i:1 	 global-step:4661	 l-p:0.14189715683460236
epoch£º233	 i:2 	 global-step:4662	 l-p:0.10018862783908844
epoch£º233	 i:3 	 global-step:4663	 l-p:0.13752995431423187
epoch£º233	 i:4 	 global-step:4664	 l-p:0.12168323248624802
epoch£º233	 i:5 	 global-step:4665	 l-p:0.14187854528427124
epoch£º233	 i:6 	 global-step:4666	 l-p:0.12266967445611954
epoch£º233	 i:7 	 global-step:4667	 l-p:0.14568157494068146
epoch£º233	 i:8 	 global-step:4668	 l-p:0.1317531019449234
epoch£º233	 i:9 	 global-step:4669	 l-p:0.14144377410411835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5677, 2.5277, 2.5601],
        [2.5677, 2.4740, 2.5342],
        [2.5677, 2.4429, 2.5118],
        [2.5677, 2.5670, 2.5677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1268889158964157 
model_pd.l_d.mean(): -24.750295639038086 
model_pd.lagr.mean(): -24.6234073638916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1566], device='cuda:0')), ('power', tensor([-24.9069], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1268889158964157
epoch£º234	 i:1 	 global-step:4681	 l-p:0.20444932579994202
epoch£º234	 i:2 	 global-step:4682	 l-p:0.1283578872680664
epoch£º234	 i:3 	 global-step:4683	 l-p:0.11859039217233658
epoch£º234	 i:4 	 global-step:4684	 l-p:0.14386257529258728
epoch£º234	 i:5 	 global-step:4685	 l-p:0.15190432965755463
epoch£º234	 i:6 	 global-step:4686	 l-p:0.12576867640018463
epoch£º234	 i:7 	 global-step:4687	 l-p:0.12396737933158875
epoch£º234	 i:8 	 global-step:4688	 l-p:0.12576797604560852
epoch£º234	 i:9 	 global-step:4689	 l-p:0.1452130228281021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6551, 2.2533, 1.8622],
        [2.6551, 2.6550, 2.6551],
        [2.6551, 2.6060, 2.6443],
        [2.6551, 2.6550, 2.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13422849774360657 
model_pd.l_d.mean(): -24.9443359375 
model_pd.lagr.mean(): -24.810108184814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0166], device='cuda:0')), ('power', tensor([-24.9610], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13422849774360657
epoch£º235	 i:1 	 global-step:4701	 l-p:0.15605121850967407
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13082391023635864
epoch£º235	 i:3 	 global-step:4703	 l-p:0.16628170013427734
epoch£º235	 i:4 	 global-step:4704	 l-p:0.1538545936346054
epoch£º235	 i:5 	 global-step:4705	 l-p:0.16160538792610168
epoch£º235	 i:6 	 global-step:4706	 l-p:0.11198809742927551
epoch£º235	 i:7 	 global-step:4707	 l-p:0.12735089659690857
epoch£º235	 i:8 	 global-step:4708	 l-p:0.1424190253019333
epoch£º235	 i:9 	 global-step:4709	 l-p:0.09806965291500092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7942, 2.3650, 2.1674],
        [2.7942, 2.7880, 2.7938],
        [2.7942, 2.4125, 2.3151],
        [2.7942, 2.6196, 2.6894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.09782025218009949 
model_pd.l_d.mean(): -24.93896484375 
model_pd.lagr.mean(): -24.841144561767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0137], device='cuda:0')), ('power', tensor([-24.9253], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.09782025218009949
epoch£º236	 i:1 	 global-step:4721	 l-p:0.11904720216989517
epoch£º236	 i:2 	 global-step:4722	 l-p:0.09402840584516525
epoch£º236	 i:3 	 global-step:4723	 l-p:0.15174615383148193
epoch£º236	 i:4 	 global-step:4724	 l-p:0.658778190612793
epoch£º236	 i:5 	 global-step:4725	 l-p:0.09640881419181824
epoch£º236	 i:6 	 global-step:4726	 l-p:0.11694833636283875
epoch£º236	 i:7 	 global-step:4727	 l-p:0.13227765262126923
epoch£º236	 i:8 	 global-step:4728	 l-p:0.038469210267066956
epoch£º236	 i:9 	 global-step:4729	 l-p:0.10796493291854858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9170, 2.6018, 2.1989],
        [2.9170, 2.6010, 2.5839],
        [2.9170, 2.9170, 2.9170],
        [2.9170, 2.5432, 2.4442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.20008526742458344 
model_pd.l_d.mean(): -24.518287658691406 
model_pd.lagr.mean(): -24.31820297241211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0117], device='cuda:0')), ('power', tensor([-24.5066], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.20008526742458344
epoch£º237	 i:1 	 global-step:4741	 l-p:0.1646943837404251
epoch£º237	 i:2 	 global-step:4742	 l-p:0.2097204476594925
epoch£º237	 i:3 	 global-step:4743	 l-p:0.12214872986078262
epoch£º237	 i:4 	 global-step:4744	 l-p:0.10491038858890533
epoch£º237	 i:5 	 global-step:4745	 l-p:0.26030775904655457
epoch£º237	 i:6 	 global-step:4746	 l-p:0.12104060500860214
epoch£º237	 i:7 	 global-step:4747	 l-p:0.06982529908418655
epoch£º237	 i:8 	 global-step:4748	 l-p:0.11254416406154633
epoch£º237	 i:9 	 global-step:4749	 l-p:0.1388004571199417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2770, 3.1210, 2.7612],
        [3.2770, 3.0485, 3.0847],
        [3.2770, 2.9347, 2.5687],
        [3.2770, 3.2770, 3.2770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.05320024490356445 
model_pd.l_d.mean(): -23.517236709594727 
model_pd.lagr.mean(): -23.46403694152832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0339], device='cuda:0')), ('power', tensor([-23.4834], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.05320024490356445
epoch£º238	 i:1 	 global-step:4761	 l-p:0.10847941786050797
epoch£º238	 i:2 	 global-step:4762	 l-p:0.11890054494142532
epoch£º238	 i:3 	 global-step:4763	 l-p:0.1281396448612213
epoch£º238	 i:4 	 global-step:4764	 l-p:0.10931448638439178
epoch£º238	 i:5 	 global-step:4765	 l-p:0.13600336015224457
epoch£º238	 i:6 	 global-step:4766	 l-p:0.1356881558895111
epoch£º238	 i:7 	 global-step:4767	 l-p:0.12399806827306747
epoch£º238	 i:8 	 global-step:4768	 l-p:0.11147837340831757
epoch£º238	 i:9 	 global-step:4769	 l-p:0.12003278732299805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[2.9707, 2.5580, 2.1783],
        [2.9707, 2.6764, 2.2735],
        [2.9707, 2.6562, 2.6388],
        [2.9707, 2.5347, 2.2125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.13297677040100098 
model_pd.l_d.mean(): -24.84192657470703 
model_pd.lagr.mean(): -24.70895004272461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0613], device='cuda:0')), ('power', tensor([-24.7806], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.13297677040100098
epoch£º239	 i:1 	 global-step:4781	 l-p:0.2475355714559555
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12788280844688416
epoch£º239	 i:3 	 global-step:4783	 l-p:0.11493663489818573
epoch£º239	 i:4 	 global-step:4784	 l-p:0.14524470269680023
epoch£º239	 i:5 	 global-step:4785	 l-p:0.13321734964847565
epoch£º239	 i:6 	 global-step:4786	 l-p:0.11832922697067261
epoch£º239	 i:7 	 global-step:4787	 l-p:0.16899190843105316
epoch£º239	 i:8 	 global-step:4788	 l-p:0.20800338685512543
epoch£º239	 i:9 	 global-step:4789	 l-p:0.23550130426883698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4786, 1.9650, 1.6486],
        [2.4786, 2.2266, 2.2871],
        [2.4786, 2.1484, 2.1580],
        [2.4786, 2.4731, 2.4784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.17730900645256042 
model_pd.l_d.mean(): -25.039287567138672 
model_pd.lagr.mean(): -24.86197853088379 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0963], device='cuda:0')), ('power', tensor([-25.1356], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.17730900645256042
epoch£º240	 i:1 	 global-step:4801	 l-p:0.16328732669353485
epoch£º240	 i:2 	 global-step:4802	 l-p:0.14503741264343262
epoch£º240	 i:3 	 global-step:4803	 l-p:0.16323333978652954
epoch£º240	 i:4 	 global-step:4804	 l-p:0.10070697218179703
epoch£º240	 i:5 	 global-step:4805	 l-p:0.13509467244148254
epoch£º240	 i:6 	 global-step:4806	 l-p:0.11509796977043152
epoch£º240	 i:7 	 global-step:4807	 l-p:-0.9165893197059631
epoch£º240	 i:8 	 global-step:4808	 l-p:0.10507798939943314
epoch£º240	 i:9 	 global-step:4809	 l-p:0.15811553597450256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0121, 2.7547, 2.7892],
        [3.0121, 3.0121, 3.0121],
        [3.0121, 2.6068, 2.2194],
        [3.0121, 2.7096, 2.3032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.12663023173809052 
model_pd.l_d.mean(): -24.8424129486084 
model_pd.lagr.mean(): -24.715782165527344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1275], device='cuda:0')), ('power', tensor([-24.7149], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.12663023173809052
epoch£º241	 i:1 	 global-step:4821	 l-p:0.15858876705169678
epoch£º241	 i:2 	 global-step:4822	 l-p:0.1017238199710846
epoch£º241	 i:3 	 global-step:4823	 l-p:0.12343073636293411
epoch£º241	 i:4 	 global-step:4824	 l-p:0.11607863008975983
epoch£º241	 i:5 	 global-step:4825	 l-p:0.6085622906684875
epoch£º241	 i:6 	 global-step:4826	 l-p:0.16970619559288025
epoch£º241	 i:7 	 global-step:4827	 l-p:0.1351449340581894
epoch£º241	 i:8 	 global-step:4828	 l-p:0.06746473163366318
epoch£º241	 i:9 	 global-step:4829	 l-p:0.7488197684288025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3930, 3.0166, 2.7502],
        [3.3930, 3.2589, 3.3229],
        [3.3930, 3.3917, 3.3930],
        [3.3930, 3.1467, 2.7694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.11433693021535873 
model_pd.l_d.mean(): -24.655776977539062 
model_pd.lagr.mean(): -24.541440963745117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2758], device='cuda:0')), ('power', tensor([-24.3800], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.11433693021535873
epoch£º242	 i:1 	 global-step:4841	 l-p:0.16209851205348969
epoch£º242	 i:2 	 global-step:4842	 l-p:0.11412474513053894
epoch£º242	 i:3 	 global-step:4843	 l-p:0.10879302769899368
epoch£º242	 i:4 	 global-step:4844	 l-p:0.038331590592861176
epoch£º242	 i:5 	 global-step:4845	 l-p:0.06104759871959686
epoch£º242	 i:6 	 global-step:4846	 l-p:0.1109887883067131
epoch£º242	 i:7 	 global-step:4847	 l-p:0.1176392138004303
epoch£º242	 i:8 	 global-step:4848	 l-p:0.10675305873155594
epoch£º242	 i:9 	 global-step:4849	 l-p:-0.5131169557571411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5359, 3.3672, 3.4245],
        [3.5359, 3.5348, 3.5359],
        [3.5359, 3.5359, 3.5359],
        [3.5359, 3.2948, 2.9301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.1074250191450119 
model_pd.l_d.mean(): -24.509288787841797 
model_pd.lagr.mean(): -24.40186309814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3521], device='cuda:0')), ('power', tensor([-24.1572], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.1074250191450119
epoch£º243	 i:1 	 global-step:4861	 l-p:0.13312672078609467
epoch£º243	 i:2 	 global-step:4862	 l-p:0.11926954239606857
epoch£º243	 i:3 	 global-step:4863	 l-p:0.11456488072872162
epoch£º243	 i:4 	 global-step:4864	 l-p:0.12423311173915863
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11642627418041229
epoch£º243	 i:6 	 global-step:4866	 l-p:0.01279525738209486
epoch£º243	 i:7 	 global-step:4867	 l-p:0.14058738946914673
epoch£º243	 i:8 	 global-step:4868	 l-p:0.10363514721393585
epoch£º243	 i:9 	 global-step:4869	 l-p:0.11523064970970154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[2.7711, 2.6207, 2.6943],
        [2.7711, 2.2957, 2.0052],
        [2.7711, 2.5394, 2.6000],
        [2.7711, 2.3273, 1.9253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.09824617207050323 
model_pd.l_d.mean(): -24.940261840820312 
model_pd.lagr.mean(): -24.842016220092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0076], device='cuda:0')), ('power', tensor([-24.9478], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.09824617207050323
epoch£º244	 i:1 	 global-step:4881	 l-p:0.1355789303779602
epoch£º244	 i:2 	 global-step:4882	 l-p:0.17117555439472198
epoch£º244	 i:3 	 global-step:4883	 l-p:0.1374950408935547
epoch£º244	 i:4 	 global-step:4884	 l-p:0.10696421563625336
epoch£º244	 i:5 	 global-step:4885	 l-p:0.19891558587551117
epoch£º244	 i:6 	 global-step:4886	 l-p:0.1501842886209488
epoch£º244	 i:7 	 global-step:4887	 l-p:0.19433097541332245
epoch£º244	 i:8 	 global-step:4888	 l-p:0.148914635181427
epoch£º244	 i:9 	 global-step:4889	 l-p:0.1593523472547531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6332, 2.6331, 2.6332],
        [2.6332, 2.1999, 2.0686],
        [2.6332, 2.1353, 1.8390],
        [2.6332, 2.6332, 2.6332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.1518785059452057 
model_pd.l_d.mean(): -24.745718002319336 
model_pd.lagr.mean(): -24.593839645385742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1406], device='cuda:0')), ('power', tensor([-24.8863], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.1518785059452057
epoch£º245	 i:1 	 global-step:4901	 l-p:0.14474725723266602
epoch£º245	 i:2 	 global-step:4902	 l-p:0.1432780623435974
epoch£º245	 i:3 	 global-step:4903	 l-p:0.13377690315246582
epoch£º245	 i:4 	 global-step:4904	 l-p:0.10322932153940201
epoch£º245	 i:5 	 global-step:4905	 l-p:0.1416265219449997
epoch£º245	 i:6 	 global-step:4906	 l-p:0.12258022278547287
epoch£º245	 i:7 	 global-step:4907	 l-p:0.15291835367679596
epoch£º245	 i:8 	 global-step:4908	 l-p:0.13059371709823608
epoch£º245	 i:9 	 global-step:4909	 l-p:0.13118039071559906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7045, 2.2161, 1.8370],
        [2.7045, 2.7000, 2.7043],
        [2.7045, 2.4393, 2.4889],
        [2.7045, 2.7044, 2.7045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.1281847506761551 
model_pd.l_d.mean(): -24.522884368896484 
model_pd.lagr.mean(): -24.394699096679688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1105], device='cuda:0')), ('power', tensor([-24.6334], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.1281847506761551
epoch£º246	 i:1 	 global-step:4921	 l-p:0.14240996539592743
epoch£º246	 i:2 	 global-step:4922	 l-p:0.14297901093959808
epoch£º246	 i:3 	 global-step:4923	 l-p:0.14649301767349243
epoch£º246	 i:4 	 global-step:4924	 l-p:0.15062886476516724
epoch£º246	 i:5 	 global-step:4925	 l-p:0.14254498481750488
epoch£º246	 i:6 	 global-step:4926	 l-p:0.16397002339363098
epoch£º246	 i:7 	 global-step:4927	 l-p:0.1454608291387558
epoch£º246	 i:8 	 global-step:4928	 l-p:0.14632339775562286
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12697605788707733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7109, 2.7109, 2.7109],
        [2.7109, 2.6032, 2.6701],
        [2.7109, 2.3576, 2.3392],
        [2.7109, 2.7109, 2.7109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.14368510246276855 
model_pd.l_d.mean(): -24.72824478149414 
model_pd.lagr.mean(): -24.58456039428711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0354], device='cuda:0')), ('power', tensor([-24.7636], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.14368510246276855
epoch£º247	 i:1 	 global-step:4941	 l-p:0.133848175406456
epoch£º247	 i:2 	 global-step:4942	 l-p:0.14764522016048431
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13432665169239044
epoch£º247	 i:4 	 global-step:4944	 l-p:0.14459048211574554
epoch£º247	 i:5 	 global-step:4945	 l-p:0.16108176112174988
epoch£º247	 i:6 	 global-step:4946	 l-p:0.12668178975582123
epoch£º247	 i:7 	 global-step:4947	 l-p:0.1366323083639145
epoch£º247	 i:8 	 global-step:4948	 l-p:0.12308697402477264
epoch£º247	 i:9 	 global-step:4949	 l-p:0.1112905889749527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7534, 2.7534, 2.7534],
        [2.7534, 2.7534, 2.7534],
        [2.7534, 2.6490, 2.7149],
        [2.7534, 2.7533, 2.7534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.14039038121700287 
model_pd.l_d.mean(): -24.981691360473633 
model_pd.lagr.mean(): -24.84130096435547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0261], device='cuda:0')), ('power', tensor([-24.9555], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.14039038121700287
epoch£º248	 i:1 	 global-step:4961	 l-p:0.1295142024755478
epoch£º248	 i:2 	 global-step:4962	 l-p:0.1271982640028
epoch£º248	 i:3 	 global-step:4963	 l-p:0.13428960740566254
epoch£º248	 i:4 	 global-step:4964	 l-p:0.12455862760543823
epoch£º248	 i:5 	 global-step:4965	 l-p:0.13507245481014252
epoch£º248	 i:6 	 global-step:4966	 l-p:0.13692830502986908
epoch£º248	 i:7 	 global-step:4967	 l-p:0.13090434670448303
epoch£º248	 i:8 	 global-step:4968	 l-p:0.12334325164556503
epoch£º248	 i:9 	 global-step:4969	 l-p:0.16158828139305115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[2.6113, 2.0801, 1.7117],
        [2.6113, 2.0856, 1.7024],
        [2.6113, 2.2622, 2.2658],
        [2.6113, 2.1729, 1.7604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.16207458078861237 
model_pd.l_d.mean(): -25.0908203125 
model_pd.lagr.mean(): -24.92874526977539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0875], device='cuda:0')), ('power', tensor([-25.1784], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.16207458078861237
epoch£º249	 i:1 	 global-step:4981	 l-p:0.15539956092834473
epoch£º249	 i:2 	 global-step:4982	 l-p:0.128158837556839
epoch£º249	 i:3 	 global-step:4983	 l-p:0.15439213812351227
epoch£º249	 i:4 	 global-step:4984	 l-p:0.14193637669086456
epoch£º249	 i:5 	 global-step:4985	 l-p:0.11977778375148773
epoch£º249	 i:6 	 global-step:4986	 l-p:0.13505278527736664
epoch£º249	 i:7 	 global-step:4987	 l-p:0.1086341068148613
epoch£º249	 i:8 	 global-step:4988	 l-p:0.1306145340204239
epoch£º249	 i:9 	 global-step:4989	 l-p:0.14382432401180267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8855, 2.4278, 2.2427],
        [2.8855, 2.8855, 2.8855],
        [2.8855, 2.7385, 2.8144],
        [2.8855, 2.8848, 2.8855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.12198013067245483 
model_pd.l_d.mean(): -24.533878326416016 
model_pd.lagr.mean(): -24.411897659301758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0195], device='cuda:0')), ('power', tensor([-24.5533], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.12198013067245483
epoch£º250	 i:1 	 global-step:5001	 l-p:0.1336095780134201
epoch£º250	 i:2 	 global-step:5002	 l-p:0.11375652998685837
epoch£º250	 i:3 	 global-step:5003	 l-p:0.05072478950023651
epoch£º250	 i:4 	 global-step:5004	 l-p:0.12902076542377472
epoch£º250	 i:5 	 global-step:5005	 l-p:0.13236300647258759
epoch£º250	 i:6 	 global-step:5006	 l-p:0.13854674994945526
epoch£º250	 i:7 	 global-step:5007	 l-p:0.1343415379524231
epoch£º250	 i:8 	 global-step:5008	 l-p:0.1861542910337448
epoch£º250	 i:9 	 global-step:5009	 l-p:0.1323264241218567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4802, 1.9989, 1.8608],
        [2.4802, 2.4780, 2.4801],
        [2.4802, 2.1079, 2.1090],
        [2.4802, 2.4802, 2.4802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): -0.29069268703460693 
model_pd.l_d.mean(): -24.875686645507812 
model_pd.lagr.mean(): -25.166379928588867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2154], device='cuda:0')), ('power', tensor([-25.0910], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:-0.29069268703460693
epoch£º251	 i:1 	 global-step:5021	 l-p:0.18403981626033783
epoch£º251	 i:2 	 global-step:5022	 l-p:0.1743924617767334
epoch£º251	 i:3 	 global-step:5023	 l-p:0.1465107649564743
epoch£º251	 i:4 	 global-step:5024	 l-p:0.16316106915473938
epoch£º251	 i:5 	 global-step:5025	 l-p:0.1217702254652977
epoch£º251	 i:6 	 global-step:5026	 l-p:0.15240071713924408
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12762102484703064
epoch£º251	 i:8 	 global-step:5028	 l-p:0.13096630573272705
epoch£º251	 i:9 	 global-step:5029	 l-p:0.08453445136547089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8072, 2.8035, 2.8071],
        [2.8072, 2.4590, 2.0289],
        [2.8072, 2.4558, 2.4540],
        [2.8072, 2.3460, 2.1902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.10520516335964203 
model_pd.l_d.mean(): -24.79075050354004 
model_pd.lagr.mean(): -24.685544967651367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0632], device='cuda:0')), ('power', tensor([-24.8539], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.10520516335964203
epoch£º252	 i:1 	 global-step:5041	 l-p:0.15136468410491943
epoch£º252	 i:2 	 global-step:5042	 l-p:0.12449728697538376
epoch£º252	 i:3 	 global-step:5043	 l-p:0.13964304327964783
epoch£º252	 i:4 	 global-step:5044	 l-p:0.15706460177898407
epoch£º252	 i:5 	 global-step:5045	 l-p:0.13119277358055115
epoch£º252	 i:6 	 global-step:5046	 l-p:0.1915951520204544
epoch£º252	 i:7 	 global-step:5047	 l-p:0.15025843679904938
epoch£º252	 i:8 	 global-step:5048	 l-p:0.14012432098388672
epoch£º252	 i:9 	 global-step:5049	 l-p:0.2635478079319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[2.5867, 2.1682, 1.7489],
        [2.5867, 2.1993, 2.1828],
        [2.5867, 2.2167, 2.2175],
        [2.5867, 2.0272, 1.6474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.14235304296016693 
model_pd.l_d.mean(): -25.049381256103516 
model_pd.lagr.mean(): -24.907028198242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1003], device='cuda:0')), ('power', tensor([-25.1497], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.14235304296016693
epoch£º253	 i:1 	 global-step:5061	 l-p:0.17838753759860992
epoch£º253	 i:2 	 global-step:5062	 l-p:0.15730252861976624
epoch£º253	 i:3 	 global-step:5063	 l-p:0.12363149225711823
epoch£º253	 i:4 	 global-step:5064	 l-p:0.14511579275131226
epoch£º253	 i:5 	 global-step:5065	 l-p:0.1328105479478836
epoch£º253	 i:6 	 global-step:5066	 l-p:0.1234561875462532
epoch£º253	 i:7 	 global-step:5067	 l-p:0.10723434388637543
epoch£º253	 i:8 	 global-step:5068	 l-p:0.12160680443048477
epoch£º253	 i:9 	 global-step:5069	 l-p:0.128105029463768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7844, 2.6425, 2.7207],
        [2.7844, 2.5032, 2.5591],
        [2.7844, 2.7843, 2.7844],
        [2.7844, 2.7843, 2.7844]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.12654832005500793 
model_pd.l_d.mean(): -24.83464241027832 
model_pd.lagr.mean(): -24.708093643188477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0176], device='cuda:0')), ('power', tensor([-24.8523], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.12654832005500793
epoch£º254	 i:1 	 global-step:5081	 l-p:0.15089938044548035
epoch£º254	 i:2 	 global-step:5082	 l-p:0.14851753413677216
epoch£º254	 i:3 	 global-step:5083	 l-p:0.14644595980644226
epoch£º254	 i:4 	 global-step:5084	 l-p:0.11570116877555847
epoch£º254	 i:5 	 global-step:5085	 l-p:0.13419032096862793
epoch£º254	 i:6 	 global-step:5086	 l-p:0.12749068439006805
epoch£º254	 i:7 	 global-step:5087	 l-p:0.1648029237985611
epoch£º254	 i:8 	 global-step:5088	 l-p:0.16745924949645996
epoch£º254	 i:9 	 global-step:5089	 l-p:0.15123286843299866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[2.6678, 2.2582, 1.8285],
        [2.6678, 2.2088, 2.1007],
        [2.6678, 2.1095, 1.7148],
        [2.6678, 2.3301, 1.9056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.14570850133895874 
model_pd.l_d.mean(): -24.286029815673828 
model_pd.lagr.mean(): -24.140321731567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1676], device='cuda:0')), ('power', tensor([-24.4536], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.14570850133895874
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11504728347063065
epoch£º255	 i:2 	 global-step:5102	 l-p:0.14132235944271088
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12722407281398773
epoch£º255	 i:4 	 global-step:5104	 l-p:0.12366244941949844
epoch£º255	 i:5 	 global-step:5105	 l-p:0.1287609338760376
epoch£º255	 i:6 	 global-step:5106	 l-p:0.09592951834201813
epoch£º255	 i:7 	 global-step:5107	 l-p:0.15076753497123718
epoch£º255	 i:8 	 global-step:5108	 l-p:0.13194435834884644
epoch£º255	 i:9 	 global-step:5109	 l-p:0.13906896114349365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7696, 2.5126, 2.5839],
        [2.7696, 2.7087, 2.7556],
        [2.7696, 2.7696, 2.7696],
        [2.7696, 2.2155, 1.8349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.14037683606147766 
model_pd.l_d.mean(): -25.01679229736328 
model_pd.lagr.mean(): -24.876415252685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0153], device='cuda:0')), ('power', tensor([-25.0015], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.14037683606147766
epoch£º256	 i:1 	 global-step:5121	 l-p:0.13419780135154724
epoch£º256	 i:2 	 global-step:5122	 l-p:0.14072977006435394
epoch£º256	 i:3 	 global-step:5123	 l-p:0.13200277090072632
epoch£º256	 i:4 	 global-step:5124	 l-p:0.1773395836353302
epoch£º256	 i:5 	 global-step:5125	 l-p:0.14545375108718872
epoch£º256	 i:6 	 global-step:5126	 l-p:0.1432064026594162
epoch£º256	 i:7 	 global-step:5127	 l-p:0.1824890524148941
epoch£º256	 i:8 	 global-step:5128	 l-p:0.12364524602890015
epoch£º256	 i:9 	 global-step:5129	 l-p:0.1743554025888443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7342, 2.7338, 2.7342],
        [2.7342, 2.7343, 2.7342],
        [2.7342, 2.2168, 2.0088],
        [2.7342, 2.6222, 2.6940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.12281037122011185 
model_pd.l_d.mean(): -24.607534408569336 
model_pd.lagr.mean(): -24.484724044799805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0430], device='cuda:0')), ('power', tensor([-24.6505], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.12281037122011185
epoch£º257	 i:1 	 global-step:5141	 l-p:0.13258150219917297
epoch£º257	 i:2 	 global-step:5142	 l-p:0.08659489452838898
epoch£º257	 i:3 	 global-step:5143	 l-p:0.12328704446554184
epoch£º257	 i:4 	 global-step:5144	 l-p:0.1285790652036667
epoch£º257	 i:5 	 global-step:5145	 l-p:0.13513599336147308
epoch£º257	 i:6 	 global-step:5146	 l-p:0.12579356133937836
epoch£º257	 i:7 	 global-step:5147	 l-p:0.13778728246688843
epoch£º257	 i:8 	 global-step:5148	 l-p:0.14359697699546814
epoch£º257	 i:9 	 global-step:5149	 l-p:0.13638938963413239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6367, 2.6309, 2.6364],
        [2.6367, 2.3680, 2.4436],
        [2.6367, 2.6367, 2.6367],
        [2.6367, 2.0477, 1.6694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.13943149149417877 
model_pd.l_d.mean(): -24.847135543823242 
model_pd.lagr.mean(): -24.707704544067383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1230], device='cuda:0')), ('power', tensor([-24.9701], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.13943149149417877
epoch£º258	 i:1 	 global-step:5161	 l-p:0.15123772621154785
epoch£º258	 i:2 	 global-step:5162	 l-p:2.057070255279541
epoch£º258	 i:3 	 global-step:5163	 l-p:-0.15157201886177063
epoch£º258	 i:4 	 global-step:5164	 l-p:0.15224997699260712
epoch£º258	 i:5 	 global-step:5165	 l-p:0.12100068479776382
epoch£º258	 i:6 	 global-step:5166	 l-p:-0.033887214958667755
epoch£º258	 i:7 	 global-step:5167	 l-p:0.14552564918994904
epoch£º258	 i:8 	 global-step:5168	 l-p:0.12712362408638
epoch£º258	 i:9 	 global-step:5169	 l-p:0.13701389729976654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8105, 2.2418, 1.8471],
        [2.8105, 2.8105, 2.8105],
        [2.8105, 2.8101, 2.8105],
        [2.8105, 2.4360, 2.4384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.1113918274641037 
model_pd.l_d.mean(): -24.343809127807617 
model_pd.lagr.mean(): -24.232418060302734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0482], device='cuda:0')), ('power', tensor([-24.3920], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.1113918274641037
epoch£º259	 i:1 	 global-step:5181	 l-p:0.13312290608882904
epoch£º259	 i:2 	 global-step:5182	 l-p:0.13141949474811554
epoch£º259	 i:3 	 global-step:5183	 l-p:0.13079985976219177
epoch£º259	 i:4 	 global-step:5184	 l-p:0.14532823860645294
epoch£º259	 i:5 	 global-step:5185	 l-p:0.13657507300376892
epoch£º259	 i:6 	 global-step:5186	 l-p:0.11926881223917007
epoch£º259	 i:7 	 global-step:5187	 l-p:0.13610534369945526
epoch£º259	 i:8 	 global-step:5188	 l-p:0.21559007465839386
epoch£º259	 i:9 	 global-step:5189	 l-p:0.17417103052139282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5215, 1.9262, 1.5192],
        [2.5215, 1.9182, 1.6049],
        [2.5215, 2.5215, 2.5215],
        [2.5215, 2.0963, 2.0769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.205917626619339 
model_pd.l_d.mean(): -24.920934677124023 
model_pd.lagr.mean(): -24.715017318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2456], device='cuda:0')), ('power', tensor([-25.1665], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.205917626619339
epoch£º260	 i:1 	 global-step:5201	 l-p:2.545597553253174
epoch£º260	 i:2 	 global-step:5202	 l-p:0.19564978778362274
epoch£º260	 i:3 	 global-step:5203	 l-p:0.11580720543861389
epoch£º260	 i:4 	 global-step:5204	 l-p:0.15486422181129456
epoch£º260	 i:5 	 global-step:5205	 l-p:0.12733274698257446
epoch£º260	 i:6 	 global-step:5206	 l-p:0.13445241749286652
epoch£º260	 i:7 	 global-step:5207	 l-p:0.14736153185367584
epoch£º260	 i:8 	 global-step:5208	 l-p:0.1219540387392044
epoch£º260	 i:9 	 global-step:5209	 l-p:0.12616808712482452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7663, 2.7663, 2.7663],
        [2.7663, 2.1774, 1.8042],
        [2.7663, 2.1758, 1.7784],
        [2.7663, 2.6347, 2.7142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.14687809348106384 
model_pd.l_d.mean(): -24.82460594177246 
model_pd.lagr.mean(): -24.6777286529541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0316], device='cuda:0')), ('power', tensor([-24.8562], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.14687809348106384
epoch£º261	 i:1 	 global-step:5221	 l-p:0.1658324897289276
epoch£º261	 i:2 	 global-step:5222	 l-p:0.1371670514345169
epoch£º261	 i:3 	 global-step:5223	 l-p:0.12242967635393143
epoch£º261	 i:4 	 global-step:5224	 l-p:0.15197224915027618
epoch£º261	 i:5 	 global-step:5225	 l-p:0.14200197160243988
epoch£º261	 i:6 	 global-step:5226	 l-p:0.131953164935112
epoch£º261	 i:7 	 global-step:5227	 l-p:0.13945285975933075
epoch£º261	 i:8 	 global-step:5228	 l-p:0.14776837825775146
epoch£º261	 i:9 	 global-step:5229	 l-p:0.1496267318725586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7082, 2.6582, 2.6987],
        [2.7082, 2.5348, 2.6239],
        [2.7082, 2.6917, 2.7067],
        [2.7082, 2.6944, 2.7071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.13391223549842834 
model_pd.l_d.mean(): -24.60210418701172 
model_pd.lagr.mean(): -24.468191146850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1039], device='cuda:0')), ('power', tensor([-24.7060], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.13391223549842834
epoch£º262	 i:1 	 global-step:5241	 l-p:0.1416594535112381
epoch£º262	 i:2 	 global-step:5242	 l-p:0.15984711050987244
epoch£º262	 i:3 	 global-step:5243	 l-p:0.004200229421257973
epoch£º262	 i:4 	 global-step:5244	 l-p:0.13720129430294037
epoch£º262	 i:5 	 global-step:5245	 l-p:0.09450194239616394
epoch£º262	 i:6 	 global-step:5246	 l-p:-0.0017004775581881404
epoch£º262	 i:7 	 global-step:5247	 l-p:0.192485973238945
epoch£º262	 i:8 	 global-step:5248	 l-p:0.25744763016700745
epoch£º262	 i:9 	 global-step:5249	 l-p:0.14163176715373993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6833, 2.0785, 1.6533],
        [2.6833, 2.0767, 1.7323],
        [2.6833, 2.5812, 2.6508],
        [2.6833, 2.0705, 1.6594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.13857316970825195 
model_pd.l_d.mean(): -24.51804542541504 
model_pd.lagr.mean(): -24.379472732543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1105], device='cuda:0')), ('power', tensor([-24.6286], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.13857316970825195
epoch£º263	 i:1 	 global-step:5261	 l-p:0.16475336253643036
epoch£º263	 i:2 	 global-step:5262	 l-p:0.11965909600257874
epoch£º263	 i:3 	 global-step:5263	 l-p:0.13814696669578552
epoch£º263	 i:4 	 global-step:5264	 l-p:0.12631233036518097
epoch£º263	 i:5 	 global-step:5265	 l-p:0.1232755035161972
epoch£º263	 i:6 	 global-step:5266	 l-p:0.14190584421157837
epoch£º263	 i:7 	 global-step:5267	 l-p:0.14127086102962494
epoch£º263	 i:8 	 global-step:5268	 l-p:0.16873224079608917
epoch£º263	 i:9 	 global-step:5269	 l-p:0.13566353917121887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7018, 2.6668, 2.6967],
        [2.7018, 2.0961, 1.6630],
        [2.7018, 2.1820, 1.7289],
        [2.7018, 2.0833, 1.7023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.14792004227638245 
model_pd.l_d.mean(): -25.009733200073242 
model_pd.lagr.mean(): -24.861812591552734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0480], device='cuda:0')), ('power', tensor([-25.0577], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.14792004227638245
epoch£º264	 i:1 	 global-step:5281	 l-p:0.1431388556957245
epoch£º264	 i:2 	 global-step:5282	 l-p:0.2800323963165283
epoch£º264	 i:3 	 global-step:5283	 l-p:0.13684792816638947
epoch£º264	 i:4 	 global-step:5284	 l-p:0.16517052054405212
epoch£º264	 i:5 	 global-step:5285	 l-p:0.15857988595962524
epoch£º264	 i:6 	 global-step:5286	 l-p:0.13694807887077332
epoch£º264	 i:7 	 global-step:5287	 l-p:0.1571614295244217
epoch£º264	 i:8 	 global-step:5288	 l-p:0.13825750350952148
epoch£º264	 i:9 	 global-step:5289	 l-p:0.12402386218309402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7357, 2.2542, 1.7947],
        [2.7357, 2.1976, 2.0250],
        [2.7357, 2.7357, 2.7357],
        [2.7357, 2.7348, 2.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.10745760053396225 
model_pd.l_d.mean(): -24.714828491210938 
model_pd.lagr.mean(): -24.607370376586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1244], device='cuda:0')), ('power', tensor([-24.8392], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.10745760053396225
epoch£º265	 i:1 	 global-step:5301	 l-p:0.15281182527542114
epoch£º265	 i:2 	 global-step:5302	 l-p:0.14397922158241272
epoch£º265	 i:3 	 global-step:5303	 l-p:0.1745026707649231
epoch£º265	 i:4 	 global-step:5304	 l-p:0.17016085982322693
epoch£º265	 i:5 	 global-step:5305	 l-p:0.13470104336738586
epoch£º265	 i:6 	 global-step:5306	 l-p:0.12861356139183044
epoch£º265	 i:7 	 global-step:5307	 l-p:0.1315889209508896
epoch£º265	 i:8 	 global-step:5308	 l-p:0.13976258039474487
epoch£º265	 i:9 	 global-step:5309	 l-p:0.13017630577087402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7043, 2.6847, 2.7024],
        [2.7043, 2.7004, 2.7041],
        [2.7043, 2.6131, 2.6783],
        [2.7043, 2.7027, 2.7042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.13223643600940704 
model_pd.l_d.mean(): -24.934375762939453 
model_pd.lagr.mean(): -24.802139282226562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0591], device='cuda:0')), ('power', tensor([-24.9935], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.13223643600940704
epoch£º266	 i:1 	 global-step:5321	 l-p:0.15002331137657166
epoch£º266	 i:2 	 global-step:5322	 l-p:0.14375965297222137
epoch£º266	 i:3 	 global-step:5323	 l-p:0.0711907148361206
epoch£º266	 i:4 	 global-step:5324	 l-p:-2.8027148246765137
epoch£º266	 i:5 	 global-step:5325	 l-p:0.08717156201601028
epoch£º266	 i:6 	 global-step:5326	 l-p:0.1661577820777893
epoch£º266	 i:7 	 global-step:5327	 l-p:0.180172860622406
epoch£º266	 i:8 	 global-step:5328	 l-p:0.14033180475234985
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13830110430717468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7671, 2.7670, 2.7671],
        [2.7671, 2.3043, 2.2525],
        [2.7671, 2.7671, 2.7671],
        [2.7671, 2.7034, 2.7531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.12516529858112335 
model_pd.l_d.mean(): -24.800039291381836 
model_pd.lagr.mean(): -24.67487335205078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1076], device='cuda:0')), ('power', tensor([-24.9077], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.12516529858112335
epoch£º267	 i:1 	 global-step:5341	 l-p:0.14847028255462646
epoch£º267	 i:2 	 global-step:5342	 l-p:0.1748979538679123
epoch£º267	 i:3 	 global-step:5343	 l-p:0.14263775944709778
epoch£º267	 i:4 	 global-step:5344	 l-p:0.12531132996082306
epoch£º267	 i:5 	 global-step:5345	 l-p:0.13987629115581512
epoch£º267	 i:6 	 global-step:5346	 l-p:0.14650984108448029
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12752388417720795
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13901233673095703
epoch£º267	 i:9 	 global-step:5349	 l-p:0.15830789506435394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7365, 2.7346, 2.7365],
        [2.7365, 2.1326, 1.8504],
        [2.7365, 2.6678, 2.7207],
        [2.7365, 2.7366, 2.7366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.1396159678697586 
model_pd.l_d.mean(): -24.90018081665039 
model_pd.lagr.mean(): -24.76056480407715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0797], device='cuda:0')), ('power', tensor([-24.9799], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.1396159678697586
epoch£º268	 i:1 	 global-step:5361	 l-p:0.14987844228744507
epoch£º268	 i:2 	 global-step:5362	 l-p:0.16421440243721008
epoch£º268	 i:3 	 global-step:5363	 l-p:0.13201352953910828
epoch£º268	 i:4 	 global-step:5364	 l-p:0.15254220366477966
epoch£º268	 i:5 	 global-step:5365	 l-p:-0.18429705500602722
epoch£º268	 i:6 	 global-step:5366	 l-p:0.14564989507198334
epoch£º268	 i:7 	 global-step:5367	 l-p:0.14107804000377655
epoch£º268	 i:8 	 global-step:5368	 l-p:0.13753603398799896
epoch£º268	 i:9 	 global-step:5369	 l-p:0.13636314868927002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7360, 2.7319, 2.7359],
        [2.7360, 2.7058, 2.7321],
        [2.7360, 2.5253, 2.6216],
        [2.7360, 2.5941, 2.6799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.16616766154766083 
model_pd.l_d.mean(): -25.02491569519043 
model_pd.lagr.mean(): -24.858747482299805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1016], device='cuda:0')), ('power', tensor([-25.1265], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.16616766154766083
epoch£º269	 i:1 	 global-step:5381	 l-p:0.18237632513046265
epoch£º269	 i:2 	 global-step:5382	 l-p:0.1562391072511673
epoch£º269	 i:3 	 global-step:5383	 l-p:0.12699556350708008
epoch£º269	 i:4 	 global-step:5384	 l-p:0.13798023760318756
epoch£º269	 i:5 	 global-step:5385	 l-p:0.1372019201517105
epoch£º269	 i:6 	 global-step:5386	 l-p:0.11848460137844086
epoch£º269	 i:7 	 global-step:5387	 l-p:0.18915046751499176
epoch£º269	 i:8 	 global-step:5388	 l-p:0.157498300075531
epoch£º269	 i:9 	 global-step:5389	 l-p:0.2284749150276184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7497, 2.7240, 2.7467],
        [2.7497, 2.1124, 1.7627],
        [2.7497, 2.7456, 2.7496],
        [2.7497, 2.2239, 2.1046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.12955816090106964 
model_pd.l_d.mean(): -25.02681541442871 
model_pd.lagr.mean(): -24.89725685119629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0043], device='cuda:0')), ('power', tensor([-25.0225], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.12955816090106964
epoch£º270	 i:1 	 global-step:5401	 l-p:0.14909212291240692
epoch£º270	 i:2 	 global-step:5402	 l-p:0.12386536598205566
epoch£º270	 i:3 	 global-step:5403	 l-p:0.1463642567396164
epoch£º270	 i:4 	 global-step:5404	 l-p:0.13136886060237885
epoch£º270	 i:5 	 global-step:5405	 l-p:0.12598371505737305
epoch£º270	 i:6 	 global-step:5406	 l-p:0.13229995965957642
epoch£º270	 i:7 	 global-step:5407	 l-p:0.12219198793172836
epoch£º270	 i:8 	 global-step:5408	 l-p:0.133444681763649
epoch£º270	 i:9 	 global-step:5409	 l-p:0.15011367201805115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6244, 2.6244, 2.6244],
        [2.6244, 2.1089, 2.0267],
        [2.6244, 1.9516, 1.5380],
        [2.6244, 2.0476, 1.5937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.21993646025657654 
model_pd.l_d.mean(): -25.02323341369629 
model_pd.lagr.mean(): -24.80329704284668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1270], device='cuda:0')), ('power', tensor([-25.1503], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.21993646025657654
epoch£º271	 i:1 	 global-step:5421	 l-p:0.12277965992689133
epoch£º271	 i:2 	 global-step:5422	 l-p:0.1692456603050232
epoch£º271	 i:3 	 global-step:5423	 l-p:0.14576689898967743
epoch£º271	 i:4 	 global-step:5424	 l-p:0.14280655980110168
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12052400410175323
epoch£º271	 i:6 	 global-step:5426	 l-p:0.16371852159500122
epoch£º271	 i:7 	 global-step:5427	 l-p:0.13058069348335266
epoch£º271	 i:8 	 global-step:5428	 l-p:0.1425601840019226
epoch£º271	 i:9 	 global-step:5429	 l-p:0.1382879912853241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6851, 2.6215, 2.6717],
        [2.6851, 2.0274, 1.5839],
        [2.6851, 2.5622, 2.6429],
        [2.6851, 2.6445, 2.6788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.18908479809761047 
model_pd.l_d.mean(): -24.488943099975586 
model_pd.lagr.mean(): -24.29985809326172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1337], device='cuda:0')), ('power', tensor([-24.6226], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.18908479809761047
epoch£º272	 i:1 	 global-step:5441	 l-p:0.14654333889484406
epoch£º272	 i:2 	 global-step:5442	 l-p:0.12419094890356064
epoch£º272	 i:3 	 global-step:5443	 l-p:0.032412391155958176
epoch£º272	 i:4 	 global-step:5444	 l-p:0.15580755472183228
epoch£º272	 i:5 	 global-step:5445	 l-p:0.14002200961112976
epoch£º272	 i:6 	 global-step:5446	 l-p:0.30857154726982117
epoch£º272	 i:7 	 global-step:5447	 l-p:0.15374931693077087
epoch£º272	 i:8 	 global-step:5448	 l-p:0.1974821239709854
epoch£º272	 i:9 	 global-step:5449	 l-p:0.15250544250011444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7228, 2.2687, 2.2564],
        [2.7228, 2.7114, 2.7220],
        [2.7228, 2.5750, 2.6643],
        [2.7228, 2.6615, 2.7102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.14265985786914825 
model_pd.l_d.mean(): -24.237947463989258 
model_pd.lagr.mean(): -24.095287322998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1847], device='cuda:0')), ('power', tensor([-24.4227], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.14265985786914825
epoch£º273	 i:1 	 global-step:5461	 l-p:0.13368159532546997
epoch£º273	 i:2 	 global-step:5462	 l-p:0.10559587925672531
epoch£º273	 i:3 	 global-step:5463	 l-p:0.10621757060289383
epoch£º273	 i:4 	 global-step:5464	 l-p:0.10379277914762497
epoch£º273	 i:5 	 global-step:5465	 l-p:0.20329606533050537
epoch£º273	 i:6 	 global-step:5466	 l-p:0.10039985179901123
epoch£º273	 i:7 	 global-step:5467	 l-p:0.14838114380836487
epoch£º273	 i:8 	 global-step:5468	 l-p:0.12382262200117111
epoch£º273	 i:9 	 global-step:5469	 l-p:0.34516364336013794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2341, 2.6555, 2.3397],
        [3.2341, 2.7840, 2.2624],
        [3.2341, 3.2197, 3.2329],
        [3.2341, 2.7829, 2.2612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.09975890815258026 
model_pd.l_d.mean(): -24.899009704589844 
model_pd.lagr.mean(): -24.799251556396484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2331], device='cuda:0')), ('power', tensor([-24.6659], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.09975890815258026
epoch£º274	 i:1 	 global-step:5481	 l-p:0.16822443902492523
epoch£º274	 i:2 	 global-step:5482	 l-p:0.1250857263803482
epoch£º274	 i:3 	 global-step:5483	 l-p:0.10332226753234863
epoch£º274	 i:4 	 global-step:5484	 l-p:0.13804346323013306
epoch£º274	 i:5 	 global-step:5485	 l-p:0.11116808652877808
epoch£º274	 i:6 	 global-step:5486	 l-p:0.09772069752216339
epoch£º274	 i:7 	 global-step:5487	 l-p:0.07066438347101212
epoch£º274	 i:8 	 global-step:5488	 l-p:0.12299173325300217
epoch£º274	 i:9 	 global-step:5489	 l-p:0.1475391536951065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7447, 2.0739, 1.6919],
        [2.7447, 2.0699, 1.6719],
        [2.7447, 2.7447, 2.7447],
        [2.7447, 2.6830, 2.7320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.1456160545349121 
model_pd.l_d.mean(): -25.101261138916016 
model_pd.lagr.mean(): -24.955644607543945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0178], device='cuda:0')), ('power', tensor([-25.0835], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.1456160545349121
epoch£º275	 i:1 	 global-step:5501	 l-p:0.15106403827667236
epoch£º275	 i:2 	 global-step:5502	 l-p:0.1504068225622177
epoch£º275	 i:3 	 global-step:5503	 l-p:0.21642114222049713
epoch£º275	 i:4 	 global-step:5504	 l-p:1.0402535200119019
epoch£º275	 i:5 	 global-step:5505	 l-p:0.1913222074508667
epoch£º275	 i:6 	 global-step:5506	 l-p:0.15820613503456116
epoch£º275	 i:7 	 global-step:5507	 l-p:0.13426195085048676
epoch£º275	 i:8 	 global-step:5508	 l-p:0.5438947081565857
epoch£º275	 i:9 	 global-step:5509	 l-p:0.15511086583137512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7118, 2.0970, 1.6295],
        [2.7118, 2.6411, 2.6959],
        [2.7118, 2.7117, 2.7118],
        [2.7118, 2.7041, 2.7114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.13962720334529877 
model_pd.l_d.mean(): -25.14256477355957 
model_pd.lagr.mean(): -25.00293731689453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0016], device='cuda:0')), ('power', tensor([-25.1410], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.13962720334529877
epoch£º276	 i:1 	 global-step:5521	 l-p:0.161259263753891
epoch£º276	 i:2 	 global-step:5522	 l-p:0.1411048322916031
epoch£º276	 i:3 	 global-step:5523	 l-p:0.1459185779094696
epoch£º276	 i:4 	 global-step:5524	 l-p:0.19758057594299316
epoch£º276	 i:5 	 global-step:5525	 l-p:0.14860200881958008
epoch£º276	 i:6 	 global-step:5526	 l-p:0.1344130039215088
epoch£º276	 i:7 	 global-step:5527	 l-p:0.12167300283908844
epoch£º276	 i:8 	 global-step:5528	 l-p:0.11309900134801865
epoch£º276	 i:9 	 global-step:5529	 l-p:0.15873517096042633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6753, 1.9967, 1.5514],
        [2.6753, 2.0207, 1.5645],
        [2.6753, 2.6743, 2.6753],
        [2.6753, 1.9820, 1.5629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.16350530087947845 
model_pd.l_d.mean(): -24.94579315185547 
model_pd.lagr.mean(): -24.78228759765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1539], device='cuda:0')), ('power', tensor([-25.0997], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.16350530087947845
epoch£º277	 i:1 	 global-step:5541	 l-p:0.1417582482099533
epoch£º277	 i:2 	 global-step:5542	 l-p:0.1486375480890274
epoch£º277	 i:3 	 global-step:5543	 l-p:0.17767655849456787
epoch£º277	 i:4 	 global-step:5544	 l-p:-0.2990097105503082
epoch£º277	 i:5 	 global-step:5545	 l-p:-0.528131365776062
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14171552658081055
epoch£º277	 i:7 	 global-step:5547	 l-p:0.18520481884479523
epoch£º277	 i:8 	 global-step:5548	 l-p:0.11513539403676987
epoch£º277	 i:9 	 global-step:5549	 l-p:0.13277624547481537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9190, 2.8859, 2.9146],
        [2.9190, 2.8060, 2.8826],
        [2.9190, 2.6344, 2.7256],
        [2.9190, 2.2881, 1.7959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.11024708300828934 
model_pd.l_d.mean(): -24.482126235961914 
model_pd.lagr.mean(): -24.37187957763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0178], device='cuda:0')), ('power', tensor([-24.5000], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.11024708300828934
epoch£º278	 i:1 	 global-step:5561	 l-p:0.12285615503787994
epoch£º278	 i:2 	 global-step:5562	 l-p:0.13225238025188446
epoch£º278	 i:3 	 global-step:5563	 l-p:0.07151751965284348
epoch£º278	 i:4 	 global-step:5564	 l-p:0.13193008303642273
epoch£º278	 i:5 	 global-step:5565	 l-p:0.11931414157152176
epoch£º278	 i:6 	 global-step:5566	 l-p:0.1277218461036682
epoch£º278	 i:7 	 global-step:5567	 l-p:0.13195949792861938
epoch£º278	 i:8 	 global-step:5568	 l-p:0.2103862762451172
epoch£º278	 i:9 	 global-step:5569	 l-p:0.15301284193992615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5210, 2.1020, 2.1463],
        [2.5210, 2.3767, 2.4676],
        [2.5210, 1.8175, 1.3972],
        [2.5210, 2.1378, 2.2045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.20752443373203278 
model_pd.l_d.mean(): -25.040184020996094 
model_pd.lagr.mean(): -24.832658767700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1526], device='cuda:0')), ('power', tensor([-25.1928], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.20752443373203278
epoch£º279	 i:1 	 global-step:5581	 l-p:0.20822933316230774
epoch£º279	 i:2 	 global-step:5582	 l-p:0.1549839973449707
epoch£º279	 i:3 	 global-step:5583	 l-p:0.29818227887153625
epoch£º279	 i:4 	 global-step:5584	 l-p:0.08936033397912979
epoch£º279	 i:5 	 global-step:5585	 l-p:0.03968720883131027
epoch£º279	 i:6 	 global-step:5586	 l-p:0.17741136252880096
epoch£º279	 i:7 	 global-step:5587	 l-p:0.127837136387825
epoch£º279	 i:8 	 global-step:5588	 l-p:0.13900057971477509
epoch£º279	 i:9 	 global-step:5589	 l-p:0.13971835374832153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8804, 2.8761, 2.8802],
        [2.8804, 2.7441, 2.8304],
        [2.8804, 2.6346, 2.7351],
        [2.8804, 2.5052, 2.5603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12404964119195938 
model_pd.l_d.mean(): -24.8160400390625 
model_pd.lagr.mean(): -24.69198989868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0253], device='cuda:0')), ('power', tensor([-24.7908], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12404964119195938
epoch£º280	 i:1 	 global-step:5601	 l-p:0.11961699277162552
epoch£º280	 i:2 	 global-step:5602	 l-p:0.14405037462711334
epoch£º280	 i:3 	 global-step:5603	 l-p:0.14035001397132874
epoch£º280	 i:4 	 global-step:5604	 l-p:0.1470019519329071
epoch£º280	 i:5 	 global-step:5605	 l-p:0.10718639194965363
epoch£º280	 i:6 	 global-step:5606	 l-p:0.1257762610912323
epoch£º280	 i:7 	 global-step:5607	 l-p:0.1376805603504181
epoch£º280	 i:8 	 global-step:5608	 l-p:0.14916494488716125
epoch£º280	 i:9 	 global-step:5609	 l-p:0.2240666151046753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6873, 2.1165, 1.9907],
        [2.6873, 2.6849, 2.6873],
        [2.6873, 2.6311, 2.6768],
        [2.6873, 2.4463, 2.5514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.05115296319127083 
model_pd.l_d.mean(): -25.084636688232422 
model_pd.lagr.mean(): -25.033483505249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0752], device='cuda:0')), ('power', tensor([-25.1599], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.05115296319127083
epoch£º281	 i:1 	 global-step:5621	 l-p:0.15081149339675903
epoch£º281	 i:2 	 global-step:5622	 l-p:0.1475229263305664
epoch£º281	 i:3 	 global-step:5623	 l-p:0.11217068135738373
epoch£º281	 i:4 	 global-step:5624	 l-p:0.1552996039390564
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11452430486679077
epoch£º281	 i:6 	 global-step:5626	 l-p:0.14737126231193542
epoch£º281	 i:7 	 global-step:5627	 l-p:0.14172819256782532
epoch£º281	 i:8 	 global-step:5628	 l-p:0.1480962187051773
epoch£º281	 i:9 	 global-step:5629	 l-p:0.1388750672340393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[2.5578, 2.2980, 2.4055],
        [2.5578, 1.9300, 1.7422],
        [2.5578, 2.2653, 2.3685],
        [2.5578, 1.9086, 1.4564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.14878402650356293 
model_pd.l_d.mean(): -24.34223175048828 
model_pd.lagr.mean(): -24.19344711303711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3843], device='cuda:0')), ('power', tensor([-24.7266], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.14878402650356293
epoch£º282	 i:1 	 global-step:5641	 l-p:0.18624334037303925
epoch£º282	 i:2 	 global-step:5642	 l-p:0.19090737402439117
epoch£º282	 i:3 	 global-step:5643	 l-p:0.11444880068302155
epoch£º282	 i:4 	 global-step:5644	 l-p:-0.5268773436546326
epoch£º282	 i:5 	 global-step:5645	 l-p:0.17046774923801422
epoch£º282	 i:6 	 global-step:5646	 l-p:0.109214186668396
epoch£º282	 i:7 	 global-step:5647	 l-p:0.11523927748203278
epoch£º282	 i:8 	 global-step:5648	 l-p:0.1269746720790863
epoch£º282	 i:9 	 global-step:5649	 l-p:0.13000167906284332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8381, 2.1540, 1.7837],
        [2.8381, 2.2272, 1.7316],
        [2.8381, 2.1540, 1.7837],
        [2.8381, 2.8192, 2.8364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.14240345358848572 
model_pd.l_d.mean(): -24.848575592041016 
model_pd.lagr.mean(): -24.706172943115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0631], device='cuda:0')), ('power', tensor([-24.9117], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.14240345358848572
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12967707216739655
epoch£º283	 i:2 	 global-step:5662	 l-p:0.14038293063640594
epoch£º283	 i:3 	 global-step:5663	 l-p:0.19188359379768372
epoch£º283	 i:4 	 global-step:5664	 l-p:0.09945543855428696
epoch£º283	 i:5 	 global-step:5665	 l-p:0.16627120971679688
epoch£º283	 i:6 	 global-step:5666	 l-p:0.1582103967666626
epoch£º283	 i:7 	 global-step:5667	 l-p:0.0759490504860878
epoch£º283	 i:8 	 global-step:5668	 l-p:0.18593905866146088
epoch£º283	 i:9 	 global-step:5669	 l-p:0.16744226217269897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8400, 2.7823, 2.8290],
        [2.8400, 2.8075, 2.8358],
        [2.8400, 2.8384, 2.8399],
        [2.8400, 2.2203, 2.0041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.1539304256439209 
model_pd.l_d.mean(): -25.117263793945312 
model_pd.lagr.mean(): -24.963333129882812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0288], device='cuda:0')), ('power', tensor([-25.0884], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.1539304256439209
epoch£º284	 i:1 	 global-step:5681	 l-p:0.13320542871952057
epoch£º284	 i:2 	 global-step:5682	 l-p:0.11699170619249344
epoch£º284	 i:3 	 global-step:5683	 l-p:0.1265825778245926
epoch£º284	 i:4 	 global-step:5684	 l-p:0.11895281821489334
epoch£º284	 i:5 	 global-step:5685	 l-p:0.13903497159481049
epoch£º284	 i:6 	 global-step:5686	 l-p:0.13842961192131042
epoch£º284	 i:7 	 global-step:5687	 l-p:0.14960357546806335
epoch£º284	 i:8 	 global-step:5688	 l-p:0.132804736495018
epoch£º284	 i:9 	 global-step:5689	 l-p:0.11257818341255188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7018, 2.7018, 2.7018],
        [2.7018, 2.6538, 2.6939],
        [2.7018, 2.4408, 2.5474],
        [2.7018, 2.0010, 1.6571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.1265047937631607 
model_pd.l_d.mean(): -25.05147933959961 
model_pd.lagr.mean(): -24.92497444152832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0737], device='cuda:0')), ('power', tensor([-25.1251], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.1265047937631607
epoch£º285	 i:1 	 global-step:5701	 l-p:0.15730518102645874
epoch£º285	 i:2 	 global-step:5702	 l-p:0.16747981309890747
epoch£º285	 i:3 	 global-step:5703	 l-p:0.138419970870018
epoch£º285	 i:4 	 global-step:5704	 l-p:0.14343930780887604
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.01530505158007145
epoch£º285	 i:6 	 global-step:5706	 l-p:0.13069410622119904
epoch£º285	 i:7 	 global-step:5707	 l-p:0.33633503317832947
epoch£º285	 i:8 	 global-step:5708	 l-p:0.1835976392030716
epoch£º285	 i:9 	 global-step:5709	 l-p:0.12967731058597565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7937, 2.7105, 2.7734],
        [2.7937, 2.7936, 2.7937],
        [2.7937, 2.7937, 2.7937],
        [2.7937, 2.7823, 2.7930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.13036233186721802 
model_pd.l_d.mean(): -25.03216552734375 
model_pd.lagr.mean(): -24.901803970336914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0162], device='cuda:0')), ('power', tensor([-25.0160], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.13036233186721802
epoch£º286	 i:1 	 global-step:5721	 l-p:0.13294580578804016
epoch£º286	 i:2 	 global-step:5722	 l-p:0.1494971364736557
epoch£º286	 i:3 	 global-step:5723	 l-p:0.16137854754924774
epoch£º286	 i:4 	 global-step:5724	 l-p:0.24955737590789795
epoch£º286	 i:5 	 global-step:5725	 l-p:0.12428561598062515
epoch£º286	 i:6 	 global-step:5726	 l-p:0.1298673003911972
epoch£º286	 i:7 	 global-step:5727	 l-p:0.1345711350440979
epoch£º286	 i:8 	 global-step:5728	 l-p:0.14829325675964355
epoch£º286	 i:9 	 global-step:5729	 l-p:0.1494888961315155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6696, 1.9713, 1.6605],
        [2.6696, 1.9381, 1.5530],
        [2.6696, 2.2877, 2.3619],
        [2.6696, 2.5353, 2.6235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.1567949801683426 
model_pd.l_d.mean(): -25.02509307861328 
model_pd.lagr.mean(): -24.868297576904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0996], device='cuda:0')), ('power', tensor([-25.1247], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.1567949801683426
epoch£º287	 i:1 	 global-step:5741	 l-p:0.1494770050048828
epoch£º287	 i:2 	 global-step:5742	 l-p:0.05069178342819214
epoch£º287	 i:3 	 global-step:5743	 l-p:0.12896619737148285
epoch£º287	 i:4 	 global-step:5744	 l-p:0.16641829907894135
epoch£º287	 i:5 	 global-step:5745	 l-p:0.17616428434848785
epoch£º287	 i:6 	 global-step:5746	 l-p:0.11779364943504333
epoch£º287	 i:7 	 global-step:5747	 l-p:0.14552687108516693
epoch£º287	 i:8 	 global-step:5748	 l-p:0.14131228625774384
epoch£º287	 i:9 	 global-step:5749	 l-p:0.15208397805690765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8453, 2.7681, 2.8275],
        [2.8453, 2.8453, 2.8453],
        [2.8453, 2.8388, 2.8450],
        [2.8453, 2.2015, 1.9644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.12856648862361908 
model_pd.l_d.mean(): -25.025556564331055 
model_pd.lagr.mean(): -24.896989822387695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0021], device='cuda:0')), ('power', tensor([-25.0234], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.12856648862361908
epoch£º288	 i:1 	 global-step:5761	 l-p:0.14258559048175812
epoch£º288	 i:2 	 global-step:5762	 l-p:0.12323352694511414
epoch£º288	 i:3 	 global-step:5763	 l-p:0.1488065868616104
epoch£º288	 i:4 	 global-step:5764	 l-p:0.14803551137447357
epoch£º288	 i:5 	 global-step:5765	 l-p:0.1448509693145752
epoch£º288	 i:6 	 global-step:5766	 l-p:0.17418010532855988
epoch£º288	 i:7 	 global-step:5767	 l-p:0.14235076308250427
epoch£º288	 i:8 	 global-step:5768	 l-p:0.13786724209785461
epoch£º288	 i:9 	 global-step:5769	 l-p:0.08107122033834457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5768, 1.8196, 1.4140],
        [2.5768, 2.5703, 2.5765],
        [2.5768, 2.5721, 2.5766],
        [2.5768, 2.4993, 2.5594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.1437232792377472 
model_pd.l_d.mean(): -24.432554244995117 
model_pd.lagr.mean(): -24.28883171081543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2409], device='cuda:0')), ('power', tensor([-24.6735], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.1437232792377472
epoch£º289	 i:1 	 global-step:5781	 l-p:0.1113942340016365
epoch£º289	 i:2 	 global-step:5782	 l-p:0.2880837321281433
epoch£º289	 i:3 	 global-step:5783	 l-p:0.14564083516597748
epoch£º289	 i:4 	 global-step:5784	 l-p:0.13174840807914734
epoch£º289	 i:5 	 global-step:5785	 l-p:0.1577332615852356
epoch£º289	 i:6 	 global-step:5786	 l-p:0.14861008524894714
epoch£º289	 i:7 	 global-step:5787	 l-p:0.14298510551452637
epoch£º289	 i:8 	 global-step:5788	 l-p:0.13515493273735046
epoch£º289	 i:9 	 global-step:5789	 l-p:0.1703713834285736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6015, 2.6015, 2.6015],
        [2.6015, 2.5816, 2.5997],
        [2.6015, 2.6015, 2.6015],
        [2.6015, 2.2174, 2.2983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): -0.07265309244394302 
model_pd.l_d.mean(): -25.06883430480957 
model_pd.lagr.mean(): -25.14148712158203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0661], device='cuda:0')), ('power', tensor([-25.1349], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:-0.07265309244394302
epoch£º290	 i:1 	 global-step:5801	 l-p:0.1527877300977707
epoch£º290	 i:2 	 global-step:5802	 l-p:0.15688061714172363
epoch£º290	 i:3 	 global-step:5803	 l-p:0.1599188596010208
epoch£º290	 i:4 	 global-step:5804	 l-p:0.1683656871318817
epoch£º290	 i:5 	 global-step:5805	 l-p:0.21485614776611328
epoch£º290	 i:6 	 global-step:5806	 l-p:0.18344008922576904
epoch£º290	 i:7 	 global-step:5807	 l-p:1.017907738685608
epoch£º290	 i:8 	 global-step:5808	 l-p:0.1400386095046997
epoch£º290	 i:9 	 global-step:5809	 l-p:0.11740630865097046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9840, 2.5514, 2.5840],
        [2.9840, 2.2767, 1.8640],
        [2.9840, 2.9051, 2.9656],
        [2.9840, 2.9839, 2.9840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.12021102756261826 
model_pd.l_d.mean(): -25.085582733154297 
model_pd.lagr.mean(): -24.96537208557129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1536], device='cuda:0')), ('power', tensor([-24.9320], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.12021102756261826
epoch£º291	 i:1 	 global-step:5821	 l-p:0.05012018233537674
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12390968948602676
epoch£º291	 i:3 	 global-step:5823	 l-p:0.1099352240562439
epoch£º291	 i:4 	 global-step:5824	 l-p:0.12542149424552917
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.6850945353507996
epoch£º291	 i:6 	 global-step:5826	 l-p:0.1103644073009491
epoch£º291	 i:7 	 global-step:5827	 l-p:0.11041764914989471
epoch£º291	 i:8 	 global-step:5828	 l-p:0.15546812117099762
epoch£º291	 i:9 	 global-step:5829	 l-p:0.04410335794091225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2672, 3.2671, 3.2672],
        [3.2672, 3.2363, 3.2633],
        [3.2672, 3.2648, 3.2671],
        [3.2672, 3.2671, 3.2672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.11248214542865753 
model_pd.l_d.mean(): -24.69854164123535 
model_pd.lagr.mean(): -24.5860595703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1853], device='cuda:0')), ('power', tensor([-24.5133], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.11248214542865753
epoch£º292	 i:1 	 global-step:5841	 l-p:0.4459439516067505
epoch£º292	 i:2 	 global-step:5842	 l-p:0.09945856779813766
epoch£º292	 i:3 	 global-step:5843	 l-p:0.09958013147115707
epoch£º292	 i:4 	 global-step:5844	 l-p:0.14344914257526398
epoch£º292	 i:5 	 global-step:5845	 l-p:0.08051836490631104
epoch£º292	 i:6 	 global-step:5846	 l-p:0.101846843957901
epoch£º292	 i:7 	 global-step:5847	 l-p:0.1331397444009781
epoch£º292	 i:8 	 global-step:5848	 l-p:0.159140482544899
epoch£º292	 i:9 	 global-step:5849	 l-p:0.11325901746749878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[3.2945, 2.8211, 2.7935],
        [3.2945, 2.8281, 2.8081],
        [3.2945, 3.0410, 3.1423],
        [3.2945, 3.0849, 3.1873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.1134348064661026 
model_pd.l_d.mean(): -24.91665267944336 
model_pd.lagr.mean(): -24.8032169342041 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2458], device='cuda:0')), ('power', tensor([-24.6708], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.1134348064661026
epoch£º293	 i:1 	 global-step:5861	 l-p:0.14903061091899872
epoch£º293	 i:2 	 global-step:5862	 l-p:0.07076147943735123
epoch£º293	 i:3 	 global-step:5863	 l-p:0.1530720293521881
epoch£º293	 i:4 	 global-step:5864	 l-p:-0.009595369920134544
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12727011740207672
epoch£º293	 i:6 	 global-step:5866	 l-p:0.10335330665111542
epoch£º293	 i:7 	 global-step:5867	 l-p:0.08762548118829727
epoch£º293	 i:8 	 global-step:5868	 l-p:0.10982076078653336
epoch£º293	 i:9 	 global-step:5869	 l-p:0.041101474314928055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[3.5594, 3.2737, 3.3621],
        [3.5594, 2.9344, 2.4799],
        [3.5594, 2.9432, 2.4316],
        [3.5594, 3.0643, 2.9786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.10274360328912735 
model_pd.l_d.mean(): -24.37501335144043 
model_pd.lagr.mean(): -24.27227020263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2950], device='cuda:0')), ('power', tensor([-24.0800], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.10274360328912735
epoch£º294	 i:1 	 global-step:5881	 l-p:0.12733124196529388
epoch£º294	 i:2 	 global-step:5882	 l-p:0.11253432929515839
epoch£º294	 i:3 	 global-step:5883	 l-p:0.13075566291809082
epoch£º294	 i:4 	 global-step:5884	 l-p:0.07202451676130295
epoch£º294	 i:5 	 global-step:5885	 l-p:0.13620953261852264
epoch£º294	 i:6 	 global-step:5886	 l-p:0.12355345487594604
epoch£º294	 i:7 	 global-step:5887	 l-p:0.12399420142173767
epoch£º294	 i:8 	 global-step:5888	 l-p:0.10177651792764664
epoch£º294	 i:9 	 global-step:5889	 l-p:0.12415942549705505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7968, 2.3743, 2.4288],
        [2.7968, 2.7954, 2.7967],
        [2.7968, 2.7874, 2.7963],
        [2.7968, 2.7967, 2.7968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.14786958694458008 
model_pd.l_d.mean(): -25.197345733642578 
model_pd.lagr.mean(): -25.049476623535156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0040], device='cuda:0')), ('power', tensor([-25.1933], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.14786958694458008
epoch£º295	 i:1 	 global-step:5901	 l-p:0.1395127773284912
epoch£º295	 i:2 	 global-step:5902	 l-p:0.11190197616815567
epoch£º295	 i:3 	 global-step:5903	 l-p:0.13540898263454437
epoch£º295	 i:4 	 global-step:5904	 l-p:0.2325534075498581
epoch£º295	 i:5 	 global-step:5905	 l-p:0.2699407637119293
epoch£º295	 i:6 	 global-step:5906	 l-p:0.2185206562280655
epoch£º295	 i:7 	 global-step:5907	 l-p:0.14627030491828918
epoch£º295	 i:8 	 global-step:5908	 l-p:0.162362203001976
epoch£º295	 i:9 	 global-step:5909	 l-p:0.14303047955036163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8448, 2.3715, 2.3858],
        [2.8448, 2.8073, 2.8397],
        [2.8448, 2.8401, 2.8446],
        [2.8448, 2.8448, 2.8448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.13774217665195465 
model_pd.l_d.mean(): -24.986207962036133 
model_pd.lagr.mean(): -24.848464965820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0121], device='cuda:0')), ('power', tensor([-24.9741], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.13774217665195465
epoch£º296	 i:1 	 global-step:5921	 l-p:0.13371790945529938
epoch£º296	 i:2 	 global-step:5922	 l-p:0.13100679218769073
epoch£º296	 i:3 	 global-step:5923	 l-p:0.1400693655014038
epoch£º296	 i:4 	 global-step:5924	 l-p:0.14721813797950745
epoch£º296	 i:5 	 global-step:5925	 l-p:0.1011544018983841
epoch£º296	 i:6 	 global-step:5926	 l-p:0.13489596545696259
epoch£º296	 i:7 	 global-step:5927	 l-p:0.1590300053358078
epoch£º296	 i:8 	 global-step:5928	 l-p:0.17344610393047333
epoch£º296	 i:9 	 global-step:5929	 l-p:0.15699593722820282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5325, 1.8565, 1.3932],
        [2.5325, 2.1295, 2.2105],
        [2.5325, 2.4507, 2.5140],
        [2.5325, 2.5277, 2.5324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.13887745141983032 
model_pd.l_d.mean(): -24.677289962768555 
model_pd.lagr.mean(): -24.53841209411621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2669], device='cuda:0')), ('power', tensor([-24.9442], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.13887745141983032
epoch£º297	 i:1 	 global-step:5941	 l-p:0.15231536328792572
epoch£º297	 i:2 	 global-step:5942	 l-p:0.17248333990573883
epoch£º297	 i:3 	 global-step:5943	 l-p:0.13883517682552338
epoch£º297	 i:4 	 global-step:5944	 l-p:0.14157548546791077
epoch£º297	 i:5 	 global-step:5945	 l-p:0.13779567182064056
epoch£º297	 i:6 	 global-step:5946	 l-p:0.07269949465990067
epoch£º297	 i:7 	 global-step:5947	 l-p:0.1627267748117447
epoch£º297	 i:8 	 global-step:5948	 l-p:0.18257489800453186
epoch£º297	 i:9 	 global-step:5949	 l-p:0.12004571408033371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7908, 2.7907, 2.7908],
        [2.7908, 2.5756, 2.6866],
        [2.7908, 2.7900, 2.7908],
        [2.7908, 2.2314, 1.7242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.13089022040367126 
model_pd.l_d.mean(): -24.725200653076172 
model_pd.lagr.mean(): -24.594310760498047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0440], device='cuda:0')), ('power', tensor([-24.7692], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.13089022040367126
epoch£º298	 i:1 	 global-step:5961	 l-p:0.1396750658750534
epoch£º298	 i:2 	 global-step:5962	 l-p:0.15107719600200653
epoch£º298	 i:3 	 global-step:5963	 l-p:0.13415294885635376
epoch£º298	 i:4 	 global-step:5964	 l-p:0.15769781172275543
epoch£º298	 i:5 	 global-step:5965	 l-p:0.13928072154521942
epoch£º298	 i:6 	 global-step:5966	 l-p:1.0426675081253052
epoch£º298	 i:7 	 global-step:5967	 l-p:0.17908038198947906
epoch£º298	 i:8 	 global-step:5968	 l-p:0.14430676400661469
epoch£º298	 i:9 	 global-step:5969	 l-p:0.18252617120742798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7644, 2.7032, 2.7530],
        [2.7644, 2.4388, 2.5441],
        [2.7644, 2.2381, 2.2186],
        [2.7644, 2.0806, 1.8411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12578590214252472 
model_pd.l_d.mean(): -25.01995849609375 
model_pd.lagr.mean(): -24.89417266845703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0492], device='cuda:0')), ('power', tensor([-25.0692], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12578590214252472
epoch£º299	 i:1 	 global-step:5981	 l-p:0.16960713267326355
epoch£º299	 i:2 	 global-step:5982	 l-p:0.15432870388031006
epoch£º299	 i:3 	 global-step:5983	 l-p:0.13392959535121918
epoch£º299	 i:4 	 global-step:5984	 l-p:0.144635409116745
epoch£º299	 i:5 	 global-step:5985	 l-p:0.1543775349855423
epoch£º299	 i:6 	 global-step:5986	 l-p:0.1478845626115799
epoch£º299	 i:7 	 global-step:5987	 l-p:0.6838589310646057
epoch£º299	 i:8 	 global-step:5988	 l-p:0.11035623401403427
epoch£º299	 i:9 	 global-step:5989	 l-p:0.2338634878396988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7340, 2.0594, 1.5637],
        [2.7340, 2.5711, 2.6718],
        [2.7340, 2.7340, 2.7340],
        [2.7340, 2.5771, 2.6758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.1738814264535904 
model_pd.l_d.mean(): -25.104158401489258 
model_pd.lagr.mean(): -24.93027687072754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0326], device='cuda:0')), ('power', tensor([-25.1367], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.1738814264535904
epoch£º300	 i:1 	 global-step:6001	 l-p:0.20519496500492096
epoch£º300	 i:2 	 global-step:6002	 l-p:0.10588978230953217
epoch£º300	 i:3 	 global-step:6003	 l-p:0.12201033532619476
epoch£º300	 i:4 	 global-step:6004	 l-p:0.1280476301908493
epoch£º300	 i:5 	 global-step:6005	 l-p:0.11497952044010162
epoch£º300	 i:6 	 global-step:6006	 l-p:-0.020277775824069977
epoch£º300	 i:7 	 global-step:6007	 l-p:0.10455833375453949
epoch£º300	 i:8 	 global-step:6008	 l-p:0.09619510173797607
epoch£º300	 i:9 	 global-step:6009	 l-p:0.1288788914680481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0193, 2.7153, 2.8207],
        [3.0193, 3.0193, 3.0193],
        [3.0193, 2.3301, 2.0324],
        [3.0193, 3.0192, 3.0193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.11719493567943573 
model_pd.l_d.mean(): -24.532718658447266 
model_pd.lagr.mean(): -24.415523529052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0177], device='cuda:0')), ('power', tensor([-24.5505], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.11719493567943573
epoch£º301	 i:1 	 global-step:6021	 l-p:0.11225879192352295
epoch£º301	 i:2 	 global-step:6022	 l-p:0.11188136786222458
epoch£º301	 i:3 	 global-step:6023	 l-p:0.15326321125030518
epoch£º301	 i:4 	 global-step:6024	 l-p:0.1225692629814148
epoch£º301	 i:5 	 global-step:6025	 l-p:0.5325360894203186
epoch£º301	 i:6 	 global-step:6026	 l-p:0.21446120738983154
epoch£º301	 i:7 	 global-step:6027	 l-p:0.06339321285486221
epoch£º301	 i:8 	 global-step:6028	 l-p:0.16582460701465607
epoch£º301	 i:9 	 global-step:6029	 l-p:0.03312920778989792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5422, 2.1556, 2.2512],
        [2.5422, 2.5352, 2.5419],
        [2.5422, 1.8187, 1.3568],
        [2.5422, 2.5360, 2.5420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.18891994655132294 
model_pd.l_d.mean(): -25.181947708129883 
model_pd.lagr.mean(): -24.99302864074707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1436], device='cuda:0')), ('power', tensor([-25.3256], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.18891994655132294
epoch£º302	 i:1 	 global-step:6041	 l-p:0.2195834368467331
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12352544814348221
epoch£º302	 i:3 	 global-step:6043	 l-p:0.19152995944023132
epoch£º302	 i:4 	 global-step:6044	 l-p:0.1384052187204361
epoch£º302	 i:5 	 global-step:6045	 l-p:0.12287062406539917
epoch£º302	 i:6 	 global-step:6046	 l-p:0.13670702278614044
epoch£º302	 i:7 	 global-step:6047	 l-p:0.12871228158473969
epoch£º302	 i:8 	 global-step:6048	 l-p:0.10460233688354492
epoch£º302	 i:9 	 global-step:6049	 l-p:0.1227639764547348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8682, 2.8681, 2.8682],
        [2.8682, 2.5256, 2.6265],
        [2.8682, 2.2901, 1.7694],
        [2.8682, 2.3627, 2.3637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.12836618721485138 
model_pd.l_d.mean(): -24.466535568237305 
model_pd.lagr.mean(): -24.33816909790039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0307], device='cuda:0')), ('power', tensor([-24.4972], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.12836618721485138
epoch£º303	 i:1 	 global-step:6061	 l-p:0.14734157919883728
epoch£º303	 i:2 	 global-step:6062	 l-p:0.14881032705307007
epoch£º303	 i:3 	 global-step:6063	 l-p:0.09034772962331772
epoch£º303	 i:4 	 global-step:6064	 l-p:0.1522677093744278
epoch£º303	 i:5 	 global-step:6065	 l-p:0.14835135638713837
epoch£º303	 i:6 	 global-step:6066	 l-p:0.7807273864746094
epoch£º303	 i:7 	 global-step:6067	 l-p:0.21686884760856628
epoch£º303	 i:8 	 global-step:6068	 l-p:1.3057605028152466
epoch£º303	 i:9 	 global-step:6069	 l-p:0.11476480960845947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7742, 2.7742, 2.7742],
        [2.7742, 2.7742, 2.7742],
        [2.7742, 2.7676, 2.7739],
        [2.7742, 2.7719, 2.7742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.1439027339220047 
model_pd.l_d.mean(): -24.4907169342041 
model_pd.lagr.mean(): -24.34681510925293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0964], device='cuda:0')), ('power', tensor([-24.5872], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.1439027339220047
epoch£º304	 i:1 	 global-step:6081	 l-p:0.11508212983608246
epoch£º304	 i:2 	 global-step:6082	 l-p:0.1372067630290985
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14315706491470337
epoch£º304	 i:4 	 global-step:6084	 l-p:0.13716629147529602
epoch£º304	 i:5 	 global-step:6085	 l-p:0.15339577198028564
epoch£º304	 i:6 	 global-step:6086	 l-p:0.1689823716878891
epoch£º304	 i:7 	 global-step:6087	 l-p:0.13944390416145325
epoch£º304	 i:8 	 global-step:6088	 l-p:0.13480642437934875
epoch£º304	 i:9 	 global-step:6089	 l-p:0.14568869769573212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7366, 2.1986, 2.1857],
        [2.7366, 2.7358, 2.7366],
        [2.7366, 2.0400, 1.5408],
        [2.7366, 2.4511, 2.5686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.15999054908752441 
model_pd.l_d.mean(): -24.920684814453125 
model_pd.lagr.mean(): -24.76069450378418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1118], device='cuda:0')), ('power', tensor([-25.0325], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.15999054908752441
epoch£º305	 i:1 	 global-step:6101	 l-p:0.14590170979499817
epoch£º305	 i:2 	 global-step:6102	 l-p:0.13440556824207306
epoch£º305	 i:3 	 global-step:6103	 l-p:0.11685927957296371
epoch£º305	 i:4 	 global-step:6104	 l-p:0.7367022037506104
epoch£º305	 i:5 	 global-step:6105	 l-p:0.15240807831287384
epoch£º305	 i:6 	 global-step:6106	 l-p:0.1483955979347229
epoch£º305	 i:7 	 global-step:6107	 l-p:0.17681246995925903
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11957748979330063
epoch£º305	 i:9 	 global-step:6109	 l-p:0.11424855887889862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8651, 2.8651, 2.8651],
        [2.8651, 2.1704, 1.6514],
        [2.8651, 2.8651, 2.8651],
        [2.8651, 2.2086, 2.0340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.13987602293491364 
model_pd.l_d.mean(): -24.431711196899414 
model_pd.lagr.mean(): -24.29183578491211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0875], device='cuda:0')), ('power', tensor([-24.5192], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.13987602293491364
epoch£º306	 i:1 	 global-step:6121	 l-p:0.11996208131313324
epoch£º306	 i:2 	 global-step:6122	 l-p:0.1313452571630478
epoch£º306	 i:3 	 global-step:6123	 l-p:0.2070799618959427
epoch£º306	 i:4 	 global-step:6124	 l-p:0.1494147777557373
epoch£º306	 i:5 	 global-step:6125	 l-p:0.1538425087928772
epoch£º306	 i:6 	 global-step:6126	 l-p:0.17537617683410645
epoch£º306	 i:7 	 global-step:6127	 l-p:0.1446523368358612
epoch£º306	 i:8 	 global-step:6128	 l-p:0.14158454537391663
epoch£º306	 i:9 	 global-step:6129	 l-p:0.1165027767419815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6591, 2.6591, 2.6591],
        [2.6591, 2.4530, 2.5677],
        [2.6591, 1.8612, 1.3994],
        [2.6591, 2.4745, 2.5840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.17072324454784393 
model_pd.l_d.mean(): -25.165369033813477 
model_pd.lagr.mean(): -24.994646072387695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1407], device='cuda:0')), ('power', tensor([-25.3061], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.17072324454784393
epoch£º307	 i:1 	 global-step:6141	 l-p:0.15369278192520142
epoch£º307	 i:2 	 global-step:6142	 l-p:0.13100457191467285
epoch£º307	 i:3 	 global-step:6143	 l-p:0.15200276672840118
epoch£º307	 i:4 	 global-step:6144	 l-p:0.1366882622241974
epoch£º307	 i:5 	 global-step:6145	 l-p:0.1641768366098404
epoch£º307	 i:6 	 global-step:6146	 l-p:0.21740931272506714
epoch£º307	 i:7 	 global-step:6147	 l-p:0.14539985358715057
epoch£º307	 i:8 	 global-step:6148	 l-p:0.13471050560474396
epoch£º307	 i:9 	 global-step:6149	 l-p:0.0976208820939064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9636, 2.9083, 2.9542],
        [2.9636, 2.8849, 2.9463],
        [2.9636, 2.7288, 2.8447],
        [2.9636, 2.3182, 2.1574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.113995760679245 
model_pd.l_d.mean(): -25.171262741088867 
model_pd.lagr.mean(): -25.057266235351562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1301], device='cuda:0')), ('power', tensor([-25.0412], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.113995760679245
epoch£º308	 i:1 	 global-step:6161	 l-p:0.11472198367118835
epoch£º308	 i:2 	 global-step:6162	 l-p:0.10462498664855957
epoch£º308	 i:3 	 global-step:6163	 l-p:0.12746462225914001
epoch£º308	 i:4 	 global-step:6164	 l-p:0.14878793060779572
epoch£º308	 i:5 	 global-step:6165	 l-p:0.1920175552368164
epoch£º308	 i:6 	 global-step:6166	 l-p:0.11907104402780533
epoch£º308	 i:7 	 global-step:6167	 l-p:0.2705451250076294
epoch£º308	 i:8 	 global-step:6168	 l-p:0.19251586496829987
epoch£º308	 i:9 	 global-step:6169	 l-p:0.19558104872703552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6454, 2.2185, 2.3024],
        [2.6454, 1.8283, 1.4325],
        [2.6454, 2.6439, 2.6454],
        [2.6454, 1.8284, 1.3773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.012345065362751484 
model_pd.l_d.mean(): -24.44818878173828 
model_pd.lagr.mean(): -24.43584442138672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1727], device='cuda:0')), ('power', tensor([-24.6209], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.012345065362751484
epoch£º309	 i:1 	 global-step:6181	 l-p:0.2031242698431015
epoch£º309	 i:2 	 global-step:6182	 l-p:0.14814811944961548
epoch£º309	 i:3 	 global-step:6183	 l-p:0.11134514212608337
epoch£º309	 i:4 	 global-step:6184	 l-p:0.13621653616428375
epoch£º309	 i:5 	 global-step:6185	 l-p:0.11836022883653641
epoch£º309	 i:6 	 global-step:6186	 l-p:0.12309694290161133
epoch£º309	 i:7 	 global-step:6187	 l-p:0.1271136999130249
epoch£º309	 i:8 	 global-step:6188	 l-p:0.12490235269069672
epoch£º309	 i:9 	 global-step:6189	 l-p:0.13422225415706635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7908, 2.7430, 2.7835],
        [2.7908, 2.7897, 2.7908],
        [2.7908, 1.9872, 1.5756],
        [2.7908, 2.0150, 1.5156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.177703395485878 
model_pd.l_d.mean(): -25.185123443603516 
model_pd.lagr.mean(): -25.00741958618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0346], device='cuda:0')), ('power', tensor([-25.2197], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.177703395485878
epoch£º310	 i:1 	 global-step:6201	 l-p:0.09613759815692902
epoch£º310	 i:2 	 global-step:6202	 l-p:0.15096712112426758
epoch£º310	 i:3 	 global-step:6203	 l-p:0.13426505029201508
epoch£º310	 i:4 	 global-step:6204	 l-p:0.04408642277121544
epoch£º310	 i:5 	 global-step:6205	 l-p:0.23546113073825836
epoch£º310	 i:6 	 global-step:6206	 l-p:-0.7202736735343933
epoch£º310	 i:7 	 global-step:6207	 l-p:0.15544165670871735
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10764459520578384
epoch£º310	 i:9 	 global-step:6209	 l-p:0.12812688946723938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[2.9924, 2.4448, 2.4171],
        [2.9924, 2.4568, 2.4415],
        [2.9924, 2.6087, 2.7006],
        [2.9924, 2.3803, 2.2751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.09952041506767273 
model_pd.l_d.mean(): -24.47641372680664 
model_pd.lagr.mean(): -24.376893997192383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0148], device='cuda:0')), ('power', tensor([-24.4912], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.09952041506767273
epoch£º311	 i:1 	 global-step:6221	 l-p:0.13407215476036072
epoch£º311	 i:2 	 global-step:6222	 l-p:0.12160851806402206
epoch£º311	 i:3 	 global-step:6223	 l-p:0.12946152687072754
epoch£º311	 i:4 	 global-step:6224	 l-p:0.11950796097517014
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11587365716695786
epoch£º311	 i:6 	 global-step:6226	 l-p:0.17382113635540009
epoch£º311	 i:7 	 global-step:6227	 l-p:0.11298991739749908
epoch£º311	 i:8 	 global-step:6228	 l-p:0.19735737144947052
epoch£º311	 i:9 	 global-step:6229	 l-p:0.17259471118450165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6658, 2.0813, 2.0469],
        [2.6658, 2.6255, 2.6604],
        [2.6658, 2.6658, 2.6658],
        [2.6658, 1.8676, 1.3923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.14008429646492004 
model_pd.l_d.mean(): -25.18598175048828 
model_pd.lagr.mean(): -25.045896530151367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0817], device='cuda:0')), ('power', tensor([-25.2677], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.14008429646492004
epoch£º312	 i:1 	 global-step:6241	 l-p:0.10669366270303726
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10600177198648453
epoch£º312	 i:3 	 global-step:6243	 l-p:0.14298592507839203
epoch£º312	 i:4 	 global-step:6244	 l-p:0.14128977060317993
epoch£º312	 i:5 	 global-step:6245	 l-p:0.1494576632976532
epoch£º312	 i:6 	 global-step:6246	 l-p:0.14755943417549133
epoch£º312	 i:7 	 global-step:6247	 l-p:0.14759701490402222
epoch£º312	 i:8 	 global-step:6248	 l-p:0.17467132210731506
epoch£º312	 i:9 	 global-step:6249	 l-p:0.17562206089496613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5657, 2.3656, 2.4816],
        [2.5657, 1.8077, 1.3341],
        [2.5657, 1.9722, 1.9390],
        [2.5657, 2.4964, 2.5525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.9450188875198364 
model_pd.l_d.mean(): -25.142311096191406 
model_pd.lagr.mean(): -24.19729232788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1369], device='cuda:0')), ('power', tensor([-25.2792], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.9450188875198364
epoch£º313	 i:1 	 global-step:6261	 l-p:0.22071072459220886
epoch£º313	 i:2 	 global-step:6262	 l-p:0.11668433248996735
epoch£º313	 i:3 	 global-step:6263	 l-p:0.21830958127975464
epoch£º313	 i:4 	 global-step:6264	 l-p:0.14938165247440338
epoch£º313	 i:5 	 global-step:6265	 l-p:0.20864754915237427
epoch£º313	 i:6 	 global-step:6266	 l-p:0.13489863276481628
epoch£º313	 i:7 	 global-step:6267	 l-p:0.12231935560703278
epoch£º313	 i:8 	 global-step:6268	 l-p:0.10782814770936966
epoch£º313	 i:9 	 global-step:6269	 l-p:0.12925012409687042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9646, 2.9646, 2.9646],
        [2.9646, 2.8793, 2.9451],
        [2.9646, 2.6790, 2.7982],
        [2.9646, 2.5921, 2.6925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.10900288075208664 
model_pd.l_d.mean(): -24.77138328552246 
model_pd.lagr.mean(): -24.66238021850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0255], device='cuda:0')), ('power', tensor([-24.7969], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.10900288075208664
epoch£º314	 i:1 	 global-step:6281	 l-p:0.13701534271240234
epoch£º314	 i:2 	 global-step:6282	 l-p:0.13603627681732178
epoch£º314	 i:3 	 global-step:6283	 l-p:0.13322071731090546
epoch£º314	 i:4 	 global-step:6284	 l-p:0.09861190617084503
epoch£º314	 i:5 	 global-step:6285	 l-p:0.21762898564338684
epoch£º314	 i:6 	 global-step:6286	 l-p:0.39587321877479553
epoch£º314	 i:7 	 global-step:6287	 l-p:0.2079697996377945
epoch£º314	 i:8 	 global-step:6288	 l-p:0.10158288478851318
epoch£º314	 i:9 	 global-step:6289	 l-p:0.5652212500572205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7452, 1.9294, 1.5394],
        [2.7452, 2.2014, 2.2068],
        [2.7452, 2.7452, 2.7452],
        [2.7452, 1.9237, 1.4434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.15170475840568542 
model_pd.l_d.mean(): -24.972766876220703 
model_pd.lagr.mean(): -24.821062088012695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0910], device='cuda:0')), ('power', tensor([-25.0637], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.15170475840568542
epoch£º315	 i:1 	 global-step:6301	 l-p:-0.015626268461346626
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11237100511789322
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12901388108730316
epoch£º315	 i:4 	 global-step:6304	 l-p:0.1283583641052246
epoch£º315	 i:5 	 global-step:6305	 l-p:0.15132316946983337
epoch£º315	 i:6 	 global-step:6306	 l-p:0.16555090248584747
epoch£º315	 i:7 	 global-step:6307	 l-p:0.15691258013248444
epoch£º315	 i:8 	 global-step:6308	 l-p:0.1341865062713623
epoch£º315	 i:9 	 global-step:6309	 l-p:0.16072070598602295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6125, 2.5533, 2.6024],
        [2.6125, 2.6032, 2.6120],
        [2.6125, 1.9673, 1.8779],
        [2.6125, 2.3976, 2.5173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.17421308159828186 
model_pd.l_d.mean(): -25.263198852539062 
model_pd.lagr.mean(): -25.088985443115234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0529], device='cuda:0')), ('power', tensor([-25.3161], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.17421308159828186
epoch£º316	 i:1 	 global-step:6321	 l-p:0.6030014157295227
epoch£º316	 i:2 	 global-step:6322	 l-p:0.2446255385875702
epoch£º316	 i:3 	 global-step:6323	 l-p:0.2761334180831909
epoch£º316	 i:4 	 global-step:6324	 l-p:0.1320851445198059
epoch£º316	 i:5 	 global-step:6325	 l-p:0.16942591965198517
epoch£º316	 i:6 	 global-step:6326	 l-p:0.10039374977350235
epoch£º316	 i:7 	 global-step:6327	 l-p:0.19900433719158173
epoch£º316	 i:8 	 global-step:6328	 l-p:0.13306036591529846
epoch£º316	 i:9 	 global-step:6329	 l-p:0.16543026268482208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7719, 2.5357, 2.6577],
        [2.7719, 2.7656, 2.7716],
        [2.7719, 2.3401, 2.4258],
        [2.7719, 2.3905, 2.4973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.17390231788158417 
model_pd.l_d.mean(): -25.008939743041992 
model_pd.lagr.mean(): -24.835037231445312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0941], device='cuda:0')), ('power', tensor([-25.1030], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.17390231788158417
epoch£º317	 i:1 	 global-step:6341	 l-p:0.12309910356998444
epoch£º317	 i:2 	 global-step:6342	 l-p:0.1505245417356491
epoch£º317	 i:3 	 global-step:6343	 l-p:0.15648068487644196
epoch£º317	 i:4 	 global-step:6344	 l-p:0.17527495324611664
epoch£º317	 i:5 	 global-step:6345	 l-p:0.13819874823093414
epoch£º317	 i:6 	 global-step:6346	 l-p:0.11448901891708374
epoch£º317	 i:7 	 global-step:6347	 l-p:0.11492015421390533
epoch£º317	 i:8 	 global-step:6348	 l-p:0.135625422000885
epoch£º317	 i:9 	 global-step:6349	 l-p:0.11504562199115753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7664, 2.5178, 2.6416],
        [2.7664, 2.7653, 2.7664],
        [2.7664, 2.7047, 2.7555],
        [2.7664, 2.7626, 2.7663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.17139707505702972 
model_pd.l_d.mean(): -24.970745086669922 
model_pd.lagr.mean(): -24.799348831176758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1056], device='cuda:0')), ('power', tensor([-25.0763], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.17139707505702972
epoch£º318	 i:1 	 global-step:6361	 l-p:0.1546589583158493
epoch£º318	 i:2 	 global-step:6362	 l-p:0.16145801544189453
epoch£º318	 i:3 	 global-step:6363	 l-p:0.1557178944349289
epoch£º318	 i:4 	 global-step:6364	 l-p:0.16436585783958435
epoch£º318	 i:5 	 global-step:6365	 l-p:0.17999473214149475
epoch£º318	 i:6 	 global-step:6366	 l-p:0.031335845589637756
epoch£º318	 i:7 	 global-step:6367	 l-p:0.12274613976478577
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12511113286018372
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12664470076560974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9532, 2.5728, 2.6756],
        [2.9532, 2.4304, 2.4467],
        [2.9532, 2.9532, 2.9532],
        [2.9532, 2.9532, 2.9532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.1441180258989334 
model_pd.l_d.mean(): -25.06117057800293 
model_pd.lagr.mean(): -24.91705322265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0115], device='cuda:0')), ('power', tensor([-25.0496], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.1441180258989334
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11483196914196014
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12992888689041138
epoch£º319	 i:3 	 global-step:6383	 l-p:0.14150625467300415
epoch£º319	 i:4 	 global-step:6384	 l-p:0.15783946216106415
epoch£º319	 i:5 	 global-step:6385	 l-p:0.1522381603717804
epoch£º319	 i:6 	 global-step:6386	 l-p:0.15385985374450684
epoch£º319	 i:7 	 global-step:6387	 l-p:0.11356313526630402
epoch£º319	 i:8 	 global-step:6388	 l-p:0.3469896912574768
epoch£º319	 i:9 	 global-step:6389	 l-p:0.20138931274414062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6757, 2.1442, 2.1741],
        [2.6757, 1.9848, 1.8401],
        [2.6757, 2.0179, 1.9176],
        [2.6757, 2.6497, 2.6731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.4436587691307068 
model_pd.l_d.mean(): -24.959896087646484 
model_pd.lagr.mean(): -24.516237258911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1001], device='cuda:0')), ('power', tensor([-25.0600], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.4436587691307068
epoch£º320	 i:1 	 global-step:6401	 l-p:0.16386069357395172
epoch£º320	 i:2 	 global-step:6402	 l-p:0.10126260668039322
epoch£º320	 i:3 	 global-step:6403	 l-p:0.13072064518928528
epoch£º320	 i:4 	 global-step:6404	 l-p:0.18770578503608704
epoch£º320	 i:5 	 global-step:6405	 l-p:0.13264450430870056
epoch£º320	 i:6 	 global-step:6406	 l-p:0.123814158141613
epoch£º320	 i:7 	 global-step:6407	 l-p:0.14473889768123627
epoch£º320	 i:8 	 global-step:6408	 l-p:0.14572736620903015
epoch£º320	 i:9 	 global-step:6409	 l-p:0.14655046164989471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7983, 2.6656, 2.7578],
        [2.7983, 1.9481, 1.4742],
        [2.7983, 2.7622, 2.7939],
        [2.7983, 2.7965, 2.7983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.12576361000537872 
model_pd.l_d.mean(): -24.683382034301758 
model_pd.lagr.mean(): -24.557619094848633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0910], device='cuda:0')), ('power', tensor([-24.7744], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.12576361000537872
epoch£º321	 i:1 	 global-step:6421	 l-p:-0.46163511276245117
epoch£º321	 i:2 	 global-step:6422	 l-p:0.12981478869915009
epoch£º321	 i:3 	 global-step:6423	 l-p:0.08777246624231339
epoch£º321	 i:4 	 global-step:6424	 l-p:0.15632513165473938
epoch£º321	 i:5 	 global-step:6425	 l-p:0.1404038369655609
epoch£º321	 i:6 	 global-step:6426	 l-p:0.15744902193546295
epoch£º321	 i:7 	 global-step:6427	 l-p:0.3081188499927521
epoch£º321	 i:8 	 global-step:6428	 l-p:0.14750367403030396
epoch£º321	 i:9 	 global-step:6429	 l-p:0.1502358764410019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8598, 2.2317, 1.6928],
        [2.8598, 2.8590, 2.8598],
        [2.8598, 2.8566, 2.8597],
        [2.8598, 2.8598, 2.8598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.15238064527511597 
model_pd.l_d.mean(): -25.09498405456543 
model_pd.lagr.mean(): -24.942604064941406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0118], device='cuda:0')), ('power', tensor([-25.0832], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.15238064527511597
epoch£º322	 i:1 	 global-step:6441	 l-p:0.15076880156993866
epoch£º322	 i:2 	 global-step:6442	 l-p:0.12145108729600906
epoch£º322	 i:3 	 global-step:6443	 l-p:0.2520413100719452
epoch£º322	 i:4 	 global-step:6444	 l-p:0.12896843254566193
epoch£º322	 i:5 	 global-step:6445	 l-p:0.17319747805595398
epoch£º322	 i:6 	 global-step:6446	 l-p:0.14536772668361664
epoch£º322	 i:7 	 global-step:6447	 l-p:0.2965790331363678
epoch£º322	 i:8 	 global-step:6448	 l-p:0.1378900706768036
epoch£º322	 i:9 	 global-step:6449	 l-p:0.1420276015996933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9617, 2.9616, 2.9617],
        [2.9617, 2.9256, 2.9573],
        [2.9617, 2.1641, 1.7949],
        [2.9617, 2.8079, 2.9088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.12304739654064178 
model_pd.l_d.mean(): -24.97616958618164 
model_pd.lagr.mean(): -24.85312271118164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0082], device='cuda:0')), ('power', tensor([-24.9679], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.12304739654064178
epoch£º323	 i:1 	 global-step:6461	 l-p:0.13107822835445404
epoch£º323	 i:2 	 global-step:6462	 l-p:0.08814454823732376
epoch£º323	 i:3 	 global-step:6463	 l-p:0.14606131613254547
epoch£º323	 i:4 	 global-step:6464	 l-p:0.14368067681789398
epoch£º323	 i:5 	 global-step:6465	 l-p:0.14883500337600708
epoch£º323	 i:6 	 global-step:6466	 l-p:0.0209346916526556
epoch£º323	 i:7 	 global-step:6467	 l-p:-0.865886390209198
epoch£º323	 i:8 	 global-step:6468	 l-p:0.2936592996120453
epoch£º323	 i:9 	 global-step:6469	 l-p:0.1415398269891739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5644, 2.5643, 2.5644],
        [2.5644, 2.5644, 2.5644],
        [2.5644, 2.4853, 2.5484],
        [2.5644, 2.1462, 2.2549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.27736812829971313 
model_pd.l_d.mean(): -24.82497787475586 
model_pd.lagr.mean(): -24.547609329223633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3350], device='cuda:0')), ('power', tensor([-25.1600], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.27736812829971313
epoch£º324	 i:1 	 global-step:6481	 l-p:0.17749568819999695
epoch£º324	 i:2 	 global-step:6482	 l-p:0.09155155718326569
epoch£º324	 i:3 	 global-step:6483	 l-p:0.1462988406419754
epoch£º324	 i:4 	 global-step:6484	 l-p:0.16438613831996918
epoch£º324	 i:5 	 global-step:6485	 l-p:0.12715928256511688
epoch£º324	 i:6 	 global-step:6486	 l-p:0.12033098191022873
epoch£º324	 i:7 	 global-step:6487	 l-p:0.11100003868341446
epoch£º324	 i:8 	 global-step:6488	 l-p:0.1266922652721405
epoch£º324	 i:9 	 global-step:6489	 l-p:0.12550809979438782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8932, 2.3168, 2.3035],
        [2.8932, 2.8157, 2.8773],
        [2.8932, 2.8920, 2.8932],
        [2.8932, 2.1352, 1.8732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.12548735737800598 
model_pd.l_d.mean(): -24.577348709106445 
model_pd.lagr.mean(): -24.451860427856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0448], device='cuda:0')), ('power', tensor([-24.6221], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.12548735737800598
epoch£º325	 i:1 	 global-step:6501	 l-p:0.1767934411764145
epoch£º325	 i:2 	 global-step:6502	 l-p:0.19102542102336884
epoch£º325	 i:3 	 global-step:6503	 l-p:0.14889654517173767
epoch£º325	 i:4 	 global-step:6504	 l-p:0.13713741302490234
epoch£º325	 i:5 	 global-step:6505	 l-p:0.1519303172826767
epoch£º325	 i:6 	 global-step:6506	 l-p:0.3352195620536804
epoch£º325	 i:7 	 global-step:6507	 l-p:0.14845499396324158
epoch£º325	 i:8 	 global-step:6508	 l-p:0.11677464097738266
epoch£º325	 i:9 	 global-step:6509	 l-p:0.12352441251277924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8126, 2.0628, 1.8344],
        [2.8126, 2.8051, 2.8122],
        [2.8126, 2.1436, 1.6082],
        [2.8126, 2.0882, 1.9004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.13224980235099792 
model_pd.l_d.mean(): -24.980588912963867 
model_pd.lagr.mean(): -24.848339080810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0650], device='cuda:0')), ('power', tensor([-25.0456], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.13224980235099792
epoch£º326	 i:1 	 global-step:6521	 l-p:0.1269664168357849
epoch£º326	 i:2 	 global-step:6522	 l-p:0.2471274882555008
epoch£º326	 i:3 	 global-step:6523	 l-p:0.17196068167686462
epoch£º326	 i:4 	 global-step:6524	 l-p:0.1337544471025467
epoch£º326	 i:5 	 global-step:6525	 l-p:0.1566917598247528
epoch£º326	 i:6 	 global-step:6526	 l-p:0.10956651717424393
epoch£º326	 i:7 	 global-step:6527	 l-p:0.12118424475193024
epoch£º326	 i:8 	 global-step:6528	 l-p:0.09832660108804703
epoch£º326	 i:9 	 global-step:6529	 l-p:0.14005911350250244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6682, 1.9901, 1.4714],
        [2.6682, 2.6670, 2.6682],
        [2.6682, 2.3527, 2.4836],
        [2.6682, 2.6600, 2.6679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.15277299284934998 
model_pd.l_d.mean(): -24.9343318939209 
model_pd.lagr.mean(): -24.781558990478516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1419], device='cuda:0')), ('power', tensor([-25.0763], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.15277299284934998
epoch£º327	 i:1 	 global-step:6541	 l-p:0.12912067770957947
epoch£º327	 i:2 	 global-step:6542	 l-p:0.008205881342291832
epoch£º327	 i:3 	 global-step:6543	 l-p:0.6744126677513123
epoch£º327	 i:4 	 global-step:6544	 l-p:0.13377855718135834
epoch£º327	 i:5 	 global-step:6545	 l-p:0.19964133203029633
epoch£º327	 i:6 	 global-step:6546	 l-p:0.1487850695848465
epoch£º327	 i:7 	 global-step:6547	 l-p:0.15994760394096375
epoch£º327	 i:8 	 global-step:6548	 l-p:0.1137714758515358
epoch£º327	 i:9 	 global-step:6549	 l-p:0.11817570775747299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1341, 3.0830, 3.1262],
        [3.1341, 3.1145, 3.1325],
        [3.1341, 2.4262, 2.2202],
        [3.1341, 2.3519, 1.7762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.11601877957582474 
model_pd.l_d.mean(): -24.80690574645996 
model_pd.lagr.mean(): -24.690887451171875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0614], device='cuda:0')), ('power', tensor([-24.7456], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.11601877957582474
epoch£º328	 i:1 	 global-step:6561	 l-p:0.1669776439666748
epoch£º328	 i:2 	 global-step:6562	 l-p:0.11107324063777924
epoch£º328	 i:3 	 global-step:6563	 l-p:-0.0026720762252807617
epoch£º328	 i:4 	 global-step:6564	 l-p:0.12490174919366837
epoch£º328	 i:5 	 global-step:6565	 l-p:0.0786285549402237
epoch£º328	 i:6 	 global-step:6566	 l-p:0.16776371002197266
epoch£º328	 i:7 	 global-step:6567	 l-p:0.09394977241754532
epoch£º328	 i:8 	 global-step:6568	 l-p:0.3328516185283661
epoch£º328	 i:9 	 global-step:6569	 l-p:0.07723632454872131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3269, 3.3268, 3.3269],
        [3.3269, 2.5187, 1.9969],
        [3.3269, 3.3203, 3.3266],
        [3.3269, 3.0809, 3.2021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.017363155260682106 
model_pd.l_d.mean(): -24.38144302368164 
model_pd.lagr.mean(): -24.36408042907715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0742], device='cuda:0')), ('power', tensor([-24.3073], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.017363155260682106
epoch£º329	 i:1 	 global-step:6581	 l-p:0.11302748322486877
epoch£º329	 i:2 	 global-step:6582	 l-p:-0.5341747999191284
epoch£º329	 i:3 	 global-step:6583	 l-p:0.11298719048500061
epoch£º329	 i:4 	 global-step:6584	 l-p:0.08847019076347351
epoch£º329	 i:5 	 global-step:6585	 l-p:0.11375568062067032
epoch£º329	 i:6 	 global-step:6586	 l-p:0.0906282290816307
epoch£º329	 i:7 	 global-step:6587	 l-p:0.12674202024936676
epoch£º329	 i:8 	 global-step:6588	 l-p:0.11972621828317642
epoch£º329	 i:9 	 global-step:6589	 l-p:0.11469253897666931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1029, 3.0972, 3.1027],
        [3.1029, 3.0944, 3.1025],
        [3.1029, 3.0980, 3.1027],
        [3.1029, 3.1029, 3.1029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.116734080016613 
model_pd.l_d.mean(): -24.970388412475586 
model_pd.lagr.mean(): -24.853654861450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1171], device='cuda:0')), ('power', tensor([-24.8533], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.116734080016613
epoch£º330	 i:1 	 global-step:6601	 l-p:0.14048491418361664
epoch£º330	 i:2 	 global-step:6602	 l-p:0.14201602339744568
epoch£º330	 i:3 	 global-step:6603	 l-p:0.058185137808322906
epoch£º330	 i:4 	 global-step:6604	 l-p:0.12713544070720673
epoch£º330	 i:5 	 global-step:6605	 l-p:0.2514835596084595
epoch£º330	 i:6 	 global-step:6606	 l-p:0.01589103415608406
epoch£º330	 i:7 	 global-step:6607	 l-p:0.1472054421901703
epoch£º330	 i:8 	 global-step:6608	 l-p:0.0781509280204773
epoch£º330	 i:9 	 global-step:6609	 l-p:0.11413843929767609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5988, 1.8721, 1.7238],
        [2.5988, 1.8788, 1.3720],
        [2.5988, 2.5726, 2.5963],
        [2.5988, 1.8266, 1.3304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.11859563738107681 
model_pd.l_d.mean(): -24.99154281616211 
model_pd.lagr.mean(): -24.872947692871094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1967], device='cuda:0')), ('power', tensor([-25.1882], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.11859563738107681
epoch£º331	 i:1 	 global-step:6621	 l-p:0.10780393332242966
epoch£º331	 i:2 	 global-step:6622	 l-p:0.12172914296388626
epoch£º331	 i:3 	 global-step:6623	 l-p:0.1515980362892151
epoch£º331	 i:4 	 global-step:6624	 l-p:0.0989178940653801
epoch£º331	 i:5 	 global-step:6625	 l-p:0.10779615491628647
epoch£º331	 i:6 	 global-step:6626	 l-p:0.11457855999469757
epoch£º331	 i:7 	 global-step:6627	 l-p:0.07776165008544922
epoch£º331	 i:8 	 global-step:6628	 l-p:0.12549293041229248
epoch£º331	 i:9 	 global-step:6629	 l-p:0.13839760422706604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4782, 2.9682, 2.9820],
        [3.4782, 2.7095, 2.2865],
        [3.4782, 2.8277, 2.6690],
        [3.4782, 3.4572, 3.4764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.0035932420287281275 
model_pd.l_d.mean(): -24.750988006591797 
model_pd.lagr.mean(): -24.747394561767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2174], device='cuda:0')), ('power', tensor([-24.5335], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.0035932420287281275
epoch£º332	 i:1 	 global-step:6641	 l-p:0.34776487946510315
epoch£º332	 i:2 	 global-step:6642	 l-p:0.1030937135219574
epoch£º332	 i:3 	 global-step:6643	 l-p:0.134876549243927
epoch£º332	 i:4 	 global-step:6644	 l-p:0.11048801988363266
epoch£º332	 i:5 	 global-step:6645	 l-p:0.03341563045978546
epoch£º332	 i:6 	 global-step:6646	 l-p:0.09594440460205078
epoch£º332	 i:7 	 global-step:6647	 l-p:0.12161396443843842
epoch£º332	 i:8 	 global-step:6648	 l-p:0.0835021585226059
epoch£º332	 i:9 	 global-step:6649	 l-p:0.9688485264778137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4611, 3.4611, 3.4611],
        [3.4611, 2.6656, 2.1390],
        [3.4611, 3.4611, 3.4611],
        [3.4611, 3.4610, 3.4611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.13589343428611755 
model_pd.l_d.mean(): -24.801605224609375 
model_pd.lagr.mean(): -24.665712356567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2483], device='cuda:0')), ('power', tensor([-24.5533], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.13589343428611755
epoch£º333	 i:1 	 global-step:6661	 l-p:0.12039320915937424
epoch£º333	 i:2 	 global-step:6662	 l-p:0.15325389802455902
epoch£º333	 i:3 	 global-step:6663	 l-p:0.11498410254716873
epoch£º333	 i:4 	 global-step:6664	 l-p:0.11964093148708344
epoch£º333	 i:5 	 global-step:6665	 l-p:0.12658454477787018
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12343411147594452
epoch£º333	 i:7 	 global-step:6667	 l-p:0.0982651337981224
epoch£º333	 i:8 	 global-step:6668	 l-p:0.16768498718738556
epoch£º333	 i:9 	 global-step:6669	 l-p:0.10403092950582504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6542, 2.6511, 2.6541],
        [2.6542, 2.6335, 2.6525],
        [2.6542, 1.9920, 1.9253],
        [2.6542, 2.1870, 2.2794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.2123522013425827 
model_pd.l_d.mean(): -25.265636444091797 
model_pd.lagr.mean(): -25.05328369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0730], device='cuda:0')), ('power', tensor([-25.3386], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.2123522013425827
epoch£º334	 i:1 	 global-step:6681	 l-p:-0.1382620930671692
epoch£º334	 i:2 	 global-step:6682	 l-p:0.17581404745578766
epoch£º334	 i:3 	 global-step:6683	 l-p:0.22070452570915222
epoch£º334	 i:4 	 global-step:6684	 l-p:0.12484999746084213
epoch£º334	 i:5 	 global-step:6685	 l-p:0.1319187730550766
epoch£º334	 i:6 	 global-step:6686	 l-p:0.1300678849220276
epoch£º334	 i:7 	 global-step:6687	 l-p:0.12814408540725708
epoch£º334	 i:8 	 global-step:6688	 l-p:0.1329016089439392
epoch£º334	 i:9 	 global-step:6689	 l-p:0.1204891949892044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7398, 2.4247, 2.5573],
        [2.7398, 2.0153, 1.4887],
        [2.7398, 2.7398, 2.7398],
        [2.7398, 2.0578, 1.5258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.11495956778526306 
model_pd.l_d.mean(): -24.55011558532715 
model_pd.lagr.mean(): -24.435155868530273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1582], device='cuda:0')), ('power', tensor([-24.7084], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.11495956778526306
epoch£º335	 i:1 	 global-step:6701	 l-p:0.2005719691514969
epoch£º335	 i:2 	 global-step:6702	 l-p:0.1599646359682083
epoch£º335	 i:3 	 global-step:6703	 l-p:0.024472016841173172
epoch£º335	 i:4 	 global-step:6704	 l-p:0.29546642303466797
epoch£º335	 i:5 	 global-step:6705	 l-p:0.261113703250885
epoch£º335	 i:6 	 global-step:6706	 l-p:0.0797240361571312
epoch£º335	 i:7 	 global-step:6707	 l-p:0.15332140028476715
epoch£º335	 i:8 	 global-step:6708	 l-p:0.1517404317855835
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12910397350788116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[2.9016, 2.6046, 2.7354],
        [2.9016, 2.4224, 2.4983],
        [2.9016, 2.0207, 1.5504],
        [2.9016, 2.5642, 2.6917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.12004676461219788 
model_pd.l_d.mean(): -24.60863494873047 
model_pd.lagr.mean(): -24.488588333129883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0029], device='cuda:0')), ('power', tensor([-24.6058], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.12004676461219788
epoch£º336	 i:1 	 global-step:6721	 l-p:0.1521724909543991
epoch£º336	 i:2 	 global-step:6722	 l-p:0.12371643632650375
epoch£º336	 i:3 	 global-step:6723	 l-p:0.21983981132507324
epoch£º336	 i:4 	 global-step:6724	 l-p:0.15549075603485107
epoch£º336	 i:5 	 global-step:6725	 l-p:0.15036286413669586
epoch£º336	 i:6 	 global-step:6726	 l-p:0.09950833767652512
epoch£º336	 i:7 	 global-step:6727	 l-p:0.15906411409378052
epoch£º336	 i:8 	 global-step:6728	 l-p:0.2089250534772873
epoch£º336	 i:9 	 global-step:6729	 l-p:0.532634437084198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6539, 2.0234, 1.4918],
        [2.6539, 2.4090, 2.5399],
        [2.6539, 1.8417, 1.5732],
        [2.6539, 1.8008, 1.3127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.16567683219909668 
model_pd.l_d.mean(): -24.3298397064209 
model_pd.lagr.mean(): -24.16416358947754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2359], device='cuda:0')), ('power', tensor([-24.5657], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.16567683219909668
epoch£º337	 i:1 	 global-step:6741	 l-p:0.18961617350578308
epoch£º337	 i:2 	 global-step:6742	 l-p:0.18019533157348633
epoch£º337	 i:3 	 global-step:6743	 l-p:0.12188142538070679
epoch£º337	 i:4 	 global-step:6744	 l-p:0.1356828808784485
epoch£º337	 i:5 	 global-step:6745	 l-p:0.06624476611614227
epoch£º337	 i:6 	 global-step:6746	 l-p:0.09027271717786789
epoch£º337	 i:7 	 global-step:6747	 l-p:0.11979209631681442
epoch£º337	 i:8 	 global-step:6748	 l-p:0.1252284049987793
epoch£º337	 i:9 	 global-step:6749	 l-p:0.11895282566547394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0454, 2.6771, 2.7962],
        [3.0454, 2.8713, 2.9814],
        [3.0454, 3.0106, 3.0413],
        [3.0454, 3.0454, 3.0454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.11665137857198715 
model_pd.l_d.mean(): -24.890634536743164 
model_pd.lagr.mean(): -24.773983001708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0468], device='cuda:0')), ('power', tensor([-24.8438], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.11665137857198715
epoch£º338	 i:1 	 global-step:6761	 l-p:0.11461412161588669
epoch£º338	 i:2 	 global-step:6762	 l-p:0.16811558604240417
epoch£º338	 i:3 	 global-step:6763	 l-p:0.1379997283220291
epoch£º338	 i:4 	 global-step:6764	 l-p:0.10289132595062256
epoch£º338	 i:5 	 global-step:6765	 l-p:0.23245856165885925
epoch£º338	 i:6 	 global-step:6766	 l-p:0.3410564363002777
epoch£º338	 i:7 	 global-step:6767	 l-p:0.28147634863853455
epoch£º338	 i:8 	 global-step:6768	 l-p:0.14122368395328522
epoch£º338	 i:9 	 global-step:6769	 l-p:0.1784353405237198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8480, 2.0423, 1.5086],
        [2.8480, 2.5751, 2.7074],
        [2.8480, 2.6645, 2.7790],
        [2.8480, 2.8480, 2.8480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.13092076778411865 
model_pd.l_d.mean(): -25.04013442993164 
model_pd.lagr.mean(): -24.90921401977539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0537], device='cuda:0')), ('power', tensor([-25.0938], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.13092076778411865
epoch£º339	 i:1 	 global-step:6781	 l-p:0.14065873622894287
epoch£º339	 i:2 	 global-step:6782	 l-p:0.12124276906251907
epoch£º339	 i:3 	 global-step:6783	 l-p:0.14201322197914124
epoch£º339	 i:4 	 global-step:6784	 l-p:0.14105916023254395
epoch£º339	 i:5 	 global-step:6785	 l-p:0.1900157481431961
epoch£º339	 i:6 	 global-step:6786	 l-p:0.16478167474269867
epoch£º339	 i:7 	 global-step:6787	 l-p:0.1510610580444336
epoch£º339	 i:8 	 global-step:6788	 l-p:-0.904322624206543
epoch£º339	 i:9 	 global-step:6789	 l-p:0.03194368630647659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7408, 1.8549, 1.3641],
        [2.7408, 2.7243, 2.7396],
        [2.7408, 2.7398, 2.7408],
        [2.7408, 2.3759, 2.5048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.14548447728157043 
model_pd.l_d.mean(): -24.859907150268555 
model_pd.lagr.mean(): -24.71442222595215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1466], device='cuda:0')), ('power', tensor([-25.0065], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.14548447728157043
epoch£º340	 i:1 	 global-step:6801	 l-p:0.14958472549915314
epoch£º340	 i:2 	 global-step:6802	 l-p:0.1840180605649948
epoch£º340	 i:3 	 global-step:6803	 l-p:0.14448055624961853
epoch£º340	 i:4 	 global-step:6804	 l-p:0.12876524031162262
epoch£º340	 i:5 	 global-step:6805	 l-p:0.15395350754261017
epoch£º340	 i:6 	 global-step:6806	 l-p:0.14363907277584076
epoch£º340	 i:7 	 global-step:6807	 l-p:0.1268131583929062
epoch£º340	 i:8 	 global-step:6808	 l-p:0.15901750326156616
epoch£º340	 i:9 	 global-step:6809	 l-p:0.13111740350723267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7233, 2.7177, 2.7231],
        [2.7233, 2.2358, 2.3210],
        [2.7233, 1.8631, 1.3613],
        [2.7233, 1.8699, 1.3658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.16975466907024384 
model_pd.l_d.mean(): -24.98497772216797 
model_pd.lagr.mean(): -24.815223693847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1847], device='cuda:0')), ('power', tensor([-25.1697], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.16975466907024384
epoch£º341	 i:1 	 global-step:6821	 l-p:0.13763798773288727
epoch£º341	 i:2 	 global-step:6822	 l-p:0.10983532667160034
epoch£º341	 i:3 	 global-step:6823	 l-p:0.17348255217075348
epoch£º341	 i:4 	 global-step:6824	 l-p:0.1583327203989029
epoch£º341	 i:5 	 global-step:6825	 l-p:0.13081468641757965
epoch£º341	 i:6 	 global-step:6826	 l-p:0.16377754509449005
epoch£º341	 i:7 	 global-step:6827	 l-p:0.1431323140859604
epoch£º341	 i:8 	 global-step:6828	 l-p:0.14647208154201508
epoch£º341	 i:9 	 global-step:6829	 l-p:0.14507868885993958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9113, 2.9113, 2.9113],
        [2.9113, 2.1035, 1.5577],
        [2.9113, 2.2137, 1.6572],
        [2.9113, 2.1040, 1.5581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.14009833335876465 
model_pd.l_d.mean(): -25.125459671020508 
model_pd.lagr.mean(): -24.985361099243164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0166], device='cuda:0')), ('power', tensor([-25.1089], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.14009833335876465
epoch£º342	 i:1 	 global-step:6841	 l-p:0.17015574872493744
epoch£º342	 i:2 	 global-step:6842	 l-p:0.14032669365406036
epoch£º342	 i:3 	 global-step:6843	 l-p:0.1247447058558464
epoch£º342	 i:4 	 global-step:6844	 l-p:0.13480566442012787
epoch£º342	 i:5 	 global-step:6845	 l-p:0.2494305819272995
epoch£º342	 i:6 	 global-step:6846	 l-p:2.355618476867676
epoch£º342	 i:7 	 global-step:6847	 l-p:-0.26072177290916443
epoch£º342	 i:8 	 global-step:6848	 l-p:0.1615661084651947
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13984690606594086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9584, 2.1515, 1.5965],
        [2.9584, 2.9584, 2.9584],
        [2.9584, 2.2572, 1.6935],
        [2.9584, 2.8111, 2.9117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13191644847393036 
model_pd.l_d.mean(): -25.111982345581055 
model_pd.lagr.mean(): -24.980066299438477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0745], device='cuda:0')), ('power', tensor([-25.0374], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13191644847393036
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13105279207229614
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13485117256641388
epoch£º343	 i:3 	 global-step:6863	 l-p:0.20974917709827423
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13672436773777008
epoch£º343	 i:5 	 global-step:6865	 l-p:0.20187871158123016
epoch£º343	 i:6 	 global-step:6866	 l-p:0.14388519525527954
epoch£º343	 i:7 	 global-step:6867	 l-p:0.9401400685310364
epoch£º343	 i:8 	 global-step:6868	 l-p:0.14259494841098785
epoch£º343	 i:9 	 global-step:6869	 l-p:0.15441259741783142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[2.7398, 2.0108, 1.4766],
        [2.7398, 1.9235, 1.4035],
        [2.7398, 2.2722, 2.3712],
        [2.7398, 2.2936, 2.4021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.1693459302186966 
model_pd.l_d.mean(): -25.18060302734375 
model_pd.lagr.mean(): -25.01125717163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0347], device='cuda:0')), ('power', tensor([-25.2153], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.1693459302186966
epoch£º344	 i:1 	 global-step:6881	 l-p:-0.5103136897087097
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13555772602558136
epoch£º344	 i:3 	 global-step:6883	 l-p:0.12356159090995789
epoch£º344	 i:4 	 global-step:6884	 l-p:0.16387026011943817
epoch£º344	 i:5 	 global-step:6885	 l-p:0.14812906086444855
epoch£º344	 i:6 	 global-step:6886	 l-p:0.12647105753421783
epoch£º344	 i:7 	 global-step:6887	 l-p:0.16639947891235352
epoch£º344	 i:8 	 global-step:6888	 l-p:0.15570871531963348
epoch£º344	 i:9 	 global-step:6889	 l-p:0.21974828839302063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7763, 2.6739, 2.7520],
        [2.7763, 2.0543, 1.5135],
        [2.7763, 2.6456, 2.7392],
        [2.7763, 1.9582, 1.6877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.11439573019742966 
model_pd.l_d.mean(): -24.69144630432129 
model_pd.lagr.mean(): -24.577051162719727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0747], device='cuda:0')), ('power', tensor([-24.7662], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.11439573019742966
epoch£º345	 i:1 	 global-step:6901	 l-p:0.14200769364833832
epoch£º345	 i:2 	 global-step:6902	 l-p:0.2388317883014679
epoch£º345	 i:3 	 global-step:6903	 l-p:0.14902126789093018
epoch£º345	 i:4 	 global-step:6904	 l-p:0.1866387575864792
epoch£º345	 i:5 	 global-step:6905	 l-p:0.1695398986339569
epoch£º345	 i:6 	 global-step:6906	 l-p:0.12636660039424896
epoch£º345	 i:7 	 global-step:6907	 l-p:0.15115471184253693
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13151513040065765
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12339851260185242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7982, 2.7374, 2.7882],
        [2.7982, 2.7975, 2.7982],
        [2.7982, 2.6206, 2.7346],
        [2.7982, 1.9274, 1.5683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.1593366414308548 
model_pd.l_d.mean(): -24.837888717651367 
model_pd.lagr.mean(): -24.678552627563477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1561], device='cuda:0')), ('power', tensor([-24.9939], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.1593366414308548
epoch£º346	 i:1 	 global-step:6921	 l-p:0.15340709686279297
epoch£º346	 i:2 	 global-step:6922	 l-p:0.19270113110542297
epoch£º346	 i:3 	 global-step:6923	 l-p:0.13475775718688965
epoch£º346	 i:4 	 global-step:6924	 l-p:-0.12493808567523956
epoch£º346	 i:5 	 global-step:6925	 l-p:0.14164255559444427
epoch£º346	 i:6 	 global-step:6926	 l-p:0.13497456908226013
epoch£º346	 i:7 	 global-step:6927	 l-p:0.14080224931240082
epoch£º346	 i:8 	 global-step:6928	 l-p:0.14519162476062775
epoch£º346	 i:9 	 global-step:6929	 l-p:0.39276567101478577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8061, 2.8061, 2.8061],
        [2.8061, 1.9782, 1.6936],
        [2.8061, 2.1122, 1.5628],
        [2.8061, 2.7803, 2.8038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.125998392701149 
model_pd.l_d.mean(): -24.870439529418945 
model_pd.lagr.mean(): -24.744441986083984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1322], device='cuda:0')), ('power', tensor([-25.0027], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.125998392701149
epoch£º347	 i:1 	 global-step:6941	 l-p:-0.15238578617572784
epoch£º347	 i:2 	 global-step:6942	 l-p:-0.18816038966178894
epoch£º347	 i:3 	 global-step:6943	 l-p:0.20419101417064667
epoch£º347	 i:4 	 global-step:6944	 l-p:0.1265331357717514
epoch£º347	 i:5 	 global-step:6945	 l-p:0.14190319180488586
epoch£º347	 i:6 	 global-step:6946	 l-p:0.16167651116847992
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12997174263000488
epoch£º347	 i:8 	 global-step:6948	 l-p:0.13134217262268066
epoch£º347	 i:9 	 global-step:6949	 l-p:0.18571379780769348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6518, 2.3477, 2.4881],
        [2.6518, 2.4808, 2.5934],
        [2.6518, 2.3734, 2.5124],
        [2.6518, 2.6518, 2.6518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.24548546969890594 
model_pd.l_d.mean(): -25.06951904296875 
model_pd.lagr.mean(): -24.824033737182617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1153], device='cuda:0')), ('power', tensor([-25.1848], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.24548546969890594
epoch£º348	 i:1 	 global-step:6961	 l-p:0.20828185975551605
epoch£º348	 i:2 	 global-step:6962	 l-p:0.12680287659168243
epoch£º348	 i:3 	 global-step:6963	 l-p:0.1286384016275406
epoch£º348	 i:4 	 global-step:6964	 l-p:0.14183782041072845
epoch£º348	 i:5 	 global-step:6965	 l-p:0.12753672897815704
epoch£º348	 i:6 	 global-step:6966	 l-p:0.15036365389823914
epoch£º348	 i:7 	 global-step:6967	 l-p:0.1478329747915268
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13514702022075653
epoch£º348	 i:9 	 global-step:6969	 l-p:0.16384257376194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9932, 2.9932, 2.9932],
        [2.9932, 2.9853, 2.9928],
        [2.9932, 2.9834, 2.9927],
        [2.9932, 2.8293, 2.9375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.12119874358177185 
model_pd.l_d.mean(): -24.431007385253906 
model_pd.lagr.mean(): -24.3098087310791 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0686], device='cuda:0')), ('power', tensor([-24.4997], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.12119874358177185
epoch£º349	 i:1 	 global-step:6981	 l-p:0.12011480331420898
epoch£º349	 i:2 	 global-step:6982	 l-p:0.15612536668777466
epoch£º349	 i:3 	 global-step:6983	 l-p:0.12729239463806152
epoch£º349	 i:4 	 global-step:6984	 l-p:0.12440524995326996
epoch£º349	 i:5 	 global-step:6985	 l-p:0.16005314886569977
epoch£º349	 i:6 	 global-step:6986	 l-p:0.27167388796806335
epoch£º349	 i:7 	 global-step:6987	 l-p:-3.648707866668701
epoch£º349	 i:8 	 global-step:6988	 l-p:0.10358802229166031
epoch£º349	 i:9 	 global-step:6989	 l-p:0.0343610979616642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5532, 2.5430, 2.5527],
        [2.5532, 1.6967, 1.2118],
        [2.5532, 1.6442, 1.1843],
        [2.5532, 1.7005, 1.2144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): -0.01560521125793457 
model_pd.l_d.mean(): -25.16487693786621 
model_pd.lagr.mean(): -25.180482864379883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2511], device='cuda:0')), ('power', tensor([-25.4160], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:-0.01560521125793457
epoch£º350	 i:1 	 global-step:7001	 l-p:-25.553373336791992
epoch£º350	 i:2 	 global-step:7002	 l-p:0.04721519351005554
epoch£º350	 i:3 	 global-step:7003	 l-p:0.095987468957901
epoch£º350	 i:4 	 global-step:7004	 l-p:0.16539351642131805
epoch£º350	 i:5 	 global-step:7005	 l-p:0.12771445512771606
epoch£º350	 i:6 	 global-step:7006	 l-p:0.1019836962223053
epoch£º350	 i:7 	 global-step:7007	 l-p:0.11946709454059601
epoch£º350	 i:8 	 global-step:7008	 l-p:0.11606802046298981
epoch£º350	 i:9 	 global-step:7009	 l-p:0.1154705211520195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9226, 2.6620, 2.7965],
        [2.9226, 2.5703, 2.7042],
        [2.9226, 2.0182, 1.5820],
        [2.9226, 2.0096, 1.5553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.12746089696884155 
model_pd.l_d.mean(): -24.942134857177734 
model_pd.lagr.mean(): -24.814674377441406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0695], device='cuda:0')), ('power', tensor([-25.0117], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.12746089696884155
epoch£º351	 i:1 	 global-step:7021	 l-p:0.2564722001552582
epoch£º351	 i:2 	 global-step:7022	 l-p:0.1444324105978012
epoch£º351	 i:3 	 global-step:7023	 l-p:-4.207667827606201
epoch£º351	 i:4 	 global-step:7024	 l-p:0.1363319754600525
epoch£º351	 i:5 	 global-step:7025	 l-p:0.17570944130420685
epoch£º351	 i:6 	 global-step:7026	 l-p:0.27193140983581543
epoch£º351	 i:7 	 global-step:7027	 l-p:0.13326221704483032
epoch£º351	 i:8 	 global-step:7028	 l-p:1.336004614830017
epoch£º351	 i:9 	 global-step:7029	 l-p:0.1369464099407196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8949, 2.6592, 2.7901],
        [2.8949, 2.0459, 1.7230],
        [2.8949, 2.4758, 2.5954],
        [2.8949, 1.9664, 1.4879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.1570265293121338 
model_pd.l_d.mean(): -25.255720138549805 
model_pd.lagr.mean(): -25.09869384765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0562], device='cuda:0')), ('power', tensor([-25.1995], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.1570265293121338
epoch£º352	 i:1 	 global-step:7041	 l-p:0.14207389950752258
epoch£º352	 i:2 	 global-step:7042	 l-p:0.13406501710414886
epoch£º352	 i:3 	 global-step:7043	 l-p:1.0744107961654663
epoch£º352	 i:4 	 global-step:7044	 l-p:0.14203596115112305
epoch£º352	 i:5 	 global-step:7045	 l-p:0.16300125420093536
epoch£º352	 i:6 	 global-step:7046	 l-p:0.1796427071094513
epoch£º352	 i:7 	 global-step:7047	 l-p:0.11588083207607269
epoch£º352	 i:8 	 global-step:7048	 l-p:0.145821675658226
epoch£º352	 i:9 	 global-step:7049	 l-p:0.12674082815647125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7951, 2.7483, 2.7888],
        [2.7951, 2.7786, 2.7940],
        [2.7951, 2.7804, 2.7942],
        [2.7951, 2.7951, 2.7951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.14722619950771332 
model_pd.l_d.mean(): -25.06360626220703 
model_pd.lagr.mean(): -24.916379928588867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0541], device='cuda:0')), ('power', tensor([-25.1177], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.14722619950771332
epoch£º353	 i:1 	 global-step:7061	 l-p:0.19093580543994904
epoch£º353	 i:2 	 global-step:7062	 l-p:0.1156633123755455
epoch£º353	 i:3 	 global-step:7063	 l-p:0.233153834939003
epoch£º353	 i:4 	 global-step:7064	 l-p:0.17385782301425934
epoch£º353	 i:5 	 global-step:7065	 l-p:0.0494430772960186
epoch£º353	 i:6 	 global-step:7066	 l-p:0.15419158339500427
epoch£º353	 i:7 	 global-step:7067	 l-p:0.142251119017601
epoch£º353	 i:8 	 global-step:7068	 l-p:0.16023819148540497
epoch£º353	 i:9 	 global-step:7069	 l-p:0.09584362804889679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0022, 2.8021, 2.9238],
        [3.0022, 2.0748, 1.5479],
        [3.0022, 2.3444, 1.7648],
        [3.0022, 2.9571, 2.9962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12570247054100037 
model_pd.l_d.mean(): -25.114425659179688 
model_pd.lagr.mean(): -24.988723754882812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1074], device='cuda:0')), ('power', tensor([-25.0070], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12570247054100037
epoch£º354	 i:1 	 global-step:7081	 l-p:0.13298314809799194
epoch£º354	 i:2 	 global-step:7082	 l-p:0.1522727757692337
epoch£º354	 i:3 	 global-step:7083	 l-p:0.15274973213672638
epoch£º354	 i:4 	 global-step:7084	 l-p:0.15517857670783997
epoch£º354	 i:5 	 global-step:7085	 l-p:0.1610189974308014
epoch£º354	 i:6 	 global-step:7086	 l-p:0.21295791864395142
epoch£º354	 i:7 	 global-step:7087	 l-p:0.16201625764369965
epoch£º354	 i:8 	 global-step:7088	 l-p:0.1329861581325531
epoch£º354	 i:9 	 global-step:7089	 l-p:0.15350085496902466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9016, 2.2299, 1.6610],
        [2.9016, 2.7757, 2.8673],
        [2.9016, 2.8769, 2.8994],
        [2.9016, 2.8930, 2.9012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.12878623604774475 
model_pd.l_d.mean(): -24.862098693847656 
model_pd.lagr.mean(): -24.733312606811523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0034], device='cuda:0')), ('power', tensor([-24.8587], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.12878623604774475
epoch£º355	 i:1 	 global-step:7101	 l-p:0.11960656940937042
epoch£º355	 i:2 	 global-step:7102	 l-p:0.8056849241256714
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12165908515453339
epoch£º355	 i:4 	 global-step:7104	 l-p:0.13897456228733063
epoch£º355	 i:5 	 global-step:7105	 l-p:0.16762998700141907
epoch£º355	 i:6 	 global-step:7106	 l-p:0.1622745245695114
epoch£º355	 i:7 	 global-step:7107	 l-p:0.08309726417064667
epoch£º355	 i:8 	 global-step:7108	 l-p:0.15612652897834778
epoch£º355	 i:9 	 global-step:7109	 l-p:0.14420491456985474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6886, 2.3193, 2.4594],
        [2.6886, 1.8031, 1.2964],
        [2.6886, 2.6885, 2.6886],
        [2.6886, 2.3016, 2.4388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.1934518814086914 
model_pd.l_d.mean(): -24.87323570251465 
model_pd.lagr.mean(): -24.67978286743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1420], device='cuda:0')), ('power', tensor([-25.0152], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.1934518814086914
epoch£º356	 i:1 	 global-step:7121	 l-p:0.16081343591213226
epoch£º356	 i:2 	 global-step:7122	 l-p:0.15549001097679138
epoch£º356	 i:3 	 global-step:7123	 l-p:0.17608535289764404
epoch£º356	 i:4 	 global-step:7124	 l-p:0.29157474637031555
epoch£º356	 i:5 	 global-step:7125	 l-p:0.24509243667125702
epoch£º356	 i:6 	 global-step:7126	 l-p:0.13126958906650543
epoch£º356	 i:7 	 global-step:7127	 l-p:0.1096077710390091
epoch£º356	 i:8 	 global-step:7128	 l-p:0.09501821547746658
epoch£º356	 i:9 	 global-step:7129	 l-p:0.11772508174180984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0832, 2.4078, 1.8165],
        [3.0832, 3.0702, 3.0825],
        [3.0832, 3.0183, 3.0721],
        [3.0832, 2.5764, 2.6538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.09326072782278061 
model_pd.l_d.mean(): -24.787555694580078 
model_pd.lagr.mean(): -24.69429588317871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0034], device='cuda:0')), ('power', tensor([-24.7909], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.09326072782278061
epoch£º357	 i:1 	 global-step:7141	 l-p:0.1424156129360199
epoch£º357	 i:2 	 global-step:7142	 l-p:0.13072793185710907
epoch£º357	 i:3 	 global-step:7143	 l-p:-1.810242772102356
epoch£º357	 i:4 	 global-step:7144	 l-p:0.1264021247625351
epoch£º357	 i:5 	 global-step:7145	 l-p:0.1852176934480667
epoch£º357	 i:6 	 global-step:7146	 l-p:0.16763347387313843
epoch£º357	 i:7 	 global-step:7147	 l-p:0.14537125825881958
epoch£º357	 i:8 	 global-step:7148	 l-p:0.1400572806596756
epoch£º357	 i:9 	 global-step:7149	 l-p:0.1388012021780014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7768, 2.7709, 2.7766],
        [2.7768, 2.7768, 2.7768],
        [2.7768, 2.0720, 2.0003],
        [2.7768, 1.9602, 1.4217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.004244685173034668 
model_pd.l_d.mean(): -25.117156982421875 
model_pd.lagr.mean(): -25.112913131713867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0548], device='cuda:0')), ('power', tensor([-25.1719], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.004244685173034668
epoch£º358	 i:1 	 global-step:7161	 l-p:0.18048375844955444
epoch£º358	 i:2 	 global-step:7162	 l-p:0.15421141684055328
epoch£º358	 i:3 	 global-step:7163	 l-p:0.12661153078079224
epoch£º358	 i:4 	 global-step:7164	 l-p:0.017385605722665787
epoch£º358	 i:5 	 global-step:7165	 l-p:0.11984850466251373
epoch£º358	 i:6 	 global-step:7166	 l-p:0.14344939589500427
epoch£º358	 i:7 	 global-step:7167	 l-p:0.13430312275886536
epoch£º358	 i:8 	 global-step:7168	 l-p:0.23174342513084412
epoch£º358	 i:9 	 global-step:7169	 l-p:0.3629245162010193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7441, 2.5706, 2.6853],
        [2.7441, 2.7441, 2.7441],
        [2.7441, 2.1203, 2.1350],
        [2.7441, 2.7440, 2.7441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.17102567851543427 
model_pd.l_d.mean(): -25.011451721191406 
model_pd.lagr.mean(): -24.840425491333008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1154], device='cuda:0')), ('power', tensor([-25.1269], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.17102567851543427
epoch£º359	 i:1 	 global-step:7181	 l-p:0.13686077296733856
epoch£º359	 i:2 	 global-step:7182	 l-p:0.13520437479019165
epoch£º359	 i:3 	 global-step:7183	 l-p:0.1248929500579834
epoch£º359	 i:4 	 global-step:7184	 l-p:0.21338070929050446
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14666011929512024
epoch£º359	 i:6 	 global-step:7186	 l-p:0.17357391119003296
epoch£º359	 i:7 	 global-step:7187	 l-p:0.13700075447559357
epoch£º359	 i:8 	 global-step:7188	 l-p:0.14486059546470642
epoch£º359	 i:9 	 global-step:7189	 l-p:0.12932436168193817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8747, 2.3634, 2.4520],
        [2.8747, 2.3505, 2.4317],
        [2.8747, 2.4849, 2.6195],
        [2.8747, 2.8745, 2.8747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.12566855549812317 
model_pd.l_d.mean(): -24.945581436157227 
model_pd.lagr.mean(): -24.81991195678711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1089], device='cuda:0')), ('power', tensor([-25.0545], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.12566855549812317
epoch£º360	 i:1 	 global-step:7201	 l-p:-0.2086067944765091
epoch£º360	 i:2 	 global-step:7202	 l-p:0.18737925589084625
epoch£º360	 i:3 	 global-step:7203	 l-p:0.28086841106414795
epoch£º360	 i:4 	 global-step:7204	 l-p:0.11706985533237457
epoch£º360	 i:5 	 global-step:7205	 l-p:0.1315646469593048
epoch£º360	 i:6 	 global-step:7206	 l-p:0.14091163873672485
epoch£º360	 i:7 	 global-step:7207	 l-p:0.15715274214744568
epoch£º360	 i:8 	 global-step:7208	 l-p:0.12538667023181915
epoch£º360	 i:9 	 global-step:7209	 l-p:0.16237860918045044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8880, 2.8880, 2.8880],
        [2.8880, 2.8879, 2.8880],
        [2.8880, 2.8869, 2.8880],
        [2.8880, 2.8774, 2.8874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.18108855187892914 
model_pd.l_d.mean(): -25.09254264831543 
model_pd.lagr.mean(): -24.911453247070312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0267], device='cuda:0')), ('power', tensor([-25.1193], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.18108855187892914
epoch£º361	 i:1 	 global-step:7221	 l-p:0.15616509318351746
epoch£º361	 i:2 	 global-step:7222	 l-p:0.15257062017917633
epoch£º361	 i:3 	 global-step:7223	 l-p:0.18317988514900208
epoch£º361	 i:4 	 global-step:7224	 l-p:0.11285325139760971
epoch£º361	 i:5 	 global-step:7225	 l-p:0.11758974939584732
epoch£º361	 i:6 	 global-step:7226	 l-p:0.1422378569841385
epoch£º361	 i:7 	 global-step:7227	 l-p:0.1578960418701172
epoch£º361	 i:8 	 global-step:7228	 l-p:0.17393991351127625
epoch£º361	 i:9 	 global-step:7229	 l-p:0.125912606716156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5816, 2.0035, 2.0687],
        [2.5816, 2.5816, 2.5816],
        [2.5816, 2.5815, 2.5816],
        [2.5816, 1.6760, 1.1885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): -0.1173633486032486 
model_pd.l_d.mean(): -25.13566017150879 
model_pd.lagr.mean(): -25.253023147583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2440], device='cuda:0')), ('power', tensor([-25.3796], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:-0.1173633486032486
epoch£º362	 i:1 	 global-step:7241	 l-p:0.30199381709098816
epoch£º362	 i:2 	 global-step:7242	 l-p:0.2803172171115875
epoch£º362	 i:3 	 global-step:7243	 l-p:1.1648684740066528
epoch£º362	 i:4 	 global-step:7244	 l-p:0.21405553817749023
epoch£º362	 i:5 	 global-step:7245	 l-p:0.11433660984039307
epoch£º362	 i:6 	 global-step:7246	 l-p:0.13290785253047943
epoch£º362	 i:7 	 global-step:7247	 l-p:0.13019151985645294
epoch£º362	 i:8 	 global-step:7248	 l-p:0.11856605112552643
epoch£º362	 i:9 	 global-step:7249	 l-p:0.12513574957847595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0186, 2.6773, 2.8179],
        [3.0186, 3.0144, 3.0185],
        [3.0186, 3.0084, 3.0181],
        [3.0186, 2.3753, 2.3601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.13767455518245697 
model_pd.l_d.mean(): -25.094377517700195 
model_pd.lagr.mean(): -24.956703186035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0921], device='cuda:0')), ('power', tensor([-25.0022], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.13767455518245697
epoch£º363	 i:1 	 global-step:7261	 l-p:0.15000386536121368
epoch£º363	 i:2 	 global-step:7262	 l-p:0.028801245614886284
epoch£º363	 i:3 	 global-step:7263	 l-p:0.12840788066387177
epoch£º363	 i:4 	 global-step:7264	 l-p:0.15171180665493011
epoch£º363	 i:5 	 global-step:7265	 l-p:0.1167096495628357
epoch£º363	 i:6 	 global-step:7266	 l-p:0.16259008646011353
epoch£º363	 i:7 	 global-step:7267	 l-p:0.05924548581242561
epoch£º363	 i:8 	 global-step:7268	 l-p:0.14133547246456146
epoch£º363	 i:9 	 global-step:7269	 l-p:0.15527625381946564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7634, 2.0877, 1.5237],
        [2.7634, 2.4116, 2.5571],
        [2.7634, 2.7626, 2.7634],
        [2.7634, 2.0443, 1.9745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.28095412254333496 
model_pd.l_d.mean(): -24.670345306396484 
model_pd.lagr.mean(): -24.38939094543457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1533], device='cuda:0')), ('power', tensor([-24.8236], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.28095412254333496
epoch£º364	 i:1 	 global-step:7281	 l-p:0.13846860826015472
epoch£º364	 i:2 	 global-step:7282	 l-p:0.11132392287254333
epoch£º364	 i:3 	 global-step:7283	 l-p:0.14364488422870636
epoch£º364	 i:4 	 global-step:7284	 l-p:0.15639548003673553
epoch£º364	 i:5 	 global-step:7285	 l-p:-6.849374294281006
epoch£º364	 i:6 	 global-step:7286	 l-p:0.16378439962863922
epoch£º364	 i:7 	 global-step:7287	 l-p:0.14969876408576965
epoch£º364	 i:8 	 global-step:7288	 l-p:0.12095774710178375
epoch£º364	 i:9 	 global-step:7289	 l-p:0.14470608532428741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9188, 2.9188, 2.9188],
        [2.9188, 2.2630, 2.2498],
        [2.9188, 2.3873, 2.4705],
        [2.9188, 2.8739, 2.9130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.14953522384166718 
model_pd.l_d.mean(): -25.124013900756836 
model_pd.lagr.mean(): -24.974477767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0373], device='cuda:0')), ('power', tensor([-25.0868], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.14953522384166718
epoch£º365	 i:1 	 global-step:7301	 l-p:0.15247437357902527
epoch£º365	 i:2 	 global-step:7302	 l-p:0.15903478860855103
epoch£º365	 i:3 	 global-step:7303	 l-p:0.5904245376586914
epoch£º365	 i:4 	 global-step:7304	 l-p:0.1427147090435028
epoch£º365	 i:5 	 global-step:7305	 l-p:0.2129247635602951
epoch£º365	 i:6 	 global-step:7306	 l-p:0.03903535380959511
epoch£º365	 i:7 	 global-step:7307	 l-p:0.04672219231724739
epoch£º365	 i:8 	 global-step:7308	 l-p:0.21449515223503113
epoch£º365	 i:9 	 global-step:7309	 l-p:0.14479099214076996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8435, 2.8160, 2.8409],
        [2.8435, 2.0766, 1.5127],
        [2.8435, 2.1304, 1.5602],
        [2.8435, 2.8217, 2.8417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11825879663228989 
model_pd.l_d.mean(): -24.429105758666992 
model_pd.lagr.mean(): -24.31084632873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2470], device='cuda:0')), ('power', tensor([-24.6761], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11825879663228989
epoch£º366	 i:1 	 global-step:7321	 l-p:0.13726195693016052
epoch£º366	 i:2 	 global-step:7322	 l-p:0.13037782907485962
epoch£º366	 i:3 	 global-step:7323	 l-p:0.14046083390712738
epoch£º366	 i:4 	 global-step:7324	 l-p:0.35065895318984985
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13966688513755798
epoch£º366	 i:6 	 global-step:7326	 l-p:0.2654240131378174
epoch£º366	 i:7 	 global-step:7327	 l-p:0.14654289186000824
epoch£º366	 i:8 	 global-step:7328	 l-p:0.2347022145986557
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12786446511745453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[2.8918, 2.1363, 1.5637],
        [2.8918, 2.2684, 2.2911],
        [2.8918, 2.4186, 2.5346],
        [2.8918, 1.9734, 1.6063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.15233366191387177 
model_pd.l_d.mean(): -24.71100616455078 
model_pd.lagr.mean(): -24.558671951293945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1461], device='cuda:0')), ('power', tensor([-24.8571], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.15233366191387177
epoch£º367	 i:1 	 global-step:7341	 l-p:0.1451893448829651
epoch£º367	 i:2 	 global-step:7342	 l-p:-2.3408639430999756
epoch£º367	 i:3 	 global-step:7343	 l-p:0.1368982344865799
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13959792256355286
epoch£º367	 i:5 	 global-step:7345	 l-p:0.15162675082683563
epoch£º367	 i:6 	 global-step:7346	 l-p:0.20378531515598297
epoch£º367	 i:7 	 global-step:7347	 l-p:0.145868182182312
epoch£º367	 i:8 	 global-step:7348	 l-p:0.15064409375190735
epoch£º367	 i:9 	 global-step:7349	 l-p:0.2140319049358368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7332, 2.7332, 2.7332],
        [2.7332, 2.7332, 2.7332],
        [2.7332, 2.7150, 2.7319],
        [2.7332, 1.8815, 1.6533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.1894761025905609 
model_pd.l_d.mean(): -25.028467178344727 
model_pd.lagr.mean(): -24.838991165161133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1315], device='cuda:0')), ('power', tensor([-25.1600], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.1894761025905609
epoch£º368	 i:1 	 global-step:7361	 l-p:0.1609041392803192
epoch£º368	 i:2 	 global-step:7362	 l-p:0.13851644098758698
epoch£º368	 i:3 	 global-step:7363	 l-p:0.19257116317749023
epoch£º368	 i:4 	 global-step:7364	 l-p:0.12227258086204529
epoch£º368	 i:5 	 global-step:7365	 l-p:0.14016130566596985
epoch£º368	 i:6 	 global-step:7366	 l-p:0.1511152982711792
epoch£º368	 i:7 	 global-step:7367	 l-p:0.12105207145214081
epoch£º368	 i:8 	 global-step:7368	 l-p:0.11721061170101166
epoch£º368	 i:9 	 global-step:7369	 l-p:0.08034728467464447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7060, 1.7419, 1.3487],
        [2.7060, 2.6931, 2.7053],
        [2.7060, 2.7060, 2.7060],
        [2.7060, 1.9373, 1.3882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.10695648193359375 
model_pd.l_d.mean(): -24.82832145690918 
model_pd.lagr.mean(): -24.721364974975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2543], device='cuda:0')), ('power', tensor([-25.0826], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.10695648193359375
epoch£º369	 i:1 	 global-step:7381	 l-p:0.15237855911254883
epoch£º369	 i:2 	 global-step:7382	 l-p:0.18059560656547546
epoch£º369	 i:3 	 global-step:7383	 l-p:0.17805607616901398
epoch£º369	 i:4 	 global-step:7384	 l-p:0.18312408030033112
epoch£º369	 i:5 	 global-step:7385	 l-p:0.13535325229167938
epoch£º369	 i:6 	 global-step:7386	 l-p:0.19455286860466003
epoch£º369	 i:7 	 global-step:7387	 l-p:0.21524757146835327
epoch£º369	 i:8 	 global-step:7388	 l-p:0.17035675048828125
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12011948972940445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1315, 2.2788, 1.9865],
        [3.1315, 3.1144, 3.1303],
        [3.1315, 2.2729, 1.6716],
        [3.1315, 2.7617, 2.9016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.11509965360164642 
model_pd.l_d.mean(): -24.132055282592773 
model_pd.lagr.mean(): -24.016956329345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0498], device='cuda:0')), ('power', tensor([-24.1819], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.11509965360164642
epoch£º370	 i:1 	 global-step:7401	 l-p:0.11615590751171112
epoch£º370	 i:2 	 global-step:7402	 l-p:0.12354692071676254
epoch£º370	 i:3 	 global-step:7403	 l-p:0.09786198288202286
epoch£º370	 i:4 	 global-step:7404	 l-p:0.13124904036521912
epoch£º370	 i:5 	 global-step:7405	 l-p:0.14317850768566132
epoch£º370	 i:6 	 global-step:7406	 l-p:0.2192990481853485
epoch£º370	 i:7 	 global-step:7407	 l-p:0.13906747102737427
epoch£º370	 i:8 	 global-step:7408	 l-p:0.13697515428066254
epoch£º370	 i:9 	 global-step:7409	 l-p:0.23131202161312103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5561, 2.5560, 2.5561],
        [2.5561, 2.5472, 2.5557],
        [2.5561, 1.7761, 1.2470],
        [2.5561, 2.4645, 2.5375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.009130358695983887 
model_pd.l_d.mean(): -25.018224716186523 
model_pd.lagr.mean(): -25.00909423828125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2191], device='cuda:0')), ('power', tensor([-25.2373], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.009130358695983887
epoch£º371	 i:1 	 global-step:7421	 l-p:0.21286647021770477
epoch£º371	 i:2 	 global-step:7422	 l-p:0.1545974165201187
epoch£º371	 i:3 	 global-step:7423	 l-p:0.2367284744977951
epoch£º371	 i:4 	 global-step:7424	 l-p:0.1143546774983406
epoch£º371	 i:5 	 global-step:7425	 l-p:0.18919913470745087
epoch£º371	 i:6 	 global-step:7426	 l-p:0.14675462245941162
epoch£º371	 i:7 	 global-step:7427	 l-p:0.13708019256591797
epoch£º371	 i:8 	 global-step:7428	 l-p:0.17130546271800995
epoch£º371	 i:9 	 global-step:7429	 l-p:0.13672402501106262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0112, 2.3472, 1.7497],
        [3.0112, 3.0110, 3.0112],
        [3.0112, 3.0021, 3.0107],
        [3.0112, 2.1734, 1.9360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.1486024260520935 
model_pd.l_d.mean(): -24.6323299407959 
model_pd.lagr.mean(): -24.483728408813477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0321], device='cuda:0')), ('power', tensor([-24.6644], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.1486024260520935
epoch£º372	 i:1 	 global-step:7441	 l-p:0.130107119679451
epoch£º372	 i:2 	 global-step:7442	 l-p:0.1097167432308197
epoch£º372	 i:3 	 global-step:7443	 l-p:0.12341759353876114
epoch£º372	 i:4 	 global-step:7444	 l-p:0.13678871095180511
epoch£º372	 i:5 	 global-step:7445	 l-p:0.1298981010913849
epoch£º372	 i:6 	 global-step:7446	 l-p:0.2501258850097656
epoch£º372	 i:7 	 global-step:7447	 l-p:0.09572011977434158
epoch£º372	 i:8 	 global-step:7448	 l-p:0.16863368451595306
epoch£º372	 i:9 	 global-step:7449	 l-p:0.08010085672140121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6609, 1.8792, 1.3347],
        [2.6609, 2.5164, 2.6199],
        [2.6609, 2.6609, 2.6609],
        [2.6609, 1.7248, 1.4000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.20559382438659668 
model_pd.l_d.mean(): -24.538774490356445 
model_pd.lagr.mean(): -24.333181381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2286], device='cuda:0')), ('power', tensor([-24.7674], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.20559382438659668
epoch£º373	 i:1 	 global-step:7461	 l-p:0.05386031046509743
epoch£º373	 i:2 	 global-step:7462	 l-p:-0.4826267957687378
epoch£º373	 i:3 	 global-step:7463	 l-p:0.14101964235305786
epoch£º373	 i:4 	 global-step:7464	 l-p:0.1093083992600441
epoch£º373	 i:5 	 global-step:7465	 l-p:0.11924723535776138
epoch£º373	 i:6 	 global-step:7466	 l-p:0.13499967753887177
epoch£º373	 i:7 	 global-step:7467	 l-p:0.13903282582759857
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14446255564689636
epoch£º373	 i:9 	 global-step:7469	 l-p:0.08601552993059158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8066, 2.0056, 1.8565],
        [2.8066, 2.5891, 2.7217],
        [2.8066, 2.6491, 2.7585],
        [2.8066, 2.0710, 1.5006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.06749849766492844 
model_pd.l_d.mean(): -25.18229866027832 
model_pd.lagr.mean(): -25.11479949951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0855], device='cuda:0')), ('power', tensor([-25.2678], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.06749849766492844
epoch£º374	 i:1 	 global-step:7481	 l-p:0.2415807694196701
epoch£º374	 i:2 	 global-step:7482	 l-p:-0.05834884196519852
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12976543605327606
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13834048807621002
epoch£º374	 i:5 	 global-step:7485	 l-p:0.1409171223640442
epoch£º374	 i:6 	 global-step:7486	 l-p:0.31780770421028137
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10692458599805832
epoch£º374	 i:8 	 global-step:7488	 l-p:0.14589464664459229
epoch£º374	 i:9 	 global-step:7489	 l-p:0.220854252576828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8402, 2.8137, 2.8379],
        [2.8402, 2.8402, 2.8402],
        [2.8402, 2.8088, 2.8371],
        [2.8402, 2.8395, 2.8402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): -1.0113167762756348 
model_pd.l_d.mean(): -25.0425968170166 
model_pd.lagr.mean(): -26.053913116455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0076], device='cuda:0')), ('power', tensor([-25.0502], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:-1.0113167762756348
epoch£º375	 i:1 	 global-step:7501	 l-p:0.18129701912403107
epoch£º375	 i:2 	 global-step:7502	 l-p:0.13584065437316895
epoch£º375	 i:3 	 global-step:7503	 l-p:0.14603525400161743
epoch£º375	 i:4 	 global-step:7504	 l-p:0.11653300374746323
epoch£º375	 i:5 	 global-step:7505	 l-p:0.16912592947483063
epoch£º375	 i:6 	 global-step:7506	 l-p:0.1252778172492981
epoch£º375	 i:7 	 global-step:7507	 l-p:0.1236225888133049
epoch£º375	 i:8 	 global-step:7508	 l-p:0.15171140432357788
epoch£º375	 i:9 	 global-step:7509	 l-p:0.13335858285427094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7742, 2.4744, 2.6242],
        [2.7742, 1.8659, 1.5680],
        [2.7742, 2.7734, 2.7742],
        [2.7742, 2.1118, 2.1222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.12314340472221375 
model_pd.l_d.mean(): -24.762479782104492 
model_pd.lagr.mean(): -24.63933563232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1376], device='cuda:0')), ('power', tensor([-24.9000], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.12314340472221375
epoch£º376	 i:1 	 global-step:7521	 l-p:0.5761484503746033
epoch£º376	 i:2 	 global-step:7522	 l-p:0.1670500487089157
epoch£º376	 i:3 	 global-step:7523	 l-p:0.7588905692100525
epoch£º376	 i:4 	 global-step:7524	 l-p:0.15018399059772491
epoch£º376	 i:5 	 global-step:7525	 l-p:0.17192727327346802
epoch£º376	 i:6 	 global-step:7526	 l-p:0.14880868792533875
epoch£º376	 i:7 	 global-step:7527	 l-p:0.10602448135614395
epoch£º376	 i:8 	 global-step:7528	 l-p:0.12798166275024414
epoch£º376	 i:9 	 global-step:7529	 l-p:0.14639897644519806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1109, 2.4253, 2.3886],
        [3.1109, 2.8266, 2.9709],
        [3.1109, 2.8758, 3.0115],
        [3.1109, 2.3439, 1.7364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.1217135339975357 
model_pd.l_d.mean(): -24.91444969177246 
model_pd.lagr.mean(): -24.792736053466797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0794], device='cuda:0')), ('power', tensor([-24.8350], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.1217135339975357
epoch£º377	 i:1 	 global-step:7541	 l-p:0.12824788689613342
epoch£º377	 i:2 	 global-step:7542	 l-p:0.1385897397994995
epoch£º377	 i:3 	 global-step:7543	 l-p:0.15858884155750275
epoch£º377	 i:4 	 global-step:7544	 l-p:0.1299075335264206
epoch£º377	 i:5 	 global-step:7545	 l-p:0.0677441656589508
epoch£º377	 i:6 	 global-step:7546	 l-p:0.23363640904426575
epoch£º377	 i:7 	 global-step:7547	 l-p:0.07528580725193024
epoch£º377	 i:8 	 global-step:7548	 l-p:0.13787926733493805
epoch£º377	 i:9 	 global-step:7549	 l-p:0.1787347048521042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8048, 1.9509, 1.7374],
        [2.8048, 2.8048, 2.8048],
        [2.8048, 2.3741, 2.5148],
        [2.8048, 2.5734, 2.7108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.2273404598236084 
model_pd.l_d.mean(): -24.877626419067383 
model_pd.lagr.mean(): -24.650285720825195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1614], device='cuda:0')), ('power', tensor([-25.0391], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.2273404598236084
epoch£º378	 i:1 	 global-step:7561	 l-p:0.2655469477176666
epoch£º378	 i:2 	 global-step:7562	 l-p:0.14296680688858032
epoch£º378	 i:3 	 global-step:7563	 l-p:0.12354980409145355
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10532084852457047
epoch£º378	 i:5 	 global-step:7565	 l-p:0.1475834995508194
epoch£º378	 i:6 	 global-step:7566	 l-p:0.15412262082099915
epoch£º378	 i:7 	 global-step:7567	 l-p:0.140590101480484
epoch£º378	 i:8 	 global-step:7568	 l-p:0.20137900114059448
epoch£º378	 i:9 	 global-step:7569	 l-p:0.19654417037963867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6425, 2.0212, 2.0774],
        [2.6425, 2.6424, 2.6425],
        [2.6425, 2.5744, 2.6313],
        [2.6425, 1.9204, 1.8894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.16079775989055634 
model_pd.l_d.mean(): -24.666473388671875 
model_pd.lagr.mean(): -24.50567626953125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3058], device='cuda:0')), ('power', tensor([-24.9722], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.16079775989055634
epoch£º379	 i:1 	 global-step:7581	 l-p:0.1334293782711029
epoch£º379	 i:2 	 global-step:7582	 l-p:0.3909362256526947
epoch£º379	 i:3 	 global-step:7583	 l-p:0.20798678696155548
epoch£º379	 i:4 	 global-step:7584	 l-p:0.1395106315612793
epoch£º379	 i:5 	 global-step:7585	 l-p:0.12908723950386047
epoch£º379	 i:6 	 global-step:7586	 l-p:0.16354094445705414
epoch£º379	 i:7 	 global-step:7587	 l-p:0.12337784469127655
epoch£º379	 i:8 	 global-step:7588	 l-p:0.12409550696611404
epoch£º379	 i:9 	 global-step:7589	 l-p:0.1488846391439438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9213, 1.9111, 1.4004],
        [2.9213, 2.8298, 2.9026],
        [2.9213, 2.9195, 2.9213],
        [2.9213, 2.9035, 2.9201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.13628660142421722 
model_pd.l_d.mean(): -25.013999938964844 
model_pd.lagr.mean(): -24.877714157104492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0122], device='cuda:0')), ('power', tensor([-25.0262], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.13628660142421722
epoch£º380	 i:1 	 global-step:7601	 l-p:0.23153093457221985
epoch£º380	 i:2 	 global-step:7602	 l-p:0.20095978677272797
epoch£º380	 i:3 	 global-step:7603	 l-p:0.1473277360200882
epoch£º380	 i:4 	 global-step:7604	 l-p:0.141679584980011
epoch£º380	 i:5 	 global-step:7605	 l-p:0.1264055073261261
epoch£º380	 i:6 	 global-step:7606	 l-p:0.13041679561138153
epoch£º380	 i:7 	 global-step:7607	 l-p:0.23587889969348907
epoch£º380	 i:8 	 global-step:7608	 l-p:0.1204778254032135
epoch£º380	 i:9 	 global-step:7609	 l-p:0.1570458710193634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9046, 2.9041, 2.9046],
        [2.9046, 1.9968, 1.4330],
        [2.9046, 2.6301, 2.7766],
        [2.9046, 2.3485, 2.4362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.2433633804321289 
model_pd.l_d.mean(): -24.93035316467285 
model_pd.lagr.mean(): -24.686988830566406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1351], device='cuda:0')), ('power', tensor([-25.0654], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.2433633804321289
epoch£º381	 i:1 	 global-step:7621	 l-p:0.14147357642650604
epoch£º381	 i:2 	 global-step:7622	 l-p:0.1363464891910553
epoch£º381	 i:3 	 global-step:7623	 l-p:0.12696361541748047
epoch£º381	 i:4 	 global-step:7624	 l-p:0.13454316556453705
epoch£º381	 i:5 	 global-step:7625	 l-p:0.13098327815532684
epoch£º381	 i:6 	 global-step:7626	 l-p:0.14239835739135742
epoch£º381	 i:7 	 global-step:7627	 l-p:0.1608818620443344
epoch£º381	 i:8 	 global-step:7628	 l-p:0.1697297841310501
epoch£º381	 i:9 	 global-step:7629	 l-p:0.22075162827968597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6817, 2.2579, 2.4056],
        [2.6817, 2.6803, 2.6817],
        [2.6817, 2.6771, 2.6816],
        [2.6817, 2.1260, 2.2252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.04905381053686142 
model_pd.l_d.mean(): -25.203731536865234 
model_pd.lagr.mean(): -25.154678344726562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0385], device='cuda:0')), ('power', tensor([-25.2422], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.04905381053686142
epoch£º382	 i:1 	 global-step:7641	 l-p:0.1340022087097168
epoch£º382	 i:2 	 global-step:7642	 l-p:0.21528220176696777
epoch£º382	 i:3 	 global-step:7643	 l-p:0.14084495604038239
epoch£º382	 i:4 	 global-step:7644	 l-p:0.131231889128685
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11103840917348862
epoch£º382	 i:6 	 global-step:7646	 l-p:0.15359529852867126
epoch£º382	 i:7 	 global-step:7647	 l-p:0.14875724911689758
epoch£º382	 i:8 	 global-step:7648	 l-p:0.20294547080993652
epoch£º382	 i:9 	 global-step:7649	 l-p:0.12138734757900238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8491, 2.8304, 2.8478],
        [2.8491, 2.0344, 1.8822],
        [2.8491, 2.8491, 2.8491],
        [2.8491, 2.5722, 2.7200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.1451542228460312 
model_pd.l_d.mean(): -24.959810256958008 
model_pd.lagr.mean(): -24.814655303955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0687], device='cuda:0')), ('power', tensor([-25.0285], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.1451542228460312
epoch£º383	 i:1 	 global-step:7661	 l-p:0.017109667882323265
epoch£º383	 i:2 	 global-step:7662	 l-p:0.15832217037677765
epoch£º383	 i:3 	 global-step:7663	 l-p:0.1461697518825531
epoch£º383	 i:4 	 global-step:7664	 l-p:0.12789195775985718
epoch£º383	 i:5 	 global-step:7665	 l-p:0.33781757950782776
epoch£º383	 i:6 	 global-step:7666	 l-p:0.12957154214382172
epoch£º383	 i:7 	 global-step:7667	 l-p:0.10030964761972427
epoch£º383	 i:8 	 global-step:7668	 l-p:0.13407453894615173
epoch£º383	 i:9 	 global-step:7669	 l-p:0.4669951796531677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7864, 1.9044, 1.3535],
        [2.7864, 2.1287, 2.1545],
        [2.7864, 1.7811, 1.2726],
        [2.7864, 2.6005, 2.7233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.02031610906124115 
model_pd.l_d.mean(): -25.249120712280273 
model_pd.lagr.mean(): -25.228805541992188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0947], device='cuda:0')), ('power', tensor([-25.3438], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.02031610906124115
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15134604275226593
epoch£º384	 i:2 	 global-step:7682	 l-p:0.12509706616401672
epoch£º384	 i:3 	 global-step:7683	 l-p:0.1353539377450943
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13036110997200012
epoch£º384	 i:5 	 global-step:7685	 l-p:0.14876730740070343
epoch£º384	 i:6 	 global-step:7686	 l-p:0.1602662205696106
epoch£º384	 i:7 	 global-step:7687	 l-p:0.3434343934059143
epoch£º384	 i:8 	 global-step:7688	 l-p:0.09537751227617264
epoch£º384	 i:9 	 global-step:7689	 l-p:0.1316676139831543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9724, 2.1434, 1.9613],
        [2.9724, 2.9600, 2.9718],
        [2.9724, 2.9723, 2.9724],
        [2.9724, 2.0727, 1.4937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.1298358291387558 
model_pd.l_d.mean(): -24.969879150390625 
model_pd.lagr.mean(): -24.840044021606445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0414], device='cuda:0')), ('power', tensor([-25.0112], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.1298358291387558
epoch£º385	 i:1 	 global-step:7701	 l-p:0.12320192158222198
epoch£º385	 i:2 	 global-step:7702	 l-p:0.15490786731243134
epoch£º385	 i:3 	 global-step:7703	 l-p:0.1431896984577179
epoch£º385	 i:4 	 global-step:7704	 l-p:0.13949988782405853
epoch£º385	 i:5 	 global-step:7705	 l-p:0.36844074726104736
epoch£º385	 i:6 	 global-step:7706	 l-p:-0.053841304033994675
epoch£º385	 i:7 	 global-step:7707	 l-p:0.48982810974121094
epoch£º385	 i:8 	 global-step:7708	 l-p:0.05039801448583603
epoch£º385	 i:9 	 global-step:7709	 l-p:0.20383727550506592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8752, 2.5698, 2.7221],
        [2.8752, 1.8566, 1.3372],
        [2.8752, 2.8752, 2.8752],
        [2.8752, 2.6505, 2.7869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.14248283207416534 
model_pd.l_d.mean(): -24.95562744140625 
model_pd.lagr.mean(): -24.81314468383789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1221], device='cuda:0')), ('power', tensor([-25.0778], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.14248283207416534
epoch£º386	 i:1 	 global-step:7721	 l-p:0.16090114414691925
epoch£º386	 i:2 	 global-step:7722	 l-p:0.13899831473827362
epoch£º386	 i:3 	 global-step:7723	 l-p:-0.02725205384194851
epoch£º386	 i:4 	 global-step:7724	 l-p:0.14615480601787567
epoch£º386	 i:5 	 global-step:7725	 l-p:0.15894007682800293
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11196348071098328
epoch£º386	 i:7 	 global-step:7727	 l-p:0.12790465354919434
epoch£º386	 i:8 	 global-step:7728	 l-p:0.21180060505867004
epoch£º386	 i:9 	 global-step:7729	 l-p:0.2160947024822235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9103, 2.7387, 2.8555],
        [2.9103, 1.9727, 1.4093],
        [2.9103, 2.9073, 2.9103],
        [2.9103, 2.5954, 2.7483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.1469847857952118 
model_pd.l_d.mean(): -25.183998107910156 
model_pd.lagr.mean(): -25.03701400756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0264], device='cuda:0')), ('power', tensor([-25.2104], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.1469847857952118
epoch£º387	 i:1 	 global-step:7741	 l-p:0.16273443400859833
epoch£º387	 i:2 	 global-step:7742	 l-p:0.14804920554161072
epoch£º387	 i:3 	 global-step:7743	 l-p:0.015031781047582626
epoch£º387	 i:4 	 global-step:7744	 l-p:0.12923981249332428
epoch£º387	 i:5 	 global-step:7745	 l-p:0.1472262144088745
epoch£º387	 i:6 	 global-step:7746	 l-p:0.11859957128763199
epoch£º387	 i:7 	 global-step:7747	 l-p:0.1852295994758606
epoch£º387	 i:8 	 global-step:7748	 l-p:0.24720215797424316
epoch£º387	 i:9 	 global-step:7749	 l-p:0.12962444126605988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8871, 2.8682, 2.8858],
        [2.8871, 2.7103, 2.8295],
        [2.8871, 2.8640, 2.8853],
        [2.8871, 2.0881, 1.9662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.1255074143409729 
model_pd.l_d.mean(): -24.697715759277344 
model_pd.lagr.mean(): -24.572208404541016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2132], device='cuda:0')), ('power', tensor([-24.9109], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.1255074143409729
epoch£º388	 i:1 	 global-step:7761	 l-p:0.0666356086730957
epoch£º388	 i:2 	 global-step:7762	 l-p:0.07060371339321136
epoch£º388	 i:3 	 global-step:7763	 l-p:0.20461253821849823
epoch£º388	 i:4 	 global-step:7764	 l-p:0.14266742765903473
epoch£º388	 i:5 	 global-step:7765	 l-p:0.14551742374897003
epoch£º388	 i:6 	 global-step:7766	 l-p:0.21032625436782837
epoch£º388	 i:7 	 global-step:7767	 l-p:0.14682671427726746
epoch£º388	 i:8 	 global-step:7768	 l-p:0.10017243027687073
epoch£º388	 i:9 	 global-step:7769	 l-p:0.13107043504714966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9053, 1.9094, 1.4897],
        [2.9053, 2.1446, 2.0692],
        [2.9053, 2.9041, 2.9053],
        [2.9053, 2.6062, 2.7584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.18871989846229553 
model_pd.l_d.mean(): -24.669139862060547 
model_pd.lagr.mean(): -24.480419158935547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1229], device='cuda:0')), ('power', tensor([-24.7921], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.18871989846229553
epoch£º389	 i:1 	 global-step:7781	 l-p:0.10920960456132889
epoch£º389	 i:2 	 global-step:7782	 l-p:0.15335725247859955
epoch£º389	 i:3 	 global-step:7783	 l-p:0.14307868480682373
epoch£º389	 i:4 	 global-step:7784	 l-p:-0.005961418151855469
epoch£º389	 i:5 	 global-step:7785	 l-p:0.1882273256778717
epoch£º389	 i:6 	 global-step:7786	 l-p:0.23428978025913239
epoch£º389	 i:7 	 global-step:7787	 l-p:0.15738622844219208
epoch£º389	 i:8 	 global-step:7788	 l-p:0.21934612095355988
epoch£º389	 i:9 	 global-step:7789	 l-p:0.18472495675086975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0654, 3.0640, 3.0654],
        [3.0654, 2.5169, 2.6110],
        [3.0654, 2.1943, 1.5932],
        [3.0654, 3.0640, 3.0654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.14625605940818787 
model_pd.l_d.mean(): -25.078657150268555 
model_pd.lagr.mean(): -24.932401657104492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0770], device='cuda:0')), ('power', tensor([-25.0017], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.14625605940818787
epoch£º390	 i:1 	 global-step:7801	 l-p:0.13502439856529236
epoch£º390	 i:2 	 global-step:7802	 l-p:0.10850831866264343
epoch£º390	 i:3 	 global-step:7803	 l-p:0.11346933245658875
epoch£º390	 i:4 	 global-step:7804	 l-p:0.10394056141376495
epoch£º390	 i:5 	 global-step:7805	 l-p:0.1147136390209198
epoch£º390	 i:6 	 global-step:7806	 l-p:0.12860047817230225
epoch£º390	 i:7 	 global-step:7807	 l-p:0.16061313450336456
epoch£º390	 i:8 	 global-step:7808	 l-p:0.16982176899909973
epoch£º390	 i:9 	 global-step:7809	 l-p:0.3820284903049469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6826, 2.6298, 2.6755],
        [2.6826, 1.8191, 1.6402],
        [2.6826, 1.9025, 1.8294],
        [2.6826, 2.6591, 2.6807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.09940356016159058 
model_pd.l_d.mean(): -25.21219825744629 
model_pd.lagr.mean(): -25.112794876098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1077], device='cuda:0')), ('power', tensor([-25.3199], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.09940356016159058
epoch£º391	 i:1 	 global-step:7821	 l-p:0.3260630667209625
epoch£º391	 i:2 	 global-step:7822	 l-p:0.16814512014389038
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12911124527454376
epoch£º391	 i:4 	 global-step:7824	 l-p:0.08243061602115631
epoch£º391	 i:5 	 global-step:7825	 l-p:0.14717674255371094
epoch£º391	 i:6 	 global-step:7826	 l-p:0.14515763521194458
epoch£º391	 i:7 	 global-step:7827	 l-p:0.09288962930440903
epoch£º391	 i:8 	 global-step:7828	 l-p:0.11087390035390854
epoch£º391	 i:9 	 global-step:7829	 l-p:0.17441505193710327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9431, 2.6708, 2.8194],
        [2.9431, 2.9416, 2.9430],
        [2.9431, 2.2287, 1.6276],
        [2.9431, 2.9431, 2.9431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.157722607254982 
model_pd.l_d.mean(): -24.814123153686523 
model_pd.lagr.mean(): -24.656400680541992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0073], device='cuda:0')), ('power', tensor([-24.8068], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.157722607254982
epoch£º392	 i:1 	 global-step:7841	 l-p:0.13965891301631927
epoch£º392	 i:2 	 global-step:7842	 l-p:0.13639701902866364
epoch£º392	 i:3 	 global-step:7843	 l-p:0.15177549421787262
epoch£º392	 i:4 	 global-step:7844	 l-p:0.14921338856220245
epoch£º392	 i:5 	 global-step:7845	 l-p:0.1230931431055069
epoch£º392	 i:6 	 global-step:7846	 l-p:0.15875785052776337
epoch£º392	 i:7 	 global-step:7847	 l-p:0.09150167554616928
epoch£º392	 i:8 	 global-step:7848	 l-p:0.0693233460187912
epoch£º392	 i:9 	 global-step:7849	 l-p:0.13531635701656342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.5884, 1.7522, 1.2136],
        [2.5884, 2.5884, 2.5884],
        [2.5884, 1.5708, 1.1027],
        [2.5884, 2.5883, 2.5884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.20737968385219574 
model_pd.l_d.mean(): -25.128074645996094 
model_pd.lagr.mean(): -24.92069435119629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2788], device='cuda:0')), ('power', tensor([-25.4069], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.20737968385219574
epoch£º393	 i:1 	 global-step:7861	 l-p:0.1304459422826767
epoch£º393	 i:2 	 global-step:7862	 l-p:0.25042226910591125
epoch£º393	 i:3 	 global-step:7863	 l-p:0.16816851496696472
epoch£º393	 i:4 	 global-step:7864	 l-p:0.12531815469264984
epoch£º393	 i:5 	 global-step:7865	 l-p:0.1413705050945282
epoch£º393	 i:6 	 global-step:7866	 l-p:0.25019699335098267
epoch£º393	 i:7 	 global-step:7867	 l-p:0.1432502567768097
epoch£º393	 i:8 	 global-step:7868	 l-p:0.1300269067287445
epoch£º393	 i:9 	 global-step:7869	 l-p:0.15532606840133667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0370, 2.7430, 2.8948],
        [3.0370, 3.0347, 3.0370],
        [3.0370, 2.3044, 2.2596],
        [3.0370, 2.2667, 1.6575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.13045938313007355 
model_pd.l_d.mean(): -25.066822052001953 
model_pd.lagr.mean(): -24.936363220214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0490], device='cuda:0')), ('power', tensor([-25.0178], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.13045938313007355
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11395585536956787
epoch£º394	 i:2 	 global-step:7882	 l-p:0.10865916311740875
epoch£º394	 i:3 	 global-step:7883	 l-p:0.10562925040721893
epoch£º394	 i:4 	 global-step:7884	 l-p:0.10284260660409927
epoch£º394	 i:5 	 global-step:7885	 l-p:-1.1865007877349854
epoch£º394	 i:6 	 global-step:7886	 l-p:0.151438370347023
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13342584669589996
epoch£º394	 i:8 	 global-step:7888	 l-p:0.12643788754940033
epoch£º394	 i:9 	 global-step:7889	 l-p:0.1534299999475479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8256, 2.8165, 2.8252],
        [2.8256, 2.4170, 2.5718],
        [2.8256, 2.8256, 2.8256],
        [2.8256, 2.5312, 2.6857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.1311657875776291 
model_pd.l_d.mean(): -24.946048736572266 
model_pd.lagr.mean(): -24.814882278442383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1291], device='cuda:0')), ('power', tensor([-25.0751], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.1311657875776291
epoch£º395	 i:1 	 global-step:7901	 l-p:0.1423875391483307
epoch£º395	 i:2 	 global-step:7902	 l-p:0.2884993255138397
epoch£º395	 i:3 	 global-step:7903	 l-p:0.06503767520189285
epoch£º395	 i:4 	 global-step:7904	 l-p:0.1905118077993393
epoch£º395	 i:5 	 global-step:7905	 l-p:0.12380434572696686
epoch£º395	 i:6 	 global-step:7906	 l-p:0.1690792292356491
epoch£º395	 i:7 	 global-step:7907	 l-p:0.14842554926872253
epoch£º395	 i:8 	 global-step:7908	 l-p:0.22291037440299988
epoch£º395	 i:9 	 global-step:7909	 l-p:0.16546012461185455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9537, 1.9097, 1.3711],
        [2.9537, 1.9278, 1.3735],
        [2.9537, 2.8361, 2.9257],
        [2.9537, 2.8840, 2.9422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.1376389116048813 
model_pd.l_d.mean(): -25.267601013183594 
model_pd.lagr.mean(): -25.129962921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0762], device='cuda:0')), ('power', tensor([-25.1914], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.1376389116048813
epoch£º396	 i:1 	 global-step:7921	 l-p:0.13730579614639282
epoch£º396	 i:2 	 global-step:7922	 l-p:0.10009761899709702
epoch£º396	 i:3 	 global-step:7923	 l-p:0.13995014131069183
epoch£º396	 i:4 	 global-step:7924	 l-p:0.1160997822880745
epoch£º396	 i:5 	 global-step:7925	 l-p:0.1393357813358307
epoch£º396	 i:6 	 global-step:7926	 l-p:0.2299071103334427
epoch£º396	 i:7 	 global-step:7927	 l-p:0.3183753788471222
epoch£º396	 i:8 	 global-step:7928	 l-p:0.1583910584449768
epoch£º396	 i:9 	 global-step:7929	 l-p:0.10531976819038391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8139, 2.0174, 1.4361],
        [2.8139, 2.7814, 2.8108],
        [2.8139, 2.4297, 2.5887],
        [2.8139, 2.4673, 2.6273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.12518902122974396 
model_pd.l_d.mean(): -24.64933967590332 
model_pd.lagr.mean(): -24.524150848388672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2448], device='cuda:0')), ('power', tensor([-24.8942], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.12518902122974396
epoch£º397	 i:1 	 global-step:7941	 l-p:0.11863626539707184
epoch£º397	 i:2 	 global-step:7942	 l-p:0.23426073789596558
epoch£º397	 i:3 	 global-step:7943	 l-p:0.14284881949424744
epoch£º397	 i:4 	 global-step:7944	 l-p:0.16565808653831482
epoch£º397	 i:5 	 global-step:7945	 l-p:0.1519824117422104
epoch£º397	 i:6 	 global-step:7946	 l-p:0.10211385786533356
epoch£º397	 i:7 	 global-step:7947	 l-p:0.13446220755577087
epoch£º397	 i:8 	 global-step:7948	 l-p:0.12935025990009308
epoch£º397	 i:9 	 global-step:7949	 l-p:0.15280961990356445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9605, 1.9193, 1.3701],
        [2.9605, 2.9605, 2.9605],
        [2.9605, 2.9514, 2.9601],
        [2.9605, 1.9060, 1.3793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.16303914785385132 
model_pd.l_d.mean(): -24.667083740234375 
model_pd.lagr.mean(): -24.504045486450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1255], device='cuda:0')), ('power', tensor([-24.7926], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.16303914785385132
epoch£º398	 i:1 	 global-step:7961	 l-p:0.11192083358764648
epoch£º398	 i:2 	 global-step:7962	 l-p:0.17610906064510345
epoch£º398	 i:3 	 global-step:7963	 l-p:0.151766836643219
epoch£º398	 i:4 	 global-step:7964	 l-p:0.1583249270915985
epoch£º398	 i:5 	 global-step:7965	 l-p:0.13889077305793762
epoch£º398	 i:6 	 global-step:7966	 l-p:0.06834623962640762
epoch£º398	 i:7 	 global-step:7967	 l-p:0.17858147621154785
epoch£º398	 i:8 	 global-step:7968	 l-p:0.15605023503303528
epoch£º398	 i:9 	 global-step:7969	 l-p:0.14563587307929993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9170, 2.9144, 2.9170],
        [2.9170, 2.5619, 2.7210],
        [2.9170, 1.8722, 1.4077],
        [2.9170, 2.0898, 1.9585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.12141301482915878 
model_pd.l_d.mean(): -24.946449279785156 
model_pd.lagr.mean(): -24.825037002563477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0835], device='cuda:0')), ('power', tensor([-25.0299], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.12141301482915878
epoch£º399	 i:1 	 global-step:7981	 l-p:0.17704147100448608
epoch£º399	 i:2 	 global-step:7982	 l-p:0.2038411796092987
epoch£º399	 i:3 	 global-step:7983	 l-p:0.13761387765407562
epoch£º399	 i:4 	 global-step:7984	 l-p:0.11237575113773346
epoch£º399	 i:5 	 global-step:7985	 l-p:0.12096395343542099
epoch£º399	 i:6 	 global-step:7986	 l-p:0.10396100580692291
epoch£º399	 i:7 	 global-step:7987	 l-p:0.1133723258972168
epoch£º399	 i:8 	 global-step:7988	 l-p:0.18563216924667358
epoch£º399	 i:9 	 global-step:7989	 l-p:0.18515630066394806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9074, 1.9609, 1.6593],
        [2.9074, 1.8490, 1.3497],
        [2.9074, 2.8067, 2.8862],
        [2.9074, 1.9876, 1.7277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.146129310131073 
model_pd.l_d.mean(): -24.78730583190918 
model_pd.lagr.mean(): -24.641176223754883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0769], device='cuda:0')), ('power', tensor([-24.8642], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.146129310131073
epoch£º400	 i:1 	 global-step:8001	 l-p:0.3728301525115967
epoch£º400	 i:2 	 global-step:8002	 l-p:0.203099325299263
epoch£º400	 i:3 	 global-step:8003	 l-p:0.13922196626663208
epoch£º400	 i:4 	 global-step:8004	 l-p:0.13685829937458038
epoch£º400	 i:5 	 global-step:8005	 l-p:-0.8314005732536316
epoch£º400	 i:6 	 global-step:8006	 l-p:0.14296817779541016
epoch£º400	 i:7 	 global-step:8007	 l-p:0.2097168266773224
epoch£º400	 i:8 	 global-step:8008	 l-p:0.027845177799463272
epoch£º400	 i:9 	 global-step:8009	 l-p:0.1387166529893875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8896, 1.8324, 1.3139],
        [2.8896, 2.8884, 2.8896],
        [2.8896, 1.8312, 1.3396],
        [2.8896, 1.8786, 1.3254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.15434114634990692 
model_pd.l_d.mean(): -24.767301559448242 
model_pd.lagr.mean(): -24.612960815429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2394], device='cuda:0')), ('power', tensor([-25.0067], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.15434114634990692
epoch£º401	 i:1 	 global-step:8021	 l-p:0.16141672432422638
epoch£º401	 i:2 	 global-step:8022	 l-p:0.15696780383586884
epoch£º401	 i:3 	 global-step:8023	 l-p:0.1667306125164032
epoch£º401	 i:4 	 global-step:8024	 l-p:0.047854457050561905
epoch£º401	 i:5 	 global-step:8025	 l-p:0.2334512323141098
epoch£º401	 i:6 	 global-step:8026	 l-p:0.15534672141075134
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11778908222913742
epoch£º401	 i:8 	 global-step:8028	 l-p:6.490272045135498
epoch£º401	 i:9 	 global-step:8029	 l-p:0.15577083826065063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8885, 2.8885, 2.8885],
        [2.8885, 2.8858, 2.8884],
        [2.8885, 2.5264, 2.6867],
        [2.8885, 2.8885, 2.8885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13389690220355988 
model_pd.l_d.mean(): -25.138608932495117 
model_pd.lagr.mean(): -25.004711151123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0413], device='cuda:0')), ('power', tensor([-25.0973], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13389690220355988
epoch£º402	 i:1 	 global-step:8041	 l-p:0.14939282834529877
epoch£º402	 i:2 	 global-step:8042	 l-p:0.15150253474712372
epoch£º402	 i:3 	 global-step:8043	 l-p:0.09722161293029785
epoch£º402	 i:4 	 global-step:8044	 l-p:0.13090547919273376
epoch£º402	 i:5 	 global-step:8045	 l-p:0.11858361214399338
epoch£º402	 i:6 	 global-step:8046	 l-p:0.1673896312713623
epoch£º402	 i:7 	 global-step:8047	 l-p:0.2139011025428772
epoch£º402	 i:8 	 global-step:8048	 l-p:-0.026831569150090218
epoch£º402	 i:9 	 global-step:8049	 l-p:0.16441303491592407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9328, 2.6359, 2.7916],
        [2.9328, 2.9326, 2.9328],
        [2.9328, 2.6636, 2.8144],
        [2.9328, 2.8360, 2.9130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.14419114589691162 
model_pd.l_d.mean(): -25.039596557617188 
model_pd.lagr.mean(): -24.895404815673828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0310], device='cuda:0')), ('power', tensor([-25.0086], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.14419114589691162
epoch£º403	 i:1 	 global-step:8061	 l-p:0.08945462852716446
epoch£º403	 i:2 	 global-step:8062	 l-p:0.13142450153827667
epoch£º403	 i:3 	 global-step:8063	 l-p:0.1283014714717865
epoch£º403	 i:4 	 global-step:8064	 l-p:0.16546784341335297
epoch£º403	 i:5 	 global-step:8065	 l-p:0.14048467576503754
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12284776568412781
epoch£º403	 i:7 	 global-step:8067	 l-p:0.26006069779396057
epoch£º403	 i:8 	 global-step:8068	 l-p:0.12703092396259308
epoch£º403	 i:9 	 global-step:8069	 l-p:0.7436785697937012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8955, 1.8331, 1.3398],
        [2.8955, 1.9574, 1.3840],
        [2.8955, 2.7017, 2.8297],
        [2.8955, 2.8671, 2.8930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.17384245991706848 
model_pd.l_d.mean(): -24.626008987426758 
model_pd.lagr.mean(): -24.452165603637695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1830], device='cuda:0')), ('power', tensor([-24.8090], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.17384245991706848
epoch£º404	 i:1 	 global-step:8081	 l-p:0.12097220867872238
epoch£º404	 i:2 	 global-step:8082	 l-p:0.49057039618492126
epoch£º404	 i:3 	 global-step:8083	 l-p:0.13748590648174286
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12869969010353088
epoch£º404	 i:5 	 global-step:8085	 l-p:-0.8570518493652344
epoch£º404	 i:6 	 global-step:8086	 l-p:0.1759684532880783
epoch£º404	 i:7 	 global-step:8087	 l-p:0.13010922074317932
epoch£º404	 i:8 	 global-step:8088	 l-p:0.3413236141204834
epoch£º404	 i:9 	 global-step:8089	 l-p:0.16157987713813782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9263, 2.0112, 1.7634],
        [2.9263, 2.9263, 2.9263],
        [2.9263, 2.9263, 2.9263],
        [2.9263, 2.2520, 2.2857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.08478002995252609 
model_pd.l_d.mean(): -25.024795532226562 
model_pd.lagr.mean(): -24.94001579284668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0507], device='cuda:0')), ('power', tensor([-25.0755], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.08478002995252609
epoch£º405	 i:1 	 global-step:8101	 l-p:0.1579950898885727
epoch£º405	 i:2 	 global-step:8102	 l-p:0.09836457669734955
epoch£º405	 i:3 	 global-step:8103	 l-p:0.13804112374782562
epoch£º405	 i:4 	 global-step:8104	 l-p:0.13159213960170746
epoch£º405	 i:5 	 global-step:8105	 l-p:0.14155632257461548
epoch£º405	 i:6 	 global-step:8106	 l-p:0.1569908857345581
epoch£º405	 i:7 	 global-step:8107	 l-p:0.15527555346488953
epoch£º405	 i:8 	 global-step:8108	 l-p:0.13396811485290527
epoch£º405	 i:9 	 global-step:8109	 l-p:0.1463341861963272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8530, 2.4843, 2.6460],
        [2.8530, 2.7331, 2.8246],
        [2.8530, 2.1784, 2.2177],
        [2.8530, 1.7884, 1.2815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.07567597180604935 
model_pd.l_d.mean(): -24.818002700805664 
model_pd.lagr.mean(): -24.742326736450195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1413], device='cuda:0')), ('power', tensor([-24.9593], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.07567597180604935
epoch£º406	 i:1 	 global-step:8121	 l-p:0.11644940078258514
epoch£º406	 i:2 	 global-step:8122	 l-p:0.16384339332580566
epoch£º406	 i:3 	 global-step:8123	 l-p:0.14705665409564972
epoch£º406	 i:4 	 global-step:8124	 l-p:0.7921339869499207
epoch£º406	 i:5 	 global-step:8125	 l-p:-4.500482559204102
epoch£º406	 i:6 	 global-step:8126	 l-p:0.12243755161762238
epoch£º406	 i:7 	 global-step:8127	 l-p:0.1139950230717659
epoch£º406	 i:8 	 global-step:8128	 l-p:0.14881911873817444
epoch£º406	 i:9 	 global-step:8129	 l-p:0.1502157598733902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8582, 2.8225, 2.8545],
        [2.8582, 2.8265, 2.8552],
        [2.8582, 1.7983, 1.3346],
        [2.8582, 1.9164, 1.6426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.22455337643623352 
model_pd.l_d.mean(): -25.143346786499023 
model_pd.lagr.mean(): -24.918792724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0282], device='cuda:0')), ('power', tensor([-25.1715], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.22455337643623352
epoch£º407	 i:1 	 global-step:8141	 l-p:0.1363837867975235
epoch£º407	 i:2 	 global-step:8142	 l-p:0.15509097278118134
epoch£º407	 i:3 	 global-step:8143	 l-p:0.1908952295780182
epoch£º407	 i:4 	 global-step:8144	 l-p:0.15476630628108978
epoch£º407	 i:5 	 global-step:8145	 l-p:0.1464473456144333
epoch£º407	 i:6 	 global-step:8146	 l-p:0.14185881614685059
epoch£º407	 i:7 	 global-step:8147	 l-p:0.14223167300224304
epoch£º407	 i:8 	 global-step:8148	 l-p:0.12002162635326385
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12742280960083008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9184, 2.7097, 2.8438],
        [2.9184, 2.7386, 2.8610],
        [2.9184, 2.2393, 2.2733],
        [2.9184, 1.8656, 1.4106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.2832522392272949 
model_pd.l_d.mean(): -25.183792114257812 
model_pd.lagr.mean(): -24.90053939819336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0694], device='cuda:0')), ('power', tensor([-25.2532], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.2832522392272949
epoch£º408	 i:1 	 global-step:8161	 l-p:0.1666654497385025
epoch£º408	 i:2 	 global-step:8162	 l-p:0.21394076943397522
epoch£º408	 i:3 	 global-step:8163	 l-p:0.1741705685853958
epoch£º408	 i:4 	 global-step:8164	 l-p:0.29089978337287903
epoch£º408	 i:5 	 global-step:8165	 l-p:0.18275009095668793
epoch£º408	 i:6 	 global-step:8166	 l-p:0.12598904967308044
epoch£º408	 i:7 	 global-step:8167	 l-p:0.12062421441078186
epoch£º408	 i:8 	 global-step:8168	 l-p:0.12809284031391144
epoch£º408	 i:9 	 global-step:8169	 l-p:0.18741358816623688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0287, 2.8994, 2.9962],
        [3.0287, 2.8844, 2.9894],
        [3.0287, 3.0192, 3.0282],
        [3.0287, 3.0220, 3.0284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.13275721669197083 
model_pd.l_d.mean(): -25.086631774902344 
model_pd.lagr.mean(): -24.953874588012695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0754], device='cuda:0')), ('power', tensor([-25.0113], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.13275721669197083
epoch£º409	 i:1 	 global-step:8181	 l-p:0.15111605823040009
epoch£º409	 i:2 	 global-step:8182	 l-p:0.16272738575935364
epoch£º409	 i:3 	 global-step:8183	 l-p:0.04120353236794472
epoch£º409	 i:4 	 global-step:8184	 l-p:0.14986589550971985
epoch£º409	 i:5 	 global-step:8185	 l-p:0.13260844349861145
epoch£º409	 i:6 	 global-step:8186	 l-p:0.02021985873579979
epoch£º409	 i:7 	 global-step:8187	 l-p:0.05527891591191292
epoch£º409	 i:8 	 global-step:8188	 l-p:0.178166002035141
epoch£º409	 i:9 	 global-step:8189	 l-p:0.13024020195007324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8640, 2.6512, 2.7872],
        [2.8640, 2.4407, 2.5989],
        [2.8640, 2.8300, 2.8606],
        [2.8640, 2.8640, 2.8640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.207540825009346 
model_pd.l_d.mean(): -25.09659194946289 
model_pd.lagr.mean(): -24.88905143737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0729], device='cuda:0')), ('power', tensor([-25.1695], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.207540825009346
epoch£º410	 i:1 	 global-step:8201	 l-p:0.15623177587985992
epoch£º410	 i:2 	 global-step:8202	 l-p:0.10162372142076492
epoch£º410	 i:3 	 global-step:8203	 l-p:0.14247506856918335
epoch£º410	 i:4 	 global-step:8204	 l-p:0.3727278411388397
epoch£º410	 i:5 	 global-step:8205	 l-p:0.14208461344242096
epoch£º410	 i:6 	 global-step:8206	 l-p:0.04729887843132019
epoch£º410	 i:7 	 global-step:8207	 l-p:0.13975702226161957
epoch£º410	 i:8 	 global-step:8208	 l-p:0.1579582691192627
epoch£º410	 i:9 	 global-step:8209	 l-p:0.048753175884485245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8565, 2.8566, 2.8565],
        [2.8565, 2.8530, 2.8565],
        [2.8565, 2.8557, 2.8565],
        [2.8565, 2.8500, 2.8563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.2265743613243103 
model_pd.l_d.mean(): -25.006135940551758 
model_pd.lagr.mean(): -24.77956199645996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0668], device='cuda:0')), ('power', tensor([-25.0729], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.2265743613243103
epoch£º411	 i:1 	 global-step:8221	 l-p:0.14546173810958862
epoch£º411	 i:2 	 global-step:8222	 l-p:0.16586609184741974
epoch£º411	 i:3 	 global-step:8223	 l-p:0.19883646070957184
epoch£º411	 i:4 	 global-step:8224	 l-p:0.07548646628856659
epoch£º411	 i:5 	 global-step:8225	 l-p:0.1874573677778244
epoch£º411	 i:6 	 global-step:8226	 l-p:0.22893325984477997
epoch£º411	 i:7 	 global-step:8227	 l-p:0.24692469835281372
epoch£º411	 i:8 	 global-step:8228	 l-p:0.1290147304534912
epoch£º411	 i:9 	 global-step:8229	 l-p:0.13705286383628845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0136, 2.9808, 3.0104],
        [3.0136, 2.2862, 1.6665],
        [3.0136, 3.0136, 3.0136],
        [3.0136, 2.6019, 2.7588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.142900750041008 
model_pd.l_d.mean(): -24.58929443359375 
model_pd.lagr.mean(): -24.446393966674805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0746], device='cuda:0')), ('power', tensor([-24.6639], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.142900750041008
epoch£º412	 i:1 	 global-step:8241	 l-p:0.14403881132602692
epoch£º412	 i:2 	 global-step:8242	 l-p:0.12798480689525604
epoch£º412	 i:3 	 global-step:8243	 l-p:0.13716712594032288
epoch£º412	 i:4 	 global-step:8244	 l-p:0.2531690299510956
epoch£º412	 i:5 	 global-step:8245	 l-p:0.1519501507282257
epoch£º412	 i:6 	 global-step:8246	 l-p:0.26626381278038025
epoch£º412	 i:7 	 global-step:8247	 l-p:-0.010487041436135769
epoch£º412	 i:8 	 global-step:8248	 l-p:0.1696523278951645
epoch£º412	 i:9 	 global-step:8249	 l-p:0.08736055344343185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8833, 2.5695, 2.7295],
        [2.8833, 2.8833, 2.8833],
        [2.8833, 2.1028, 1.5032],
        [2.8833, 2.8833, 2.8833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.14895842969417572 
model_pd.l_d.mean(): -24.873626708984375 
model_pd.lagr.mean(): -24.724668502807617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0988], device='cuda:0')), ('power', tensor([-24.9724], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.14895842969417572
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14509180188179016
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13835519552230835
epoch£º413	 i:3 	 global-step:8263	 l-p:0.5826318264007568
epoch£º413	 i:4 	 global-step:8264	 l-p:0.12012698501348495
epoch£º413	 i:5 	 global-step:8265	 l-p:0.17200449109077454
epoch£º413	 i:6 	 global-step:8266	 l-p:0.17519016563892365
epoch£º413	 i:7 	 global-step:8267	 l-p:0.15010768175125122
epoch£º413	 i:8 	 global-step:8268	 l-p:0.025689467787742615
epoch£º413	 i:9 	 global-step:8269	 l-p:0.13036654889583588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8985, 2.8787, 2.8971],
        [2.8985, 1.8230, 1.3103],
        [2.8985, 2.8985, 2.8985],
        [2.8985, 2.7529, 2.8590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): -0.1417408287525177 
model_pd.l_d.mean(): -24.737337112426758 
model_pd.lagr.mean(): -24.879077911376953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1148], device='cuda:0')), ('power', tensor([-24.8522], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:-0.1417408287525177
epoch£º414	 i:1 	 global-step:8281	 l-p:0.14130380749702454
epoch£º414	 i:2 	 global-step:8282	 l-p:0.1867581456899643
epoch£º414	 i:3 	 global-step:8283	 l-p:0.12729477882385254
epoch£º414	 i:4 	 global-step:8284	 l-p:0.12583962082862854
epoch£º414	 i:5 	 global-step:8285	 l-p:0.12399128824472427
epoch£º414	 i:6 	 global-step:8286	 l-p:0.14512228965759277
epoch£º414	 i:7 	 global-step:8287	 l-p:0.14244337379932404
epoch£º414	 i:8 	 global-step:8288	 l-p:0.12723691761493683
epoch£º414	 i:9 	 global-step:8289	 l-p:0.14278236031532288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9180, 1.8429, 1.3457],
        [2.9180, 1.9931, 1.4095],
        [2.9180, 1.8633, 1.4160],
        [2.9180, 2.8381, 2.9039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.17269039154052734 
model_pd.l_d.mean(): -24.864866256713867 
model_pd.lagr.mean(): -24.692176818847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1663], device='cuda:0')), ('power', tensor([-25.0312], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.17269039154052734
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12328078597784042
epoch£º415	 i:2 	 global-step:8302	 l-p:0.1526837944984436
epoch£º415	 i:3 	 global-step:8303	 l-p:0.10713993012905121
epoch£º415	 i:4 	 global-step:8304	 l-p:0.15039244294166565
epoch£º415	 i:5 	 global-step:8305	 l-p:0.1883665770292282
epoch£º415	 i:6 	 global-step:8306	 l-p:0.107346311211586
epoch£º415	 i:7 	 global-step:8307	 l-p:0.1267315298318863
epoch£º415	 i:8 	 global-step:8308	 l-p:0.1621718406677246
epoch£º415	 i:9 	 global-step:8309	 l-p:0.1250593364238739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9805, 2.7858, 2.9146],
        [2.9805, 2.9804, 2.9805],
        [2.9805, 1.9089, 1.4119],
        [2.9805, 2.9805, 2.9805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.15191712975502014 
model_pd.l_d.mean(): -25.152692794799805 
model_pd.lagr.mean(): -25.000776290893555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0269], device='cuda:0')), ('power', tensor([-25.1796], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.15191712975502014
epoch£º416	 i:1 	 global-step:8321	 l-p:0.1520131528377533
epoch£º416	 i:2 	 global-step:8322	 l-p:-0.02309577912092209
epoch£º416	 i:3 	 global-step:8323	 l-p:0.1557297557592392
epoch£º416	 i:4 	 global-step:8324	 l-p:0.1134224683046341
epoch£º416	 i:5 	 global-step:8325	 l-p:0.13300390541553497
epoch£º416	 i:6 	 global-step:8326	 l-p:0.1402510404586792
epoch£º416	 i:7 	 global-step:8327	 l-p:0.16237643361091614
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11693931370973587
epoch£º416	 i:9 	 global-step:8329	 l-p:0.1270834505558014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9200, 2.8728, 2.9142],
        [2.9200, 2.8173, 2.8984],
        [2.9200, 2.6051, 2.7654],
        [2.9200, 2.9200, 2.9200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.19098074734210968 
model_pd.l_d.mean(): -25.02570343017578 
model_pd.lagr.mean(): -24.8347225189209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0232], device='cuda:0')), ('power', tensor([-25.0489], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.19098074734210968
epoch£º417	 i:1 	 global-step:8341	 l-p:0.14875812828540802
epoch£º417	 i:2 	 global-step:8342	 l-p:0.14143827557563782
epoch£º417	 i:3 	 global-step:8343	 l-p:0.14401502907276154
epoch£º417	 i:4 	 global-step:8344	 l-p:0.14245162904262543
epoch£º417	 i:5 	 global-step:8345	 l-p:0.22739535570144653
epoch£º417	 i:6 	 global-step:8346	 l-p:1.5899529457092285
epoch£º417	 i:7 	 global-step:8347	 l-p:0.16931025683879852
epoch£º417	 i:8 	 global-step:8348	 l-p:0.0906076431274414
epoch£º417	 i:9 	 global-step:8349	 l-p:0.046394433826208115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7023, 2.7014, 2.7023],
        [2.7023, 2.7017, 2.7023],
        [2.7023, 1.6485, 1.1472],
        [2.7023, 2.1615, 2.2944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.14035138487815857 
model_pd.l_d.mean(): -24.855722427368164 
model_pd.lagr.mean(): -24.715370178222656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2283], device='cuda:0')), ('power', tensor([-25.0840], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.14035138487815857
epoch£º418	 i:1 	 global-step:8361	 l-p:0.199625626206398
epoch£º418	 i:2 	 global-step:8362	 l-p:0.13114386796951294
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13957135379314423
epoch£º418	 i:4 	 global-step:8364	 l-p:0.14329062402248383
epoch£º418	 i:5 	 global-step:8365	 l-p:0.19696231186389923
epoch£º418	 i:6 	 global-step:8366	 l-p:0.11583507061004639
epoch£º418	 i:7 	 global-step:8367	 l-p:0.14728869497776031
epoch£º418	 i:8 	 global-step:8368	 l-p:0.1284475475549698
epoch£º418	 i:9 	 global-step:8369	 l-p:0.15280228853225708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0341, 2.5104, 2.6386],
        [3.0341, 1.9832, 1.5202],
        [3.0341, 2.0917, 1.8077],
        [3.0341, 2.9838, 3.0276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.19514887034893036 
model_pd.l_d.mean(): -25.03401756286621 
model_pd.lagr.mean(): -24.838869094848633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0567], device='cuda:0')), ('power', tensor([-25.0907], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.19514887034893036
epoch£º419	 i:1 	 global-step:8381	 l-p:0.1640998274087906
epoch£º419	 i:2 	 global-step:8382	 l-p:0.14463192224502563
epoch£º419	 i:3 	 global-step:8383	 l-p:0.12003656476736069
epoch£º419	 i:4 	 global-step:8384	 l-p:0.12948627769947052
epoch£º419	 i:5 	 global-step:8385	 l-p:0.1373768150806427
epoch£º419	 i:6 	 global-step:8386	 l-p:0.11968864500522614
epoch£º419	 i:7 	 global-step:8387	 l-p:0.11862049996852875
epoch£º419	 i:8 	 global-step:8388	 l-p:0.1311756819486618
epoch£º419	 i:9 	 global-step:8389	 l-p:0.1593698412179947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8532, 1.7753, 1.2610],
        [2.8532, 2.3862, 2.5389],
        [2.8532, 2.1612, 2.1974],
        [2.8532, 2.8507, 2.8531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): -0.015786800533533096 
model_pd.l_d.mean(): -24.772449493408203 
model_pd.lagr.mean(): -24.788236618041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0978], device='cuda:0')), ('power', tensor([-24.8703], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:-0.015786800533533096
epoch£º420	 i:1 	 global-step:8401	 l-p:0.14210282266139984
epoch£º420	 i:2 	 global-step:8402	 l-p:0.1665668785572052
epoch£º420	 i:3 	 global-step:8403	 l-p:0.26826727390289307
epoch£º420	 i:4 	 global-step:8404	 l-p:0.14308638870716095
epoch£º420	 i:5 	 global-step:8405	 l-p:0.23157401382923126
epoch£º420	 i:6 	 global-step:8406	 l-p:0.1458103507757187
epoch£º420	 i:7 	 global-step:8407	 l-p:0.16189849376678467
epoch£º420	 i:8 	 global-step:8408	 l-p:0.12480464577674866
epoch£º420	 i:9 	 global-step:8409	 l-p:0.08313299715518951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8514, 2.0930, 2.0703],
        [2.8514, 1.9939, 1.4074],
        [2.8514, 2.6205, 2.7635],
        [2.8514, 2.8169, 2.8480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.10476626455783844 
model_pd.l_d.mean(): -24.98851203918457 
model_pd.lagr.mean(): -24.883745193481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0556], device='cuda:0')), ('power', tensor([-25.0441], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.10476626455783844
epoch£º421	 i:1 	 global-step:8421	 l-p:0.14327472448349
epoch£º421	 i:2 	 global-step:8422	 l-p:0.12902507185935974
epoch£º421	 i:3 	 global-step:8423	 l-p:0.1688910275697708
epoch£º421	 i:4 	 global-step:8424	 l-p:-1.1895402669906616
epoch£º421	 i:5 	 global-step:8425	 l-p:0.12433425337076187
epoch£º421	 i:6 	 global-step:8426	 l-p:-0.23604576289653778
epoch£º421	 i:7 	 global-step:8427	 l-p:0.1554618924856186
epoch£º421	 i:8 	 global-step:8428	 l-p:0.14653383195400238
epoch£º421	 i:9 	 global-step:8429	 l-p:0.3249664902687073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9376, 2.8507, 2.9214],
        [2.9376, 2.9376, 2.9376],
        [2.9376, 2.9339, 2.9375],
        [2.9376, 1.9016, 1.4927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.1545139104127884 
model_pd.l_d.mean(): -24.979331970214844 
model_pd.lagr.mean(): -24.824817657470703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0494], device='cuda:0')), ('power', tensor([-25.0287], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.1545139104127884
epoch£º422	 i:1 	 global-step:8441	 l-p:0.13472628593444824
epoch£º422	 i:2 	 global-step:8442	 l-p:0.11657780408859253
epoch£º422	 i:3 	 global-step:8443	 l-p:0.037336789071559906
epoch£º422	 i:4 	 global-step:8444	 l-p:0.14186100661754608
epoch£º422	 i:5 	 global-step:8445	 l-p:0.17470677196979523
epoch£º422	 i:6 	 global-step:8446	 l-p:0.16290231049060822
epoch£º422	 i:7 	 global-step:8447	 l-p:0.1384028047323227
epoch£º422	 i:8 	 global-step:8448	 l-p:0.13866649568080902
epoch£º422	 i:9 	 global-step:8449	 l-p:0.1256566196680069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9742, 2.9740, 2.9742],
        [2.9742, 1.9007, 1.4195],
        [2.9742, 2.8706, 2.9523],
        [2.9742, 2.0399, 1.7838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.04795125871896744 
model_pd.l_d.mean(): -25.033916473388672 
model_pd.lagr.mean(): -24.985965728759766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1343], device='cuda:0')), ('power', tensor([-25.1682], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.04795125871896744
epoch£º423	 i:1 	 global-step:8461	 l-p:0.1921442747116089
epoch£º423	 i:2 	 global-step:8462	 l-p:0.1703474074602127
epoch£º423	 i:3 	 global-step:8463	 l-p:0.15079374611377716
epoch£º423	 i:4 	 global-step:8464	 l-p:0.1669827252626419
epoch£º423	 i:5 	 global-step:8465	 l-p:0.12194440513849258
epoch£º423	 i:6 	 global-step:8466	 l-p:0.13421736657619476
epoch£º423	 i:7 	 global-step:8467	 l-p:0.13839997351169586
epoch£º423	 i:8 	 global-step:8468	 l-p:0.14014612138271332
epoch£º423	 i:9 	 global-step:8469	 l-p:0.13397251069545746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8617, 2.3510, 2.4920],
        [2.8617, 2.8494, 2.8611],
        [2.8617, 1.8075, 1.2644],
        [2.8617, 2.8617, 2.8617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.16614806652069092 
model_pd.l_d.mean(): -25.0455322265625 
model_pd.lagr.mean(): -24.879384994506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1737], device='cuda:0')), ('power', tensor([-25.2192], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.16614806652069092
epoch£º424	 i:1 	 global-step:8481	 l-p:0.18140949308872223
epoch£º424	 i:2 	 global-step:8482	 l-p:0.17991115152835846
epoch£º424	 i:3 	 global-step:8483	 l-p:0.13349872827529907
epoch£º424	 i:4 	 global-step:8484	 l-p:0.176681786775589
epoch£º424	 i:5 	 global-step:8485	 l-p:0.25953948497772217
epoch£º424	 i:6 	 global-step:8486	 l-p:0.16071470081806183
epoch£º424	 i:7 	 global-step:8487	 l-p:0.15000207722187042
epoch£º424	 i:8 	 global-step:8488	 l-p:-3.396653413772583
epoch£º424	 i:9 	 global-step:8489	 l-p:0.30855387449264526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9075, 2.4621, 2.6196],
        [2.9075, 2.1274, 2.0821],
        [2.9075, 1.8223, 1.2965],
        [2.9075, 2.9075, 2.9075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.18016277253627777 
model_pd.l_d.mean(): -24.776180267333984 
model_pd.lagr.mean(): -24.596017837524414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1008], device='cuda:0')), ('power', tensor([-24.8770], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.18016277253627777
epoch£º425	 i:1 	 global-step:8501	 l-p:0.1407576948404312
epoch£º425	 i:2 	 global-step:8502	 l-p:0.15006640553474426
epoch£º425	 i:3 	 global-step:8503	 l-p:0.13522228598594666
epoch£º425	 i:4 	 global-step:8504	 l-p:0.15312287211418152
epoch£º425	 i:5 	 global-step:8505	 l-p:0.4093017876148224
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11527276039123535
epoch£º425	 i:7 	 global-step:8507	 l-p:0.130225270986557
epoch£º425	 i:8 	 global-step:8508	 l-p:0.15983740985393524
epoch£º425	 i:9 	 global-step:8509	 l-p:0.1282166987657547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9863, 2.9394, 2.9806],
        [2.9863, 2.9827, 2.9862],
        [2.9863, 2.9726, 2.9855],
        [2.9863, 1.9699, 1.3876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.22154392302036285 
model_pd.l_d.mean(): -24.608457565307617 
model_pd.lagr.mean(): -24.386913299560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1586], device='cuda:0')), ('power', tensor([-24.7670], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.22154392302036285
epoch£º426	 i:1 	 global-step:8521	 l-p:0.1638985425233841
epoch£º426	 i:2 	 global-step:8522	 l-p:0.14382217824459076
epoch£º426	 i:3 	 global-step:8523	 l-p:0.15484900772571564
epoch£º426	 i:4 	 global-step:8524	 l-p:0.13460996747016907
epoch£º426	 i:5 	 global-step:8525	 l-p:0.14867883920669556
epoch£º426	 i:6 	 global-step:8526	 l-p:0.1569216102361679
epoch£º426	 i:7 	 global-step:8527	 l-p:0.14518338441848755
epoch£º426	 i:8 	 global-step:8528	 l-p:0.16836032271385193
epoch£º426	 i:9 	 global-step:8529	 l-p:0.14180642366409302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6808, 2.6479, 2.6777],
        [2.6808, 2.6664, 2.6800],
        [2.6808, 2.3556, 2.5217],
        [2.6808, 1.6563, 1.3152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): -0.1711694747209549 
model_pd.l_d.mean(): -24.872316360473633 
model_pd.lagr.mean(): -25.043485641479492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1336], device='cuda:0')), ('power', tensor([-25.0059], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:-0.1711694747209549
epoch£º427	 i:1 	 global-step:8541	 l-p:0.006700372323393822
epoch£º427	 i:2 	 global-step:8542	 l-p:0.42532315850257874
epoch£º427	 i:3 	 global-step:8543	 l-p:0.1486559510231018
epoch£º427	 i:4 	 global-step:8544	 l-p:0.14672894775867462
epoch£º427	 i:5 	 global-step:8545	 l-p:0.11515124887228012
epoch£º427	 i:6 	 global-step:8546	 l-p:4.08887243270874
epoch£º427	 i:7 	 global-step:8547	 l-p:0.36558955907821655
epoch£º427	 i:8 	 global-step:8548	 l-p:0.24058350920677185
epoch£º427	 i:9 	 global-step:8549	 l-p:0.16151590645313263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[2.9016, 1.9558, 1.3734],
        [2.9016, 2.1178, 2.0730],
        [2.9016, 1.8809, 1.3145],
        [2.9016, 2.0397, 1.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.17318230867385864 
model_pd.l_d.mean(): -25.13520622253418 
model_pd.lagr.mean(): -24.962024688720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0348], device='cuda:0')), ('power', tensor([-25.1700], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.17318230867385864
epoch£º428	 i:1 	 global-step:8561	 l-p:0.17580822110176086
epoch£º428	 i:2 	 global-step:8562	 l-p:0.14205187559127808
epoch£º428	 i:3 	 global-step:8563	 l-p:0.11866618692874908
epoch£º428	 i:4 	 global-step:8564	 l-p:0.13781969249248505
epoch£º428	 i:5 	 global-step:8565	 l-p:0.10977104306221008
epoch£º428	 i:6 	 global-step:8566	 l-p:0.11924571543931961
epoch£º428	 i:7 	 global-step:8567	 l-p:0.11431229114532471
epoch£º428	 i:8 	 global-step:8568	 l-p:0.14196544885635376
epoch£º428	 i:9 	 global-step:8569	 l-p:0.12942448258399963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0007, 2.0631, 1.8099],
        [3.0007, 3.0002, 3.0007],
        [3.0007, 1.9189, 1.3587],
        [3.0007, 2.2097, 1.5912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.09855006635189056 
model_pd.l_d.mean(): -24.672592163085938 
model_pd.lagr.mean(): -24.57404136657715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1558], device='cuda:0')), ('power', tensor([-24.8284], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.09855006635189056
epoch£º429	 i:1 	 global-step:8581	 l-p:0.16784639656543732
epoch£º429	 i:2 	 global-step:8582	 l-p:0.14691995084285736
epoch£º429	 i:3 	 global-step:8583	 l-p:0.16579949855804443
epoch£º429	 i:4 	 global-step:8584	 l-p:0.214074969291687
epoch£º429	 i:5 	 global-step:8585	 l-p:0.12721586227416992
epoch£º429	 i:6 	 global-step:8586	 l-p:0.1265338659286499
epoch£º429	 i:7 	 global-step:8587	 l-p:0.1240733414888382
epoch£º429	 i:8 	 global-step:8588	 l-p:-0.28548914194107056
epoch£º429	 i:9 	 global-step:8589	 l-p:0.1062472015619278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9202, 2.7353, 2.8611],
        [2.9202, 2.1185, 1.5106],
        [2.9202, 2.4924, 2.6543],
        [2.9202, 2.9133, 2.9199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.7315363883972168 
model_pd.l_d.mean(): -24.670747756958008 
model_pd.lagr.mean(): -23.939210891723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2136], device='cuda:0')), ('power', tensor([-24.8844], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.7315363883972168
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14430129528045654
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13690491020679474
epoch£º430	 i:3 	 global-step:8603	 l-p:0.1384536474943161
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13370037078857422
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12104703485965729
epoch£º430	 i:6 	 global-step:8606	 l-p:-0.012418479658663273
epoch£º430	 i:7 	 global-step:8607	 l-p:0.30374765396118164
epoch£º430	 i:8 	 global-step:8608	 l-p:0.08948204666376114
epoch£º430	 i:9 	 global-step:8609	 l-p:0.13079684972763062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9091, 1.8142, 1.2945],
        [2.9091, 2.1140, 1.5063],
        [2.9091, 2.8437, 2.8992],
        [2.9091, 2.0014, 1.8073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): -0.13093319535255432 
model_pd.l_d.mean(): -24.9187068939209 
model_pd.lagr.mean(): -25.049640655517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0844], device='cuda:0')), ('power', tensor([-25.0031], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:-0.13093319535255432
epoch£º431	 i:1 	 global-step:8621	 l-p:0.2302667498588562
epoch£º431	 i:2 	 global-step:8622	 l-p:0.12696851789951324
epoch£º431	 i:3 	 global-step:8623	 l-p:0.1519841104745865
epoch£º431	 i:4 	 global-step:8624	 l-p:0.139382466673851
epoch£º431	 i:5 	 global-step:8625	 l-p:0.13048040866851807
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12176936119794846
epoch£º431	 i:7 	 global-step:8627	 l-p:0.049346741288900375
epoch£º431	 i:8 	 global-step:8628	 l-p:0.11951318383216858
epoch£º431	 i:9 	 global-step:8629	 l-p:0.15448527038097382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9010, 2.0574, 1.4575],
        [2.9010, 2.9007, 2.9011],
        [2.9010, 2.8003, 2.8805],
        [2.9010, 2.7065, 2.8366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.12323642522096634 
model_pd.l_d.mean(): -24.981182098388672 
model_pd.lagr.mean(): -24.857946395874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1127], device='cuda:0')), ('power', tensor([-25.0939], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.12323642522096634
epoch£º432	 i:1 	 global-step:8641	 l-p:0.15670299530029297
epoch£º432	 i:2 	 global-step:8642	 l-p:0.06879068166017532
epoch£º432	 i:3 	 global-step:8643	 l-p:0.1296745240688324
epoch£º432	 i:4 	 global-step:8644	 l-p:0.11875473707914352
epoch£º432	 i:5 	 global-step:8645	 l-p:0.14419661462306976
epoch£º432	 i:6 	 global-step:8646	 l-p:0.14565980434417725
epoch£º432	 i:7 	 global-step:8647	 l-p:0.23602551221847534
epoch£º432	 i:8 	 global-step:8648	 l-p:14.244473457336426
epoch£º432	 i:9 	 global-step:8649	 l-p:0.13515816628932953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9797, 2.2178, 2.1929],
        [2.9797, 1.8962, 1.3389],
        [2.9797, 2.9798, 2.9798],
        [2.9797, 2.8315, 2.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13895244896411896 
model_pd.l_d.mean(): -24.927993774414062 
model_pd.lagr.mean(): -24.78904151916504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0697], device='cuda:0')), ('power', tensor([-24.9977], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13895244896411896
epoch£º433	 i:1 	 global-step:8661	 l-p:0.1364535093307495
epoch£º433	 i:2 	 global-step:8662	 l-p:0.13045358657836914
epoch£º433	 i:3 	 global-step:8663	 l-p:0.15704473853111267
epoch£º433	 i:4 	 global-step:8664	 l-p:0.09302294254302979
epoch£º433	 i:5 	 global-step:8665	 l-p:0.22005027532577515
epoch£º433	 i:6 	 global-step:8666	 l-p:0.17789849638938904
epoch£º433	 i:7 	 global-step:8667	 l-p:0.14638742804527283
epoch£º433	 i:8 	 global-step:8668	 l-p:0.13304418325424194
epoch£º433	 i:9 	 global-step:8669	 l-p:0.13676565885543823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[3.0254, 2.0522, 1.7447],
        [3.0254, 1.9596, 1.3814],
        [3.0254, 2.4537, 2.5676],
        [3.0254, 2.2774, 2.2635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.14493335783481598 
model_pd.l_d.mean(): -25.263290405273438 
model_pd.lagr.mean(): -25.118356704711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0355], device='cuda:0')), ('power', tensor([-25.2278], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.14493335783481598
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14275695383548737
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11925000697374344
epoch£º434	 i:3 	 global-step:8683	 l-p:0.01614919677376747
epoch£º434	 i:4 	 global-step:8684	 l-p:-0.4454247057437897
epoch£º434	 i:5 	 global-step:8685	 l-p:0.21012412011623383
epoch£º434	 i:6 	 global-step:8686	 l-p:0.1342090666294098
epoch£º434	 i:7 	 global-step:8687	 l-p:0.17489401996135712
epoch£º434	 i:8 	 global-step:8688	 l-p:0.15774299204349518
epoch£º434	 i:9 	 global-step:8689	 l-p:0.08854034543037415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8886, 2.8878, 2.8886],
        [2.8886, 2.8886, 2.8886],
        [2.8886, 2.8883, 2.8886],
        [2.8886, 2.0270, 1.4305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.16052667796611786 
model_pd.l_d.mean(): -25.1804141998291 
model_pd.lagr.mean(): -25.019887924194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0226], device='cuda:0')), ('power', tensor([-25.1578], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.16052667796611786
epoch£º435	 i:1 	 global-step:8701	 l-p:0.3439527750015259
epoch£º435	 i:2 	 global-step:8702	 l-p:0.09634315222501755
epoch£º435	 i:3 	 global-step:8703	 l-p:0.12383560091257095
epoch£º435	 i:4 	 global-step:8704	 l-p:0.13043732941150665
epoch£º435	 i:5 	 global-step:8705	 l-p:0.1628345102071762
epoch£º435	 i:6 	 global-step:8706	 l-p:0.1848621517419815
epoch£º435	 i:7 	 global-step:8707	 l-p:0.14403994381427765
epoch£º435	 i:8 	 global-step:8708	 l-p:0.13098226487636566
epoch£º435	 i:9 	 global-step:8709	 l-p:0.20442703366279602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0045, 2.1332, 1.5213],
        [3.0045, 1.9776, 1.3902],
        [3.0045, 2.9627, 2.9998],
        [3.0045, 1.9034, 1.3794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): -0.1453191041946411 
model_pd.l_d.mean(): -25.069368362426758 
model_pd.lagr.mean(): -25.21468734741211 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0650], device='cuda:0')), ('power', tensor([-25.1344], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:-0.1453191041946411
epoch£º436	 i:1 	 global-step:8721	 l-p:0.14420059323310852
epoch£º436	 i:2 	 global-step:8722	 l-p:0.1150638684630394
epoch£º436	 i:3 	 global-step:8723	 l-p:0.1313062161207199
epoch£º436	 i:4 	 global-step:8724	 l-p:0.1166905015707016
epoch£º436	 i:5 	 global-step:8725	 l-p:0.13827796280384064
epoch£º436	 i:6 	 global-step:8726	 l-p:0.1533440202474594
epoch£º436	 i:7 	 global-step:8727	 l-p:0.1884501725435257
epoch£º436	 i:8 	 global-step:8728	 l-p:0.1908794343471527
epoch£º436	 i:9 	 global-step:8729	 l-p:0.1521465927362442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7999, 2.7793, 2.7984],
        [2.7999, 2.0315, 1.4299],
        [2.7999, 1.7205, 1.1968],
        [2.7999, 1.7317, 1.2003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.1396774798631668 
model_pd.l_d.mean(): -24.732851028442383 
model_pd.lagr.mean(): -24.59317398071289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1193], device='cuda:0')), ('power', tensor([-24.8522], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.1396774798631668
epoch£º437	 i:1 	 global-step:8741	 l-p:0.13144274055957794
epoch£º437	 i:2 	 global-step:8742	 l-p:-0.9078543782234192
epoch£º437	 i:3 	 global-step:8743	 l-p:0.09029778093099594
epoch£º437	 i:4 	 global-step:8744	 l-p:0.352401465177536
epoch£º437	 i:5 	 global-step:8745	 l-p:0.003815736621618271
epoch£º437	 i:6 	 global-step:8746	 l-p:0.14166173338890076
epoch£º437	 i:7 	 global-step:8747	 l-p:0.12688302993774414
epoch£º437	 i:8 	 global-step:8748	 l-p:0.13222265243530273
epoch£º437	 i:9 	 global-step:8749	 l-p:0.10751233994960785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1439, 2.4006, 2.3848],
        [3.1439, 2.0691, 1.5685],
        [3.1439, 2.9965, 3.1039],
        [3.1439, 3.1440, 3.1439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.1243300512433052 
model_pd.l_d.mean(): -24.608413696289062 
model_pd.lagr.mean(): -24.48408317565918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0737], device='cuda:0')), ('power', tensor([-24.5347], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.1243300512433052
epoch£º438	 i:1 	 global-step:8761	 l-p:0.1296824812889099
epoch£º438	 i:2 	 global-step:8762	 l-p:0.15071040391921997
epoch£º438	 i:3 	 global-step:8763	 l-p:0.10697097331285477
epoch£º438	 i:4 	 global-step:8764	 l-p:0.03156226500868797
epoch£º438	 i:5 	 global-step:8765	 l-p:0.15379592776298523
epoch£º438	 i:6 	 global-step:8766	 l-p:-0.298086941242218
epoch£º438	 i:7 	 global-step:8767	 l-p:0.152765154838562
epoch£º438	 i:8 	 global-step:8768	 l-p:0.0757666826248169
epoch£º438	 i:9 	 global-step:8769	 l-p:0.1448257565498352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7893, 1.8408, 1.6109],
        [2.7893, 1.7843, 1.4761],
        [2.7893, 1.7767, 1.4575],
        [2.7893, 2.7881, 2.7893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.17441833019256592 
model_pd.l_d.mean(): -25.231792449951172 
model_pd.lagr.mean(): -25.057374954223633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0349], device='cuda:0')), ('power', tensor([-25.2667], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.17441833019256592
epoch£º439	 i:1 	 global-step:8781	 l-p:0.15925176441669464
epoch£º439	 i:2 	 global-step:8782	 l-p:0.16023506224155426
epoch£º439	 i:3 	 global-step:8783	 l-p:0.16552913188934326
epoch£º439	 i:4 	 global-step:8784	 l-p:0.17463582754135132
epoch£º439	 i:5 	 global-step:8785	 l-p:1.1075499057769775
epoch£º439	 i:6 	 global-step:8786	 l-p:0.1534327268600464
epoch£º439	 i:7 	 global-step:8787	 l-p:0.17699025571346283
epoch£º439	 i:8 	 global-step:8788	 l-p:0.13262619078159332
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12451174110174179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0524, 2.9038, 3.0121],
        [3.0524, 2.9356, 3.0258],
        [3.0524, 2.4582, 2.5619],
        [3.0524, 1.9553, 1.3892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.1531650573015213 
model_pd.l_d.mean(): -24.62858772277832 
model_pd.lagr.mean(): -24.475421905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0005], device='cuda:0')), ('power', tensor([-24.6291], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.1531650573015213
epoch£º440	 i:1 	 global-step:8801	 l-p:0.12385932356119156
epoch£º440	 i:2 	 global-step:8802	 l-p:0.20876197516918182
epoch£º440	 i:3 	 global-step:8803	 l-p:0.11843272298574448
epoch£º440	 i:4 	 global-step:8804	 l-p:0.14307081699371338
epoch£º440	 i:5 	 global-step:8805	 l-p:0.13296596705913544
epoch£º440	 i:6 	 global-step:8806	 l-p:0.14789380133152008
epoch£º440	 i:7 	 global-step:8807	 l-p:0.21840137243270874
epoch£º440	 i:8 	 global-step:8808	 l-p:0.1531633883714676
epoch£º440	 i:9 	 global-step:8809	 l-p:1.5883748531341553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9250, 2.8828, 2.9203],
        [2.9250, 1.8275, 1.2873],
        [2.9250, 2.9208, 2.9249],
        [2.9250, 2.0403, 1.8870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 1.9535536766052246 
model_pd.l_d.mean(): -24.601469039916992 
model_pd.lagr.mean(): -22.64791488647461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1519], device='cuda:0')), ('power', tensor([-24.7534], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:1.9535536766052246
epoch£º441	 i:1 	 global-step:8821	 l-p:0.12866555154323578
epoch£º441	 i:2 	 global-step:8822	 l-p:0.15659581124782562
epoch£º441	 i:3 	 global-step:8823	 l-p:0.12958718836307526
epoch£º441	 i:4 	 global-step:8824	 l-p:0.15241891145706177
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13241881132125854
epoch£º441	 i:6 	 global-step:8826	 l-p:0.1304735690355301
epoch£º441	 i:7 	 global-step:8827	 l-p:0.16108666360378265
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11956829577684402
epoch£º441	 i:9 	 global-step:8829	 l-p:0.13506962358951569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9013, 2.8933, 2.9010],
        [2.9013, 1.9066, 1.3292],
        [2.9013, 1.8154, 1.3667],
        [2.9013, 2.7883, 2.8765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.14054353535175323 
model_pd.l_d.mean(): -25.035459518432617 
model_pd.lagr.mean(): -24.894916534423828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0413], device='cuda:0')), ('power', tensor([-25.0768], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.14054353535175323
epoch£º442	 i:1 	 global-step:8841	 l-p:0.15485425293445587
epoch£º442	 i:2 	 global-step:8842	 l-p:0.10923106968402863
epoch£º442	 i:3 	 global-step:8843	 l-p:0.03773912042379379
epoch£º442	 i:4 	 global-step:8844	 l-p:0.31526702642440796
epoch£º442	 i:5 	 global-step:8845	 l-p:0.3999876379966736
epoch£º442	 i:6 	 global-step:8846	 l-p:0.014129631221294403
epoch£º442	 i:7 	 global-step:8847	 l-p:0.019668536260724068
epoch£º442	 i:8 	 global-step:8848	 l-p:0.274211585521698
epoch£º442	 i:9 	 global-step:8849	 l-p:0.1206195205450058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8850, 2.6586, 2.8017],
        [2.8850, 2.8813, 2.8849],
        [2.8850, 2.8798, 2.8848],
        [2.8850, 2.5717, 2.7361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.12516723573207855 
model_pd.l_d.mean(): -24.95265007019043 
model_pd.lagr.mean(): -24.827482223510742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0498], device='cuda:0')), ('power', tensor([-25.0024], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.12516723573207855
epoch£º443	 i:1 	 global-step:8861	 l-p:0.1541331559419632
epoch£º443	 i:2 	 global-step:8862	 l-p:0.14474736154079437
epoch£º443	 i:3 	 global-step:8863	 l-p:0.2776332497596741
epoch£º443	 i:4 	 global-step:8864	 l-p:0.1636338233947754
epoch£º443	 i:5 	 global-step:8865	 l-p:0.14418798685073853
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11870540678501129
epoch£º443	 i:7 	 global-step:8867	 l-p:0.15812978148460388
epoch£º443	 i:8 	 global-step:8868	 l-p:0.13401684165000916
epoch£º443	 i:9 	 global-step:8869	 l-p:-0.02879103645682335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0167, 1.9123, 1.3578],
        [3.0167, 2.7141, 2.8755],
        [3.0167, 2.2923, 2.3082],
        [3.0167, 1.9085, 1.3811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 29.443397521972656 
model_pd.l_d.mean(): -24.54558753967285 
model_pd.lagr.mean(): 4.897809982299805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1164], device='cuda:0')), ('power', tensor([-24.6620], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:29.443397521972656
epoch£º444	 i:1 	 global-step:8881	 l-p:0.1356302946805954
epoch£º444	 i:2 	 global-step:8882	 l-p:0.13186374306678772
epoch£º444	 i:3 	 global-step:8883	 l-p:0.11931664496660233
epoch£º444	 i:4 	 global-step:8884	 l-p:0.13066045939922333
epoch£º444	 i:5 	 global-step:8885	 l-p:0.12683840095996857
epoch£º444	 i:6 	 global-step:8886	 l-p:0.15192680060863495
epoch£º444	 i:7 	 global-step:8887	 l-p:0.16844087839126587
epoch£º444	 i:8 	 global-step:8888	 l-p:0.19315822422504425
epoch£º444	 i:9 	 global-step:8889	 l-p:0.026499733328819275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8222, 2.8201, 2.8221],
        [2.8222, 1.9598, 1.8487],
        [2.8222, 2.7671, 2.8148],
        [2.8222, 2.3074, 2.4562]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.07309836894273758 
model_pd.l_d.mean(): -25.182920455932617 
model_pd.lagr.mean(): -25.109821319580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0965], device='cuda:0')), ('power', tensor([-25.2794], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.07309836894273758
epoch£º445	 i:1 	 global-step:8901	 l-p:0.16080047190189362
epoch£º445	 i:2 	 global-step:8902	 l-p:0.09630469977855682
epoch£º445	 i:3 	 global-step:8903	 l-p:0.16774873435497284
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13276301324367523
epoch£º445	 i:5 	 global-step:8905	 l-p:0.15311698615550995
epoch£º445	 i:6 	 global-step:8906	 l-p:0.1618870347738266
epoch£º445	 i:7 	 global-step:8907	 l-p:0.15266309678554535
epoch£º445	 i:8 	 global-step:8908	 l-p:0.11600611358880997
epoch£º445	 i:9 	 global-step:8909	 l-p:0.11976764351129532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[3.0127, 2.1662, 2.0563],
        [3.0127, 2.0682, 1.4622],
        [3.0127, 2.0130, 1.6819],
        [3.0127, 2.0848, 1.4765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): -0.2874671220779419 
model_pd.l_d.mean(): -24.960311889648438 
model_pd.lagr.mean(): -25.247779846191406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0470], device='cuda:0')), ('power', tensor([-25.0074], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:-0.2874671220779419
epoch£º446	 i:1 	 global-step:8921	 l-p:0.12337897717952728
epoch£º446	 i:2 	 global-step:8922	 l-p:0.12048162519931793
epoch£º446	 i:3 	 global-step:8923	 l-p:0.16304956376552582
epoch£º446	 i:4 	 global-step:8924	 l-p:0.13823716342449188
epoch£º446	 i:5 	 global-step:8925	 l-p:-0.10297910124063492
epoch£º446	 i:6 	 global-step:8926	 l-p:0.1469082534313202
epoch£º446	 i:7 	 global-step:8927	 l-p:0.15032246708869934
epoch£º446	 i:8 	 global-step:8928	 l-p:0.23059211671352386
epoch£º446	 i:9 	 global-step:8929	 l-p:0.20539100468158722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9136, 2.8884, 2.9116],
        [2.9136, 2.6890, 2.8315],
        [2.9136, 1.9920, 1.7955],
        [2.9136, 1.9792, 1.7645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): -0.17264360189437866 
model_pd.l_d.mean(): -25.07621192932129 
model_pd.lagr.mean(): -25.248855590820312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0833], device='cuda:0')), ('power', tensor([-25.1595], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:-0.17264360189437866
epoch£º447	 i:1 	 global-step:8941	 l-p:0.13679836690425873
epoch£º447	 i:2 	 global-step:8942	 l-p:0.13522300124168396
epoch£º447	 i:3 	 global-step:8943	 l-p:0.1323973387479782
epoch£º447	 i:4 	 global-step:8944	 l-p:0.23432835936546326
epoch£º447	 i:5 	 global-step:8945	 l-p:0.1313568353652954
epoch£º447	 i:6 	 global-step:8946	 l-p:0.13080699741840363
epoch£º447	 i:7 	 global-step:8947	 l-p:0.14961668848991394
epoch£º447	 i:8 	 global-step:8948	 l-p:0.1494031846523285
epoch£º447	 i:9 	 global-step:8949	 l-p:0.17078641057014465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0080, 2.1433, 1.5270],
        [3.0080, 2.0248, 1.4251],
        [3.0080, 2.8482, 2.9626],
        [3.0080, 3.0076, 3.0080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.12624695897102356 
model_pd.l_d.mean(): -24.821210861206055 
model_pd.lagr.mean(): -24.694963455200195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0855], device='cuda:0')), ('power', tensor([-24.9067], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.12624695897102356
epoch£º448	 i:1 	 global-step:8961	 l-p:0.14215736091136932
epoch£º448	 i:2 	 global-step:8962	 l-p:0.13779287040233612
epoch£º448	 i:3 	 global-step:8963	 l-p:0.12955453991889954
epoch£º448	 i:4 	 global-step:8964	 l-p:0.026561670005321503
epoch£º448	 i:5 	 global-step:8965	 l-p:0.20882678031921387
epoch£º448	 i:6 	 global-step:8966	 l-p:0.12827537953853607
epoch£º448	 i:7 	 global-step:8967	 l-p:0.11858724057674408
epoch£º448	 i:8 	 global-step:8968	 l-p:0.16605396568775177
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13524700701236725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9242, 2.9136, 2.9237],
        [2.9242, 2.3015, 2.3994],
        [2.9242, 2.6959, 2.8397],
        [2.9242, 2.6365, 2.7966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.2646482288837433 
model_pd.l_d.mean(): -25.19510269165039 
model_pd.lagr.mean(): -24.93045425415039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0280], device='cuda:0')), ('power', tensor([-25.2232], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.2646482288837433
epoch£º449	 i:1 	 global-step:8981	 l-p:0.012693185359239578
epoch£º449	 i:2 	 global-step:8982	 l-p:0.1741866022348404
epoch£º449	 i:3 	 global-step:8983	 l-p:0.16565728187561035
epoch£º449	 i:4 	 global-step:8984	 l-p:0.15302234888076782
epoch£º449	 i:5 	 global-step:8985	 l-p:0.13237382471561432
epoch£º449	 i:6 	 global-step:8986	 l-p:0.1332676112651825
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12278938293457031
epoch£º449	 i:8 	 global-step:8988	 l-p:0.16098035871982574
epoch£º449	 i:9 	 global-step:8989	 l-p:-0.1031767800450325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9477, 2.9456, 2.9476],
        [2.9477, 2.9469, 2.9476],
        [2.9477, 2.9319, 2.9467],
        [2.9477, 1.8875, 1.3158]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.17494381964206696 
model_pd.l_d.mean(): -25.195261001586914 
model_pd.lagr.mean(): -25.02031707763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0523], device='cuda:0')), ('power', tensor([-25.2476], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.17494381964206696
epoch£º450	 i:1 	 global-step:9001	 l-p:0.1427941471338272
epoch£º450	 i:2 	 global-step:9002	 l-p:0.11709210276603699
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12633097171783447
epoch£º450	 i:4 	 global-step:9004	 l-p:0.24317309260368347
epoch£º450	 i:5 	 global-step:9005	 l-p:0.13236203789710999
epoch£º450	 i:6 	 global-step:9006	 l-p:0.15174263715744019
epoch£º450	 i:7 	 global-step:9007	 l-p:0.1455073356628418
epoch£º450	 i:8 	 global-step:9008	 l-p:0.31717294454574585
epoch£º450	 i:9 	 global-step:9009	 l-p:0.124544657766819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9002, 2.7762, 2.8713],
        [2.9002, 2.5721, 2.7393],
        [2.9002, 2.8998, 2.9002],
        [2.9002, 2.8990, 2.9002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.14472778141498566 
model_pd.l_d.mean(): -24.47675132751465 
model_pd.lagr.mean(): -24.33202362060547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2165], device='cuda:0')), ('power', tensor([-24.6932], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.14472778141498566
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10445752739906311
epoch£º451	 i:2 	 global-step:9022	 l-p:0.16260114312171936
epoch£º451	 i:3 	 global-step:9023	 l-p:0.17475925385951996
epoch£º451	 i:4 	 global-step:9024	 l-p:-0.050476741045713425
epoch£º451	 i:5 	 global-step:9025	 l-p:0.1207016184926033
epoch£º451	 i:6 	 global-step:9026	 l-p:0.18214450776576996
epoch£º451	 i:7 	 global-step:9027	 l-p:0.14025823771953583
epoch£º451	 i:8 	 global-step:9028	 l-p:0.1339729279279709
epoch£º451	 i:9 	 global-step:9029	 l-p:0.12897197902202606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9842, 2.5690, 2.7361],
        [2.9842, 2.9774, 2.9840],
        [2.9842, 2.9704, 2.9834],
        [2.9842, 2.9842, 2.9842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.15087875723838806 
model_pd.l_d.mean(): -24.923059463500977 
model_pd.lagr.mean(): -24.772180557250977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0687], device='cuda:0')), ('power', tensor([-24.9918], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.15087875723838806
epoch£º452	 i:1 	 global-step:9041	 l-p:0.2654138207435608
epoch£º452	 i:2 	 global-step:9042	 l-p:0.1096966341137886
epoch£º452	 i:3 	 global-step:9043	 l-p:0.13302485644817352
epoch£º452	 i:4 	 global-step:9044	 l-p:0.14490340650081635
epoch£º452	 i:5 	 global-step:9045	 l-p:0.09356847405433655
epoch£º452	 i:6 	 global-step:9046	 l-p:0.14739471673965454
epoch£º452	 i:7 	 global-step:9047	 l-p:0.13289178907871246
epoch£º452	 i:8 	 global-step:9048	 l-p:0.1499616801738739
epoch£º452	 i:9 	 global-step:9049	 l-p:0.15213771164417267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8921, 2.8851, 2.8919],
        [2.8921, 2.8921, 2.8921],
        [2.8921, 2.8815, 2.8916],
        [2.8921, 2.6728, 2.8140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.2960818111896515 
model_pd.l_d.mean(): -25.203136444091797 
model_pd.lagr.mean(): -24.907054901123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0553], device='cuda:0')), ('power', tensor([-25.2584], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.2960818111896515
epoch£º453	 i:1 	 global-step:9061	 l-p:0.1594599336385727
epoch£º453	 i:2 	 global-step:9062	 l-p:-0.6464994549751282
epoch£º453	 i:3 	 global-step:9063	 l-p:0.21051815152168274
epoch£º453	 i:4 	 global-step:9064	 l-p:0.13002729415893555
epoch£º453	 i:5 	 global-step:9065	 l-p:0.12741312384605408
epoch£º453	 i:6 	 global-step:9066	 l-p:0.1357637643814087
epoch£º453	 i:7 	 global-step:9067	 l-p:0.151401549577713
epoch£º453	 i:8 	 global-step:9068	 l-p:0.21223968267440796
epoch£º453	 i:9 	 global-step:9069	 l-p:0.14012853801250458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0837, 2.5695, 2.7137],
        [3.0837, 1.9659, 1.4003],
        [3.0837, 3.0697, 3.0829],
        [3.0837, 3.0208, 3.0745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.12693315744400024 
model_pd.l_d.mean(): -24.960098266601562 
model_pd.lagr.mean(): -24.83316421508789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0998], device='cuda:0')), ('power', tensor([-24.8603], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.12693315744400024
epoch£º454	 i:1 	 global-step:9081	 l-p:0.16196510195732117
epoch£º454	 i:2 	 global-step:9082	 l-p:0.13756689429283142
epoch£º454	 i:3 	 global-step:9083	 l-p:0.14009594917297363
epoch£º454	 i:4 	 global-step:9084	 l-p:0.15679526329040527
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12208857387304306
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11950235068798065
epoch£º454	 i:7 	 global-step:9087	 l-p:0.0999995619058609
epoch£º454	 i:8 	 global-step:9088	 l-p:0.0803847685456276
epoch£º454	 i:9 	 global-step:9089	 l-p:0.15651436150074005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8208, 1.9413, 1.8214],
        [2.8208, 2.8183, 2.8208],
        [2.8208, 2.8200, 2.8208],
        [2.8208, 2.8208, 2.8208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.17480504512786865 
model_pd.l_d.mean(): -25.033798217773438 
model_pd.lagr.mean(): -24.858993530273438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1513], device='cuda:0')), ('power', tensor([-25.1851], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.17480504512786865
epoch£º455	 i:1 	 global-step:9101	 l-p:0.13362383842468262
epoch£º455	 i:2 	 global-step:9102	 l-p:0.2535993158817291
epoch£º455	 i:3 	 global-step:9103	 l-p:0.14062748849391937
epoch£º455	 i:4 	 global-step:9104	 l-p:0.1330287754535675
epoch£º455	 i:5 	 global-step:9105	 l-p:-0.003133859485387802
epoch£º455	 i:6 	 global-step:9106	 l-p:0.18219593167304993
epoch£º455	 i:7 	 global-step:9107	 l-p:-0.0745435282588005
epoch£º455	 i:8 	 global-step:9108	 l-p:0.1786646544933319
epoch£º455	 i:9 	 global-step:9109	 l-p:0.12912806868553162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8991, 2.8991, 2.8991],
        [2.8991, 2.4845, 2.6547],
        [2.8991, 2.8965, 2.8990],
        [2.8991, 2.8852, 2.8983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.06019217520952225 
model_pd.l_d.mean(): -25.034433364868164 
model_pd.lagr.mean(): -24.974241256713867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0314], device='cuda:0')), ('power', tensor([-25.0658], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.06019217520952225
epoch£º456	 i:1 	 global-step:9121	 l-p:0.14424774050712585
epoch£º456	 i:2 	 global-step:9122	 l-p:0.12878158688545227
epoch£º456	 i:3 	 global-step:9123	 l-p:0.1726064234972
epoch£º456	 i:4 	 global-step:9124	 l-p:0.1252964586019516
epoch£º456	 i:5 	 global-step:9125	 l-p:0.14550600945949554
epoch£º456	 i:6 	 global-step:9126	 l-p:0.18037055432796478
epoch£º456	 i:7 	 global-step:9127	 l-p:0.15235589444637299
epoch£º456	 i:8 	 global-step:9128	 l-p:0.1573295146226883
epoch£º456	 i:9 	 global-step:9129	 l-p:0.20634868741035461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9928, 1.8942, 1.3234],
        [2.9928, 2.5332, 2.6954],
        [2.9928, 1.8845, 1.4060],
        [2.9928, 2.9928, 2.9928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.15800869464874268 
model_pd.l_d.mean(): -25.23459815979004 
model_pd.lagr.mean(): -25.076589584350586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0076], device='cuda:0')), ('power', tensor([-25.2270], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.15800869464874268
epoch£º457	 i:1 	 global-step:9141	 l-p:0.12594062089920044
epoch£º457	 i:2 	 global-step:9142	 l-p:0.1349635273218155
epoch£º457	 i:3 	 global-step:9143	 l-p:0.13897931575775146
epoch£º457	 i:4 	 global-step:9144	 l-p:0.2902272045612335
epoch£º457	 i:5 	 global-step:9145	 l-p:0.1568281650543213
epoch£º457	 i:6 	 global-step:9146	 l-p:0.0658654049038887
epoch£º457	 i:7 	 global-step:9147	 l-p:0.12139802426099777
epoch£º457	 i:8 	 global-step:9148	 l-p:-0.0686357319355011
epoch£º457	 i:9 	 global-step:9149	 l-p:0.14930564165115356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8990, 2.8987, 2.8990],
        [2.8990, 2.6508, 2.8022],
        [2.8990, 2.8990, 2.8990],
        [2.8990, 2.8986, 2.8990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.15768244862556458 
model_pd.l_d.mean(): -25.18634033203125 
model_pd.lagr.mean(): -25.028657913208008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0295], device='cuda:0')), ('power', tensor([-25.2159], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.15768244862556458
epoch£º458	 i:1 	 global-step:9161	 l-p:0.12789484858512878
epoch£º458	 i:2 	 global-step:9162	 l-p:0.14695531129837036
epoch£º458	 i:3 	 global-step:9163	 l-p:0.20127597451210022
epoch£º458	 i:4 	 global-step:9164	 l-p:0.12109672278165817
epoch£º458	 i:5 	 global-step:9165	 l-p:0.13747793436050415
epoch£º458	 i:6 	 global-step:9166	 l-p:0.015844030305743217
epoch£º458	 i:7 	 global-step:9167	 l-p:0.15665364265441895
epoch£º458	 i:8 	 global-step:9168	 l-p:0.13943012058734894
epoch£º458	 i:9 	 global-step:9169	 l-p:0.20157168805599213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9748, 2.9748, 2.9748],
        [2.9748, 1.8515, 1.3067],
        [2.9748, 2.1908, 1.5637],
        [2.9748, 2.9746, 2.9748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.1469266712665558 
model_pd.l_d.mean(): -24.980737686157227 
model_pd.lagr.mean(): -24.833810806274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0168], device='cuda:0')), ('power', tensor([-24.9639], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.1469266712665558
epoch£º459	 i:1 	 global-step:9181	 l-p:0.1475607305765152
epoch£º459	 i:2 	 global-step:9182	 l-p:0.07313288748264313
epoch£º459	 i:3 	 global-step:9183	 l-p:0.14915940165519714
epoch£º459	 i:4 	 global-step:9184	 l-p:0.13257426023483276
epoch£º459	 i:5 	 global-step:9185	 l-p:0.14226087927818298
epoch£º459	 i:6 	 global-step:9186	 l-p:0.12846173346042633
epoch£º459	 i:7 	 global-step:9187	 l-p:0.09950798004865646
epoch£º459	 i:8 	 global-step:9188	 l-p:0.13323964178562164
epoch£º459	 i:9 	 global-step:9189	 l-p:0.1299864798784256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1264, 3.0781, 3.1205],
        [3.1264, 2.2706, 2.1546],
        [3.1264, 2.3177, 1.6750],
        [3.1264, 2.0139, 1.4977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.09814514219760895 
model_pd.l_d.mean(): -24.44288444519043 
model_pd.lagr.mean(): -24.34473991394043 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0895], device='cuda:0')), ('power', tensor([-24.5324], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.09814514219760895
epoch£º460	 i:1 	 global-step:9201	 l-p:0.12196742743253708
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12409514933824539
epoch£º460	 i:3 	 global-step:9203	 l-p:0.2203742414712906
epoch£º460	 i:4 	 global-step:9204	 l-p:0.09545014053583145
epoch£º460	 i:5 	 global-step:9205	 l-p:0.15149392187595367
epoch£º460	 i:6 	 global-step:9206	 l-p:0.1536223590373993
epoch£º460	 i:7 	 global-step:9207	 l-p:0.15192456543445587
epoch£º460	 i:8 	 global-step:9208	 l-p:0.11339426040649414
epoch£º460	 i:9 	 global-step:9209	 l-p:0.15151964128017426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8416, 2.8316, 2.8411],
        [2.8416, 1.9897, 1.9084],
        [2.8416, 2.5779, 2.7345],
        [2.8416, 1.7188, 1.2079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.19585371017456055 
model_pd.l_d.mean(): -24.40313720703125 
model_pd.lagr.mean(): -24.20728302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1946], device='cuda:0')), ('power', tensor([-24.5978], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.19585371017456055
epoch£º461	 i:1 	 global-step:9221	 l-p:1.0016082525253296
epoch£º461	 i:2 	 global-step:9222	 l-p:0.06253116577863693
epoch£º461	 i:3 	 global-step:9223	 l-p:0.14618359506130219
epoch£º461	 i:4 	 global-step:9224	 l-p:0.1363336741924286
epoch£º461	 i:5 	 global-step:9225	 l-p:0.17184396088123322
epoch£º461	 i:6 	 global-step:9226	 l-p:0.14723896980285645
epoch£º461	 i:7 	 global-step:9227	 l-p:0.14839652180671692
epoch£º461	 i:8 	 global-step:9228	 l-p:-0.021585101261734962
epoch£º461	 i:9 	 global-step:9229	 l-p:0.15142211318016052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0510, 2.0121, 1.6412],
        [3.0510, 2.1014, 1.8702],
        [3.0510, 2.8206, 2.9656],
        [3.0510, 2.8240, 2.9679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.27565324306488037 
model_pd.l_d.mean(): -25.065771102905273 
model_pd.lagr.mean(): -24.790117263793945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0137], device='cuda:0')), ('power', tensor([-25.0795], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.27565324306488037
epoch£º462	 i:1 	 global-step:9241	 l-p:0.12839016318321228
epoch£º462	 i:2 	 global-step:9242	 l-p:0.13241085410118103
epoch£º462	 i:3 	 global-step:9243	 l-p:0.12748472392559052
epoch£º462	 i:4 	 global-step:9244	 l-p:0.14597278833389282
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11384103447198868
epoch£º462	 i:6 	 global-step:9246	 l-p:0.1367097795009613
epoch£º462	 i:7 	 global-step:9247	 l-p:0.1737065464258194
epoch£º462	 i:8 	 global-step:9248	 l-p:-0.1781754046678543
epoch£º462	 i:9 	 global-step:9249	 l-p:0.1533345729112625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8889, 2.8812, 2.8886],
        [2.8889, 2.8328, 2.8814],
        [2.8889, 2.8389, 2.8828],
        [2.8889, 2.8882, 2.8889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.7816988825798035 
model_pd.l_d.mean(): -24.280378341674805 
model_pd.lagr.mean(): -23.498680114746094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2461], device='cuda:0')), ('power', tensor([-24.5264], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.7816988825798035
epoch£º463	 i:1 	 global-step:9261	 l-p:0.08110776543617249
epoch£º463	 i:2 	 global-step:9262	 l-p:0.1861030012369156
epoch£º463	 i:3 	 global-step:9263	 l-p:0.08993647992610931
epoch£º463	 i:4 	 global-step:9264	 l-p:0.13778994977474213
epoch£º463	 i:5 	 global-step:9265	 l-p:0.12382188439369202
epoch£º463	 i:6 	 global-step:9266	 l-p:0.13235951960086823
epoch£º463	 i:7 	 global-step:9267	 l-p:0.14454925060272217
epoch£º463	 i:8 	 global-step:9268	 l-p:0.11063161492347717
epoch£º463	 i:9 	 global-step:9269	 l-p:0.1287907212972641
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8757, 2.8575, 2.8745],
        [2.8757, 1.7471, 1.2410],
        [2.8757, 1.7724, 1.2251],
        [2.8757, 2.8757, 2.8757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.0634736493229866 
model_pd.l_d.mean(): -25.135311126708984 
model_pd.lagr.mean(): -25.07183837890625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0519], device='cuda:0')), ('power', tensor([-25.1872], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.0634736493229866
epoch£º464	 i:1 	 global-step:9281	 l-p:0.04145491123199463
epoch£º464	 i:2 	 global-step:9282	 l-p:0.12047358602285385
epoch£º464	 i:3 	 global-step:9283	 l-p:0.1563456654548645
epoch£º464	 i:4 	 global-step:9284	 l-p:0.1260942965745926
epoch£º464	 i:5 	 global-step:9285	 l-p:0.13714073598384857
epoch£º464	 i:6 	 global-step:9286	 l-p:0.08376636356115341
epoch£º464	 i:7 	 global-step:9287	 l-p:0.18237736821174622
epoch£º464	 i:8 	 global-step:9288	 l-p:0.48434188961982727
epoch£º464	 i:9 	 global-step:9289	 l-p:0.1521947681903839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9182, 2.9182, 2.9182],
        [2.9182, 2.9110, 2.9179],
        [2.9182, 2.8094, 2.8953],
        [2.9182, 2.1598, 2.1701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.1458103209733963 
model_pd.l_d.mean(): -24.972257614135742 
model_pd.lagr.mean(): -24.826446533203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1281], device='cuda:0')), ('power', tensor([-25.1004], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.1458103209733963
epoch£º465	 i:1 	 global-step:9301	 l-p:0.24972619116306305
epoch£º465	 i:2 	 global-step:9302	 l-p:0.13032953441143036
epoch£º465	 i:3 	 global-step:9303	 l-p:0.14584486186504364
epoch£º465	 i:4 	 global-step:9304	 l-p:0.12429396063089371
epoch£º465	 i:5 	 global-step:9305	 l-p:0.14819279313087463
epoch£º465	 i:6 	 global-step:9306	 l-p:-0.051411207765340805
epoch£º465	 i:7 	 global-step:9307	 l-p:0.21350890398025513
epoch£º465	 i:8 	 global-step:9308	 l-p:0.17270679771900177
epoch£º465	 i:9 	 global-step:9309	 l-p:0.11424516141414642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8705, 2.2564, 2.3719],
        [2.8705, 2.1076, 2.1185],
        [2.8705, 2.7895, 2.8567],
        [2.8705, 1.7630, 1.3264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.1575048863887787 
model_pd.l_d.mean(): -24.621885299682617 
model_pd.lagr.mean(): -24.464380264282227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2001], device='cuda:0')), ('power', tensor([-24.8220], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.1575048863887787
epoch£º466	 i:1 	 global-step:9321	 l-p:0.10198816657066345
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13628001511096954
epoch£º466	 i:3 	 global-step:9323	 l-p:0.1300107091665268
epoch£º466	 i:4 	 global-step:9324	 l-p:0.14144137501716614
epoch£º466	 i:5 	 global-step:9325	 l-p:0.16291621327400208
epoch£º466	 i:6 	 global-step:9326	 l-p:0.17772798240184784
epoch£º466	 i:7 	 global-step:9327	 l-p:0.15317244827747345
epoch£º466	 i:8 	 global-step:9328	 l-p:0.1687256544828415
epoch£º466	 i:9 	 global-step:9329	 l-p:0.09797707945108414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0281, 3.0278, 3.0281],
        [3.0281, 1.8972, 1.3344],
        [3.0281, 2.9037, 2.9992],
        [3.0281, 2.3369, 2.3968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12384044378995895 
model_pd.l_d.mean(): -25.01070785522461 
model_pd.lagr.mean(): -24.88686752319336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0149], device='cuda:0')), ('power', tensor([-25.0256], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12384044378995895
epoch£º467	 i:1 	 global-step:9341	 l-p:0.20434948801994324
epoch£º467	 i:2 	 global-step:9342	 l-p:0.13710804283618927
epoch£º467	 i:3 	 global-step:9343	 l-p:0.08242359757423401
epoch£º467	 i:4 	 global-step:9344	 l-p:0.1389908790588379
epoch£º467	 i:5 	 global-step:9345	 l-p:0.16178502142429352
epoch£º467	 i:6 	 global-step:9346	 l-p:0.18420283496379852
epoch£º467	 i:7 	 global-step:9347	 l-p:0.1942991018295288
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11865464597940445
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11292560398578644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0460, 2.8546, 2.9848],
        [3.0460, 1.9153, 1.3465],
        [3.0460, 2.7145, 2.8834],
        [3.0460, 2.2008, 1.5687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.1724633425474167 
model_pd.l_d.mean(): -25.122591018676758 
model_pd.lagr.mean(): -24.95012855529785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0168], device='cuda:0')), ('power', tensor([-25.1394], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.1724633425474167
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10571031272411346
epoch£º468	 i:2 	 global-step:9362	 l-p:0.12926001846790314
epoch£º468	 i:3 	 global-step:9363	 l-p:0.1654835045337677
epoch£º468	 i:4 	 global-step:9364	 l-p:0.14923788607120514
epoch£º468	 i:5 	 global-step:9365	 l-p:0.14859387278556824
epoch£º468	 i:6 	 global-step:9366	 l-p:0.14217112958431244
epoch£º468	 i:7 	 global-step:9367	 l-p:0.12640275061130524
epoch£º468	 i:8 	 global-step:9368	 l-p:0.061810173094272614
epoch£º468	 i:9 	 global-step:9369	 l-p:0.12575870752334595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8936, 2.6418, 2.7954],
        [2.8936, 2.8936, 2.8936],
        [2.8936, 2.8791, 2.8928],
        [2.8936, 2.8936, 2.8936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.10146883130073547 
model_pd.l_d.mean(): -25.01128578186035 
model_pd.lagr.mean(): -24.90981674194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0269], device='cuda:0')), ('power', tensor([-24.9844], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.10146883130073547
epoch£º469	 i:1 	 global-step:9381	 l-p:0.1650456190109253
epoch£º469	 i:2 	 global-step:9382	 l-p:0.1705624908208847
epoch£º469	 i:3 	 global-step:9383	 l-p:0.1745043396949768
epoch£º469	 i:4 	 global-step:9384	 l-p:0.1289645880460739
epoch£º469	 i:5 	 global-step:9385	 l-p:0.6027611494064331
epoch£º469	 i:6 	 global-step:9386	 l-p:-0.061736173927783966
epoch£º469	 i:7 	 global-step:9387	 l-p:0.13820017874240875
epoch£º469	 i:8 	 global-step:9388	 l-p:0.15542066097259521
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11215276271104813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1021, 3.1009, 3.1021],
        [3.1021, 2.4763, 2.5766],
        [3.1021, 2.7836, 2.9504],
        [3.1021, 3.1013, 3.1021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.11076890677213669 
model_pd.l_d.mean(): -25.035831451416016 
model_pd.lagr.mean(): -24.92506217956543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0947], device='cuda:0')), ('power', tensor([-24.9411], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.11076890677213669
epoch£º470	 i:1 	 global-step:9401	 l-p:0.12509645521640778
epoch£º470	 i:2 	 global-step:9402	 l-p:0.1513662487268448
epoch£º470	 i:3 	 global-step:9403	 l-p:0.1478448510169983
epoch£º470	 i:4 	 global-step:9404	 l-p:0.14342017471790314
epoch£º470	 i:5 	 global-step:9405	 l-p:0.1525125503540039
epoch£º470	 i:6 	 global-step:9406	 l-p:0.15831376612186432
epoch£º470	 i:7 	 global-step:9407	 l-p:0.14005689322948456
epoch£º470	 i:8 	 global-step:9408	 l-p:-0.17104992270469666
epoch£º470	 i:9 	 global-step:9409	 l-p:0.20538878440856934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9143, 2.9143, 2.9143],
        [2.9143, 2.9143, 2.9143],
        [2.9143, 2.9143, 2.9143],
        [2.9143, 2.9143, 2.9143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.18438835442066193 
model_pd.l_d.mean(): -24.93807601928711 
model_pd.lagr.mean(): -24.753686904907227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1110], device='cuda:0')), ('power', tensor([-25.0491], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.18438835442066193
epoch£º471	 i:1 	 global-step:9421	 l-p:0.1472700834274292
epoch£º471	 i:2 	 global-step:9422	 l-p:-0.33659255504608154
epoch£º471	 i:3 	 global-step:9423	 l-p:0.14154715836048126
epoch£º471	 i:4 	 global-step:9424	 l-p:0.20020131766796112
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13521385192871094
epoch£º471	 i:6 	 global-step:9426	 l-p:0.1309787929058075
epoch£º471	 i:7 	 global-step:9427	 l-p:0.13525591790676117
epoch£º471	 i:8 	 global-step:9428	 l-p:0.23019050061702728
epoch£º471	 i:9 	 global-step:9429	 l-p:0.14826416969299316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9311, 1.9547, 1.7127],
        [2.9311, 2.8214, 2.9080],
        [2.9311, 1.8964, 1.3111],
        [2.9311, 2.9311, 2.9311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.13902002573013306 
model_pd.l_d.mean(): -25.15822410583496 
model_pd.lagr.mean(): -25.019203186035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0307], device='cuda:0')), ('power', tensor([-25.1889], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.13902002573013306
epoch£º472	 i:1 	 global-step:9441	 l-p:1.0881561040878296
epoch£º472	 i:2 	 global-step:9442	 l-p:0.14647479355335236
epoch£º472	 i:3 	 global-step:9443	 l-p:0.16004616022109985
epoch£º472	 i:4 	 global-step:9444	 l-p:0.16121980547904968
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12929068505764008
epoch£º472	 i:6 	 global-step:9446	 l-p:0.3075008690357208
epoch£º472	 i:7 	 global-step:9447	 l-p:-0.0385802686214447
epoch£º472	 i:8 	 global-step:9448	 l-p:0.15667203068733215
epoch£º472	 i:9 	 global-step:9449	 l-p:0.14612381160259247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0142, 2.2185, 2.1927],
        [3.0142, 3.0141, 3.0142],
        [3.0142, 3.0140, 3.0142],
        [3.0142, 2.8992, 2.9891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.1609676480293274 
model_pd.l_d.mean(): -24.84592628479004 
model_pd.lagr.mean(): -24.684959411621094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2069], device='cuda:0')), ('power', tensor([-25.0528], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.1609676480293274
epoch£º473	 i:1 	 global-step:9461	 l-p:0.11987532675266266
epoch£º473	 i:2 	 global-step:9462	 l-p:0.22959037125110626
epoch£º473	 i:3 	 global-step:9463	 l-p:0.13272887468338013
epoch£º473	 i:4 	 global-step:9464	 l-p:0.11861073225736618
epoch£º473	 i:5 	 global-step:9465	 l-p:0.15814273059368134
epoch£º473	 i:6 	 global-step:9466	 l-p:0.12059743702411652
epoch£º473	 i:7 	 global-step:9467	 l-p:0.11964687705039978
epoch£º473	 i:8 	 global-step:9468	 l-p:0.150544673204422
epoch£º473	 i:9 	 global-step:9469	 l-p:0.18686465919017792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9174, 2.9075, 2.9169],
        [2.9174, 2.9173, 2.9174],
        [2.9174, 2.9174, 2.9174],
        [2.9174, 2.5547, 2.7286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.12615293264389038 
model_pd.l_d.mean(): -25.026159286499023 
model_pd.lagr.mean(): -24.900007247924805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1137], device='cuda:0')), ('power', tensor([-25.1399], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.12615293264389038
epoch£º474	 i:1 	 global-step:9481	 l-p:0.1403559297323227
epoch£º474	 i:2 	 global-step:9482	 l-p:0.1528942734003067
epoch£º474	 i:3 	 global-step:9483	 l-p:0.17323461174964905
epoch£º474	 i:4 	 global-step:9484	 l-p:0.16553077101707458
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12042346596717834
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12729187309741974
epoch£º474	 i:7 	 global-step:9487	 l-p:0.16003760695457458
epoch£º474	 i:8 	 global-step:9488	 l-p:0.22981341183185577
epoch£º474	 i:9 	 global-step:9489	 l-p:0.17797237634658813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9657, 2.4958, 2.6620],
        [2.9657, 2.3252, 2.4258],
        [2.9657, 2.9657, 2.9657],
        [2.9657, 2.9657, 2.9657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.13820742070674896 
model_pd.l_d.mean(): -24.96905517578125 
model_pd.lagr.mean(): -24.830848693847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1325], device='cuda:0')), ('power', tensor([-25.1016], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.13820742070674896
epoch£º475	 i:1 	 global-step:9501	 l-p:0.1456083208322525
epoch£º475	 i:2 	 global-step:9502	 l-p:0.1391821652650833
epoch£º475	 i:3 	 global-step:9503	 l-p:0.5639635920524597
epoch£º475	 i:4 	 global-step:9504	 l-p:-0.1286928951740265
epoch£º475	 i:5 	 global-step:9505	 l-p:0.4316173195838928
epoch£º475	 i:6 	 global-step:9506	 l-p:0.15068580210208893
epoch£º475	 i:7 	 global-step:9507	 l-p:0.14326630532741547
epoch£º475	 i:8 	 global-step:9508	 l-p:0.16628620028495789
epoch£º475	 i:9 	 global-step:9509	 l-p:0.06391652673482895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0243, 1.9975, 1.3924],
        [3.0243, 3.0242, 3.0243],
        [3.0243, 1.9592, 1.3620],
        [3.0243, 2.7293, 2.8934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12337344884872437 
model_pd.l_d.mean(): -25.045215606689453 
model_pd.lagr.mean(): -24.921842575073242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0190], device='cuda:0')), ('power', tensor([-25.0262], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12337344884872437
epoch£º476	 i:1 	 global-step:9521	 l-p:0.13507947325706482
epoch£º476	 i:2 	 global-step:9522	 l-p:0.17496247589588165
epoch£º476	 i:3 	 global-step:9523	 l-p:0.14043055474758148
epoch£º476	 i:4 	 global-step:9524	 l-p:0.15526580810546875
epoch£º476	 i:5 	 global-step:9525	 l-p:0.13694334030151367
epoch£º476	 i:6 	 global-step:9526	 l-p:1.6262470483779907
epoch£º476	 i:7 	 global-step:9527	 l-p:0.13401389122009277
epoch£º476	 i:8 	 global-step:9528	 l-p:0.09937272220849991
epoch£º476	 i:9 	 global-step:9529	 l-p:0.1444614827632904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8964, 2.8920, 2.8963],
        [2.8964, 1.7575, 1.2320],
        [2.8964, 2.6448, 2.7988],
        [2.8964, 2.1012, 2.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.1532505750656128 
model_pd.l_d.mean(): -24.6688175201416 
model_pd.lagr.mean(): -24.515567779541016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1290], device='cuda:0')), ('power', tensor([-24.7979], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.1532505750656128
epoch£º477	 i:1 	 global-step:9541	 l-p:0.15134790539741516
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11835402250289917
epoch£º477	 i:3 	 global-step:9543	 l-p:0.14787478744983673
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11836475133895874
epoch£º477	 i:5 	 global-step:9545	 l-p:0.10412642359733582
epoch£º477	 i:6 	 global-step:9546	 l-p:0.19994667172431946
epoch£º477	 i:7 	 global-step:9547	 l-p:0.46150103211402893
epoch£º477	 i:8 	 global-step:9548	 l-p:0.20481917262077332
epoch£º477	 i:9 	 global-step:9549	 l-p:0.18758079409599304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8902, 2.8902, 2.8902],
        [2.8902, 1.9160, 1.3242],
        [2.8902, 2.8902, 2.8902],
        [2.8902, 1.8733, 1.5842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.06080502271652222 
model_pd.l_d.mean(): -24.532691955566406 
model_pd.lagr.mean(): -24.471887588500977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2349], device='cuda:0')), ('power', tensor([-24.7676], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.06080502271652222
epoch£º478	 i:1 	 global-step:9561	 l-p:0.14669884741306305
epoch£º478	 i:2 	 global-step:9562	 l-p:0.17395007610321045
epoch£º478	 i:3 	 global-step:9563	 l-p:0.1337774246931076
epoch£º478	 i:4 	 global-step:9564	 l-p:-0.33053281903266907
epoch£º478	 i:5 	 global-step:9565	 l-p:0.11821351945400238
epoch£º478	 i:6 	 global-step:9566	 l-p:0.1619090586900711
epoch£º478	 i:7 	 global-step:9567	 l-p:0.12982255220413208
epoch£º478	 i:8 	 global-step:9568	 l-p:0.10815408825874329
epoch£º478	 i:9 	 global-step:9569	 l-p:0.14746572077274323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0081, 2.0274, 1.4168],
        [3.0081, 2.9269, 2.9943],
        [3.0081, 1.9990, 1.7044],
        [3.0081, 2.5969, 2.7700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.1625358909368515 
model_pd.l_d.mean(): -24.82451057434082 
model_pd.lagr.mean(): -24.66197395324707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0671], device='cuda:0')), ('power', tensor([-24.8916], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.1625358909368515
epoch£º479	 i:1 	 global-step:9581	 l-p:0.21274420619010925
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1397944837808609
epoch£º479	 i:3 	 global-step:9583	 l-p:0.24551823735237122
epoch£º479	 i:4 	 global-step:9584	 l-p:0.14224538207054138
epoch£º479	 i:5 	 global-step:9585	 l-p:0.1425705999135971
epoch£º479	 i:6 	 global-step:9586	 l-p:0.09581506252288818
epoch£º479	 i:7 	 global-step:9587	 l-p:0.1544492244720459
epoch£º479	 i:8 	 global-step:9588	 l-p:0.17317132651805878
epoch£º479	 i:9 	 global-step:9589	 l-p:0.20728309452533722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9583, 2.9581, 2.9583],
        [2.9583, 2.3354, 2.4479],
        [2.9583, 2.9581, 2.9583],
        [2.9583, 2.6120, 2.7848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.3608940839767456 
model_pd.l_d.mean(): -24.690093994140625 
model_pd.lagr.mean(): -24.329200744628906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0859], device='cuda:0')), ('power', tensor([-24.7760], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.3608940839767456
epoch£º480	 i:1 	 global-step:9601	 l-p:0.12775282561779022
epoch£º480	 i:2 	 global-step:9602	 l-p:0.14948341250419617
epoch£º480	 i:3 	 global-step:9603	 l-p:-0.18011172115802765
epoch£º480	 i:4 	 global-step:9604	 l-p:0.1518147736787796
epoch£º480	 i:5 	 global-step:9605	 l-p:0.1398940235376358
epoch£º480	 i:6 	 global-step:9606	 l-p:0.12968413531780243
epoch£º480	 i:7 	 global-step:9607	 l-p:0.13620512187480927
epoch£º480	 i:8 	 global-step:9608	 l-p:0.19314004480838776
epoch£º480	 i:9 	 global-step:9609	 l-p:0.24342799186706543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9731, 2.7900, 2.9173],
        [2.9731, 2.9731, 2.9731],
        [2.9731, 2.9528, 2.9717],
        [2.9731, 2.9730, 2.9731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.12146686017513275 
model_pd.l_d.mean(): -25.254335403442383 
model_pd.lagr.mean(): -25.13286781311035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0167], device='cuda:0')), ('power', tensor([-25.2710], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.12146686017513275
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13208003342151642
epoch£º481	 i:2 	 global-step:9622	 l-p:0.14037761092185974
epoch£º481	 i:3 	 global-step:9623	 l-p:0.1515824794769287
epoch£º481	 i:4 	 global-step:9624	 l-p:0.19442464411258698
epoch£º481	 i:5 	 global-step:9625	 l-p:0.14643873274326324
epoch£º481	 i:6 	 global-step:9626	 l-p:0.13926465809345245
epoch£º481	 i:7 	 global-step:9627	 l-p:0.14490889012813568
epoch£º481	 i:8 	 global-step:9628	 l-p:0.049662161618471146
epoch£º481	 i:9 	 global-step:9629	 l-p:0.15721556544303894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8693, 2.7551, 2.8448],
        [2.8693, 2.6007, 2.7602],
        [2.8693, 1.9121, 1.7152],
        [2.8693, 1.7275, 1.2089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): -0.058740291744470596 
model_pd.l_d.mean(): -25.262590408325195 
model_pd.lagr.mean(): -25.321331024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0129], device='cuda:0')), ('power', tensor([-25.2755], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:-0.058740291744470596
epoch£º482	 i:1 	 global-step:9641	 l-p:0.10304854065179825
epoch£º482	 i:2 	 global-step:9642	 l-p:0.13132454454898834
epoch£º482	 i:3 	 global-step:9643	 l-p:0.15010066330432892
epoch£º482	 i:4 	 global-step:9644	 l-p:0.1539035439491272
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11160153895616531
epoch£º482	 i:6 	 global-step:9646	 l-p:0.15657053887844086
epoch£º482	 i:7 	 global-step:9647	 l-p:0.1193835586309433
epoch£º482	 i:8 	 global-step:9648	 l-p:0.11350343376398087
epoch£º482	 i:9 	 global-step:9649	 l-p:0.118355393409729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1335, 3.1335, 3.1335],
        [3.1335, 1.9845, 1.4122],
        [3.1335, 3.1315, 3.1335],
        [3.1335, 3.1335, 3.1335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.11930997669696808 
model_pd.l_d.mean(): -24.37818145751953 
model_pd.lagr.mean(): -24.25887107849121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0572], device='cuda:0')), ('power', tensor([-24.4354], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.11930997669696808
epoch£º483	 i:1 	 global-step:9661	 l-p:0.12796616554260254
epoch£º483	 i:2 	 global-step:9662	 l-p:0.14815950393676758
epoch£º483	 i:3 	 global-step:9663	 l-p:0.12440189719200134
epoch£º483	 i:4 	 global-step:9664	 l-p:0.15034247934818268
epoch£º483	 i:5 	 global-step:9665	 l-p:0.09395985305309296
epoch£º483	 i:6 	 global-step:9666	 l-p:0.18038055300712585
epoch£º483	 i:7 	 global-step:9667	 l-p:0.14943493902683258
epoch£º483	 i:8 	 global-step:9668	 l-p:0.4946008324623108
epoch£º483	 i:9 	 global-step:9669	 l-p:0.4957331418991089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[2.7080, 1.6237, 1.0956],
        [2.7080, 1.7614, 1.5936],
        [2.7080, 1.6287, 1.2756],
        [2.7080, 1.6351, 1.1022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.08561315387487411 
model_pd.l_d.mean(): -24.82489013671875 
model_pd.lagr.mean(): -24.739276885986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3006], device='cuda:0')), ('power', tensor([-25.1255], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.08561315387487411
epoch£º484	 i:1 	 global-step:9681	 l-p:0.12855705618858337
epoch£º484	 i:2 	 global-step:9682	 l-p:0.11974168568849564
epoch£º484	 i:3 	 global-step:9683	 l-p:0.1508740335702896
epoch£º484	 i:4 	 global-step:9684	 l-p:0.1082543134689331
epoch£º484	 i:5 	 global-step:9685	 l-p:-0.5415034890174866
epoch£º484	 i:6 	 global-step:9686	 l-p:0.13478238880634308
epoch£º484	 i:7 	 global-step:9687	 l-p:0.13132250308990479
epoch£º484	 i:8 	 global-step:9688	 l-p:0.13716451823711395
epoch£º484	 i:9 	 global-step:9689	 l-p:0.1238660216331482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0464, 2.8802, 2.9992],
        [3.0464, 2.5536, 2.7150],
        [3.0464, 3.0464, 3.0463],
        [3.0464, 3.0464, 3.0464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.12933602929115295 
model_pd.l_d.mean(): -25.284852981567383 
model_pd.lagr.mean(): -25.155517578125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1423], device='cuda:0')), ('power', tensor([-25.1426], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.12933602929115295
epoch£º485	 i:1 	 global-step:9701	 l-p:0.14277368783950806
epoch£º485	 i:2 	 global-step:9702	 l-p:0.11334666609764099
epoch£º485	 i:3 	 global-step:9703	 l-p:0.0877336636185646
epoch£º485	 i:4 	 global-step:9704	 l-p:0.17508918046951294
epoch£º485	 i:5 	 global-step:9705	 l-p:0.1223405972123146
epoch£º485	 i:6 	 global-step:9706	 l-p:0.14327116310596466
epoch£º485	 i:7 	 global-step:9707	 l-p:0.15784195065498352
epoch£º485	 i:8 	 global-step:9708	 l-p:4.626398086547852
epoch£º485	 i:9 	 global-step:9709	 l-p:-0.32231414318084717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9501, 1.8459, 1.4278],
        [2.9501, 2.2115, 2.2498],
        [2.9501, 1.8373, 1.2671],
        [2.9501, 2.6293, 2.7997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.11237575113773346 
model_pd.l_d.mean(): -24.948701858520508 
model_pd.lagr.mean(): -24.836326599121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1355], device='cuda:0')), ('power', tensor([-25.0842], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.11237575113773346
epoch£º486	 i:1 	 global-step:9721	 l-p:0.17253974080085754
epoch£º486	 i:2 	 global-step:9722	 l-p:0.12322206050157547
epoch£º486	 i:3 	 global-step:9723	 l-p:0.15265941619873047
epoch£º486	 i:4 	 global-step:9724	 l-p:0.1618485003709793
epoch£º486	 i:5 	 global-step:9725	 l-p:0.11398201435804367
epoch£º486	 i:6 	 global-step:9726	 l-p:0.14324849843978882
epoch£º486	 i:7 	 global-step:9727	 l-p:0.17132973670959473
epoch£º486	 i:8 	 global-step:9728	 l-p:0.20203837752342224
epoch£º486	 i:9 	 global-step:9729	 l-p:0.16929897665977478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0464, 2.3107, 2.3456],
        [3.0464, 3.0457, 3.0464],
        [3.0464, 3.0464, 3.0464],
        [3.0464, 1.8948, 1.3626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.1389121264219284 
model_pd.l_d.mean(): -24.919647216796875 
model_pd.lagr.mean(): -24.78073501586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0231], device='cuda:0')), ('power', tensor([-24.9427], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.1389121264219284
epoch£º487	 i:1 	 global-step:9741	 l-p:0.129324272274971
epoch£º487	 i:2 	 global-step:9742	 l-p:0.13796700537204742
epoch£º487	 i:3 	 global-step:9743	 l-p:0.09108926355838776
epoch£º487	 i:4 	 global-step:9744	 l-p:0.14289648830890656
epoch£º487	 i:5 	 global-step:9745	 l-p:0.18120570480823517
epoch£º487	 i:6 	 global-step:9746	 l-p:0.14068835973739624
epoch£º487	 i:7 	 global-step:9747	 l-p:0.14673848450183868
epoch£º487	 i:8 	 global-step:9748	 l-p:0.1383952647447586
epoch£º487	 i:9 	 global-step:9749	 l-p:0.1401120126247406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8586, 2.8531, 2.8584],
        [2.8586, 1.7990, 1.4645],
        [2.8586, 2.7471, 2.8352],
        [2.8586, 1.8349, 1.2564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.1632223129272461 
model_pd.l_d.mean(): -25.096628189086914 
model_pd.lagr.mean(): -24.933406829833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1210], device='cuda:0')), ('power', tensor([-25.2177], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.1632223129272461
epoch£º488	 i:1 	 global-step:9761	 l-p:0.14250922203063965
epoch£º488	 i:2 	 global-step:9762	 l-p:0.2856760621070862
epoch£º488	 i:3 	 global-step:9763	 l-p:0.14039228856563568
epoch£º488	 i:4 	 global-step:9764	 l-p:0.18985870480537415
epoch£º488	 i:5 	 global-step:9765	 l-p:0.018008489161729813
epoch£º488	 i:6 	 global-step:9766	 l-p:0.39961233735084534
epoch£º488	 i:7 	 global-step:9767	 l-p:0.2886522114276886
epoch£º488	 i:8 	 global-step:9768	 l-p:0.15655939280986786
epoch£º488	 i:9 	 global-step:9769	 l-p:0.1360798478126526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1616, 2.9962, 3.1145],
        [3.1616, 3.1599, 3.1615],
        [3.1616, 2.9797, 3.1060],
        [3.1616, 3.1616, 3.1616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.13297131657600403 
model_pd.l_d.mean(): -25.056367874145508 
model_pd.lagr.mean(): -24.923397064208984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1396], device='cuda:0')), ('power', tensor([-24.9167], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.13297131657600403
epoch£º489	 i:1 	 global-step:9781	 l-p:0.0885457769036293
epoch£º489	 i:2 	 global-step:9782	 l-p:0.13546453416347504
epoch£º489	 i:3 	 global-step:9783	 l-p:0.125764399766922
epoch£º489	 i:4 	 global-step:9784	 l-p:0.1132839024066925
epoch£º489	 i:5 	 global-step:9785	 l-p:0.1299135535955429
epoch£º489	 i:6 	 global-step:9786	 l-p:0.12385127693414688
epoch£º489	 i:7 	 global-step:9787	 l-p:0.132383793592453
epoch£º489	 i:8 	 global-step:9788	 l-p:0.170730322599411
epoch£º489	 i:9 	 global-step:9789	 l-p:0.13006626069545746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8990, 2.8990, 2.8990],
        [2.8990, 2.8968, 2.8990],
        [2.8990, 2.8956, 2.8989],
        [2.8990, 2.8990, 2.8990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.150892972946167 
model_pd.l_d.mean(): -25.22214698791504 
model_pd.lagr.mean(): -25.07125473022461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0007], device='cuda:0')), ('power', tensor([-25.2215], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.150892972946167
epoch£º490	 i:1 	 global-step:9801	 l-p:0.12653310596942902
epoch£º490	 i:2 	 global-step:9802	 l-p:-0.58223956823349
epoch£º490	 i:3 	 global-step:9803	 l-p:0.03299708291888237
epoch£º490	 i:4 	 global-step:9804	 l-p:0.13776978850364685
epoch£º490	 i:5 	 global-step:9805	 l-p:0.10754527896642685
epoch£º490	 i:6 	 global-step:9806	 l-p:0.1376984566450119
epoch£º490	 i:7 	 global-step:9807	 l-p:0.24492697417736053
epoch£º490	 i:8 	 global-step:9808	 l-p:0.13850045204162598
epoch£º490	 i:9 	 global-step:9809	 l-p:0.15011224150657654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9491, 1.9569, 1.3554],
        [2.9491, 2.2558, 2.3305],
        [2.9491, 2.7490, 2.8843],
        [2.9491, 1.8265, 1.2594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.27988025546073914 
model_pd.l_d.mean(): -24.726301193237305 
model_pd.lagr.mean(): -24.446420669555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0882], device='cuda:0')), ('power', tensor([-24.8145], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.27988025546073914
epoch£º491	 i:1 	 global-step:9821	 l-p:0.1318565458059311
epoch£º491	 i:2 	 global-step:9822	 l-p:0.1204429492354393
epoch£º491	 i:3 	 global-step:9823	 l-p:0.1319778859615326
epoch£º491	 i:4 	 global-step:9824	 l-p:0.15108032524585724
epoch£º491	 i:5 	 global-step:9825	 l-p:0.14504964649677277
epoch£º491	 i:6 	 global-step:9826	 l-p:0.13677918910980225
epoch£º491	 i:7 	 global-step:9827	 l-p:0.08835364878177643
epoch£º491	 i:8 	 global-step:9828	 l-p:0.13018031418323517
epoch£º491	 i:9 	 global-step:9829	 l-p:0.13325135409832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9246, 2.9245, 2.9246],
        [2.9246, 1.8853, 1.5737],
        [2.9246, 2.8886, 2.9211],
        [2.9246, 2.8983, 2.9225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.12747055292129517 
model_pd.l_d.mean(): -25.221595764160156 
model_pd.lagr.mean(): -25.094125747680664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0822], device='cuda:0')), ('power', tensor([-25.3038], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.12747055292129517
epoch£º492	 i:1 	 global-step:9841	 l-p:0.15817825496196747
epoch£º492	 i:2 	 global-step:9842	 l-p:0.12388624995946884
epoch£º492	 i:3 	 global-step:9843	 l-p:0.11403980851173401
epoch£º492	 i:4 	 global-step:9844	 l-p:-0.11489133536815643
epoch£º492	 i:5 	 global-step:9845	 l-p:0.1403505504131317
epoch£º492	 i:6 	 global-step:9846	 l-p:44.24334716796875
epoch£º492	 i:7 	 global-step:9847	 l-p:0.6517261862754822
epoch£º492	 i:8 	 global-step:9848	 l-p:0.22025248408317566
epoch£º492	 i:9 	 global-step:9849	 l-p:0.09138455986976624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9197, 2.1157, 1.4863],
        [2.9197, 2.9184, 2.9196],
        [2.9197, 2.2819, 2.3943],
        [2.9197, 2.0061, 1.3943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.11750886589288712 
model_pd.l_d.mean(): -24.009532928466797 
model_pd.lagr.mean(): -23.892024993896484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2672], device='cuda:0')), ('power', tensor([-24.2768], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.11750886589288712
epoch£º493	 i:1 	 global-step:9861	 l-p:0.12066563963890076
epoch£º493	 i:2 	 global-step:9862	 l-p:0.14003951847553253
epoch£º493	 i:3 	 global-step:9863	 l-p:-0.19378910958766937
epoch£º493	 i:4 	 global-step:9864	 l-p:0.12553957104682922
epoch£º493	 i:5 	 global-step:9865	 l-p:0.18959948420524597
epoch£º493	 i:6 	 global-step:9866	 l-p:0.10960834473371506
epoch£º493	 i:7 	 global-step:9867	 l-p:0.13043242692947388
epoch£º493	 i:8 	 global-step:9868	 l-p:0.13115358352661133
epoch£º493	 i:9 	 global-step:9869	 l-p:0.1368580460548401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0872, 2.0540, 1.7294],
        [3.0872, 2.2478, 1.6028],
        [3.0872, 2.5912, 2.7535],
        [3.0872, 3.0021, 3.0724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.11439147591590881 
model_pd.l_d.mean(): -24.30529022216797 
model_pd.lagr.mean(): -24.190898895263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2046], device='cuda:0')), ('power', tensor([-24.5099], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.11439147591590881
epoch£º494	 i:1 	 global-step:9881	 l-p:0.1419905424118042
epoch£º494	 i:2 	 global-step:9882	 l-p:0.17969171702861786
epoch£º494	 i:3 	 global-step:9883	 l-p:-0.03419370949268341
epoch£º494	 i:4 	 global-step:9884	 l-p:0.1564057320356369
epoch£º494	 i:5 	 global-step:9885	 l-p:-0.1132977083325386
epoch£º494	 i:6 	 global-step:9886	 l-p:0.09534164518117905
epoch£º494	 i:7 	 global-step:9887	 l-p:0.16541877388954163
epoch£º494	 i:8 	 global-step:9888	 l-p:0.09370534867048264
epoch£º494	 i:9 	 global-step:9889	 l-p:0.16433300077915192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9645, 2.3600, 2.4883],
        [2.9645, 2.9619, 2.9644],
        [2.9645, 1.9069, 1.3134],
        [2.9645, 2.9103, 2.9575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.14225737750530243 
model_pd.l_d.mean(): -25.072031021118164 
model_pd.lagr.mean(): -24.929773330688477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0225], device='cuda:0')), ('power', tensor([-25.0946], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.14225737750530243
epoch£º495	 i:1 	 global-step:9901	 l-p:0.13846087455749512
epoch£º495	 i:2 	 global-step:9902	 l-p:0.1520616114139557
epoch£º495	 i:3 	 global-step:9903	 l-p:0.14200907945632935
epoch£º495	 i:4 	 global-step:9904	 l-p:-0.015496301464736462
epoch£º495	 i:5 	 global-step:9905	 l-p:0.24097564816474915
epoch£º495	 i:6 	 global-step:9906	 l-p:0.12008783221244812
epoch£º495	 i:7 	 global-step:9907	 l-p:0.1415301412343979
epoch£º495	 i:8 	 global-step:9908	 l-p:0.17494353652000427
epoch£º495	 i:9 	 global-step:9909	 l-p:0.1578036993741989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8800, 2.8800, 2.8800],
        [2.8800, 2.5424, 2.7173],
        [2.8800, 2.8799, 2.8800],
        [2.8800, 1.8253, 1.2469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.15818236768245697 
model_pd.l_d.mean(): -25.126121520996094 
model_pd.lagr.mean(): -24.967939376831055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0479], device='cuda:0')), ('power', tensor([-25.1741], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.15818236768245697
epoch£º496	 i:1 	 global-step:9921	 l-p:0.14119505882263184
epoch£º496	 i:2 	 global-step:9922	 l-p:0.10382752865552902
epoch£º496	 i:3 	 global-step:9923	 l-p:-0.4009677767753601
epoch£º496	 i:4 	 global-step:9924	 l-p:-1.7291399240493774
epoch£º496	 i:5 	 global-step:9925	 l-p:0.06656524538993835
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13417953252792358
epoch£º496	 i:7 	 global-step:9927	 l-p:0.1966620236635208
epoch£º496	 i:8 	 global-step:9928	 l-p:0.14572836458683014
epoch£º496	 i:9 	 global-step:9929	 l-p:0.22043636441230774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1184, 2.1326, 1.8808],
        [3.1184, 3.1184, 3.1184],
        [3.1184, 3.1183, 3.1184],
        [3.1184, 3.1111, 3.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.1221671849489212 
model_pd.l_d.mean(): -24.991714477539062 
model_pd.lagr.mean(): -24.86954689025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0230], device='cuda:0')), ('power', tensor([-25.0147], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.1221671849489212
epoch£º497	 i:1 	 global-step:9941	 l-p:0.15103329718112946
epoch£º497	 i:2 	 global-step:9942	 l-p:0.13250847160816193
epoch£º497	 i:3 	 global-step:9943	 l-p:0.13738420605659485
epoch£º497	 i:4 	 global-step:9944	 l-p:0.11906582862138748
epoch£º497	 i:5 	 global-step:9945	 l-p:0.1318538635969162
epoch£º497	 i:6 	 global-step:9946	 l-p:0.2906387150287628
epoch£º497	 i:7 	 global-step:9947	 l-p:0.14711329340934753
epoch£º497	 i:8 	 global-step:9948	 l-p:0.08538457006216049
epoch£º497	 i:9 	 global-step:9949	 l-p:0.1938198208808899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8675, 2.6322, 2.7823],
        [2.8675, 1.8702, 1.2809],
        [2.8675, 1.7379, 1.1882],
        [2.8675, 2.8329, 2.8643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.1323586404323578 
model_pd.l_d.mean(): -25.075138092041016 
model_pd.lagr.mean(): -24.942779541015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1109], device='cuda:0')), ('power', tensor([-25.1860], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.1323586404323578
epoch£º498	 i:1 	 global-step:9961	 l-p:0.14160950481891632
epoch£º498	 i:2 	 global-step:9962	 l-p:0.12391367554664612
epoch£º498	 i:3 	 global-step:9963	 l-p:0.1556762456893921
epoch£º498	 i:4 	 global-step:9964	 l-p:0.1606925129890442
epoch£º498	 i:5 	 global-step:9965	 l-p:0.13313741981983185
epoch£º498	 i:6 	 global-step:9966	 l-p:0.17255721986293793
epoch£º498	 i:7 	 global-step:9967	 l-p:0.39694321155548096
epoch£º498	 i:8 	 global-step:9968	 l-p:-0.17463302612304688
epoch£º498	 i:9 	 global-step:9969	 l-p:-0.4429433047771454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0067, 3.0032, 3.0066],
        [3.0067, 1.9873, 1.3779],
        [3.0067, 3.0067, 3.0067],
        [3.0067, 3.0067, 3.0067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.1479378491640091 
model_pd.l_d.mean(): -25.23070526123047 
model_pd.lagr.mean(): -25.082767486572266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1006], device='cuda:0')), ('power', tensor([-25.1301], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.1479378491640091
epoch£º499	 i:1 	 global-step:9981	 l-p:0.022965606302022934
epoch£º499	 i:2 	 global-step:9982	 l-p:0.14912135899066925
epoch£º499	 i:3 	 global-step:9983	 l-p:0.13243205845355988
epoch£º499	 i:4 	 global-step:9984	 l-p:0.1390165239572525
epoch£º499	 i:5 	 global-step:9985	 l-p:0.16908545792102814
epoch£º499	 i:6 	 global-step:9986	 l-p:0.11537044495344162
epoch£º499	 i:7 	 global-step:9987	 l-p:0.14422327280044556
epoch£º499	 i:8 	 global-step:9988	 l-p:0.1320430189371109
epoch£º499	 i:9 	 global-step:9989	 l-p:0.1312899887561798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9584, 2.5030, 2.6772],
        [2.9584, 1.9679, 1.7330],
        [2.9584, 2.7242, 2.8735],
        [2.9584, 1.8019, 1.2523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.005422415677458048 
model_pd.l_d.mean(): -25.27835464477539 
model_pd.lagr.mean(): -25.272932052612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0398], device='cuda:0')), ('power', tensor([-25.3182], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.005422415677458048
epoch£º500	 i:1 	 global-step:10001	 l-p:0.14481201767921448
epoch£º500	 i:2 	 global-step:10002	 l-p:-0.16952909529209137
epoch£º500	 i:3 	 global-step:10003	 l-p:0.19728098809719086
epoch£º500	 i:4 	 global-step:10004	 l-p:0.13277354836463928
epoch£º500	 i:5 	 global-step:10005	 l-p:0.154359370470047
epoch£º500	 i:6 	 global-step:10006	 l-p:0.13240835070610046
epoch£º500	 i:7 	 global-step:10007	 l-p:0.36744797229766846
epoch£º500	 i:8 	 global-step:10008	 l-p:0.11234216392040253
epoch£º500	 i:9 	 global-step:10009	 l-p:0.177064448595047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8610, 2.2802, 2.4244],
        [2.8610, 1.8444, 1.5842],
        [2.8610, 1.7893, 1.4528],
        [2.8610, 2.7301, 2.8304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.07294102758169174 
model_pd.l_d.mean(): -24.93994140625 
model_pd.lagr.mean(): -24.867000579833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1487], device='cuda:0')), ('power', tensor([-25.0887], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.07294102758169174
epoch£º501	 i:1 	 global-step:10021	 l-p:0.1956913024187088
epoch£º501	 i:2 	 global-step:10022	 l-p:0.018625229597091675
epoch£º501	 i:3 	 global-step:10023	 l-p:0.10722681879997253
epoch£º501	 i:4 	 global-step:10024	 l-p:0.1379256248474121
epoch£º501	 i:5 	 global-step:10025	 l-p:0.15638551115989685
epoch£º501	 i:6 	 global-step:10026	 l-p:0.10822363942861557
epoch£º501	 i:7 	 global-step:10027	 l-p:0.1218908503651619
epoch£º501	 i:8 	 global-step:10028	 l-p:-0.5424217581748962
epoch£º501	 i:9 	 global-step:10029	 l-p:0.1273040771484375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0277, 2.1556, 2.0698],
        [3.0277, 3.0276, 3.0277],
        [3.0277, 2.0004, 1.3880],
        [3.0277, 3.0266, 3.0276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.129940927028656 
model_pd.l_d.mean(): -24.697120666503906 
model_pd.lagr.mean(): -24.567180633544922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0348], device='cuda:0')), ('power', tensor([-24.7319], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.129940927028656
epoch£º502	 i:1 	 global-step:10041	 l-p:0.28782960772514343
epoch£º502	 i:2 	 global-step:10042	 l-p:0.17155510187149048
epoch£º502	 i:3 	 global-step:10043	 l-p:0.14747443795204163
epoch£º502	 i:4 	 global-step:10044	 l-p:0.12291856855154037
epoch£º502	 i:5 	 global-step:10045	 l-p:0.17540499567985535
epoch£º502	 i:6 	 global-step:10046	 l-p:0.026363572105765343
epoch£º502	 i:7 	 global-step:10047	 l-p:0.14167290925979614
epoch£º502	 i:8 	 global-step:10048	 l-p:0.14450789988040924
epoch£º502	 i:9 	 global-step:10049	 l-p:0.12056567519903183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9089, 2.9086, 2.9089],
        [2.9089, 1.8718, 1.2823],
        [2.9089, 2.9089, 2.9089],
        [2.9089, 2.7960, 2.8852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.151234433054924 
model_pd.l_d.mean(): -24.768970489501953 
model_pd.lagr.mean(): -24.61773681640625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0412], device='cuda:0')), ('power', tensor([-24.8101], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.151234433054924
epoch£º503	 i:1 	 global-step:10061	 l-p:0.05724211409687996
epoch£º503	 i:2 	 global-step:10062	 l-p:0.4454978406429291
epoch£º503	 i:3 	 global-step:10063	 l-p:0.13671468198299408
epoch£º503	 i:4 	 global-step:10064	 l-p:0.14245940744876862
epoch£º503	 i:5 	 global-step:10065	 l-p:0.14515329897403717
epoch£º503	 i:6 	 global-step:10066	 l-p:0.1371336728334427
epoch£º503	 i:7 	 global-step:10067	 l-p:0.15186741948127747
epoch£º503	 i:8 	 global-step:10068	 l-p:0.1233755499124527
epoch£º503	 i:9 	 global-step:10069	 l-p:0.29527026414871216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9653, 2.9144, 2.9591],
        [2.9653, 1.9508, 1.6837],
        [2.9653, 2.8215, 2.9292],
        [2.9653, 2.3982, 2.5449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): -0.020744265988469124 
model_pd.l_d.mean(): -25.040178298950195 
model_pd.lagr.mean(): -25.060922622680664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0606], device='cuda:0')), ('power', tensor([-25.1007], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:-0.020744265988469124
epoch£º504	 i:1 	 global-step:10081	 l-p:0.10934360325336456
epoch£º504	 i:2 	 global-step:10082	 l-p:0.1548895239830017
epoch£º504	 i:3 	 global-step:10083	 l-p:0.06447304785251617
epoch£º504	 i:4 	 global-step:10084	 l-p:0.1652478724718094
epoch£º504	 i:5 	 global-step:10085	 l-p:0.03179825097322464
epoch£º504	 i:6 	 global-step:10086	 l-p:0.1344555765390396
epoch£º504	 i:7 	 global-step:10087	 l-p:0.10715659707784653
epoch£º504	 i:8 	 global-step:10088	 l-p:0.17513352632522583
epoch£º504	 i:9 	 global-step:10089	 l-p:0.16595520079135895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9048, 2.2242, 2.3171],
        [2.9048, 2.6534, 2.8091],
        [2.9048, 2.8847, 2.9035],
        [2.9048, 2.5478, 2.7256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.1908186823129654 
model_pd.l_d.mean(): -25.14217185974121 
model_pd.lagr.mean(): -24.951353073120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0485], device='cuda:0')), ('power', tensor([-25.1907], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.1908186823129654
epoch£º505	 i:1 	 global-step:10101	 l-p:0.07454096525907516
epoch£º505	 i:2 	 global-step:10102	 l-p:0.22015473246574402
epoch£º505	 i:3 	 global-step:10103	 l-p:0.1442996710538864
epoch£º505	 i:4 	 global-step:10104	 l-p:0.11581823974847794
epoch£º505	 i:5 	 global-step:10105	 l-p:0.12251643091440201
epoch£º505	 i:6 	 global-step:10106	 l-p:0.12008069455623627
epoch£º505	 i:7 	 global-step:10107	 l-p:0.13013620674610138
epoch£º505	 i:8 	 global-step:10108	 l-p:0.20792438089847565
epoch£º505	 i:9 	 global-step:10109	 l-p:0.14231237769126892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0558, 3.0051, 3.0496],
        [3.0558, 2.2327, 1.5871],
        [3.0558, 2.1322, 1.4990],
        [3.0558, 1.9973, 1.6511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.1381542682647705 
model_pd.l_d.mean(): -24.759532928466797 
model_pd.lagr.mean(): -24.62137794494629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0557], device='cuda:0')), ('power', tensor([-24.8152], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.1381542682647705
epoch£º506	 i:1 	 global-step:10121	 l-p:0.1650901585817337
epoch£º506	 i:2 	 global-step:10122	 l-p:0.15579953789710999
epoch£º506	 i:3 	 global-step:10123	 l-p:-0.16839607059955597
epoch£º506	 i:4 	 global-step:10124	 l-p:0.17380084097385406
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11568649113178253
epoch£º506	 i:6 	 global-step:10126	 l-p:0.1527801752090454
epoch£º506	 i:7 	 global-step:10127	 l-p:0.11489096283912659
epoch£º506	 i:8 	 global-step:10128	 l-p:0.12887711822986603
epoch£º506	 i:9 	 global-step:10129	 l-p:0.5538514256477356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8636, 2.6338, 2.7823],
        [2.8636, 2.0692, 2.0789],
        [2.8636, 1.9324, 1.7943],
        [2.8636, 1.9858, 1.3721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.2235894799232483 
model_pd.l_d.mean(): -25.133962631225586 
model_pd.lagr.mean(): -24.91037368774414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1376], device='cuda:0')), ('power', tensor([-25.2715], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.2235894799232483
epoch£º507	 i:1 	 global-step:10141	 l-p:0.11867894977331161
epoch£º507	 i:2 	 global-step:10142	 l-p:0.17111743986606598
epoch£º507	 i:3 	 global-step:10143	 l-p:0.11684813350439072
epoch£º507	 i:4 	 global-step:10144	 l-p:0.1683504432439804
epoch£º507	 i:5 	 global-step:10145	 l-p:0.08633463084697723
epoch£º507	 i:6 	 global-step:10146	 l-p:0.23357586562633514
epoch£º507	 i:7 	 global-step:10147	 l-p:0.14961937069892883
epoch£º507	 i:8 	 global-step:10148	 l-p:0.1268642395734787
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12174446135759354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1338, 3.0216, 3.1101],
        [3.1338, 3.1238, 3.1333],
        [3.1338, 3.1312, 3.1337],
        [3.1338, 1.9712, 1.4414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.14224153757095337 
model_pd.l_d.mean(): -24.68904685974121 
model_pd.lagr.mean(): -24.546804428100586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0448], device='cuda:0')), ('power', tensor([-24.7338], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.14224153757095337
epoch£º508	 i:1 	 global-step:10161	 l-p:0.1260165572166443
epoch£º508	 i:2 	 global-step:10162	 l-p:0.14063121378421783
epoch£º508	 i:3 	 global-step:10163	 l-p:0.14020010828971863
epoch£º508	 i:4 	 global-step:10164	 l-p:0.1437303125858307
epoch£º508	 i:5 	 global-step:10165	 l-p:0.12762615084648132
epoch£º508	 i:6 	 global-step:10166	 l-p:0.18286897242069244
epoch£º508	 i:7 	 global-step:10167	 l-p:0.11555662751197815
epoch£º508	 i:8 	 global-step:10168	 l-p:0.18014438450336456
epoch£º508	 i:9 	 global-step:10169	 l-p:0.24661189317703247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9155, 1.8062, 1.2327],
        [2.9155, 2.9134, 2.9155],
        [2.9155, 2.0942, 1.4636],
        [2.9155, 2.7153, 2.8516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): -0.004043499007821083 
model_pd.l_d.mean(): -24.910486221313477 
model_pd.lagr.mean(): -24.91452980041504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1106], device='cuda:0')), ('power', tensor([-25.0211], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:-0.004043499007821083
epoch£º509	 i:1 	 global-step:10181	 l-p:0.12183074653148651
epoch£º509	 i:2 	 global-step:10182	 l-p:0.13338911533355713
epoch£º509	 i:3 	 global-step:10183	 l-p:0.18457584083080292
epoch£º509	 i:4 	 global-step:10184	 l-p:0.10507481545209885
epoch£º509	 i:5 	 global-step:10185	 l-p:0.13169698417186737
epoch£º509	 i:6 	 global-step:10186	 l-p:0.1206044927239418
epoch£º509	 i:7 	 global-step:10187	 l-p:0.20495986938476562
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10765073448419571
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14311566948890686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0304, 3.0241, 3.0301],
        [3.0304, 1.8733, 1.2973],
        [3.0304, 1.8873, 1.3025],
        [3.0304, 1.8622, 1.3452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.11943885684013367 
model_pd.l_d.mean(): -24.4027042388916 
model_pd.lagr.mean(): -24.283266067504883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0053], device='cuda:0')), ('power', tensor([-24.3974], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.11943885684013367
epoch£º510	 i:1 	 global-step:10201	 l-p:0.34944045543670654
epoch£º510	 i:2 	 global-step:10202	 l-p:0.13941113650798798
epoch£º510	 i:3 	 global-step:10203	 l-p:0.23094360530376434
epoch£º510	 i:4 	 global-step:10204	 l-p:0.13552679121494293
epoch£º510	 i:5 	 global-step:10205	 l-p:0.15008436143398285
epoch£º510	 i:6 	 global-step:10206	 l-p:0.17125551402568817
epoch£º510	 i:7 	 global-step:10207	 l-p:0.1563534140586853
epoch£º510	 i:8 	 global-step:10208	 l-p:0.18522821366786957
epoch£º510	 i:9 	 global-step:10209	 l-p:-0.015459489077329636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9170, 2.5372, 2.7172],
        [2.9170, 1.7532, 1.2142],
        [2.9170, 2.7559, 2.8734],
        [2.9170, 2.9170, 2.9170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): -21.25925064086914 
model_pd.l_d.mean(): -25.011438369750977 
model_pd.lagr.mean(): -46.27069091796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1811], device='cuda:0')), ('power', tensor([-25.1925], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:-21.25925064086914
epoch£º511	 i:1 	 global-step:10221	 l-p:-0.03243011236190796
epoch£º511	 i:2 	 global-step:10222	 l-p:0.1149502620100975
epoch£º511	 i:3 	 global-step:10223	 l-p:0.1197967380285263
epoch£º511	 i:4 	 global-step:10224	 l-p:0.12238017469644547
epoch£º511	 i:5 	 global-step:10225	 l-p:0.13768412172794342
epoch£º511	 i:6 	 global-step:10226	 l-p:0.1209230050444603
epoch£º511	 i:7 	 global-step:10227	 l-p:0.12904295325279236
epoch£º511	 i:8 	 global-step:10228	 l-p:-0.2644904553890228
epoch£º511	 i:9 	 global-step:10229	 l-p:0.11004571616649628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9354, 2.8833, 2.9290],
        [2.9354, 2.7350, 2.8714],
        [2.9354, 1.7663, 1.2315],
        [2.9354, 2.3564, 2.5020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.15657342970371246 
model_pd.l_d.mean(): -25.174760818481445 
model_pd.lagr.mean(): -25.018186569213867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0240], device='cuda:0')), ('power', tensor([-25.1987], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.15657342970371246
epoch£º512	 i:1 	 global-step:10241	 l-p:0.07848353683948517
epoch£º512	 i:2 	 global-step:10242	 l-p:0.1854066401720047
epoch£º512	 i:3 	 global-step:10243	 l-p:0.1466170698404312
epoch£º512	 i:4 	 global-step:10244	 l-p:0.23398837447166443
epoch£º512	 i:5 	 global-step:10245	 l-p:-0.2114490419626236
epoch£º512	 i:6 	 global-step:10246	 l-p:0.12342988699674606
epoch£º512	 i:7 	 global-step:10247	 l-p:0.13890008628368378
epoch£º512	 i:8 	 global-step:10248	 l-p:0.011223754845559597
epoch£º512	 i:9 	 global-step:10249	 l-p:7.902537822723389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9949, 2.2285, 2.2575],
        [2.9949, 1.9616, 1.3533],
        [2.9949, 2.3144, 2.4064],
        [2.9949, 2.9855, 2.9945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.22229453921318054 
model_pd.l_d.mean(): -25.0720272064209 
model_pd.lagr.mean(): -24.849733352661133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0453], device='cuda:0')), ('power', tensor([-25.1173], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.22229453921318054
epoch£º513	 i:1 	 global-step:10261	 l-p:0.18476887047290802
epoch£º513	 i:2 	 global-step:10262	 l-p:0.12112356722354889
epoch£º513	 i:3 	 global-step:10263	 l-p:0.15572696924209595
epoch£º513	 i:4 	 global-step:10264	 l-p:0.12266755104064941
epoch£º513	 i:5 	 global-step:10265	 l-p:0.13403615355491638
epoch£º513	 i:6 	 global-step:10266	 l-p:0.2234291434288025
epoch£º513	 i:7 	 global-step:10267	 l-p:0.1230972483754158
epoch£º513	 i:8 	 global-step:10268	 l-p:0.12697359919548035
epoch£º513	 i:9 	 global-step:10269	 l-p:0.15138639509677887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0202, 2.1173, 1.4842],
        [3.0202, 1.9729, 1.3624],
        [3.0202, 2.4092, 2.5386],
        [3.0202, 3.0193, 3.0202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.13196906447410583 
model_pd.l_d.mean(): -25.008934020996094 
model_pd.lagr.mean(): -24.876964569091797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0325], device='cuda:0')), ('power', tensor([-24.9765], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.13196906447410583
epoch£º514	 i:1 	 global-step:10281	 l-p:0.1525975614786148
epoch£º514	 i:2 	 global-step:10282	 l-p:0.17244794964790344
epoch£º514	 i:3 	 global-step:10283	 l-p:-0.5525261163711548
epoch£º514	 i:4 	 global-step:10284	 l-p:0.1397639364004135
epoch£º514	 i:5 	 global-step:10285	 l-p:0.1754738986492157
epoch£º514	 i:6 	 global-step:10286	 l-p:0.12983790040016174
epoch£º514	 i:7 	 global-step:10287	 l-p:0.20532187819480896
epoch£º514	 i:8 	 global-step:10288	 l-p:0.18187640607357025
epoch£º514	 i:9 	 global-step:10289	 l-p:0.15706048905849457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8717, 1.8103, 1.2303],
        [2.8717, 1.7826, 1.2100],
        [2.8717, 2.6113, 2.7707],
        [2.8717, 1.7142, 1.2505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.08207941800355911 
model_pd.l_d.mean(): -24.63866424560547 
model_pd.lagr.mean(): -24.55658531188965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2357], device='cuda:0')), ('power', tensor([-24.8743], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.08207941800355911
epoch£º515	 i:1 	 global-step:10301	 l-p:0.1099894717335701
epoch£º515	 i:2 	 global-step:10302	 l-p:0.12335269898176193
epoch£º515	 i:3 	 global-step:10303	 l-p:0.16211386024951935
epoch£º515	 i:4 	 global-step:10304	 l-p:0.13985103368759155
epoch£º515	 i:5 	 global-step:10305	 l-p:0.1416708081960678
epoch£º515	 i:6 	 global-step:10306	 l-p:0.03198998421430588
epoch£º515	 i:7 	 global-step:10307	 l-p:0.2228260636329651
epoch£º515	 i:8 	 global-step:10308	 l-p:0.2233615219593048
epoch£º515	 i:9 	 global-step:10309	 l-p:0.12276578694581985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9221, 2.8808, 2.9177],
        [2.9221, 2.2559, 2.3613],
        [2.9221, 1.8558, 1.2672],
        [2.9221, 2.8444, 2.9096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 1.5135819911956787 
model_pd.l_d.mean(): -24.955881118774414 
model_pd.lagr.mean(): -23.442298889160156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1737], device='cuda:0')), ('power', tensor([-25.1296], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:1.5135819911956787
epoch£º516	 i:1 	 global-step:10321	 l-p:0.15353746712207794
epoch£º516	 i:2 	 global-step:10322	 l-p:0.1346450299024582
epoch£º516	 i:3 	 global-step:10323	 l-p:0.2108696848154068
epoch£º516	 i:4 	 global-step:10324	 l-p:0.19797450304031372
epoch£º516	 i:5 	 global-step:10325	 l-p:0.12289000302553177
epoch£º516	 i:6 	 global-step:10326	 l-p:0.14840540289878845
epoch£º516	 i:7 	 global-step:10327	 l-p:0.17272424697875977
epoch£º516	 i:8 	 global-step:10328	 l-p:0.1200469583272934
epoch£º516	 i:9 	 global-step:10329	 l-p:0.12508299946784973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1796, 3.1478, 3.1768],
        [3.1796, 2.1589, 1.8644],
        [3.1796, 2.2130, 2.0000],
        [3.1796, 3.1795, 3.1796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.12819619476795197 
model_pd.l_d.mean(): -24.829557418823242 
model_pd.lagr.mean(): -24.70136070251465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0628], device='cuda:0')), ('power', tensor([-24.7667], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.12819619476795197
epoch£º517	 i:1 	 global-step:10341	 l-p:0.11730539798736572
epoch£º517	 i:2 	 global-step:10342	 l-p:0.13061058521270752
epoch£º517	 i:3 	 global-step:10343	 l-p:0.1524294763803482
epoch£º517	 i:4 	 global-step:10344	 l-p:0.16039393842220306
epoch£º517	 i:5 	 global-step:10345	 l-p:0.1286301612854004
epoch£º517	 i:6 	 global-step:10346	 l-p:0.10133903473615646
epoch£º517	 i:7 	 global-step:10347	 l-p:0.09350625425577164
epoch£º517	 i:8 	 global-step:10348	 l-p:0.113620825111866
epoch£º517	 i:9 	 global-step:10349	 l-p:0.2888262867927551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[2.8370, 1.7013, 1.1536],
        [2.8370, 1.7065, 1.2999],
        [2.8370, 2.4035, 2.5854],
        [2.8370, 2.0367, 2.0491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.14704406261444092 
model_pd.l_d.mean(): -25.085525512695312 
model_pd.lagr.mean(): -24.9384822845459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1586], device='cuda:0')), ('power', tensor([-25.2441], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.14704406261444092
epoch£º518	 i:1 	 global-step:10361	 l-p:0.1219705268740654
epoch£º518	 i:2 	 global-step:10362	 l-p:0.15672269463539124
epoch£º518	 i:3 	 global-step:10363	 l-p:0.019070595502853394
epoch£º518	 i:4 	 global-step:10364	 l-p:0.21880406141281128
epoch£º518	 i:5 	 global-step:10365	 l-p:0.2846071422100067
epoch£º518	 i:6 	 global-step:10366	 l-p:0.14267320930957794
epoch£º518	 i:7 	 global-step:10367	 l-p:0.15109221637248993
epoch£º518	 i:8 	 global-step:10368	 l-p:0.13728705048561096
epoch£º518	 i:9 	 global-step:10369	 l-p:-1.088644027709961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9989, 2.9866, 2.9983],
        [2.9989, 2.1419, 1.5041],
        [2.9989, 2.9855, 2.9982],
        [2.9989, 2.6515, 2.8287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.14146225154399872 
model_pd.l_d.mean(): -25.066417694091797 
model_pd.lagr.mean(): -24.924955368041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0087], device='cuda:0')), ('power', tensor([-25.0751], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.14146225154399872
epoch£º519	 i:1 	 global-step:10381	 l-p:0.18691733479499817
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11295224726200104
epoch£º519	 i:3 	 global-step:10383	 l-p:0.15473978221416473
epoch£º519	 i:4 	 global-step:10384	 l-p:0.1905282586812973
epoch£º519	 i:5 	 global-step:10385	 l-p:0.1765018254518509
epoch£º519	 i:6 	 global-step:10386	 l-p:0.12543050944805145
epoch£º519	 i:7 	 global-step:10387	 l-p:0.1724456548690796
epoch£º519	 i:8 	 global-step:10388	 l-p:0.11538644880056381
epoch£º519	 i:9 	 global-step:10389	 l-p:2.037071704864502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0903, 3.0903, 3.0903],
        [3.0903, 2.1565, 1.5162],
        [3.0903, 3.0903, 3.0903],
        [3.0903, 3.0583, 3.0874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13057424128055573 
model_pd.l_d.mean(): -24.731346130371094 
model_pd.lagr.mean(): -24.600770950317383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0204], device='cuda:0')), ('power', tensor([-24.7109], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13057424128055573
epoch£º520	 i:1 	 global-step:10401	 l-p:0.1252502202987671
epoch£º520	 i:2 	 global-step:10402	 l-p:0.11495460569858551
epoch£º520	 i:3 	 global-step:10403	 l-p:0.13050653040409088
epoch£º520	 i:4 	 global-step:10404	 l-p:0.3101775646209717
epoch£º520	 i:5 	 global-step:10405	 l-p:0.18332809209823608
epoch£º520	 i:6 	 global-step:10406	 l-p:0.12931500375270844
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12433988600969315
epoch£º520	 i:8 	 global-step:10408	 l-p:0.13536562025547028
epoch£º520	 i:9 	 global-step:10409	 l-p:0.15845604240894318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0051, 2.1149, 2.0248],
        [3.0051, 2.8783, 2.9763],
        [3.0051, 2.1241, 1.4884],
        [3.0051, 2.2525, 2.2961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.14037230610847473 
model_pd.l_d.mean(): -25.16105079650879 
model_pd.lagr.mean(): -25.02067756652832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0360], device='cuda:0')), ('power', tensor([-25.1251], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.14037230610847473
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12439493089914322
epoch£º521	 i:2 	 global-step:10422	 l-p:0.01894555054605007
epoch£º521	 i:3 	 global-step:10423	 l-p:0.20964519679546356
epoch£º521	 i:4 	 global-step:10424	 l-p:0.15134122967720032
epoch£º521	 i:5 	 global-step:10425	 l-p:-0.06256146728992462
epoch£º521	 i:6 	 global-step:10426	 l-p:0.32136160135269165
epoch£º521	 i:7 	 global-step:10427	 l-p:0.16380806267261505
epoch£º521	 i:8 	 global-step:10428	 l-p:0.082839734852314
epoch£º521	 i:9 	 global-step:10429	 l-p:0.1699409782886505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0065, 2.9910, 3.0056],
        [3.0065, 3.0065, 3.0065],
        [3.0065, 2.9698, 3.0029],
        [3.0065, 2.9652, 3.0022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.23309847712516785 
model_pd.l_d.mean(): -25.194564819335938 
model_pd.lagr.mean(): -24.96146583557129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0196], device='cuda:0')), ('power', tensor([-25.2141], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.23309847712516785
epoch£º522	 i:1 	 global-step:10441	 l-p:0.13016963005065918
epoch£º522	 i:2 	 global-step:10442	 l-p:0.1373303234577179
epoch£º522	 i:3 	 global-step:10443	 l-p:0.13542403280735016
epoch£º522	 i:4 	 global-step:10444	 l-p:0.12130023539066315
epoch£º522	 i:5 	 global-step:10445	 l-p:0.12764739990234375
epoch£º522	 i:6 	 global-step:10446	 l-p:0.10623558610677719
epoch£º522	 i:7 	 global-step:10447	 l-p:0.12025850266218185
epoch£º522	 i:8 	 global-step:10448	 l-p:0.1129322201013565
epoch£º522	 i:9 	 global-step:10449	 l-p:0.1379818320274353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0936, 3.0141, 3.0806],
        [3.0936, 3.0936, 3.0936],
        [3.0936, 2.2165, 1.5682],
        [3.0936, 3.0929, 3.0936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.13227437436580658 
model_pd.l_d.mean(): -24.286907196044922 
model_pd.lagr.mean(): -24.154632568359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1723], device='cuda:0')), ('power', tensor([-24.4592], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.13227437436580658
epoch£º523	 i:1 	 global-step:10461	 l-p:0.14262664318084717
epoch£º523	 i:2 	 global-step:10462	 l-p:0.21047581732273102
epoch£º523	 i:3 	 global-step:10463	 l-p:0.1459004431962967
epoch£º523	 i:4 	 global-step:10464	 l-p:0.13708873093128204
epoch£º523	 i:5 	 global-step:10465	 l-p:0.5005260109901428
epoch£º523	 i:6 	 global-step:10466	 l-p:0.17129798233509064
epoch£º523	 i:7 	 global-step:10467	 l-p:0.1599845439195633
epoch£º523	 i:8 	 global-step:10468	 l-p:0.15690714120864868
epoch£º523	 i:9 	 global-step:10469	 l-p:0.15817758440971375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8819, 2.8336, 2.8763],
        [2.8819, 1.8121, 1.2296],
        [2.8819, 2.8807, 2.8819],
        [2.8819, 2.8361, 2.8768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.24036571383476257 
model_pd.l_d.mean(): -25.065492630004883 
model_pd.lagr.mean(): -24.82512664794922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1031], device='cuda:0')), ('power', tensor([-25.1686], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.24036571383476257
epoch£º524	 i:1 	 global-step:10481	 l-p:0.13830524682998657
epoch£º524	 i:2 	 global-step:10482	 l-p:0.5282888412475586
epoch£º524	 i:3 	 global-step:10483	 l-p:0.1950657069683075
epoch£º524	 i:4 	 global-step:10484	 l-p:0.1317717581987381
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13007695972919464
epoch£º524	 i:6 	 global-step:10486	 l-p:0.11774766445159912
epoch£º524	 i:7 	 global-step:10487	 l-p:0.12392150610685349
epoch£º524	 i:8 	 global-step:10488	 l-p:0.13753022253513336
epoch£º524	 i:9 	 global-step:10489	 l-p:0.03139183297753334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9466, 2.9368, 2.9462],
        [2.9466, 2.9465, 2.9466],
        [2.9466, 2.9332, 2.9459],
        [2.9466, 2.1202, 2.1063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16465012729167938 
model_pd.l_d.mean(): -24.851716995239258 
model_pd.lagr.mean(): -24.68706703186035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1185], device='cuda:0')), ('power', tensor([-24.9702], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16465012729167938
epoch£º525	 i:1 	 global-step:10501	 l-p:0.3898758292198181
epoch£º525	 i:2 	 global-step:10502	 l-p:0.13260045647621155
epoch£º525	 i:3 	 global-step:10503	 l-p:0.13989819586277008
epoch£º525	 i:4 	 global-step:10504	 l-p:0.1302831768989563
epoch£º525	 i:5 	 global-step:10505	 l-p:0.1377866268157959
epoch£º525	 i:6 	 global-step:10506	 l-p:0.21997642517089844
epoch£º525	 i:7 	 global-step:10507	 l-p:0.15410034358501434
epoch£º525	 i:8 	 global-step:10508	 l-p:-0.16026738286018372
epoch£º525	 i:9 	 global-step:10509	 l-p:0.1827840805053711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8391, 2.7913, 2.8336],
        [2.8391, 2.1672, 2.2779],
        [2.8391, 1.6757, 1.1419],
        [2.8391, 1.7042, 1.1510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.30231019854545593 
model_pd.l_d.mean(): -25.236581802368164 
model_pd.lagr.mean(): -24.93427085876465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1357], device='cuda:0')), ('power', tensor([-25.3723], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.30231019854545593
epoch£º526	 i:1 	 global-step:10521	 l-p:0.16744288802146912
epoch£º526	 i:2 	 global-step:10522	 l-p:0.0183307696133852
epoch£º526	 i:3 	 global-step:10523	 l-p:0.1304064691066742
epoch£º526	 i:4 	 global-step:10524	 l-p:-0.7217208743095398
epoch£º526	 i:5 	 global-step:10525	 l-p:0.14055104553699493
epoch£º526	 i:6 	 global-step:10526	 l-p:0.138688325881958
epoch£º526	 i:7 	 global-step:10527	 l-p:0.13283047080039978
epoch£º526	 i:8 	 global-step:10528	 l-p:0.6184707283973694
epoch£º526	 i:9 	 global-step:10529	 l-p:0.13390786945819855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0901, 3.0901, 3.0901],
        [3.0901, 2.0758, 1.8198],
        [3.0901, 3.0902, 3.0901],
        [3.0901, 3.0875, 3.0901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12804555892944336 
model_pd.l_d.mean(): -25.05369758605957 
model_pd.lagr.mean(): -24.92565155029297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0500], device='cuda:0')), ('power', tensor([-25.0037], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12804555892944336
epoch£º527	 i:1 	 global-step:10541	 l-p:0.1392933875322342
epoch£º527	 i:2 	 global-step:10542	 l-p:0.14273017644882202
epoch£º527	 i:3 	 global-step:10543	 l-p:0.15128584206104279
epoch£º527	 i:4 	 global-step:10544	 l-p:0.04607430472970009
epoch£º527	 i:5 	 global-step:10545	 l-p:0.17415709793567657
epoch£º527	 i:6 	 global-step:10546	 l-p:0.16976295411586761
epoch£º527	 i:7 	 global-step:10547	 l-p:0.058917608112096786
epoch£º527	 i:8 	 global-step:10548	 l-p:0.15057535469532013
epoch£º527	 i:9 	 global-step:10549	 l-p:0.14994998276233673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9665, 2.7581, 2.8987],
        [2.9665, 2.9384, 2.9643],
        [2.9665, 1.9092, 1.3062],
        [2.9665, 1.9093, 1.3063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.02958734519779682 
model_pd.l_d.mean(): -24.83934783935547 
model_pd.lagr.mean(): -24.868934631347656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1248], device='cuda:0')), ('power', tensor([-24.9642], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.02958734519779682
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12191397696733475
epoch£º528	 i:2 	 global-step:10562	 l-p:0.12609770894050598
epoch£º528	 i:3 	 global-step:10563	 l-p:0.1571841537952423
epoch£º528	 i:4 	 global-step:10564	 l-p:0.11416871845722198
epoch£º528	 i:5 	 global-step:10565	 l-p:0.13487876951694489
epoch£º528	 i:6 	 global-step:10566	 l-p:0.12246081978082657
epoch£º528	 i:7 	 global-step:10567	 l-p:0.15274778008460999
epoch£º528	 i:8 	 global-step:10568	 l-p:0.07793191075325012
epoch£º528	 i:9 	 global-step:10569	 l-p:0.13767334818840027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[2.9923, 1.9350, 1.6304],
        [2.9923, 1.8120, 1.3133],
        [2.9923, 2.6133, 2.7949],
        [2.9923, 1.8877, 1.2902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.23005156219005585 
model_pd.l_d.mean(): -25.17854118347168 
model_pd.lagr.mean(): -24.948490142822266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0381], device='cuda:0')), ('power', tensor([-25.2166], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.23005156219005585
epoch£º529	 i:1 	 global-step:10581	 l-p:-1.0146764516830444
epoch£º529	 i:2 	 global-step:10582	 l-p:0.1432347595691681
epoch£º529	 i:3 	 global-step:10583	 l-p:0.13644203543663025
epoch£º529	 i:4 	 global-step:10584	 l-p:0.18195663392543793
epoch£º529	 i:5 	 global-step:10585	 l-p:0.15826955437660217
epoch£º529	 i:6 	 global-step:10586	 l-p:0.14918102324008942
epoch£º529	 i:7 	 global-step:10587	 l-p:0.15241293609142303
epoch£º529	 i:8 	 global-step:10588	 l-p:0.14503884315490723
epoch£º529	 i:9 	 global-step:10589	 l-p:0.130278542637825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9286, 2.9286, 2.9286],
        [2.9286, 2.0239, 1.9338],
        [2.9286, 2.9286, 2.9286],
        [2.9286, 1.7717, 1.2063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.14461851119995117 
model_pd.l_d.mean(): -25.070131301879883 
model_pd.lagr.mean(): -24.925512313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1044], device='cuda:0')), ('power', tensor([-25.1745], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.14461851119995117
epoch£º530	 i:1 	 global-step:10601	 l-p:-0.12061610817909241
epoch£º530	 i:2 	 global-step:10602	 l-p:0.12774214148521423
epoch£º530	 i:3 	 global-step:10603	 l-p:0.06854064762592316
epoch£º530	 i:4 	 global-step:10604	 l-p:0.20814746618270874
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13719649612903595
epoch£º530	 i:6 	 global-step:10606	 l-p:0.1637454330921173
epoch£º530	 i:7 	 global-step:10607	 l-p:0.3825889825820923
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13213109970092773
epoch£º530	 i:9 	 global-step:10609	 l-p:0.1113007515668869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0625, 3.0411, 3.0611],
        [3.0625, 3.0625, 3.0625],
        [3.0625, 3.0625, 3.0625],
        [3.0625, 3.0625, 3.0625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.13905376195907593 
model_pd.l_d.mean(): -24.692138671875 
model_pd.lagr.mean(): -24.553085327148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0152], device='cuda:0')), ('power', tensor([-24.7073], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.13905376195907593
epoch£º531	 i:1 	 global-step:10621	 l-p:0.12050261348485947
epoch£º531	 i:2 	 global-step:10622	 l-p:0.05855906009674072
epoch£º531	 i:3 	 global-step:10623	 l-p:0.13039471209049225
epoch£º531	 i:4 	 global-step:10624	 l-p:0.14076076447963715
epoch£º531	 i:5 	 global-step:10625	 l-p:0.1653212457895279
epoch£º531	 i:6 	 global-step:10626	 l-p:0.14267006516456604
epoch£º531	 i:7 	 global-step:10627	 l-p:0.13409458100795746
epoch£º531	 i:8 	 global-step:10628	 l-p:0.12859585881233215
epoch£º531	 i:9 	 global-step:10629	 l-p:0.13908778131008148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0206, 1.8285, 1.2942],
        [3.0206, 2.8033, 2.9476],
        [3.0206, 1.8366, 1.2687],
        [3.0206, 3.0088, 3.0201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.20355373620986938 
model_pd.l_d.mean(): -24.739444732666016 
model_pd.lagr.mean(): -24.535890579223633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0902], device='cuda:0')), ('power', tensor([-24.8296], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.20355373620986938
epoch£º532	 i:1 	 global-step:10641	 l-p:0.13884182274341583
epoch£º532	 i:2 	 global-step:10642	 l-p:0.22038477659225464
epoch£º532	 i:3 	 global-step:10643	 l-p:0.13538005948066711
epoch£º532	 i:4 	 global-step:10644	 l-p:0.2578926086425781
epoch£º532	 i:5 	 global-step:10645	 l-p:0.14522340893745422
epoch£º532	 i:6 	 global-step:10646	 l-p:0.12280115485191345
epoch£º532	 i:7 	 global-step:10647	 l-p:0.12210601568222046
epoch£º532	 i:8 	 global-step:10648	 l-p:0.0515529066324234
epoch£º532	 i:9 	 global-step:10649	 l-p:0.15894964337348938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0912, 2.0064, 1.3845],
        [3.0912, 3.0776, 3.0905],
        [3.0912, 2.9817, 3.0689],
        [3.0912, 2.2964, 2.3076]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.13209852576255798 
model_pd.l_d.mean(): -24.841520309448242 
model_pd.lagr.mean(): -24.709421157836914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0257], device='cuda:0')), ('power', tensor([-24.8672], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.13209852576255798
epoch£º533	 i:1 	 global-step:10661	 l-p:0.11893939971923828
epoch£º533	 i:2 	 global-step:10662	 l-p:0.1185527965426445
epoch£º533	 i:3 	 global-step:10663	 l-p:0.15197890996932983
epoch£º533	 i:4 	 global-step:10664	 l-p:0.21067190170288086
epoch£º533	 i:5 	 global-step:10665	 l-p:0.14898870885372162
epoch£º533	 i:6 	 global-step:10666	 l-p:0.05851311981678009
epoch£º533	 i:7 	 global-step:10667	 l-p:0.1036556214094162
epoch£º533	 i:8 	 global-step:10668	 l-p:0.1579056829214096
epoch£º533	 i:9 	 global-step:10669	 l-p:-0.10862188041210175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9495, 2.1775, 2.2181],
        [2.9495, 1.8118, 1.4031],
        [2.9495, 2.9152, 2.9464],
        [2.9495, 2.0204, 1.9026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.0883793979883194 
model_pd.l_d.mean(): -24.96013641357422 
model_pd.lagr.mean(): -24.87175750732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1610], device='cuda:0')), ('power', tensor([-25.1211], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.0883793979883194
epoch£º534	 i:1 	 global-step:10681	 l-p:0.15474748611450195
epoch£º534	 i:2 	 global-step:10682	 l-p:0.13589461147785187
epoch£º534	 i:3 	 global-step:10683	 l-p:0.1409899890422821
epoch£º534	 i:4 	 global-step:10684	 l-p:0.12240655720233917
epoch£º534	 i:5 	 global-step:10685	 l-p:0.10203065723180771
epoch£º534	 i:6 	 global-step:10686	 l-p:0.17032180726528168
epoch£º534	 i:7 	 global-step:10687	 l-p:0.13432346284389496
epoch£º534	 i:8 	 global-step:10688	 l-p:0.3390689790248871
epoch£º534	 i:9 	 global-step:10689	 l-p:0.31655260920524597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8569, 2.8559, 2.8569],
        [2.8569, 2.7661, 2.8409],
        [2.8569, 2.8567, 2.8569],
        [2.8569, 2.6513, 2.7914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.09403805434703827 
model_pd.l_d.mean(): -25.010488510131836 
model_pd.lagr.mean(): -24.91645050048828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0529], device='cuda:0')), ('power', tensor([-25.0634], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.09403805434703827
epoch£º535	 i:1 	 global-step:10701	 l-p:0.1673351675271988
epoch£º535	 i:2 	 global-step:10702	 l-p:0.16414062678813934
epoch£º535	 i:3 	 global-step:10703	 l-p:0.15323032438755035
epoch£º535	 i:4 	 global-step:10704	 l-p:0.09896198660135269
epoch£º535	 i:5 	 global-step:10705	 l-p:0.1266011744737625
epoch£º535	 i:6 	 global-step:10706	 l-p:0.1553076207637787
epoch£º535	 i:7 	 global-step:10707	 l-p:0.13588887453079224
epoch£º535	 i:8 	 global-step:10708	 l-p:0.1711558848619461
epoch£º535	 i:9 	 global-step:10709	 l-p:0.13043774664402008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0915, 3.0457, 3.0864],
        [3.0915, 3.0693, 3.0899],
        [3.0915, 3.0498, 3.0871],
        [3.0915, 1.9212, 1.3232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.13226991891860962 
model_pd.l_d.mean(): -24.78194236755371 
model_pd.lagr.mean(): -24.64967155456543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0837], device='cuda:0')), ('power', tensor([-24.8656], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.13226991891860962
epoch£º536	 i:1 	 global-step:10721	 l-p:-0.0007085180259309709
epoch£º536	 i:2 	 global-step:10722	 l-p:0.14204826951026917
epoch£º536	 i:3 	 global-step:10723	 l-p:0.1712053120136261
epoch£º536	 i:4 	 global-step:10724	 l-p:0.1763577163219452
epoch£º536	 i:5 	 global-step:10725	 l-p:0.16923107206821442
epoch£º536	 i:6 	 global-step:10726	 l-p:0.1251964569091797
epoch£º536	 i:7 	 global-step:10727	 l-p:0.1384490728378296
epoch£º536	 i:8 	 global-step:10728	 l-p:0.12656797468662262
epoch£º536	 i:9 	 global-step:10729	 l-p:0.16680628061294556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0541, 3.0539, 3.0541],
        [3.0541, 2.2130, 2.1860],
        [3.0541, 2.2497, 2.2577],
        [3.0541, 3.0541, 3.0541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.16160473227500916 
model_pd.l_d.mean(): -25.114620208740234 
model_pd.lagr.mean(): -24.95301628112793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1321], device='cuda:0')), ('power', tensor([-24.9826], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.16160473227500916
epoch£º537	 i:1 	 global-step:10741	 l-p:0.1259133368730545
epoch£º537	 i:2 	 global-step:10742	 l-p:0.16775481402873993
epoch£º537	 i:3 	 global-step:10743	 l-p:0.1706259697675705
epoch£º537	 i:4 	 global-step:10744	 l-p:0.142752543091774
epoch£º537	 i:5 	 global-step:10745	 l-p:0.23554611206054688
epoch£º537	 i:6 	 global-step:10746	 l-p:0.13286732137203217
epoch£º537	 i:7 	 global-step:10747	 l-p:0.1021537259221077
epoch£º537	 i:8 	 global-step:10748	 l-p:0.1460161656141281
epoch£º537	 i:9 	 global-step:10749	 l-p:0.1900589019060135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9933, 2.8454, 2.9562],
        [2.9933, 2.9912, 2.9933],
        [2.9933, 2.9932, 2.9933],
        [2.9933, 2.7468, 2.9025]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.14377371966838837 
model_pd.l_d.mean(): -25.156696319580078 
model_pd.lagr.mean(): -25.012922286987305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0044], device='cuda:0')), ('power', tensor([-25.1523], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.14377371966838837
epoch£º538	 i:1 	 global-step:10761	 l-p:-0.00018414974329061806
epoch£º538	 i:2 	 global-step:10762	 l-p:0.1498192995786667
epoch£º538	 i:3 	 global-step:10763	 l-p:0.12362601608037949
epoch£º538	 i:4 	 global-step:10764	 l-p:0.12077564001083374
epoch£º538	 i:5 	 global-step:10765	 l-p:-2.170896053314209
epoch£º538	 i:6 	 global-step:10766	 l-p:0.15262804925441742
epoch£º538	 i:7 	 global-step:10767	 l-p:0.15342549979686737
epoch£º538	 i:8 	 global-step:10768	 l-p:0.16361784934997559
epoch£º538	 i:9 	 global-step:10769	 l-p:0.042504824697971344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0068, 2.5725, 2.7556],
        [3.0068, 3.0067, 3.0068],
        [3.0068, 2.9508, 2.9997],
        [3.0068, 3.0065, 3.0068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.1177472472190857 
model_pd.l_d.mean(): -25.06993865966797 
model_pd.lagr.mean(): -24.952192306518555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0146], device='cuda:0')), ('power', tensor([-25.0846], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.1177472472190857
epoch£º539	 i:1 	 global-step:10781	 l-p:0.2098826915025711
epoch£º539	 i:2 	 global-step:10782	 l-p:0.13028182089328766
epoch£º539	 i:3 	 global-step:10783	 l-p:0.1303614228963852
epoch£º539	 i:4 	 global-step:10784	 l-p:0.10855649411678314
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12050013989210129
epoch£º539	 i:6 	 global-step:10786	 l-p:0.10883782058954239
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11753034591674805
epoch£º539	 i:8 	 global-step:10788	 l-p:0.16090361773967743
epoch£º539	 i:9 	 global-step:10789	 l-p:0.17698508501052856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0013, 1.9034, 1.5498],
        [3.0013, 3.0013, 3.0013],
        [3.0013, 2.4065, 2.5534],
        [3.0013, 1.8331, 1.2525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.16726695001125336 
model_pd.l_d.mean(): -24.67047119140625 
model_pd.lagr.mean(): -24.503204345703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1860], device='cuda:0')), ('power', tensor([-24.8565], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.16726695001125336
epoch£º540	 i:1 	 global-step:10801	 l-p:-2.1982662677764893
epoch£º540	 i:2 	 global-step:10802	 l-p:0.14886774122714996
epoch£º540	 i:3 	 global-step:10803	 l-p:0.13903459906578064
epoch£º540	 i:4 	 global-step:10804	 l-p:0.08751266449689865
epoch£º540	 i:5 	 global-step:10805	 l-p:0.1322939395904541
epoch£º540	 i:6 	 global-step:10806	 l-p:0.15173311531543732
epoch£º540	 i:7 	 global-step:10807	 l-p:0.2068803906440735
epoch£º540	 i:8 	 global-step:10808	 l-p:0.2113201916217804
epoch£º540	 i:9 	 global-step:10809	 l-p:0.10523729771375656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8798, 2.7140, 2.8349],
        [2.8798, 2.7495, 2.8502],
        [2.8798, 1.9306, 1.3183],
        [2.8798, 2.8798, 2.8798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.12805147469043732 
model_pd.l_d.mean(): -24.60198402404785 
model_pd.lagr.mean(): -24.47393226623535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0869], device='cuda:0')), ('power', tensor([-24.6889], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.12805147469043732
epoch£º541	 i:1 	 global-step:10821	 l-p:0.18339622020721436
epoch£º541	 i:2 	 global-step:10822	 l-p:0.10320581495761871
epoch£º541	 i:3 	 global-step:10823	 l-p:-0.014430188573896885
epoch£º541	 i:4 	 global-step:10824	 l-p:0.15368328988552094
epoch£º541	 i:5 	 global-step:10825	 l-p:-0.29834699630737305
epoch£º541	 i:6 	 global-step:10826	 l-p:0.1886974424123764
epoch£º541	 i:7 	 global-step:10827	 l-p:0.13988468050956726
epoch£º541	 i:8 	 global-step:10828	 l-p:0.23406948149204254
epoch£º541	 i:9 	 global-step:10829	 l-p:0.14200042188167572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1695, 3.1695, 3.1695],
        [3.1695, 2.1360, 1.8557],
        [3.1695, 2.3613, 1.6914],
        [3.1695, 2.0346, 1.5920]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.10697052627801895 
model_pd.l_d.mean(): -24.322174072265625 
model_pd.lagr.mean(): -24.2152042388916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0265], device='cuda:0')), ('power', tensor([-24.3487], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.10697052627801895
epoch£º542	 i:1 	 global-step:10841	 l-p:0.12135007232427597
epoch£º542	 i:2 	 global-step:10842	 l-p:0.11996318399906158
epoch£º542	 i:3 	 global-step:10843	 l-p:0.18321755528450012
epoch£º542	 i:4 	 global-step:10844	 l-p:0.125470831990242
epoch£º542	 i:5 	 global-step:10845	 l-p:0.12365198135375977
epoch£º542	 i:6 	 global-step:10846	 l-p:0.1718854457139969
epoch£º542	 i:7 	 global-step:10847	 l-p:0.265686571598053
epoch£º542	 i:8 	 global-step:10848	 l-p:0.1596446931362152
epoch£º542	 i:9 	 global-step:10849	 l-p:0.12287315726280212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0528, 1.9222, 1.5135],
        [3.0528, 2.1892, 1.5390],
        [3.0528, 2.6876, 2.8691],
        [3.0528, 3.0473, 3.0526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.1259298324584961 
model_pd.l_d.mean(): -24.93044662475586 
model_pd.lagr.mean(): -24.804515838623047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0349], device='cuda:0')), ('power', tensor([-24.9654], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.1259298324584961
epoch£º543	 i:1 	 global-step:10861	 l-p:0.20094625651836395
epoch£º543	 i:2 	 global-step:10862	 l-p:0.16765961050987244
epoch£º543	 i:3 	 global-step:10863	 l-p:0.20707426965236664
epoch£º543	 i:4 	 global-step:10864	 l-p:0.11764264851808548
epoch£º543	 i:5 	 global-step:10865	 l-p:0.10683763027191162
epoch£º543	 i:6 	 global-step:10866	 l-p:0.1398019641637802
epoch£º543	 i:7 	 global-step:10867	 l-p:0.16004809737205505
epoch£º543	 i:8 	 global-step:10868	 l-p:0.12778444588184357
epoch£º543	 i:9 	 global-step:10869	 l-p:0.14174522459506989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9572, 2.9525, 2.9570],
        [2.9572, 1.7878, 1.3353],
        [2.9572, 2.9572, 2.9572],
        [2.9572, 1.9402, 1.3279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.06935889273881912 
model_pd.l_d.mean(): -25.131317138671875 
model_pd.lagr.mean(): -25.06195831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0160], device='cuda:0')), ('power', tensor([-25.1473], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.06935889273881912
epoch£º544	 i:1 	 global-step:10881	 l-p:0.13881529867649078
epoch£º544	 i:2 	 global-step:10882	 l-p:0.06666848808526993
epoch£º544	 i:3 	 global-step:10883	 l-p:0.13748489320278168
epoch£º544	 i:4 	 global-step:10884	 l-p:0.14561611413955688
epoch£º544	 i:5 	 global-step:10885	 l-p:0.1298971176147461
epoch£º544	 i:6 	 global-step:10886	 l-p:2.059812307357788
epoch£º544	 i:7 	 global-step:10887	 l-p:0.22480379045009613
epoch£º544	 i:8 	 global-step:10888	 l-p:0.5135793685913086
epoch£º544	 i:9 	 global-step:10889	 l-p:0.12397494167089462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0911, 3.0422, 3.0854],
        [3.0911, 2.8871, 3.0259],
        [3.0911, 2.4172, 2.5225],
        [3.0911, 2.4386, 2.5558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.1473625749349594 
model_pd.l_d.mean(): -25.183626174926758 
model_pd.lagr.mean(): -25.036264419555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0275], device='cuda:0')), ('power', tensor([-25.1561], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.1473625749349594
epoch£º545	 i:1 	 global-step:10901	 l-p:0.13109098374843597
epoch£º545	 i:2 	 global-step:10902	 l-p:0.13039955496788025
epoch£º545	 i:3 	 global-step:10903	 l-p:-0.057624634355306625
epoch£º545	 i:4 	 global-step:10904	 l-p:0.1841762810945511
epoch£º545	 i:5 	 global-step:10905	 l-p:0.1476883590221405
epoch£º545	 i:6 	 global-step:10906	 l-p:0.144503653049469
epoch£º545	 i:7 	 global-step:10907	 l-p:0.16733548045158386
epoch£º545	 i:8 	 global-step:10908	 l-p:0.12689898908138275
epoch£º545	 i:9 	 global-step:10909	 l-p:0.11632757633924484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0111, 2.8948, 2.9866],
        [3.0111, 2.9001, 2.9885],
        [3.0111, 3.0105, 3.0111],
        [3.0111, 1.9866, 1.7435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.13573527336120605 
model_pd.l_d.mean(): -24.84357261657715 
model_pd.lagr.mean(): -24.70783805847168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0403], device='cuda:0')), ('power', tensor([-24.8033], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.13573527336120605
epoch£º546	 i:1 	 global-step:10921	 l-p:0.15180401504039764
epoch£º546	 i:2 	 global-step:10922	 l-p:0.1598622053861618
epoch£º546	 i:3 	 global-step:10923	 l-p:0.17774724960327148
epoch£º546	 i:4 	 global-step:10924	 l-p:0.10176949948072433
epoch£º546	 i:5 	 global-step:10925	 l-p:1.4844584465026855
epoch£º546	 i:6 	 global-step:10926	 l-p:0.2647152543067932
epoch£º546	 i:7 	 global-step:10927	 l-p:0.47381719946861267
epoch£º546	 i:8 	 global-step:10928	 l-p:0.18290063738822937
epoch£º546	 i:9 	 global-step:10929	 l-p:0.06135598197579384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9347, 2.4339, 2.6114],
        [2.9347, 1.9969, 1.3734],
        [2.9347, 2.9306, 2.9346],
        [2.9347, 2.9347, 2.9347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.14894726872444153 
model_pd.l_d.mean(): -25.12185287475586 
model_pd.lagr.mean(): -24.9729061126709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0547], device='cuda:0')), ('power', tensor([-25.1766], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.14894726872444153
epoch£º547	 i:1 	 global-step:10941	 l-p:-0.22285106778144836
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11439359188079834
epoch£º547	 i:3 	 global-step:10943	 l-p:0.15009143948554993
epoch£º547	 i:4 	 global-step:10944	 l-p:0.12241286784410477
epoch£º547	 i:5 	 global-step:10945	 l-p:0.133029043674469
epoch£º547	 i:6 	 global-step:10946	 l-p:0.12180272489786148
epoch£º547	 i:7 	 global-step:10947	 l-p:0.12806400656700134
epoch£º547	 i:8 	 global-step:10948	 l-p:0.1346321851015091
epoch£º547	 i:9 	 global-step:10949	 l-p:0.21797309815883636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9964, 2.9955, 2.9964],
        [2.9964, 2.5568, 2.7407],
        [2.9964, 2.8595, 2.9641],
        [2.9964, 2.7909, 2.9308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.5862598419189453 
model_pd.l_d.mean(): -24.681684494018555 
model_pd.lagr.mean(): -24.09542465209961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0854], device='cuda:0')), ('power', tensor([-24.7671], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.5862598419189453
epoch£º548	 i:1 	 global-step:10961	 l-p:0.13330398499965668
epoch£º548	 i:2 	 global-step:10962	 l-p:0.1577540785074234
epoch£º548	 i:3 	 global-step:10963	 l-p:0.1291508674621582
epoch£º548	 i:4 	 global-step:10964	 l-p:0.17456424236297607
epoch£º548	 i:5 	 global-step:10965	 l-p:0.11450240761041641
epoch£º548	 i:6 	 global-step:10966	 l-p:0.1373380869626999
epoch£º548	 i:7 	 global-step:10967	 l-p:-0.6760693788528442
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12180233001708984
epoch£º548	 i:9 	 global-step:10969	 l-p:0.23792746663093567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9563, 1.9259, 1.3154],
        [2.9563, 2.9356, 2.9549],
        [2.9563, 2.9236, 2.9534],
        [2.9563, 2.1516, 2.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.2815013825893402 
model_pd.l_d.mean(): -25.087181091308594 
model_pd.lagr.mean(): -24.805679321289062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0019], device='cuda:0')), ('power', tensor([-25.0852], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.2815013825893402
epoch£º549	 i:1 	 global-step:10981	 l-p:0.12082624435424805
epoch£º549	 i:2 	 global-step:10982	 l-p:0.14465908706188202
epoch£º549	 i:3 	 global-step:10983	 l-p:0.1870798021554947
epoch£º549	 i:4 	 global-step:10984	 l-p:0.1381690502166748
epoch£º549	 i:5 	 global-step:10985	 l-p:0.0988488495349884
epoch£º549	 i:6 	 global-step:10986	 l-p:0.13930705189704895
epoch£º549	 i:7 	 global-step:10987	 l-p:0.12991248071193695
epoch£º549	 i:8 	 global-step:10988	 l-p:0.1442066878080368
epoch£º549	 i:9 	 global-step:10989	 l-p:0.1297948658466339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0890, 2.9454, 3.0538],
        [3.0890, 2.0412, 1.7566],
        [3.0890, 2.1858, 2.0965],
        [3.0890, 2.8456, 3.0003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.12925979495048523 
model_pd.l_d.mean(): -24.403823852539062 
model_pd.lagr.mean(): -24.274564743041992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1251], device='cuda:0')), ('power', tensor([-24.5289], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.12925979495048523
epoch£º550	 i:1 	 global-step:11001	 l-p:0.1977129727602005
epoch£º550	 i:2 	 global-step:11002	 l-p:0.15306401252746582
epoch£º550	 i:3 	 global-step:11003	 l-p:0.13160920143127441
epoch£º550	 i:4 	 global-step:11004	 l-p:0.12287897616624832
epoch£º550	 i:5 	 global-step:11005	 l-p:0.1527530699968338
epoch£º550	 i:6 	 global-step:11006	 l-p:0.3035517632961273
epoch£º550	 i:7 	 global-step:11007	 l-p:0.15674512088298798
epoch£º550	 i:8 	 global-step:11008	 l-p:0.31743699312210083
epoch£º550	 i:9 	 global-step:11009	 l-p:-0.2005620002746582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7646, 2.1886, 2.3528],
        [2.7646, 1.6121, 1.0763],
        [2.7646, 1.8839, 1.2718],
        [2.7646, 2.7646, 2.7646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.09622632712125778 
model_pd.l_d.mean(): -24.277013778686523 
model_pd.lagr.mean(): -24.180788040161133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2743], device='cuda:0')), ('power', tensor([-24.5513], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.09622632712125778
epoch£º551	 i:1 	 global-step:11021	 l-p:0.08759210258722305
epoch£º551	 i:2 	 global-step:11022	 l-p:0.15943209826946259
epoch£º551	 i:3 	 global-step:11023	 l-p:0.24655045568943024
epoch£º551	 i:4 	 global-step:11024	 l-p:0.09155727177858353
epoch£º551	 i:5 	 global-step:11025	 l-p:0.1416948288679123
epoch£º551	 i:6 	 global-step:11026	 l-p:0.2822403013706207
epoch£º551	 i:7 	 global-step:11027	 l-p:0.16324138641357422
epoch£º551	 i:8 	 global-step:11028	 l-p:0.12632739543914795
epoch£º551	 i:9 	 global-step:11029	 l-p:0.11121167987585068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2043, 2.1552, 1.8537],
        [3.2043, 3.2042, 3.2043],
        [3.2043, 3.2043, 3.2043],
        [3.2043, 3.2043, 3.2043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11422458291053772 
model_pd.l_d.mean(): -24.77113914489746 
model_pd.lagr.mean(): -24.65691375732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0560], device='cuda:0')), ('power', tensor([-24.7152], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11422458291053772
epoch£º552	 i:1 	 global-step:11041	 l-p:0.13555359840393066
epoch£º552	 i:2 	 global-step:11042	 l-p:0.1216411367058754
epoch£º552	 i:3 	 global-step:11043	 l-p:0.1506076455116272
epoch£º552	 i:4 	 global-step:11044	 l-p:0.1259639412164688
epoch£º552	 i:5 	 global-step:11045	 l-p:0.13332411646842957
epoch£º552	 i:6 	 global-step:11046	 l-p:0.15042535960674286
epoch£º552	 i:7 	 global-step:11047	 l-p:0.1567721664905548
epoch£º552	 i:8 	 global-step:11048	 l-p:0.13289874792099
epoch£º552	 i:9 	 global-step:11049	 l-p:0.13879045844078064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9661, 2.9661, 2.9661],
        [2.9661, 1.8927, 1.5901],
        [2.9661, 2.0626, 1.4275],
        [2.9661, 2.7715, 2.9068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.21190856397151947 
model_pd.l_d.mean(): -25.071674346923828 
model_pd.lagr.mean(): -24.859766006469727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1486], device='cuda:0')), ('power', tensor([-25.2203], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.21190856397151947
epoch£º553	 i:1 	 global-step:11061	 l-p:0.09121020138263702
epoch£º553	 i:2 	 global-step:11062	 l-p:0.08358494937419891
epoch£º553	 i:3 	 global-step:11063	 l-p:0.17076943814754486
epoch£º553	 i:4 	 global-step:11064	 l-p:0.14262014627456665
epoch£º553	 i:5 	 global-step:11065	 l-p:0.4504229724407196
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13946522772312164
epoch£º553	 i:7 	 global-step:11067	 l-p:0.134760782122612
epoch£º553	 i:8 	 global-step:11068	 l-p:0.08700574934482574
epoch£º553	 i:9 	 global-step:11069	 l-p:0.20372657477855682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9291, 2.8389, 2.9134],
        [2.9291, 2.9254, 2.9290],
        [2.9291, 2.8164, 2.9061],
        [2.9291, 1.7351, 1.2462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): -0.023034172132611275 
model_pd.l_d.mean(): -24.883995056152344 
model_pd.lagr.mean(): -24.90703010559082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0131], device='cuda:0')), ('power', tensor([-24.8971], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:-0.023034172132611275
epoch£º554	 i:1 	 global-step:11081	 l-p:0.13662990927696228
epoch£º554	 i:2 	 global-step:11082	 l-p:4.264967918395996
epoch£º554	 i:3 	 global-step:11083	 l-p:0.01239791326224804
epoch£º554	 i:4 	 global-step:11084	 l-p:0.25473839044570923
epoch£º554	 i:5 	 global-step:11085	 l-p:0.015992693603038788
epoch£º554	 i:6 	 global-step:11086	 l-p:0.11181171238422394
epoch£º554	 i:7 	 global-step:11087	 l-p:0.12992015480995178
epoch£º554	 i:8 	 global-step:11088	 l-p:0.11294135451316833
epoch£º554	 i:9 	 global-step:11089	 l-p:0.1242956817150116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1798, 3.1797, 3.1798],
        [3.1798, 2.8255, 3.0056],
        [3.1798, 3.1267, 3.1733],
        [3.1798, 2.7403, 2.9219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.12550993263721466 
model_pd.l_d.mean(): -24.98151206970215 
model_pd.lagr.mean(): -24.856002807617188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0872], device='cuda:0')), ('power', tensor([-24.8943], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.12550993263721466
epoch£º555	 i:1 	 global-step:11101	 l-p:0.14601168036460876
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10473290085792542
epoch£º555	 i:3 	 global-step:11103	 l-p:0.13415928184986115
epoch£º555	 i:4 	 global-step:11104	 l-p:0.1626879870891571
epoch£º555	 i:5 	 global-step:11105	 l-p:0.1091674417257309
epoch£º555	 i:6 	 global-step:11106	 l-p:0.18888019025325775
epoch£º555	 i:7 	 global-step:11107	 l-p:-0.9078879952430725
epoch£º555	 i:8 	 global-step:11108	 l-p:0.12644566595554352
epoch£º555	 i:9 	 global-step:11109	 l-p:0.17036937177181244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9489, 1.9678, 1.8004],
        [2.9489, 2.9489, 2.9489],
        [2.9489, 1.7531, 1.2003],
        [2.9489, 2.9414, 2.9486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.6298608779907227 
model_pd.l_d.mean(): -25.170238494873047 
model_pd.lagr.mean(): -24.54037857055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0576], device='cuda:0')), ('power', tensor([-25.2278], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.6298608779907227
epoch£º556	 i:1 	 global-step:11121	 l-p:0.03643084689974785
epoch£º556	 i:2 	 global-step:11122	 l-p:0.21247871220111847
epoch£º556	 i:3 	 global-step:11123	 l-p:0.12487674504518509
epoch£º556	 i:4 	 global-step:11124	 l-p:0.14210397005081177
epoch£º556	 i:5 	 global-step:11125	 l-p:0.13984321057796478
epoch£º556	 i:6 	 global-step:11126	 l-p:0.22605997323989868
epoch£º556	 i:7 	 global-step:11127	 l-p:0.12780432403087616
epoch£º556	 i:8 	 global-step:11128	 l-p:0.1153460219502449
epoch£º556	 i:9 	 global-step:11129	 l-p:0.135801300406456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0692, 3.0268, 3.0647],
        [3.0692, 1.9184, 1.4894],
        [3.0692, 2.8014, 2.9646],
        [3.0692, 1.8687, 1.2880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.17353910207748413 
model_pd.l_d.mean(): -24.949140548706055 
model_pd.lagr.mean(): -24.775602340698242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1208], device='cuda:0')), ('power', tensor([-25.0700], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.17353910207748413
epoch£º557	 i:1 	 global-step:11141	 l-p:0.13338743150234222
epoch£º557	 i:2 	 global-step:11142	 l-p:0.18633705377578735
epoch£º557	 i:3 	 global-step:11143	 l-p:0.13394120335578918
epoch£º557	 i:4 	 global-step:11144	 l-p:0.12624312937259674
epoch£º557	 i:5 	 global-step:11145	 l-p:0.08585912734270096
epoch£º557	 i:6 	 global-step:11146	 l-p:0.024937180802226067
epoch£º557	 i:7 	 global-step:11147	 l-p:0.12547636032104492
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11603928357362747
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12246358394622803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9859, 2.0409, 1.9175],
        [2.9859, 1.7812, 1.2314],
        [2.9859, 2.8621, 2.9589],
        [2.9859, 2.9581, 2.9837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): -0.20548568665981293 
model_pd.l_d.mean(): -24.88887596130371 
model_pd.lagr.mean(): -25.094362258911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0574], device='cuda:0')), ('power', tensor([-24.9463], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:-0.20548568665981293
epoch£º558	 i:1 	 global-step:11161	 l-p:0.35840940475463867
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12765586376190186
epoch£º558	 i:3 	 global-step:11163	 l-p:0.13666638731956482
epoch£º558	 i:4 	 global-step:11164	 l-p:0.09199081361293793
epoch£º558	 i:5 	 global-step:11165	 l-p:0.1504935771226883
epoch£º558	 i:6 	 global-step:11166	 l-p:0.13051700592041016
epoch£º558	 i:7 	 global-step:11167	 l-p:0.1898137480020523
epoch£º558	 i:8 	 global-step:11168	 l-p:0.20828568935394287
epoch£º558	 i:9 	 global-step:11169	 l-p:0.17602157592773438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9099, 1.7152, 1.2316],
        [2.9099, 2.8728, 2.9064],
        [2.9099, 2.9098, 2.9099],
        [2.9099, 2.9072, 2.9099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.15842050313949585 
model_pd.l_d.mean(): -24.950439453125 
model_pd.lagr.mean(): -24.79201889038086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2367], device='cuda:0')), ('power', tensor([-25.1871], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.15842050313949585
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15560495853424072
epoch£º559	 i:2 	 global-step:11182	 l-p:0.1828647255897522
epoch£º559	 i:3 	 global-step:11183	 l-p:0.15604358911514282
epoch£º559	 i:4 	 global-step:11184	 l-p:0.14980489015579224
epoch£º559	 i:5 	 global-step:11185	 l-p:-0.01906798779964447
epoch£º559	 i:6 	 global-step:11186	 l-p:-18.386463165283203
epoch£º559	 i:7 	 global-step:11187	 l-p:0.13546521961688995
epoch£º559	 i:8 	 global-step:11188	 l-p:0.24520441889762878
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11481078714132309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[3.0638, 2.2280, 1.5691],
        [3.0638, 1.9748, 1.6419],
        [3.0638, 2.2327, 1.5732],
        [3.0638, 1.8580, 1.2819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.051536377519369125 
model_pd.l_d.mean(): -24.41419219970703 
model_pd.lagr.mean(): -24.362655639648438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0790], device='cuda:0')), ('power', tensor([-24.4932], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.051536377519369125
epoch£º560	 i:1 	 global-step:11201	 l-p:0.17540547251701355
epoch£º560	 i:2 	 global-step:11202	 l-p:0.1282554268836975
epoch£º560	 i:3 	 global-step:11203	 l-p:0.12097063660621643
epoch£º560	 i:4 	 global-step:11204	 l-p:0.1282762736082077
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11301003396511078
epoch£º560	 i:6 	 global-step:11206	 l-p:0.14614585041999817
epoch£º560	 i:7 	 global-step:11207	 l-p:0.22578498721122742
epoch£º560	 i:8 	 global-step:11208	 l-p:0.17945100367069244
epoch£º560	 i:9 	 global-step:11209	 l-p:0.143671452999115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9108, 2.9108, 2.9108],
        [2.9108, 1.8527, 1.5869],
        [2.9108, 2.8726, 2.9070],
        [2.9108, 1.7184, 1.1680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.04842712730169296 
model_pd.l_d.mean(): -24.839765548706055 
model_pd.lagr.mean(): -24.791337966918945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0811], device='cuda:0')), ('power', tensor([-24.9209], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.04842712730169296
epoch£º561	 i:1 	 global-step:11221	 l-p:0.18506081402301788
epoch£º561	 i:2 	 global-step:11222	 l-p:0.25836601853370667
epoch£º561	 i:3 	 global-step:11223	 l-p:0.13056884706020355
epoch£º561	 i:4 	 global-step:11224	 l-p:0.3286248445510864
epoch£º561	 i:5 	 global-step:11225	 l-p:0.16080917418003082
epoch£º561	 i:6 	 global-step:11226	 l-p:0.14114603400230408
epoch£º561	 i:7 	 global-step:11227	 l-p:0.5298174023628235
epoch£º561	 i:8 	 global-step:11228	 l-p:0.09987569600343704
epoch£º561	 i:9 	 global-step:11229	 l-p:0.17383737862110138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8689, 2.7005, 2.8233],
        [2.8689, 2.8667, 2.8689],
        [2.8689, 2.8687, 2.8689],
        [2.8689, 1.6845, 1.2323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.3548576533794403 
model_pd.l_d.mean(): -24.874914169311523 
model_pd.lagr.mean(): -24.520055770874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0593], device='cuda:0')), ('power', tensor([-24.9343], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.3548576533794403
epoch£º562	 i:1 	 global-step:11241	 l-p:0.15159879624843597
epoch£º562	 i:2 	 global-step:11242	 l-p:1.6885297298431396
epoch£º562	 i:3 	 global-step:11243	 l-p:0.07179398089647293
epoch£º562	 i:4 	 global-step:11244	 l-p:0.11177770048379898
epoch£º562	 i:5 	 global-step:11245	 l-p:0.13153481483459473
epoch£º562	 i:6 	 global-step:11246	 l-p:0.13344138860702515
epoch£º562	 i:7 	 global-step:11247	 l-p:0.11601091176271439
epoch£º562	 i:8 	 global-step:11248	 l-p:0.14853864908218384
epoch£º562	 i:9 	 global-step:11249	 l-p:0.1615617573261261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1065, 3.0779, 3.1042],
        [3.1065, 2.6286, 2.8092],
        [3.1065, 2.7892, 2.9651],
        [3.1065, 2.9680, 3.0736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.41300979256629944 
model_pd.l_d.mean(): -25.11037826538086 
model_pd.lagr.mean(): -24.697368621826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0118], device='cuda:0')), ('power', tensor([-25.0986], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.41300979256629944
epoch£º563	 i:1 	 global-step:11261	 l-p:0.12741783261299133
epoch£º563	 i:2 	 global-step:11262	 l-p:0.13402636349201202
epoch£º563	 i:3 	 global-step:11263	 l-p:0.1358673870563507
epoch£º563	 i:4 	 global-step:11264	 l-p:0.1433771550655365
epoch£º563	 i:5 	 global-step:11265	 l-p:0.2808285057544708
epoch£º563	 i:6 	 global-step:11266	 l-p:0.14056791365146637
epoch£º563	 i:7 	 global-step:11267	 l-p:0.15693970024585724
epoch£º563	 i:8 	 global-step:11268	 l-p:0.11790426075458527
epoch£º563	 i:9 	 global-step:11269	 l-p:0.1532280445098877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[2.8743, 1.7091, 1.2942],
        [2.8743, 1.6795, 1.1409],
        [2.8743, 1.9185, 1.7958],
        [2.8743, 2.3133, 2.4823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.10017001628875732 
model_pd.l_d.mean(): -24.844051361083984 
model_pd.lagr.mean(): -24.743881225585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1016], device='cuda:0')), ('power', tensor([-24.9457], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.10017001628875732
epoch£º564	 i:1 	 global-step:11281	 l-p:0.337735116481781
epoch£º564	 i:2 	 global-step:11282	 l-p:0.14464645087718964
epoch£º564	 i:3 	 global-step:11283	 l-p:0.15262438356876373
epoch£º564	 i:4 	 global-step:11284	 l-p:0.171746626496315
epoch£º564	 i:5 	 global-step:11285	 l-p:0.2526673376560211
epoch£º564	 i:6 	 global-step:11286	 l-p:0.10772246867418289
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12123585492372513
epoch£º564	 i:8 	 global-step:11288	 l-p:0.14434872567653656
epoch£º564	 i:9 	 global-step:11289	 l-p:0.15342654287815094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1650, 3.1456, 3.1638],
        [3.1650, 2.6900, 2.8702],
        [3.1650, 2.2851, 1.6179],
        [3.1650, 2.2891, 1.6215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.11407867074012756 
model_pd.l_d.mean(): -24.882747650146484 
model_pd.lagr.mean(): -24.76866912841797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0043], device='cuda:0')), ('power', tensor([-24.8785], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.11407867074012756
epoch£º565	 i:1 	 global-step:11301	 l-p:0.15267594158649445
epoch£º565	 i:2 	 global-step:11302	 l-p:0.11816787719726562
epoch£º565	 i:3 	 global-step:11303	 l-p:0.12343629449605942
epoch£º565	 i:4 	 global-step:11304	 l-p:0.14041604101657867
epoch£º565	 i:5 	 global-step:11305	 l-p:0.06415969878435135
epoch£º565	 i:6 	 global-step:11306	 l-p:0.1379939764738083
epoch£º565	 i:7 	 global-step:11307	 l-p:0.1423937976360321
epoch£º565	 i:8 	 global-step:11308	 l-p:0.19277942180633545
epoch£º565	 i:9 	 global-step:11309	 l-p:0.08315826207399368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9025, 2.8488, 2.8959],
        [2.9025, 2.8959, 2.9023],
        [2.9025, 2.4392, 2.6265],
        [2.9025, 2.6887, 2.8333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.06034858524799347 
model_pd.l_d.mean(): -25.032100677490234 
model_pd.lagr.mean(): -24.971752166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0771], device='cuda:0')), ('power', tensor([-25.1092], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.06034858524799347
epoch£º566	 i:1 	 global-step:11321	 l-p:0.12737824022769928
epoch£º566	 i:2 	 global-step:11322	 l-p:0.11721689254045486
epoch£º566	 i:3 	 global-step:11323	 l-p:0.21074004471302032
epoch£º566	 i:4 	 global-step:11324	 l-p:0.20546817779541016
epoch£º566	 i:5 	 global-step:11325	 l-p:0.12747223675251007
epoch£º566	 i:6 	 global-step:11326	 l-p:0.1811375617980957
epoch£º566	 i:7 	 global-step:11327	 l-p:0.20504321157932281
epoch£º566	 i:8 	 global-step:11328	 l-p:0.17269167304039001
epoch£º566	 i:9 	 global-step:11329	 l-p:0.12995143234729767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[3.1521, 2.2832, 1.6160],
        [3.1521, 2.2356, 1.5738],
        [3.1521, 2.5495, 2.6954],
        [3.1521, 2.1054, 1.4599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.12222162634134293 
model_pd.l_d.mean(): -24.351966857910156 
model_pd.lagr.mean(): -24.229745864868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0986], device='cuda:0')), ('power', tensor([-24.4506], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.12222162634134293
epoch£º567	 i:1 	 global-step:11341	 l-p:0.12962406873703003
epoch£º567	 i:2 	 global-step:11342	 l-p:0.12355425208806992
epoch£º567	 i:3 	 global-step:11343	 l-p:0.1256536841392517
epoch£º567	 i:4 	 global-step:11344	 l-p:0.36276811361312866
epoch£º567	 i:5 	 global-step:11345	 l-p:0.12791146337985992
epoch£º567	 i:6 	 global-step:11346	 l-p:0.18967869877815247
epoch£º567	 i:7 	 global-step:11347	 l-p:0.1452006846666336
epoch£º567	 i:8 	 global-step:11348	 l-p:0.14570572972297668
epoch£º567	 i:9 	 global-step:11349	 l-p:0.008449721150100231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9737, 1.8876, 1.5802],
        [2.9737, 2.9369, 2.9702],
        [2.9737, 1.8392, 1.2441],
        [2.9737, 1.8518, 1.4942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.10853046178817749 
model_pd.l_d.mean(): -24.99748992919922 
model_pd.lagr.mean(): -24.888959884643555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1379], device='cuda:0')), ('power', tensor([-25.1354], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.10853046178817749
epoch£º568	 i:1 	 global-step:11361	 l-p:0.17700645327568054
epoch£º568	 i:2 	 global-step:11362	 l-p:0.12440957874059677
epoch£º568	 i:3 	 global-step:11363	 l-p:0.12465843558311462
epoch£º568	 i:4 	 global-step:11364	 l-p:0.2579748034477234
epoch£º568	 i:5 	 global-step:11365	 l-p:0.1450517624616623
epoch£º568	 i:6 	 global-step:11366	 l-p:0.04127161577343941
epoch£º568	 i:7 	 global-step:11367	 l-p:0.005220532417297363
epoch£º568	 i:8 	 global-step:11368	 l-p:0.3739166259765625
epoch£º568	 i:9 	 global-step:11369	 l-p:0.1300843358039856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8182, 2.8181, 2.8182],
        [2.8182, 2.6836, 2.7875],
        [2.8182, 1.9232, 1.3028],
        [2.8182, 2.8182, 2.8182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.15175099670886993 
model_pd.l_d.mean(): -24.802030563354492 
model_pd.lagr.mean(): -24.650279998779297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1673], device='cuda:0')), ('power', tensor([-24.9694], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.15175099670886993
epoch£º569	 i:1 	 global-step:11381	 l-p:0.18240533769130707
epoch£º569	 i:2 	 global-step:11382	 l-p:0.29073354601860046
epoch£º569	 i:3 	 global-step:11383	 l-p:0.20156171917915344
epoch£º569	 i:4 	 global-step:11384	 l-p:0.0849931538105011
epoch£º569	 i:5 	 global-step:11385	 l-p:0.1652321070432663
epoch£º569	 i:6 	 global-step:11386	 l-p:0.1464204043149948
epoch£º569	 i:7 	 global-step:11387	 l-p:0.1672441065311432
epoch£º569	 i:8 	 global-step:11388	 l-p:0.17187240719795227
epoch£º569	 i:9 	 global-step:11389	 l-p:0.12667499482631683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0519, 3.0106, 3.0476],
        [3.0519, 3.0519, 3.0519],
        [3.0519, 2.0023, 1.7415],
        [3.0519, 1.9785, 1.3542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.13140803575515747 
model_pd.l_d.mean(): -24.852066040039062 
model_pd.lagr.mean(): -24.720657348632812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0274], device='cuda:0')), ('power', tensor([-24.8247], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.13140803575515747
epoch£º570	 i:1 	 global-step:11401	 l-p:0.1382863074541092
epoch£º570	 i:2 	 global-step:11402	 l-p:0.28447225689888
epoch£º570	 i:3 	 global-step:11403	 l-p:0.4638335406780243
epoch£º570	 i:4 	 global-step:11404	 l-p:0.1320793628692627
epoch£º570	 i:5 	 global-step:11405	 l-p:0.15229089558124542
epoch£º570	 i:6 	 global-step:11406	 l-p:0.1416550725698471
epoch£º570	 i:7 	 global-step:11407	 l-p:0.14142648875713348
epoch£º570	 i:8 	 global-step:11408	 l-p:0.1003933772444725
epoch£º570	 i:9 	 global-step:11409	 l-p:0.21276015043258667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8898, 2.0288, 2.0162],
        [2.8898, 2.8897, 2.8898],
        [2.8898, 2.7639, 2.8623],
        [2.8898, 2.8685, 2.8884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.23848022520542145 
model_pd.l_d.mean(): -24.714275360107422 
model_pd.lagr.mean(): -24.47579574584961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1197], device='cuda:0')), ('power', tensor([-24.8340], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.23848022520542145
epoch£º571	 i:1 	 global-step:11421	 l-p:0.07482101768255234
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11947160214185715
epoch£º571	 i:3 	 global-step:11423	 l-p:-1.28278648853302
epoch£º571	 i:4 	 global-step:11424	 l-p:0.08655852824449539
epoch£º571	 i:5 	 global-step:11425	 l-p:0.13240782916545868
epoch£º571	 i:6 	 global-step:11426	 l-p:0.15437452495098114
epoch£º571	 i:7 	 global-step:11427	 l-p:0.20105361938476562
epoch£º571	 i:8 	 global-step:11428	 l-p:0.14254023134708405
epoch£º571	 i:9 	 global-step:11429	 l-p:0.12229280918836594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0006, 2.9969, 3.0005],
        [3.0006, 2.1529, 1.4995],
        [3.0006, 2.0868, 1.4437],
        [3.0006, 2.9503, 2.9948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.13100403547286987 
model_pd.l_d.mean(): -24.773096084594727 
model_pd.lagr.mean(): -24.642091751098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1114], device='cuda:0')), ('power', tensor([-24.8845], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.13100403547286987
epoch£º572	 i:1 	 global-step:11441	 l-p:0.07939820736646652
epoch£º572	 i:2 	 global-step:11442	 l-p:-1.065921425819397
epoch£º572	 i:3 	 global-step:11443	 l-p:0.09872858971357346
epoch£º572	 i:4 	 global-step:11444	 l-p:0.13100053369998932
epoch£º572	 i:5 	 global-step:11445	 l-p:0.13314969837665558
epoch£º572	 i:6 	 global-step:11446	 l-p:0.16254942119121552
epoch£º572	 i:7 	 global-step:11447	 l-p:-3.4552035331726074
epoch£º572	 i:8 	 global-step:11448	 l-p:0.12738482654094696
epoch£º572	 i:9 	 global-step:11449	 l-p:0.17586329579353333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9550, 2.7138, 2.8697],
        [2.9550, 2.2211, 2.3087],
        [2.9550, 2.6098, 2.7933],
        [2.9550, 2.9540, 2.9550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.15879647433757782 
model_pd.l_d.mean(): -24.787342071533203 
model_pd.lagr.mean(): -24.6285457611084 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0497], device='cuda:0')), ('power', tensor([-24.8371], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.15879647433757782
epoch£º573	 i:1 	 global-step:11461	 l-p:0.14920459687709808
epoch£º573	 i:2 	 global-step:11462	 l-p:0.1830737590789795
epoch£º573	 i:3 	 global-step:11463	 l-p:0.23266969621181488
epoch£º573	 i:4 	 global-step:11464	 l-p:0.156060591340065
epoch£º573	 i:5 	 global-step:11465	 l-p:0.12726037204265594
epoch£º573	 i:6 	 global-step:11466	 l-p:0.14356780052185059
epoch£º573	 i:7 	 global-step:11467	 l-p:0.12359418720006943
epoch£º573	 i:8 	 global-step:11468	 l-p:0.13110218942165375
epoch£º573	 i:9 	 global-step:11469	 l-p:0.08557698875665665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1458, 2.1312, 1.9157],
        [3.1458, 1.9391, 1.3316],
        [3.1458, 3.1458, 3.1458],
        [3.1458, 3.1305, 3.1449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.12763413786888123 
model_pd.l_d.mean(): -24.980112075805664 
model_pd.lagr.mean(): -24.85247802734375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0430], device='cuda:0')), ('power', tensor([-24.9371], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.12763413786888123
epoch£º574	 i:1 	 global-step:11481	 l-p:0.13312219083309174
epoch£º574	 i:2 	 global-step:11482	 l-p:0.14378564059734344
epoch£º574	 i:3 	 global-step:11483	 l-p:-0.03362835943698883
epoch£º574	 i:4 	 global-step:11484	 l-p:0.3394087851047516
epoch£º574	 i:5 	 global-step:11485	 l-p:0.16053886711597443
epoch£º574	 i:6 	 global-step:11486	 l-p:0.10640179365873337
epoch£º574	 i:7 	 global-step:11487	 l-p:0.13643048703670502
epoch£º574	 i:8 	 global-step:11488	 l-p:0.1593499779701233
epoch£º574	 i:9 	 global-step:11489	 l-p:0.1195346787571907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9836, 2.9361, 2.9783],
        [2.9836, 2.2885, 2.3997],
        [2.9836, 2.9836, 2.9836],
        [2.9836, 2.9297, 2.9770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.06325837224721909 
model_pd.l_d.mean(): -25.084171295166016 
model_pd.lagr.mean(): -25.020912170410156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1182], device='cuda:0')), ('power', tensor([-25.2024], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.06325837224721909
epoch£º575	 i:1 	 global-step:11501	 l-p:0.07006660103797913
epoch£º575	 i:2 	 global-step:11502	 l-p:-0.038989074528217316
epoch£º575	 i:3 	 global-step:11503	 l-p:0.11530277878046036
epoch£º575	 i:4 	 global-step:11504	 l-p:0.15062406659126282
epoch£º575	 i:5 	 global-step:11505	 l-p:0.13105899095535278
epoch£º575	 i:6 	 global-step:11506	 l-p:0.13046376407146454
epoch£º575	 i:7 	 global-step:11507	 l-p:0.1820618063211441
epoch£º575	 i:8 	 global-step:11508	 l-p:0.8953348398208618
epoch£º575	 i:9 	 global-step:11509	 l-p:0.16120648384094238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9695, 2.9684, 2.9695],
        [2.9695, 2.9560, 2.9689],
        [2.9695, 2.0289, 1.3936],
        [2.9695, 1.7542, 1.2499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.08430177718400955 
model_pd.l_d.mean(): -25.04738426208496 
model_pd.lagr.mean(): -24.963083267211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0772], device='cuda:0')), ('power', tensor([-25.1246], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.08430177718400955
epoch£º576	 i:1 	 global-step:11521	 l-p:0.15367092192173004
epoch£º576	 i:2 	 global-step:11522	 l-p:0.14826121926307678
epoch£º576	 i:3 	 global-step:11523	 l-p:0.06175868958234787
epoch£º576	 i:4 	 global-step:11524	 l-p:0.14051593840122223
epoch£º576	 i:5 	 global-step:11525	 l-p:0.1589953899383545
epoch£º576	 i:6 	 global-step:11526	 l-p:0.18158014118671417
epoch£º576	 i:7 	 global-step:11527	 l-p:0.056750331073999405
epoch£º576	 i:8 	 global-step:11528	 l-p:0.07787109166383743
epoch£º576	 i:9 	 global-step:11529	 l-p:0.15909549593925476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0250, 2.5685, 2.7562],
        [3.0250, 3.0250, 3.0250],
        [3.0250, 1.8314, 1.2429],
        [3.0250, 3.0250, 3.0250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.5099191665649414 
model_pd.l_d.mean(): -25.174224853515625 
model_pd.lagr.mean(): -24.664306640625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0149], device='cuda:0')), ('power', tensor([-25.1891], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.5099191665649414
epoch£º577	 i:1 	 global-step:11541	 l-p:0.14945144951343536
epoch£º577	 i:2 	 global-step:11542	 l-p:0.32064953446388245
epoch£º577	 i:3 	 global-step:11543	 l-p:0.1053086444735527
epoch£º577	 i:4 	 global-step:11544	 l-p:0.1181797981262207
epoch£º577	 i:5 	 global-step:11545	 l-p:0.1274503618478775
epoch£º577	 i:6 	 global-step:11546	 l-p:0.13859152793884277
epoch£º577	 i:7 	 global-step:11547	 l-p:0.11824139207601547
epoch£º577	 i:8 	 global-step:11548	 l-p:0.11584760248661041
epoch£º577	 i:9 	 global-step:11549	 l-p:0.1602906584739685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0114, 3.0115, 3.0114],
        [3.0114, 1.8089, 1.2283],
        [3.0114, 2.2129, 2.2523],
        [3.0114, 1.8929, 1.5463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.20920421183109283 
model_pd.l_d.mean(): -25.110706329345703 
model_pd.lagr.mean(): -24.90150260925293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0199], device='cuda:0')), ('power', tensor([-25.1306], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.20920421183109283
epoch£º578	 i:1 	 global-step:11561	 l-p:0.11405505985021591
epoch£º578	 i:2 	 global-step:11562	 l-p:0.28409308195114136
epoch£º578	 i:3 	 global-step:11563	 l-p:0.11653178930282593
epoch£º578	 i:4 	 global-step:11564	 l-p:0.11448320001363754
epoch£º578	 i:5 	 global-step:11565	 l-p:0.15144574642181396
epoch£º578	 i:6 	 global-step:11566	 l-p:0.15270400047302246
epoch£º578	 i:7 	 global-step:11567	 l-p:0.14642615616321564
epoch£º578	 i:8 	 global-step:11568	 l-p:0.18496447801589966
epoch£º578	 i:9 	 global-step:11569	 l-p:0.09318322688341141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9518, 2.0645, 1.4217],
        [2.9518, 2.6266, 2.8072],
        [2.9518, 1.8544, 1.5475],
        [2.9518, 2.7225, 2.8741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.15391792356967926 
model_pd.l_d.mean(): -25.110647201538086 
model_pd.lagr.mean(): -24.956729888916016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0558], device='cuda:0')), ('power', tensor([-25.1665], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.15391792356967926
epoch£º579	 i:1 	 global-step:11581	 l-p:0.2646206021308899
epoch£º579	 i:2 	 global-step:11582	 l-p:0.14133769273757935
epoch£º579	 i:3 	 global-step:11583	 l-p:-0.009736742824316025
epoch£º579	 i:4 	 global-step:11584	 l-p:0.22810693085193634
epoch£º579	 i:5 	 global-step:11585	 l-p:0.14418289065361023
epoch£º579	 i:6 	 global-step:11586	 l-p:0.1420297622680664
epoch£º579	 i:7 	 global-step:11587	 l-p:0.0959206148982048
epoch£º579	 i:8 	 global-step:11588	 l-p:0.13479529321193695
epoch£º579	 i:9 	 global-step:11589	 l-p:0.15041682124137878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0938, 3.0127, 3.0808],
        [3.0938, 3.0716, 3.0923],
        [3.0938, 1.8675, 1.3293],
        [3.0938, 2.9896, 3.0738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.16317927837371826 
model_pd.l_d.mean(): -25.238405227661133 
model_pd.lagr.mean(): -25.075225830078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0851], device='cuda:0')), ('power', tensor([-25.1533], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.16317927837371826
epoch£º580	 i:1 	 global-step:11601	 l-p:0.13797180354595184
epoch£º580	 i:2 	 global-step:11602	 l-p:0.1314503699541092
epoch£º580	 i:3 	 global-step:11603	 l-p:-0.31948286294937134
epoch£º580	 i:4 	 global-step:11604	 l-p:0.7115955352783203
epoch£º580	 i:5 	 global-step:11605	 l-p:0.1683228313922882
epoch£º580	 i:6 	 global-step:11606	 l-p:0.15647822618484497
epoch£º580	 i:7 	 global-step:11607	 l-p:0.50772625207901
epoch£º580	 i:8 	 global-step:11608	 l-p:0.07596028596162796
epoch£º580	 i:9 	 global-step:11609	 l-p:0.11732292920351028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1568, 3.1568, 3.1568],
        [3.1568, 2.0285, 1.3920],
        [3.1568, 2.0907, 1.8049],
        [3.1568, 2.7325, 2.9200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.1307218074798584 
model_pd.l_d.mean(): -24.942365646362305 
model_pd.lagr.mean(): -24.811643600463867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0142], device='cuda:0')), ('power', tensor([-24.9282], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.1307218074798584
epoch£º581	 i:1 	 global-step:11621	 l-p:0.10225681215524673
epoch£º581	 i:2 	 global-step:11622	 l-p:0.1192830502986908
epoch£º581	 i:3 	 global-step:11623	 l-p:0.12268762290477753
epoch£º581	 i:4 	 global-step:11624	 l-p:0.14682723581790924
epoch£º581	 i:5 	 global-step:11625	 l-p:0.2149200439453125
epoch£º581	 i:6 	 global-step:11626	 l-p:0.3290150761604309
epoch£º581	 i:7 	 global-step:11627	 l-p:0.14408019185066223
epoch£º581	 i:8 	 global-step:11628	 l-p:0.12418748438358307
epoch£º581	 i:9 	 global-step:11629	 l-p:0.1442326009273529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9761, 2.7674, 2.9102],
        [2.9761, 2.0589, 1.4174],
        [2.9761, 2.9747, 2.9761],
        [2.9761, 2.9342, 2.9718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.1492615044116974 
model_pd.l_d.mean(): -24.38081932067871 
model_pd.lagr.mean(): -24.231557846069336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1539], device='cuda:0')), ('power', tensor([-24.5347], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.1492615044116974
epoch£º582	 i:1 	 global-step:11641	 l-p:0.12024195492267609
epoch£º582	 i:2 	 global-step:11642	 l-p:0.12616457045078278
epoch£º582	 i:3 	 global-step:11643	 l-p:0.14511151611804962
epoch£º582	 i:4 	 global-step:11644	 l-p:0.15018759667873383
epoch£º582	 i:5 	 global-step:11645	 l-p:1.8085416555404663
epoch£º582	 i:6 	 global-step:11646	 l-p:-0.2425035983324051
epoch£º582	 i:7 	 global-step:11647	 l-p:0.08570481836795807
epoch£º582	 i:8 	 global-step:11648	 l-p:0.17973190546035767
epoch£º582	 i:9 	 global-step:11649	 l-p:0.136164590716362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1151, 2.9785, 3.0834],
        [3.1151, 1.9323, 1.3166],
        [3.1151, 1.9444, 1.3250],
        [3.1151, 1.9725, 1.5793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.12444505095481873 
model_pd.l_d.mean(): -25.06089973449707 
model_pd.lagr.mean(): -24.93645477294922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0270], device='cuda:0')), ('power', tensor([-25.0879], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.12444505095481873
epoch£º583	 i:1 	 global-step:11661	 l-p:0.12366484105587006
epoch£º583	 i:2 	 global-step:11662	 l-p:0.14018143713474274
epoch£º583	 i:3 	 global-step:11663	 l-p:0.13936449587345123
epoch£º583	 i:4 	 global-step:11664	 l-p:0.18549677729606628
epoch£º583	 i:5 	 global-step:11665	 l-p:0.14175663888454437
epoch£º583	 i:6 	 global-step:11666	 l-p:0.1321157068014145
epoch£º583	 i:7 	 global-step:11667	 l-p:-0.01270091999322176
epoch£º583	 i:8 	 global-step:11668	 l-p:-0.05912887305021286
epoch£º583	 i:9 	 global-step:11669	 l-p:0.13717912137508392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0410, 2.9592, 3.0278],
        [3.0410, 3.0410, 3.0410],
        [3.0410, 2.8145, 2.9649],
        [3.0410, 2.7660, 2.9336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.14209771156311035 
model_pd.l_d.mean(): -25.08702278137207 
model_pd.lagr.mean(): -24.94492530822754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0257], device='cuda:0')), ('power', tensor([-25.0614], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.14209771156311035
epoch£º584	 i:1 	 global-step:11681	 l-p:0.17250581085681915
epoch£º584	 i:2 	 global-step:11682	 l-p:0.16527165472507477
epoch£º584	 i:3 	 global-step:11683	 l-p:0.1627102643251419
epoch£º584	 i:4 	 global-step:11684	 l-p:0.10654456913471222
epoch£º584	 i:5 	 global-step:11685	 l-p:0.14485716819763184
epoch£º584	 i:6 	 global-step:11686	 l-p:0.12844106554985046
epoch£º584	 i:7 	 global-step:11687	 l-p:0.11755328625440598
epoch£º584	 i:8 	 global-step:11688	 l-p:0.1563047617673874
epoch£º584	 i:9 	 global-step:11689	 l-p:0.1089729592204094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0359, 2.7155, 2.8951],
        [3.0359, 3.0076, 3.0337],
        [3.0359, 3.0300, 3.0357],
        [3.0359, 2.9986, 3.0324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.16819097101688385 
model_pd.l_d.mean(): -25.02249526977539 
model_pd.lagr.mean(): -24.85430335998535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0407], device='cuda:0')), ('power', tensor([-24.9818], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.16819097101688385
epoch£º585	 i:1 	 global-step:11701	 l-p:0.1318729817867279
epoch£º585	 i:2 	 global-step:11702	 l-p:0.2576429545879364
epoch£º585	 i:3 	 global-step:11703	 l-p:-0.0065293810330331326
epoch£º585	 i:4 	 global-step:11704	 l-p:0.13250406086444855
epoch£º585	 i:5 	 global-step:11705	 l-p:0.13452593982219696
epoch£º585	 i:6 	 global-step:11706	 l-p:0.15530213713645935
epoch£º585	 i:7 	 global-step:11707	 l-p:0.12790173292160034
epoch£º585	 i:8 	 global-step:11708	 l-p:0.15572786331176758
epoch£º585	 i:9 	 global-step:11709	 l-p:0.10259316861629486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9325, 2.9301, 2.9325],
        [2.9325, 2.0080, 1.3718],
        [2.9325, 2.2778, 2.4177],
        [2.9325, 2.8038, 2.9042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.1805369108915329 
model_pd.l_d.mean(): -25.00693130493164 
model_pd.lagr.mean(): -24.82639503479004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0669], device='cuda:0')), ('power', tensor([-25.0739], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.1805369108915329
epoch£º586	 i:1 	 global-step:11721	 l-p:0.19538894295692444
epoch£º586	 i:2 	 global-step:11722	 l-p:0.10682418942451477
epoch£º586	 i:3 	 global-step:11723	 l-p:0.20605288445949554
epoch£º586	 i:4 	 global-step:11724	 l-p:-0.7433699369430542
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11055015027523041
epoch£º586	 i:6 	 global-step:11726	 l-p:0.1460537165403366
epoch£º586	 i:7 	 global-step:11727	 l-p:0.12599940598011017
epoch£º586	 i:8 	 global-step:11728	 l-p:0.2736189067363739
epoch£º586	 i:9 	 global-step:11729	 l-p:0.13189628720283508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1385, 2.0596, 1.7668],
        [3.1385, 1.9341, 1.3196],
        [3.1385, 1.9093, 1.3114],
        [3.1385, 2.9169, 3.0650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.11213494837284088 
model_pd.l_d.mean(): -24.940366744995117 
model_pd.lagr.mean(): -24.828231811523438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0199], device='cuda:0')), ('power', tensor([-24.9603], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.11213494837284088
epoch£º587	 i:1 	 global-step:11741	 l-p:0.14875687658786774
epoch£º587	 i:2 	 global-step:11742	 l-p:0.16681461036205292
epoch£º587	 i:3 	 global-step:11743	 l-p:0.11995483189821243
epoch£º587	 i:4 	 global-step:11744	 l-p:0.12727080285549164
epoch£º587	 i:5 	 global-step:11745	 l-p:0.14955748617649078
epoch£º587	 i:6 	 global-step:11746	 l-p:0.15372353792190552
epoch£º587	 i:7 	 global-step:11747	 l-p:-0.6534420847892761
epoch£º587	 i:8 	 global-step:11748	 l-p:0.12439074367284775
epoch£º587	 i:9 	 global-step:11749	 l-p:0.1653166115283966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9383, 2.9383, 2.9383],
        [2.9383, 2.5749, 2.7632],
        [2.9383, 2.0288, 1.9775],
        [2.9383, 2.6322, 2.8096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.12066364288330078 
model_pd.l_d.mean(): -25.056501388549805 
model_pd.lagr.mean(): -24.935836791992188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0560], device='cuda:0')), ('power', tensor([-25.1125], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.12066364288330078
epoch£º588	 i:1 	 global-step:11761	 l-p:0.0628850907087326
epoch£º588	 i:2 	 global-step:11762	 l-p:0.07345884293317795
epoch£º588	 i:3 	 global-step:11763	 l-p:0.10989020764827728
epoch£º588	 i:4 	 global-step:11764	 l-p:0.33175933361053467
epoch£º588	 i:5 	 global-step:11765	 l-p:0.1506584733724594
epoch£º588	 i:6 	 global-step:11766	 l-p:0.1387133151292801
epoch£º588	 i:7 	 global-step:11767	 l-p:0.13043348491191864
epoch£º588	 i:8 	 global-step:11768	 l-p:0.14033125340938568
epoch£º588	 i:9 	 global-step:11769	 l-p:0.3789576292037964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9381, 2.8825, 2.9312],
        [2.9381, 2.9350, 2.9380],
        [2.9381, 2.7662, 2.8915],
        [2.9381, 2.9380, 2.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.09202698618173599 
model_pd.l_d.mean(): -24.984100341796875 
model_pd.lagr.mean(): -24.892072677612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1081], device='cuda:0')), ('power', tensor([-25.0922], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.09202698618173599
epoch£º589	 i:1 	 global-step:11781	 l-p:0.12992212176322937
epoch£º589	 i:2 	 global-step:11782	 l-p:0.15113700926303864
epoch£º589	 i:3 	 global-step:11783	 l-p:-0.002063116990029812
epoch£º589	 i:4 	 global-step:11784	 l-p:0.1675841510295868
epoch£º589	 i:5 	 global-step:11785	 l-p:0.16788223385810852
epoch£º589	 i:6 	 global-step:11786	 l-p:0.13765616714954376
epoch£º589	 i:7 	 global-step:11787	 l-p:0.13592545688152313
epoch£º589	 i:8 	 global-step:11788	 l-p:0.1329483985900879
epoch£º589	 i:9 	 global-step:11789	 l-p:0.25532135367393494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0501, 2.9158, 3.0195],
        [3.0501, 1.8136, 1.2510],
        [3.0501, 2.8207, 2.9725],
        [3.0501, 3.0126, 3.0465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.15094497799873352 
model_pd.l_d.mean(): -25.052936553955078 
model_pd.lagr.mean(): -24.90199089050293 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0514], device='cuda:0')), ('power', tensor([-25.1043], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.15094497799873352
epoch£º590	 i:1 	 global-step:11801	 l-p:0.186629980802536
epoch£º590	 i:2 	 global-step:11802	 l-p:0.10628119111061096
epoch£º590	 i:3 	 global-step:11803	 l-p:0.13124707341194153
epoch£º590	 i:4 	 global-step:11804	 l-p:0.12865228950977325
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13888207077980042
epoch£º590	 i:6 	 global-step:11806	 l-p:0.1276213526725769
epoch£º590	 i:7 	 global-step:11807	 l-p:0.1800411343574524
epoch£º590	 i:8 	 global-step:11808	 l-p:0.15172511339187622
epoch£º590	 i:9 	 global-step:11809	 l-p:0.14307743310928345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9468, 1.7594, 1.1793],
        [2.9468, 1.8635, 1.5912],
        [2.9468, 2.9464, 2.9468],
        [2.9468, 2.9455, 2.9468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.1326657384634018 
model_pd.l_d.mean(): -24.495235443115234 
model_pd.lagr.mean(): -24.36256980895996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1099], device='cuda:0')), ('power', tensor([-24.6051], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.1326657384634018
epoch£º591	 i:1 	 global-step:11821	 l-p:0.21960823237895966
epoch£º591	 i:2 	 global-step:11822	 l-p:0.12814874947071075
epoch£º591	 i:3 	 global-step:11823	 l-p:0.29548972845077515
epoch£º591	 i:4 	 global-step:11824	 l-p:0.12396378070116043
epoch£º591	 i:5 	 global-step:11825	 l-p:0.22504922747612
epoch£º591	 i:6 	 global-step:11826	 l-p:0.12594430148601532
epoch£º591	 i:7 	 global-step:11827	 l-p:0.21964120864868164
epoch£º591	 i:8 	 global-step:11828	 l-p:0.06966694444417953
epoch£º591	 i:9 	 global-step:11829	 l-p:0.137135311961174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[3.0917, 2.2781, 2.3094],
        [3.0917, 2.2774, 2.3080],
        [3.0917, 2.2331, 1.5647],
        [3.0917, 2.1902, 2.1382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.1275673508644104 
model_pd.l_d.mean(): -24.952972412109375 
model_pd.lagr.mean(): -24.82540512084961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0246], device='cuda:0')), ('power', tensor([-24.9283], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.1275673508644104
epoch£º592	 i:1 	 global-step:11841	 l-p:0.1652258038520813
epoch£º592	 i:2 	 global-step:11842	 l-p:0.12072795629501343
epoch£º592	 i:3 	 global-step:11843	 l-p:0.12153919041156769
epoch£º592	 i:4 	 global-step:11844	 l-p:0.14935335516929626
epoch£º592	 i:5 	 global-step:11845	 l-p:0.23801353573799133
epoch£º592	 i:6 	 global-step:11846	 l-p:0.1272183656692505
epoch£º592	 i:7 	 global-step:11847	 l-p:0.12169675529003143
epoch£º592	 i:8 	 global-step:11848	 l-p:0.15002518892288208
epoch£º592	 i:9 	 global-step:11849	 l-p:0.5100752115249634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0114, 2.7189, 2.8926],
        [3.0114, 2.5333, 2.7228],
        [3.0114, 3.0113, 3.0114],
        [3.0114, 3.0114, 3.0114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.17168837785720825 
model_pd.l_d.mean(): -25.21668243408203 
model_pd.lagr.mean(): -25.044994354248047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0125], device='cuda:0')), ('power', tensor([-25.2292], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.17168837785720825
epoch£º593	 i:1 	 global-step:11861	 l-p:0.03933825343847275
epoch£º593	 i:2 	 global-step:11862	 l-p:0.21081851422786713
epoch£º593	 i:3 	 global-step:11863	 l-p:0.15782976150512695
epoch£º593	 i:4 	 global-step:11864	 l-p:0.13552556931972504
epoch£º593	 i:5 	 global-step:11865	 l-p:0.17428721487522125
epoch£º593	 i:6 	 global-step:11866	 l-p:0.11048625409603119
epoch£º593	 i:7 	 global-step:11867	 l-p:0.11394061148166656
epoch£º593	 i:8 	 global-step:11868	 l-p:0.13558703660964966
epoch£º593	 i:9 	 global-step:11869	 l-p:0.3821443021297455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0505, 2.6137, 2.8052],
        [3.0505, 3.0504, 3.0505],
        [3.0505, 2.8041, 2.9627],
        [3.0505, 1.9586, 1.3316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.2508655786514282 
model_pd.l_d.mean(): -25.064712524414062 
model_pd.lagr.mean(): -24.813846588134766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0993], device='cuda:0')), ('power', tensor([-25.1640], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.2508655786514282
epoch£º594	 i:1 	 global-step:11881	 l-p:0.12881338596343994
epoch£º594	 i:2 	 global-step:11882	 l-p:0.17338542640209198
epoch£º594	 i:3 	 global-step:11883	 l-p:0.1273730993270874
epoch£º594	 i:4 	 global-step:11884	 l-p:0.1433592289686203
epoch£º594	 i:5 	 global-step:11885	 l-p:0.16731618344783783
epoch£º594	 i:6 	 global-step:11886	 l-p:0.13708297908306122
epoch£º594	 i:7 	 global-step:11887	 l-p:0.14513961970806122
epoch£º594	 i:8 	 global-step:11888	 l-p:0.08902433514595032
epoch£º594	 i:9 	 global-step:11889	 l-p:0.13484707474708557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0184, 1.9299, 1.3079],
        [3.0184, 2.1978, 2.2295],
        [3.0184, 3.0183, 3.0184],
        [3.0184, 3.0125, 3.0182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.14171253144741058 
model_pd.l_d.mean(): -25.121397018432617 
model_pd.lagr.mean(): -24.979684829711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0012], device='cuda:0')), ('power', tensor([-25.1202], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.14171253144741058
epoch£º595	 i:1 	 global-step:11901	 l-p:-0.0659770593047142
epoch£º595	 i:2 	 global-step:11902	 l-p:0.1680816113948822
epoch£º595	 i:3 	 global-step:11903	 l-p:0.12097522616386414
epoch£º595	 i:4 	 global-step:11904	 l-p:0.1286659836769104
epoch£º595	 i:5 	 global-step:11905	 l-p:0.13434526324272156
epoch£º595	 i:6 	 global-step:11906	 l-p:0.11110693961381912
epoch£º595	 i:7 	 global-step:11907	 l-p:0.2161352038383484
epoch£º595	 i:8 	 global-step:11908	 l-p:0.11572400480508804
epoch£º595	 i:9 	 global-step:11909	 l-p:0.10683659464120865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0924, 2.9496, 3.0585],
        [3.0924, 3.0895, 3.0923],
        [3.0924, 1.8549, 1.3252],
        [3.0924, 3.0923, 3.0924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.07681120187044144 
model_pd.l_d.mean(): -25.070444107055664 
model_pd.lagr.mean(): -24.993633270263672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0418], device='cuda:0')), ('power', tensor([-25.0287], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.07681120187044144
epoch£º596	 i:1 	 global-step:11921	 l-p:0.1265007108449936
epoch£º596	 i:2 	 global-step:11922	 l-p:0.12928971648216248
epoch£º596	 i:3 	 global-step:11923	 l-p:0.12051059305667877
epoch£º596	 i:4 	 global-step:11924	 l-p:0.1398461014032364
epoch£º596	 i:5 	 global-step:11925	 l-p:0.390786349773407
epoch£º596	 i:6 	 global-step:11926	 l-p:0.19600045680999756
epoch£º596	 i:7 	 global-step:11927	 l-p:0.1989801675081253
epoch£º596	 i:8 	 global-step:11928	 l-p:0.14704909920692444
epoch£º596	 i:9 	 global-step:11929	 l-p:0.12550751864910126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0218, 2.3133, 2.4251],
        [3.0218, 2.9287, 3.0055],
        [3.0218, 3.0216, 3.0218],
        [3.0218, 3.0158, 3.0216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): -0.18810531497001648 
model_pd.l_d.mean(): -25.01902961730957 
model_pd.lagr.mean(): -25.207134246826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0179], device='cuda:0')), ('power', tensor([-25.0011], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:-0.18810531497001648
epoch£º597	 i:1 	 global-step:11941	 l-p:0.13432374596595764
epoch£º597	 i:2 	 global-step:11942	 l-p:0.014504671096801758
epoch£º597	 i:3 	 global-step:11943	 l-p:0.13079650700092316
epoch£º597	 i:4 	 global-step:11944	 l-p:0.24754589796066284
epoch£º597	 i:5 	 global-step:11945	 l-p:0.15478011965751648
epoch£º597	 i:6 	 global-step:11946	 l-p:0.18530787527561188
epoch£º597	 i:7 	 global-step:11947	 l-p:0.1348305642604828
epoch£º597	 i:8 	 global-step:11948	 l-p:0.11771436780691147
epoch£º597	 i:9 	 global-step:11949	 l-p:0.12125089019536972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0769, 3.0741, 3.0768],
        [3.0769, 1.9117, 1.2946],
        [3.0769, 2.4960, 2.6631],
        [3.0769, 2.1136, 1.4607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.10729820281267166 
model_pd.l_d.mean(): -24.62262535095215 
model_pd.lagr.mean(): -24.51532745361328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0500], device='cuda:0')), ('power', tensor([-24.6726], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.10729820281267166
epoch£º598	 i:1 	 global-step:11961	 l-p:0.20173117518424988
epoch£º598	 i:2 	 global-step:11962	 l-p:-0.10140630602836609
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13267959654331207
epoch£º598	 i:4 	 global-step:11964	 l-p:-0.04153835028409958
epoch£º598	 i:5 	 global-step:11965	 l-p:0.21481411159038544
epoch£º598	 i:6 	 global-step:11966	 l-p:0.13840821385383606
epoch£º598	 i:7 	 global-step:11967	 l-p:0.1426672786474228
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15851755440235138
epoch£º598	 i:9 	 global-step:11969	 l-p:0.1413773000240326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9288, 2.9288, 2.9288],
        [2.9288, 2.3057, 2.4630],
        [2.9288, 1.8433, 1.2367],
        [2.9288, 2.9150, 2.9281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.1692461371421814 
model_pd.l_d.mean(): -24.83649444580078 
model_pd.lagr.mean(): -24.667247772216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1677], device='cuda:0')), ('power', tensor([-25.0042], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.1692461371421814
epoch£º599	 i:1 	 global-step:11981	 l-p:0.2114035189151764
epoch£º599	 i:2 	 global-step:11982	 l-p:0.13560016453266144
epoch£º599	 i:3 	 global-step:11983	 l-p:-2.5252206325531006
epoch£º599	 i:4 	 global-step:11984	 l-p:0.13812017440795898
epoch£º599	 i:5 	 global-step:11985	 l-p:0.09588714689016342
epoch£º599	 i:6 	 global-step:11986	 l-p:0.15103350579738617
epoch£º599	 i:7 	 global-step:11987	 l-p:0.14631886780261993
epoch£º599	 i:8 	 global-step:11988	 l-p:0.0988239273428917
epoch£º599	 i:9 	 global-step:11989	 l-p:0.13204002380371094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9287, 1.7354, 1.1597],
        [2.9287, 2.9286, 2.9287],
        [2.9287, 2.9287, 2.9287],
        [2.9287, 2.9287, 2.9287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.1446799486875534 
model_pd.l_d.mean(): -24.804969787597656 
model_pd.lagr.mean(): -24.660289764404297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0720], device='cuda:0')), ('power', tensor([-24.8769], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.1446799486875534
epoch£º600	 i:1 	 global-step:12001	 l-p:0.15835382044315338
epoch£º600	 i:2 	 global-step:12002	 l-p:0.19221621751785278
epoch£º600	 i:3 	 global-step:12003	 l-p:0.06934177130460739
epoch£º600	 i:4 	 global-step:12004	 l-p:0.026521576568484306
epoch£º600	 i:5 	 global-step:12005	 l-p:0.11243074387311935
epoch£º600	 i:6 	 global-step:12006	 l-p:0.2022886574268341
epoch£º600	 i:7 	 global-step:12007	 l-p:0.05660862475633621
epoch£º600	 i:8 	 global-step:12008	 l-p:0.11897235363721848
epoch£º600	 i:9 	 global-step:12009	 l-p:0.1262672245502472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0656, 1.8268, 1.2500],
        [3.0656, 1.9551, 1.6358],
        [3.0656, 2.9900, 3.0542],
        [3.0656, 3.0634, 3.0656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.15405616164207458 
model_pd.l_d.mean(): -25.111366271972656 
model_pd.lagr.mean(): -24.95730972290039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0552], device='cuda:0')), ('power', tensor([-25.0562], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.15405616164207458
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12484966218471527
epoch£º601	 i:2 	 global-step:12022	 l-p:0.2149403691291809
epoch£º601	 i:3 	 global-step:12023	 l-p:0.13710570335388184
epoch£º601	 i:4 	 global-step:12024	 l-p:0.1289113610982895
epoch£º601	 i:5 	 global-step:12025	 l-p:0.15231196582317352
epoch£º601	 i:6 	 global-step:12026	 l-p:0.14867830276489258
epoch£º601	 i:7 	 global-step:12027	 l-p:0.27260905504226685
epoch£º601	 i:8 	 global-step:12028	 l-p:0.26608139276504517
epoch£º601	 i:9 	 global-step:12029	 l-p:0.09026116132736206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0659, 3.0659, 3.0659],
        [3.0659, 1.8520, 1.2538],
        [3.0659, 2.9758, 3.0505],
        [3.0659, 1.8337, 1.2483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.16769930720329285 
model_pd.l_d.mean(): -25.031633377075195 
model_pd.lagr.mean(): -24.863933563232422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0214], device='cuda:0')), ('power', tensor([-25.0531], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.16769930720329285
epoch£º602	 i:1 	 global-step:12041	 l-p:0.13533857464790344
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11299144476652145
epoch£º602	 i:3 	 global-step:12043	 l-p:0.10672938823699951
epoch£º602	 i:4 	 global-step:12044	 l-p:0.13467437028884888
epoch£º602	 i:5 	 global-step:12045	 l-p:0.14618763327598572
epoch£º602	 i:6 	 global-step:12046	 l-p:0.1701551228761673
epoch£º602	 i:7 	 global-step:12047	 l-p:0.14526911079883575
epoch£º602	 i:8 	 global-step:12048	 l-p:0.1422855108976364
epoch£º602	 i:9 	 global-step:12049	 l-p:0.14767761528491974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0566, 2.5372, 2.7210],
        [3.0566, 3.0164, 3.0526],
        [3.0566, 3.0563, 3.0566],
        [3.0566, 2.5777, 2.7674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.16016745567321777 
model_pd.l_d.mean(): -25.181964874267578 
model_pd.lagr.mean(): -25.02179718017578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0535], device='cuda:0')), ('power', tensor([-25.1285], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.16016745567321777
epoch£º603	 i:1 	 global-step:12061	 l-p:0.15280118584632874
epoch£º603	 i:2 	 global-step:12062	 l-p:0.13125084340572357
epoch£º603	 i:3 	 global-step:12063	 l-p:0.1343194842338562
epoch£º603	 i:4 	 global-step:12064	 l-p:-1.7600446939468384
epoch£º603	 i:5 	 global-step:12065	 l-p:0.0876191109418869
epoch£º603	 i:6 	 global-step:12066	 l-p:0.15577565133571625
epoch£º603	 i:7 	 global-step:12067	 l-p:0.6648769974708557
epoch£º603	 i:8 	 global-step:12068	 l-p:0.12676408886909485
epoch£º603	 i:9 	 global-step:12069	 l-p:0.13247884809970856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9230, 1.8586, 1.2481],
        [2.9230, 2.9230, 2.9230],
        [2.9230, 2.6627, 2.8273],
        [2.9230, 2.0401, 2.0246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.05102343484759331 
model_pd.l_d.mean(): -25.106887817382812 
model_pd.lagr.mean(): -25.055864334106445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0461], device='cuda:0')), ('power', tensor([-25.1530], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.05102343484759331
epoch£º604	 i:1 	 global-step:12081	 l-p:0.2075151801109314
epoch£º604	 i:2 	 global-step:12082	 l-p:0.160862535238266
epoch£º604	 i:3 	 global-step:12083	 l-p:0.14047236740589142
epoch£º604	 i:4 	 global-step:12084	 l-p:0.17611262202262878
epoch£º604	 i:5 	 global-step:12085	 l-p:0.12226586043834686
epoch£º604	 i:6 	 global-step:12086	 l-p:-0.05258842185139656
epoch£º604	 i:7 	 global-step:12087	 l-p:0.117154560983181
epoch£º604	 i:8 	 global-step:12088	 l-p:0.1516622006893158
epoch£º604	 i:9 	 global-step:12089	 l-p:0.127492293715477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0124, 1.7730, 1.2138],
        [3.0124, 2.5561, 2.7486],
        [3.0124, 2.7977, 2.9439],
        [3.0124, 2.8398, 2.9656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.11067405343055725 
model_pd.l_d.mean(): -25.05721092224121 
model_pd.lagr.mean(): -24.946537017822266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-25.0657], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.11067405343055725
epoch£º605	 i:1 	 global-step:12101	 l-p:0.012308306060731411
epoch£º605	 i:2 	 global-step:12102	 l-p:0.1426418125629425
epoch£º605	 i:3 	 global-step:12103	 l-p:0.1360601782798767
epoch£º605	 i:4 	 global-step:12104	 l-p:0.42669203877449036
epoch£º605	 i:5 	 global-step:12105	 l-p:0.15636613965034485
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11659122258424759
epoch£º605	 i:7 	 global-step:12107	 l-p:0.26781243085861206
epoch£º605	 i:8 	 global-step:12108	 l-p:0.13924682140350342
epoch£º605	 i:9 	 global-step:12109	 l-p:-0.038950689136981964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9510, 2.9510, 2.9510],
        [2.9510, 1.8664, 1.2550],
        [2.9510, 2.0720, 2.0587],
        [2.9510, 2.9497, 2.9510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.11722823232412338 
model_pd.l_d.mean(): -25.202192306518555 
model_pd.lagr.mean(): -25.084964752197266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0273], device='cuda:0')), ('power', tensor([-25.2295], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.11722823232412338
epoch£º606	 i:1 	 global-step:12121	 l-p:0.15788203477859497
epoch£º606	 i:2 	 global-step:12122	 l-p:0.14128442108631134
epoch£º606	 i:3 	 global-step:12123	 l-p:0.08122772723436356
epoch£º606	 i:4 	 global-step:12124	 l-p:0.14024312794208527
epoch£º606	 i:5 	 global-step:12125	 l-p:0.2202344536781311
epoch£º606	 i:6 	 global-step:12126	 l-p:0.1508997231721878
epoch£º606	 i:7 	 global-step:12127	 l-p:0.14415353536605835
epoch£º606	 i:8 	 global-step:12128	 l-p:0.13476507365703583
epoch£º606	 i:9 	 global-step:12129	 l-p:0.1611117273569107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0209, 2.9868, 3.0179],
        [3.0209, 2.0728, 1.9809],
        [3.0209, 3.0161, 3.0208],
        [3.0209, 2.1981, 2.2310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.1480950266122818 
model_pd.l_d.mean(): -25.065141677856445 
model_pd.lagr.mean(): -24.91704750061035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0295], device='cuda:0')), ('power', tensor([-25.0947], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.1480950266122818
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13339756429195404
epoch£º607	 i:2 	 global-step:12142	 l-p:0.13552497327327728
epoch£º607	 i:3 	 global-step:12143	 l-p:0.14105463027954102
epoch£º607	 i:4 	 global-step:12144	 l-p:0.23029716312885284
epoch£º607	 i:5 	 global-step:12145	 l-p:0.05614454299211502
epoch£º607	 i:6 	 global-step:12146	 l-p:0.41765594482421875
epoch£º607	 i:7 	 global-step:12147	 l-p:0.1134268045425415
epoch£º607	 i:8 	 global-step:12148	 l-p:-0.007628927007317543
epoch£º607	 i:9 	 global-step:12149	 l-p:0.143299862742424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0289, 3.0237, 3.0288],
        [3.0289, 3.0087, 3.0276],
        [3.0289, 2.7829, 2.9418],
        [3.0289, 1.9246, 1.3026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.14786076545715332 
model_pd.l_d.mean(): -24.785198211669922 
model_pd.lagr.mean(): -24.63733673095703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1406], device='cuda:0')), ('power', tensor([-24.9258], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.14786076545715332
epoch£º608	 i:1 	 global-step:12161	 l-p:0.14185775816440582
epoch£º608	 i:2 	 global-step:12162	 l-p:0.25021857023239136
epoch£º608	 i:3 	 global-step:12163	 l-p:0.14841219782829285
epoch£º608	 i:4 	 global-step:12164	 l-p:0.18301734328269958
epoch£º608	 i:5 	 global-step:12165	 l-p:0.07090476900339127
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13967493176460266
epoch£º608	 i:7 	 global-step:12167	 l-p:0.024929508566856384
epoch£º608	 i:8 	 global-step:12168	 l-p:0.14638034999370575
epoch£º608	 i:9 	 global-step:12169	 l-p:0.11429141461849213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0112, 3.0112, 3.0112],
        [3.0112, 2.9467, 3.0024],
        [3.0112, 1.9073, 1.2884],
        [3.0112, 2.5444, 2.7365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): -0.054713573306798935 
model_pd.l_d.mean(): -24.606611251831055 
model_pd.lagr.mean(): -24.661325454711914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1316], device='cuda:0')), ('power', tensor([-24.7382], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:-0.054713573306798935
epoch£º609	 i:1 	 global-step:12181	 l-p:0.1403961330652237
epoch£º609	 i:2 	 global-step:12182	 l-p:0.1324400156736374
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13361714780330658
epoch£º609	 i:4 	 global-step:12184	 l-p:0.1038934513926506
epoch£º609	 i:5 	 global-step:12185	 l-p:0.1457623392343521
epoch£º609	 i:6 	 global-step:12186	 l-p:0.16753756999969482
epoch£º609	 i:7 	 global-step:12187	 l-p:0.14680184423923492
epoch£º609	 i:8 	 global-step:12188	 l-p:0.03697596862912178
epoch£º609	 i:9 	 global-step:12189	 l-p:0.03333718329668045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9331, 2.4637, 2.6572],
        [2.9331, 2.9319, 2.9331],
        [2.9331, 2.9330, 2.9331],
        [2.9331, 2.8041, 2.9050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.049183424562215805 
model_pd.l_d.mean(): -25.053184509277344 
model_pd.lagr.mean(): -25.00400161743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0532], device='cuda:0')), ('power', tensor([-25.1064], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.049183424562215805
epoch£º610	 i:1 	 global-step:12201	 l-p:0.1564246267080307
epoch£º610	 i:2 	 global-step:12202	 l-p:0.13491059839725494
epoch£º610	 i:3 	 global-step:12203	 l-p:0.13293173909187317
epoch£º610	 i:4 	 global-step:12204	 l-p:0.11296937614679337
epoch£º610	 i:5 	 global-step:12205	 l-p:0.11297562718391418
epoch£º610	 i:6 	 global-step:12206	 l-p:0.17124627530574799
epoch£º610	 i:7 	 global-step:12207	 l-p:0.16972261667251587
epoch£º610	 i:8 	 global-step:12208	 l-p:0.15346874296665192
epoch£º610	 i:9 	 global-step:12209	 l-p:0.14240434765815735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0481, 2.1529, 2.1181],
        [3.0481, 1.9856, 1.3521],
        [3.0481, 2.1443, 1.4853],
        [3.0481, 3.0443, 3.0480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.16230925917625427 
model_pd.l_d.mean(): -25.014698028564453 
model_pd.lagr.mean(): -24.852388381958008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1064], device='cuda:0')), ('power', tensor([-24.9083], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.16230925917625427
epoch£º611	 i:1 	 global-step:12221	 l-p:0.1202431172132492
epoch£º611	 i:2 	 global-step:12222	 l-p:0.12831555306911469
epoch£º611	 i:3 	 global-step:12223	 l-p:0.14654824137687683
epoch£º611	 i:4 	 global-step:12224	 l-p:0.13303634524345398
epoch£º611	 i:5 	 global-step:12225	 l-p:0.19398212432861328
epoch£º611	 i:6 	 global-step:12226	 l-p:0.13075678050518036
epoch£º611	 i:7 	 global-step:12227	 l-p:0.12793117761611938
epoch£º611	 i:8 	 global-step:12228	 l-p:2.126044988632202
epoch£º611	 i:9 	 global-step:12229	 l-p:0.1562449038028717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0218, 1.7797, 1.2186],
        [3.0218, 3.0218, 3.0218],
        [3.0218, 2.2146, 2.2616],
        [3.0218, 3.0186, 3.0218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14741921424865723 
model_pd.l_d.mean(): -24.762510299682617 
model_pd.lagr.mean(): -24.61509132385254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0706], device='cuda:0')), ('power', tensor([-24.8331], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14741921424865723
epoch£º612	 i:1 	 global-step:12241	 l-p:0.13770096004009247
epoch£º612	 i:2 	 global-step:12242	 l-p:0.07569656521081924
epoch£º612	 i:3 	 global-step:12243	 l-p:0.1367189586162567
epoch£º612	 i:4 	 global-step:12244	 l-p:0.05846729129552841
epoch£º612	 i:5 	 global-step:12245	 l-p:0.1479560136795044
epoch£º612	 i:6 	 global-step:12246	 l-p:0.1429755687713623
epoch£º612	 i:7 	 global-step:12247	 l-p:0.13756175339221954
epoch£º612	 i:8 	 global-step:12248	 l-p:0.19327333569526672
epoch£º612	 i:9 	 global-step:12249	 l-p:0.1816015988588333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9974, 2.9969, 2.9974],
        [2.9974, 2.9067, 2.9819],
        [2.9974, 2.9860, 2.9969],
        [2.9974, 2.7778, 2.9263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.18367667496204376 
model_pd.l_d.mean(): -25.13507080078125 
model_pd.lagr.mean(): -24.95139503479004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0357], device='cuda:0')), ('power', tensor([-25.1708], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.18367667496204376
epoch£º613	 i:1 	 global-step:12261	 l-p:0.152769073843956
epoch£º613	 i:2 	 global-step:12262	 l-p:0.12010755389928818
epoch£º613	 i:3 	 global-step:12263	 l-p:0.2698725461959839
epoch£º613	 i:4 	 global-step:12264	 l-p:0.1826721578836441
epoch£º613	 i:5 	 global-step:12265	 l-p:0.11773835867643356
epoch£º613	 i:6 	 global-step:12266	 l-p:0.13442812860012054
epoch£º613	 i:7 	 global-step:12267	 l-p:0.1693989485502243
epoch£º613	 i:8 	 global-step:12268	 l-p:0.14918728172779083
epoch£º613	 i:9 	 global-step:12269	 l-p:0.7848070859909058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1482, 1.9014, 1.3525],
        [3.1482, 2.1590, 2.0073],
        [3.1482, 3.1362, 3.1476],
        [3.1482, 3.1422, 3.1480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.1215403825044632 
model_pd.l_d.mean(): -25.014469146728516 
model_pd.lagr.mean(): -24.892929077148438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0293], device='cuda:0')), ('power', tensor([-24.9852], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.1215403825044632
epoch£º614	 i:1 	 global-step:12281	 l-p:0.1311754286289215
epoch£º614	 i:2 	 global-step:12282	 l-p:0.1386338323354721
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12138492614030838
epoch£º614	 i:4 	 global-step:12284	 l-p:0.11582349240779877
epoch£º614	 i:5 	 global-step:12285	 l-p:0.12511777877807617
epoch£º614	 i:6 	 global-step:12286	 l-p:0.14372001588344574
epoch£º614	 i:7 	 global-step:12287	 l-p:0.14668092131614685
epoch£º614	 i:8 	 global-step:12288	 l-p:0.2244693636894226
epoch£º614	 i:9 	 global-step:12289	 l-p:0.312113493680954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0285, 1.9838, 1.7700],
        [3.0285, 2.0487, 1.9204],
        [3.0285, 1.8225, 1.2275],
        [3.0285, 3.0285, 3.0285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.13495266437530518 
model_pd.l_d.mean(): -24.579431533813477 
model_pd.lagr.mean(): -24.44447898864746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0613], device='cuda:0')), ('power', tensor([-24.6407], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.13495266437530518
epoch£º615	 i:1 	 global-step:12301	 l-p:0.16774390637874603
epoch£º615	 i:2 	 global-step:12302	 l-p:0.04919612780213356
epoch£º615	 i:3 	 global-step:12303	 l-p:0.03861398622393608
epoch£º615	 i:4 	 global-step:12304	 l-p:0.13541148602962494
epoch£º615	 i:5 	 global-step:12305	 l-p:0.13567493855953217
epoch£º615	 i:6 	 global-step:12306	 l-p:0.1530362218618393
epoch£º615	 i:7 	 global-step:12307	 l-p:0.1713068187236786
epoch£º615	 i:8 	 global-step:12308	 l-p:-0.3794708549976349
epoch£º615	 i:9 	 global-step:12309	 l-p:0.11063163727521896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9606, 2.9533, 2.9603],
        [2.9606, 2.8378, 2.9347],
        [2.9606, 1.8243, 1.2219],
        [2.9606, 1.7469, 1.2927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.14113964140415192 
model_pd.l_d.mean(): -24.466829299926758 
model_pd.lagr.mean(): -24.3256893157959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2207], device='cuda:0')), ('power', tensor([-24.6875], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.14113964140415192
epoch£º616	 i:1 	 global-step:12321	 l-p:0.16039544343948364
epoch£º616	 i:2 	 global-step:12322	 l-p:0.14928457140922546
epoch£º616	 i:3 	 global-step:12323	 l-p:-0.4367122948169708
epoch£º616	 i:4 	 global-step:12324	 l-p:0.1685696542263031
epoch£º616	 i:5 	 global-step:12325	 l-p:0.2981186509132385
epoch£º616	 i:6 	 global-step:12326	 l-p:0.12232840061187744
epoch£º616	 i:7 	 global-step:12327	 l-p:0.2633679211139679
epoch£º616	 i:8 	 global-step:12328	 l-p:0.18934230506420135
epoch£º616	 i:9 	 global-step:12329	 l-p:0.04276849702000618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1298, 2.9007, 3.0527],
        [3.1298, 2.5051, 2.6568],
        [3.1298, 2.2519, 1.5785],
        [3.1298, 2.9050, 3.0553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.12232835590839386 
model_pd.l_d.mean(): -24.97407341003418 
model_pd.lagr.mean(): -24.85174560546875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0168], device='cuda:0')), ('power', tensor([-24.9572], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.12232835590839386
epoch£º617	 i:1 	 global-step:12341	 l-p:0.13619758188724518
epoch£º617	 i:2 	 global-step:12342	 l-p:0.1307782083749771
epoch£º617	 i:3 	 global-step:12343	 l-p:0.12314991652965546
epoch£º617	 i:4 	 global-step:12344	 l-p:0.11381940543651581
epoch£º617	 i:5 	 global-step:12345	 l-p:0.17495068907737732
epoch£º617	 i:6 	 global-step:12346	 l-p:0.14310748875141144
epoch£º617	 i:7 	 global-step:12347	 l-p:0.14346782863140106
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11457549035549164
epoch£º617	 i:9 	 global-step:12349	 l-p:0.15681391954421997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0400, 2.0124, 1.8232],
        [3.0400, 2.8040, 2.9592],
        [3.0400, 2.1402, 1.4809],
        [3.0400, 2.2266, 2.2691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.10836241394281387 
model_pd.l_d.mean(): -25.01201820373535 
model_pd.lagr.mean(): -24.903656005859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0280], device='cuda:0')), ('power', tensor([-25.0401], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.10836241394281387
epoch£º618	 i:1 	 global-step:12361	 l-p:0.16371913254261017
epoch£º618	 i:2 	 global-step:12362	 l-p:0.026699203997850418
epoch£º618	 i:3 	 global-step:12363	 l-p:0.1809144914150238
epoch£º618	 i:4 	 global-step:12364	 l-p:0.3243182599544525
epoch£º618	 i:5 	 global-step:12365	 l-p:0.13310548663139343
epoch£º618	 i:6 	 global-step:12366	 l-p:0.06250368058681488
epoch£º618	 i:7 	 global-step:12367	 l-p:0.15596647560596466
epoch£º618	 i:8 	 global-step:12368	 l-p:0.16547046601772308
epoch£º618	 i:9 	 global-step:12369	 l-p:0.18225815892219543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0294, 2.8031, 2.9545],
        [3.0294, 2.5539, 2.7457],
        [3.0294, 2.5339, 2.7236],
        [3.0294, 1.8497, 1.2442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): -0.36264240741729736 
model_pd.l_d.mean(): -25.084138870239258 
model_pd.lagr.mean(): -25.446781158447266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0963], device='cuda:0')), ('power', tensor([-25.1804], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:-0.36264240741729736
epoch£º619	 i:1 	 global-step:12381	 l-p:0.09598856419324875
epoch£º619	 i:2 	 global-step:12382	 l-p:0.11668270081281662
epoch£º619	 i:3 	 global-step:12383	 l-p:0.12129160016775131
epoch£º619	 i:4 	 global-step:12384	 l-p:0.1526598483324051
epoch£º619	 i:5 	 global-step:12385	 l-p:0.21417781710624695
epoch£º619	 i:6 	 global-step:12386	 l-p:0.12762165069580078
epoch£º619	 i:7 	 global-step:12387	 l-p:0.1567603051662445
epoch£º619	 i:8 	 global-step:12388	 l-p:0.16064342856407166
epoch£º619	 i:9 	 global-step:12389	 l-p:0.13832610845565796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0704, 2.9873, 3.0571],
        [3.0704, 2.4568, 2.6155],
        [3.0704, 3.0704, 3.0704],
        [3.0704, 1.8283, 1.2468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.21282073855400085 
model_pd.l_d.mean(): -25.267162322998047 
model_pd.lagr.mean(): -25.05434226989746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0015], device='cuda:0')), ('power', tensor([-25.2656], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.21282073855400085
epoch£º620	 i:1 	 global-step:12401	 l-p:0.20004646480083466
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12394942343235016
epoch£º620	 i:3 	 global-step:12403	 l-p:0.14067290723323822
epoch£º620	 i:4 	 global-step:12404	 l-p:0.1226925477385521
epoch£º620	 i:5 	 global-step:12405	 l-p:0.15933547914028168
epoch£º620	 i:6 	 global-step:12406	 l-p:0.13355502486228943
epoch£º620	 i:7 	 global-step:12407	 l-p:0.1082693487405777
epoch£º620	 i:8 	 global-step:12408	 l-p:0.13305050134658813
epoch£º620	 i:9 	 global-step:12409	 l-p:0.10630883276462555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0076, 1.8055, 1.2124],
        [3.0076, 3.0044, 3.0075],
        [3.0076, 3.0076, 3.0076],
        [3.0076, 3.0075, 3.0076]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.1440858691930771 
model_pd.l_d.mean(): -25.214534759521484 
model_pd.lagr.mean(): -25.070449829101562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0562], device='cuda:0')), ('power', tensor([-25.1583], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.1440858691930771
epoch£º621	 i:1 	 global-step:12421	 l-p:0.0712798684835434
epoch£º621	 i:2 	 global-step:12422	 l-p:-0.2042016237974167
epoch£º621	 i:3 	 global-step:12423	 l-p:0.13376225531101227
epoch£º621	 i:4 	 global-step:12424	 l-p:0.16489139199256897
epoch£º621	 i:5 	 global-step:12425	 l-p:0.17251451313495636
epoch£º621	 i:6 	 global-step:12426	 l-p:0.1099579855799675
epoch£º621	 i:7 	 global-step:12427	 l-p:0.1283717304468155
epoch£º621	 i:8 	 global-step:12428	 l-p:0.1466502994298935
epoch£º621	 i:9 	 global-step:12429	 l-p:-0.0521988682448864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0244, 3.0244, 3.0244],
        [3.0244, 2.1421, 2.1255],
        [3.0244, 1.7789, 1.2539],
        [3.0244, 2.3325, 2.4580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.15382005274295807 
model_pd.l_d.mean(): -25.187971115112305 
model_pd.lagr.mean(): -25.034151077270508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0198], device='cuda:0')), ('power', tensor([-25.1681], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.15382005274295807
epoch£º622	 i:1 	 global-step:12441	 l-p:0.13378244638442993
epoch£º622	 i:2 	 global-step:12442	 l-p:0.06857170909643173
epoch£º622	 i:3 	 global-step:12443	 l-p:0.7170538902282715
epoch£º622	 i:4 	 global-step:12444	 l-p:0.1235123798251152
epoch£º622	 i:5 	 global-step:12445	 l-p:0.3037182688713074
epoch£º622	 i:6 	 global-step:12446	 l-p:0.1506887674331665
epoch£º622	 i:7 	 global-step:12447	 l-p:0.1534186601638794
epoch£º622	 i:8 	 global-step:12448	 l-p:0.09075640141963959
epoch£º622	 i:9 	 global-step:12449	 l-p:0.09729543328285217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9876, 2.8616, 2.9606],
        [2.9876, 2.5175, 2.7111],
        [2.9876, 2.9648, 2.9860],
        [2.9876, 2.9871, 2.9876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.15271881222724915 
model_pd.l_d.mean(): -24.975069046020508 
model_pd.lagr.mean(): -24.822349548339844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0262], device='cuda:0')), ('power', tensor([-24.9489], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.15271881222724915
epoch£º623	 i:1 	 global-step:12461	 l-p:0.10894255340099335
epoch£º623	 i:2 	 global-step:12462	 l-p:0.1605384647846222
epoch£º623	 i:3 	 global-step:12463	 l-p:0.15731927752494812
epoch£º623	 i:4 	 global-step:12464	 l-p:-0.053248632699251175
epoch£º623	 i:5 	 global-step:12465	 l-p:0.17354723811149597
epoch£º623	 i:6 	 global-step:12466	 l-p:0.1135081872344017
epoch£º623	 i:7 	 global-step:12467	 l-p:0.1453963816165924
epoch£º623	 i:8 	 global-step:12468	 l-p:0.1277448534965515
epoch£º623	 i:9 	 global-step:12469	 l-p:0.15446868538856506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1285, 3.0988, 3.1261],
        [3.1285, 2.1281, 1.4703],
        [3.1285, 3.1226, 3.1284],
        [3.1285, 3.1286, 3.1285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.13400834798812866 
model_pd.l_d.mean(): -25.021757125854492 
model_pd.lagr.mean(): -24.88774871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0429], device='cuda:0')), ('power', tensor([-24.9788], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.13400834798812866
epoch£º624	 i:1 	 global-step:12481	 l-p:0.12320252507925034
epoch£º624	 i:2 	 global-step:12482	 l-p:0.1342853158712387
epoch£º624	 i:3 	 global-step:12483	 l-p:0.18631397187709808
epoch£º624	 i:4 	 global-step:12484	 l-p:-0.02752614952623844
epoch£º624	 i:5 	 global-step:12485	 l-p:0.11664262413978577
epoch£º624	 i:6 	 global-step:12486	 l-p:0.126718208193779
epoch£º624	 i:7 	 global-step:12487	 l-p:0.13158608973026276
epoch£º624	 i:8 	 global-step:12488	 l-p:0.15767304599285126
epoch£º624	 i:9 	 global-step:12489	 l-p:0.19746385514736176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0827, 3.0827, 3.0827],
        [3.0827, 3.0384, 3.0780],
        [3.0827, 3.0784, 3.0826],
        [3.0827, 1.9517, 1.6103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.16558541357517242 
model_pd.l_d.mean(): -25.006752014160156 
model_pd.lagr.mean(): -24.841167449951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0254], device='cuda:0')), ('power', tensor([-25.0322], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.16558541357517242
epoch£º625	 i:1 	 global-step:12501	 l-p:0.037622518837451935
epoch£º625	 i:2 	 global-step:12502	 l-p:0.15087056159973145
epoch£º625	 i:3 	 global-step:12503	 l-p:0.13351385295391083
epoch£º625	 i:4 	 global-step:12504	 l-p:0.1662316918373108
epoch£º625	 i:5 	 global-step:12505	 l-p:0.13377651572227478
epoch£º625	 i:6 	 global-step:12506	 l-p:0.1801164150238037
epoch£º625	 i:7 	 global-step:12507	 l-p:0.11658758670091629
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12648636102676392
epoch£º625	 i:9 	 global-step:12509	 l-p:0.1612001359462738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1018, 2.9461, 3.0627],
        [3.1018, 2.2121, 2.1837],
        [3.1018, 3.1018, 3.1018],
        [3.1018, 2.1146, 1.4589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.14056232571601868 
model_pd.l_d.mean(): -25.1588077545166 
model_pd.lagr.mean(): -25.018245697021484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0935], device='cuda:0')), ('power', tensor([-25.0653], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.14056232571601868
epoch£º626	 i:1 	 global-step:12521	 l-p:0.17180249094963074
epoch£º626	 i:2 	 global-step:12522	 l-p:0.09593863040208817
epoch£º626	 i:3 	 global-step:12523	 l-p:0.17567916214466095
epoch£º626	 i:4 	 global-step:12524	 l-p:0.1486343890428543
epoch£º626	 i:5 	 global-step:12525	 l-p:0.10123781114816666
epoch£º626	 i:6 	 global-step:12526	 l-p:0.13296657800674438
epoch£º626	 i:7 	 global-step:12527	 l-p:0.14007709920406342
epoch£º626	 i:8 	 global-step:12528	 l-p:0.1170017197728157
epoch£º626	 i:9 	 global-step:12529	 l-p:0.12378312647342682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1100, 2.7269, 2.9180],
        [3.1100, 1.9731, 1.6195],
        [3.1100, 2.7600, 2.9468],
        [3.1100, 2.9929, 3.0860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.1431266814470291 
model_pd.l_d.mean(): -25.09515380859375 
model_pd.lagr.mean(): -24.9520263671875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1183], device='cuda:0')), ('power', tensor([-24.9769], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.1431266814470291
epoch£º627	 i:1 	 global-step:12541	 l-p:0.1434846818447113
epoch£º627	 i:2 	 global-step:12542	 l-p:0.15002016723155975
epoch£º627	 i:3 	 global-step:12543	 l-p:0.08774196356534958
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12447572499513626
epoch£º627	 i:5 	 global-step:12545	 l-p:-2.66758131980896
epoch£º627	 i:6 	 global-step:12546	 l-p:0.1442590355873108
epoch£º627	 i:7 	 global-step:12547	 l-p:0.1755792200565338
epoch£º627	 i:8 	 global-step:12548	 l-p:0.15067686140537262
epoch£º627	 i:9 	 global-step:12549	 l-p:0.15163959562778473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9805, 1.9704, 1.8135],
        [2.9805, 2.9793, 2.9804],
        [2.9805, 2.9774, 2.9804],
        [2.9805, 2.7298, 2.8912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.10807415843009949 
model_pd.l_d.mean(): -25.130962371826172 
model_pd.lagr.mean(): -25.02288818359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0442], device='cuda:0')), ('power', tensor([-25.0868], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.10807415843009949
epoch£º628	 i:1 	 global-step:12561	 l-p:0.14971525967121124
epoch£º628	 i:2 	 global-step:12562	 l-p:0.18982481956481934
epoch£º628	 i:3 	 global-step:12563	 l-p:0.18312960863113403
epoch£º628	 i:4 	 global-step:12564	 l-p:0.09546918421983719
epoch£º628	 i:5 	 global-step:12565	 l-p:0.1461963802576065
epoch£º628	 i:6 	 global-step:12566	 l-p:0.14051446318626404
epoch£º628	 i:7 	 global-step:12567	 l-p:0.11905700713396072
epoch£º628	 i:8 	 global-step:12568	 l-p:0.13428078591823578
epoch£º628	 i:9 	 global-step:12569	 l-p:0.021165603771805763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9738, 2.9707, 2.9737],
        [2.9738, 2.0877, 2.0730],
        [2.9738, 2.9580, 2.9729],
        [2.9738, 1.8667, 1.2535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.150088369846344 
model_pd.l_d.mean(): -25.311176300048828 
model_pd.lagr.mean(): -25.161087036132812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0574], device='cuda:0')), ('power', tensor([-25.2537], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.150088369846344
epoch£º629	 i:1 	 global-step:12581	 l-p:1.4726496934890747
epoch£º629	 i:2 	 global-step:12582	 l-p:0.1345529407262802
epoch£º629	 i:3 	 global-step:12583	 l-p:0.13394513726234436
epoch£º629	 i:4 	 global-step:12584	 l-p:0.1601400375366211
epoch£º629	 i:5 	 global-step:12585	 l-p:0.047294966876506805
epoch£º629	 i:6 	 global-step:12586	 l-p:-0.034241463989019394
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12182110548019409
epoch£º629	 i:8 	 global-step:12588	 l-p:0.1736135482788086
epoch£º629	 i:9 	 global-step:12589	 l-p:0.23878413438796997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9987, 2.9986, 2.9987],
        [2.9987, 2.1145, 2.1004],
        [2.9987, 1.9468, 1.7329],
        [2.9987, 2.9987, 2.9987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.15546217560768127 
model_pd.l_d.mean(): -24.964269638061523 
model_pd.lagr.mean(): -24.808807373046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0779], device='cuda:0')), ('power', tensor([-25.0421], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.15546217560768127
epoch£º630	 i:1 	 global-step:12601	 l-p:0.7903259992599487
epoch£º630	 i:2 	 global-step:12602	 l-p:0.17723806202411652
epoch£º630	 i:3 	 global-step:12603	 l-p:-0.050918083637952805
epoch£º630	 i:4 	 global-step:12604	 l-p:0.16177763044834137
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13368289172649384
epoch£º630	 i:6 	 global-step:12606	 l-p:0.09231829643249512
epoch£º630	 i:7 	 global-step:12607	 l-p:0.11657979339361191
epoch£º630	 i:8 	 global-step:12608	 l-p:0.1354721635580063
epoch£º630	 i:9 	 global-step:12609	 l-p:0.146829292178154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1325, 3.0735, 3.1250],
        [3.1325, 1.8782, 1.2939],
        [3.1325, 3.1326, 3.1326],
        [3.1325, 2.6106, 2.7950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.15013210475444794 
model_pd.l_d.mean(): -25.17987823486328 
model_pd.lagr.mean(): -25.029747009277344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1495], device='cuda:0')), ('power', tensor([-25.0304], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.15013210475444794
epoch£º631	 i:1 	 global-step:12621	 l-p:0.13535365462303162
epoch£º631	 i:2 	 global-step:12622	 l-p:0.1477123200893402
epoch£º631	 i:3 	 global-step:12623	 l-p:0.1246338039636612
epoch£º631	 i:4 	 global-step:12624	 l-p:0.21361255645751953
epoch£º631	 i:5 	 global-step:12625	 l-p:0.13640737533569336
epoch£º631	 i:6 	 global-step:12626	 l-p:0.1447673887014389
epoch£º631	 i:7 	 global-step:12627	 l-p:0.11154131591320038
epoch£º631	 i:8 	 global-step:12628	 l-p:0.13257063925266266
epoch£º631	 i:9 	 global-step:12629	 l-p:0.09093404561281204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9855, 2.7811, 2.9232],
        [2.9855, 2.9251, 2.9777],
        [2.9855, 2.9673, 2.9844],
        [2.9855, 2.9711, 2.9847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): -0.06719883531332016 
model_pd.l_d.mean(): -24.719404220581055 
model_pd.lagr.mean(): -24.786603927612305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0545], device='cuda:0')), ('power', tensor([-24.7740], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:-0.06719883531332016
epoch£º632	 i:1 	 global-step:12641	 l-p:0.14967775344848633
epoch£º632	 i:2 	 global-step:12642	 l-p:0.13659986853599548
epoch£º632	 i:3 	 global-step:12643	 l-p:0.15263426303863525
epoch£º632	 i:4 	 global-step:12644	 l-p:0.3341647982597351
epoch£º632	 i:5 	 global-step:12645	 l-p:0.1446286141872406
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12900184094905853
epoch£º632	 i:7 	 global-step:12647	 l-p:0.13414640724658966
epoch£º632	 i:8 	 global-step:12648	 l-p:0.09698288887739182
epoch£º632	 i:9 	 global-step:12649	 l-p:0.19731593132019043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9440, 2.8931, 2.9382],
        [2.9440, 1.8391, 1.5569],
        [2.9440, 2.4973, 2.6936],
        [2.9440, 2.0715, 1.4180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.14459320902824402 
model_pd.l_d.mean(): -24.924205780029297 
model_pd.lagr.mean(): -24.779613494873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1180], device='cuda:0')), ('power', tensor([-25.0422], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.14459320902824402
epoch£º633	 i:1 	 global-step:12661	 l-p:0.14298562705516815
epoch£º633	 i:2 	 global-step:12662	 l-p:-0.11327226459980011
epoch£º633	 i:3 	 global-step:12663	 l-p:0.07679609209299088
epoch£º633	 i:4 	 global-step:12664	 l-p:0.13126403093338013
epoch£º633	 i:5 	 global-step:12665	 l-p:0.14477339386940002
epoch£º633	 i:6 	 global-step:12666	 l-p:0.14722777903079987
epoch£º633	 i:7 	 global-step:12667	 l-p:0.16216005384922028
epoch£º633	 i:8 	 global-step:12668	 l-p:0.05842919647693634
epoch£º633	 i:9 	 global-step:12669	 l-p:0.21946488320827484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0168, 3.0168, 3.0168],
        [3.0168, 3.0034, 3.0161],
        [3.0168, 2.6112, 2.8057],
        [3.0168, 2.6547, 2.8448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.12404884397983551 
model_pd.l_d.mean(): -24.897239685058594 
model_pd.lagr.mean(): -24.773191452026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0390], device='cuda:0')), ('power', tensor([-24.9362], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.12404884397983551
epoch£º634	 i:1 	 global-step:12681	 l-p:0.145010843873024
epoch£º634	 i:2 	 global-step:12682	 l-p:0.14926046133041382
epoch£º634	 i:3 	 global-step:12683	 l-p:0.1636018306016922
epoch£º634	 i:4 	 global-step:12684	 l-p:1.9871147871017456
epoch£º634	 i:5 	 global-step:12685	 l-p:0.13573172688484192
epoch£º634	 i:6 	 global-step:12686	 l-p:0.1880122274160385
epoch£º634	 i:7 	 global-step:12687	 l-p:0.8123436570167542
epoch£º634	 i:8 	 global-step:12688	 l-p:0.20964570343494415
epoch£º634	 i:9 	 global-step:12689	 l-p:0.13748860359191895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0809, 2.4494, 2.6032],
        [3.0809, 2.5037, 2.6765],
        [3.0809, 3.0778, 3.0808],
        [3.0809, 1.8380, 1.3267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.13767972588539124 
model_pd.l_d.mean(): -25.12110137939453 
model_pd.lagr.mean(): -24.983421325683594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0029], device='cuda:0')), ('power', tensor([-25.1240], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.13767972588539124
epoch£º635	 i:1 	 global-step:12701	 l-p:0.1244521513581276
epoch£º635	 i:2 	 global-step:12702	 l-p:0.19274196028709412
epoch£º635	 i:3 	 global-step:12703	 l-p:0.13486064970493317
epoch£º635	 i:4 	 global-step:12704	 l-p:0.14366750419139862
epoch£º635	 i:5 	 global-step:12705	 l-p:0.16883715987205505
epoch£º635	 i:6 	 global-step:12706	 l-p:0.23125335574150085
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12765318155288696
epoch£º635	 i:8 	 global-step:12708	 l-p:0.2736765444278717
epoch£º635	 i:9 	 global-step:12709	 l-p:0.15663862228393555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0627, 2.8353, 2.9874],
        [3.0627, 2.9995, 3.0543],
        [3.0627, 1.8104, 1.2400],
        [3.0627, 3.0624, 3.0627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.0918506607413292 
model_pd.l_d.mean(): -25.0039119720459 
model_pd.lagr.mean(): -24.91206169128418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0738], device='cuda:0')), ('power', tensor([-24.9301], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.0918506607413292
epoch£º636	 i:1 	 global-step:12721	 l-p:0.12006387114524841
epoch£º636	 i:2 	 global-step:12722	 l-p:0.18224118649959564
epoch£º636	 i:3 	 global-step:12723	 l-p:0.11727573722600937
epoch£º636	 i:4 	 global-step:12724	 l-p:0.19149573147296906
epoch£º636	 i:5 	 global-step:12725	 l-p:0.1811847984790802
epoch£º636	 i:6 	 global-step:12726	 l-p:0.12963062524795532
epoch£º636	 i:7 	 global-step:12727	 l-p:0.14025424420833588
epoch£º636	 i:8 	 global-step:12728	 l-p:0.1437072902917862
epoch£º636	 i:9 	 global-step:12729	 l-p:0.14556260406970978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1010, 2.2845, 2.3259],
        [3.1010, 2.2317, 1.5586],
        [3.1010, 2.5701, 2.7541],
        [3.1010, 3.0410, 3.0933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.14011089503765106 
model_pd.l_d.mean(): -24.706947326660156 
model_pd.lagr.mean(): -24.566837310791016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0148], device='cuda:0')), ('power', tensor([-24.7217], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.14011089503765106
epoch£º637	 i:1 	 global-step:12741	 l-p:0.11811350286006927
epoch£º637	 i:2 	 global-step:12742	 l-p:0.2372979074716568
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14008845388889313
epoch£º637	 i:4 	 global-step:12744	 l-p:0.12206146121025085
epoch£º637	 i:5 	 global-step:12745	 l-p:0.13092707097530365
epoch£º637	 i:6 	 global-step:12746	 l-p:0.38877761363983154
epoch£º637	 i:7 	 global-step:12747	 l-p:0.13174311816692352
epoch£º637	 i:8 	 global-step:12748	 l-p:0.206659734249115
epoch£º637	 i:9 	 global-step:12749	 l-p:0.11307784169912338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9383, 2.9373, 2.9383],
        [2.9383, 2.9384, 2.9383],
        [2.9383, 1.8313, 1.2239],
        [2.9383, 1.9632, 1.3290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.1667127013206482 
model_pd.l_d.mean(): -25.170209884643555 
model_pd.lagr.mean(): -25.003498077392578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1917], device='cuda:0')), ('power', tensor([-25.3619], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.1667127013206482
epoch£º638	 i:1 	 global-step:12761	 l-p:0.07327062636613846
epoch£º638	 i:2 	 global-step:12762	 l-p:0.4837189316749573
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12918376922607422
epoch£º638	 i:4 	 global-step:12764	 l-p:0.14690738916397095
epoch£º638	 i:5 	 global-step:12765	 l-p:0.22814850509166718
epoch£º638	 i:6 	 global-step:12766	 l-p:0.13374078273773193
epoch£º638	 i:7 	 global-step:12767	 l-p:0.143063023686409
epoch£º638	 i:8 	 global-step:12768	 l-p:41.26328659057617
epoch£º638	 i:9 	 global-step:12769	 l-p:0.1158081591129303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0631, 2.1128, 1.4561],
        [3.0631, 3.0498, 3.0625],
        [3.0631, 2.1423, 2.0888],
        [3.0631, 2.2181, 2.2383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.07693890482187271 
model_pd.l_d.mean(): -24.317760467529297 
model_pd.lagr.mean(): -24.240821838378906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2184], device='cuda:0')), ('power', tensor([-24.5361], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.07693890482187271
epoch£º639	 i:1 	 global-step:12781	 l-p:0.1547633856534958
epoch£º639	 i:2 	 global-step:12782	 l-p:0.16516944766044617
epoch£º639	 i:3 	 global-step:12783	 l-p:0.14776672422885895
epoch£º639	 i:4 	 global-step:12784	 l-p:0.13829223811626434
epoch£º639	 i:5 	 global-step:12785	 l-p:0.2432040125131607
epoch£º639	 i:6 	 global-step:12786	 l-p:0.130639910697937
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14260996878147125
epoch£º639	 i:8 	 global-step:12788	 l-p:0.14485810697078705
epoch£º639	 i:9 	 global-step:12789	 l-p:0.1331413984298706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0395, 2.1525, 2.1356],
        [3.0395, 1.9377, 1.6515],
        [3.0395, 3.0395, 3.0395],
        [3.0395, 2.9761, 3.0310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.41189196705818176 
model_pd.l_d.mean(): -24.956335067749023 
model_pd.lagr.mean(): -24.544443130493164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1420], device='cuda:0')), ('power', tensor([-25.0983], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.41189196705818176
epoch£º640	 i:1 	 global-step:12801	 l-p:0.1463395059108734
epoch£º640	 i:2 	 global-step:12802	 l-p:0.25763487815856934
epoch£º640	 i:3 	 global-step:12803	 l-p:-2.2803218364715576
epoch£º640	 i:4 	 global-step:12804	 l-p:0.14300021529197693
epoch£º640	 i:5 	 global-step:12805	 l-p:0.1231120303273201
epoch£º640	 i:6 	 global-step:12806	 l-p:0.1251993030309677
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12624870240688324
epoch£º640	 i:8 	 global-step:12808	 l-p:0.09551993757486343
epoch£º640	 i:9 	 global-step:12809	 l-p:0.14599740505218506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0071, 3.0022, 3.0070],
        [3.0071, 1.7625, 1.1941],
        [3.0071, 3.0071, 3.0071],
        [3.0071, 2.7786, 2.9315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.530495285987854 
model_pd.l_d.mean(): -25.18523406982422 
model_pd.lagr.mean(): -24.654739379882812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0569], device='cuda:0')), ('power', tensor([-25.2422], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.530495285987854
epoch£º641	 i:1 	 global-step:12821	 l-p:0.03151782974600792
epoch£º641	 i:2 	 global-step:12822	 l-p:0.10698346793651581
epoch£º641	 i:3 	 global-step:12823	 l-p:0.0011405515251681209
epoch£º641	 i:4 	 global-step:12824	 l-p:0.14574526250362396
epoch£º641	 i:5 	 global-step:12825	 l-p:0.1691114455461502
epoch£º641	 i:6 	 global-step:12826	 l-p:0.13703931868076324
epoch£º641	 i:7 	 global-step:12827	 l-p:0.19207696616649628
epoch£º641	 i:8 	 global-step:12828	 l-p:0.009397439658641815
epoch£º641	 i:9 	 global-step:12829	 l-p:0.14031703770160675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0185, 3.0177, 3.0185],
        [3.0185, 2.1697, 2.1905],
        [3.0185, 3.0185, 3.0185],
        [3.0185, 2.0823, 1.4291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.006756219547241926 
model_pd.l_d.mean(): -24.67055892944336 
model_pd.lagr.mean(): -24.663803100585938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1938], device='cuda:0')), ('power', tensor([-24.8644], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.006756219547241926
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15957488119602203
epoch£º642	 i:2 	 global-step:12842	 l-p:0.07653142511844635
epoch£º642	 i:3 	 global-step:12843	 l-p:0.1357704997062683
epoch£º642	 i:4 	 global-step:12844	 l-p:0.24258777499198914
epoch£º642	 i:5 	 global-step:12845	 l-p:0.12589852511882782
epoch£º642	 i:6 	 global-step:12846	 l-p:0.13086842000484467
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11015775799751282
epoch£º642	 i:8 	 global-step:12848	 l-p:-0.002086319960653782
epoch£º642	 i:9 	 global-step:12849	 l-p:0.15368878841400146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0294, 2.7781, 2.9399],
        [3.0294, 3.0294, 3.0294],
        [3.0294, 1.7791, 1.2593],
        [3.0294, 3.0294, 3.0294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.1664278209209442 
model_pd.l_d.mean(): -24.52677345275879 
model_pd.lagr.mean(): -24.3603458404541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1177], device='cuda:0')), ('power', tensor([-24.6444], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.1664278209209442
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12279259413480759
epoch£º643	 i:2 	 global-step:12862	 l-p:0.13324101269245148
epoch£º643	 i:3 	 global-step:12863	 l-p:0.3249194324016571
epoch£º643	 i:4 	 global-step:12864	 l-p:0.18855248391628265
epoch£º643	 i:5 	 global-step:12865	 l-p:0.05294153466820717
epoch£º643	 i:6 	 global-step:12866	 l-p:0.09888812154531479
epoch£º643	 i:7 	 global-step:12867	 l-p:0.07023615390062332
epoch£º643	 i:8 	 global-step:12868	 l-p:0.12386354058980942
epoch£º643	 i:9 	 global-step:12869	 l-p:0.13846330344676971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[3.0219, 2.0138, 1.8624],
        [3.0219, 1.7717, 1.2533],
        [3.0219, 2.1603, 2.1699],
        [3.0219, 2.6336, 2.8272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.03239123150706291 
model_pd.l_d.mean(): -25.04545021057129 
model_pd.lagr.mean(): -25.013059616088867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0962], device='cuda:0')), ('power', tensor([-25.1417], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.03239123150706291
epoch£º644	 i:1 	 global-step:12881	 l-p:0.1585507094860077
epoch£º644	 i:2 	 global-step:12882	 l-p:0.1405440717935562
epoch£º644	 i:3 	 global-step:12883	 l-p:0.1292540729045868
epoch£º644	 i:4 	 global-step:12884	 l-p:0.1673911064863205
epoch£º644	 i:5 	 global-step:12885	 l-p:0.15413884818553925
epoch£º644	 i:6 	 global-step:12886	 l-p:0.1453603208065033
epoch£º644	 i:7 	 global-step:12887	 l-p:0.04023974761366844
epoch£º644	 i:8 	 global-step:12888	 l-p:0.07536781579256058
epoch£º644	 i:9 	 global-step:12889	 l-p:0.10182901471853256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9302, 2.7113, 2.8604],
        [2.9302, 2.1584, 2.2449],
        [2.9302, 1.6970, 1.1372],
        [2.9302, 2.9188, 2.9296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.17689479887485504 
model_pd.l_d.mean(): -24.962833404541016 
model_pd.lagr.mean(): -24.785938262939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1406], device='cuda:0')), ('power', tensor([-25.1035], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.17689479887485504
epoch£º645	 i:1 	 global-step:12901	 l-p:0.0433025024831295
epoch£º645	 i:2 	 global-step:12902	 l-p:0.12826327979564667
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11879824101924896
epoch£º645	 i:4 	 global-step:12904	 l-p:0.13035443425178528
epoch£º645	 i:5 	 global-step:12905	 l-p:0.29236677289009094
epoch£º645	 i:6 	 global-step:12906	 l-p:0.14842748641967773
epoch£º645	 i:7 	 global-step:12907	 l-p:0.39272746443748474
epoch£º645	 i:8 	 global-step:12908	 l-p:0.0239584818482399
epoch£º645	 i:9 	 global-step:12909	 l-p:0.13301977515220642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[3.0443, 1.7869, 1.2329],
        [3.0443, 2.1045, 1.4478],
        [3.0443, 2.1319, 1.4709],
        [3.0443, 2.2222, 2.2652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.739592432975769 
model_pd.l_d.mean(): -24.695350646972656 
model_pd.lagr.mean(): -23.955759048461914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1259], device='cuda:0')), ('power', tensor([-24.8212], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.739592432975769
epoch£º646	 i:1 	 global-step:12921	 l-p:0.1672113984823227
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12754006683826447
epoch£º646	 i:3 	 global-step:12923	 l-p:0.13821029663085938
epoch£º646	 i:4 	 global-step:12924	 l-p:0.1399984210729599
epoch£º646	 i:5 	 global-step:12925	 l-p:0.5562291741371155
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12500648200511932
epoch£º646	 i:7 	 global-step:12927	 l-p:0.1308363378047943
epoch£º646	 i:8 	 global-step:12928	 l-p:0.14145375788211823
epoch£º646	 i:9 	 global-step:12929	 l-p:-1.097746729850769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0265, 3.0265, 3.0265],
        [3.0265, 3.0255, 3.0264],
        [3.0265, 1.7768, 1.2636],
        [3.0265, 3.0253, 3.0264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.04548841714859009 
model_pd.l_d.mean(): -24.94331932067871 
model_pd.lagr.mean(): -24.897830963134766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1143], device='cuda:0')), ('power', tensor([-25.0576], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.04548841714859009
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1344759166240692
epoch£º647	 i:2 	 global-step:12942	 l-p:0.1283494532108307
epoch£º647	 i:3 	 global-step:12943	 l-p:0.13204726576805115
epoch£º647	 i:4 	 global-step:12944	 l-p:0.09570475667715073
epoch£º647	 i:5 	 global-step:12945	 l-p:0.12308281660079956
epoch£º647	 i:6 	 global-step:12946	 l-p:0.25361600518226624
epoch£º647	 i:7 	 global-step:12947	 l-p:-0.01951180398464203
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14657285809516907
epoch£º647	 i:9 	 global-step:12949	 l-p:0.2196207195520401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0159, 1.7756, 1.1970],
        [3.0159, 3.0158, 3.0159],
        [3.0159, 2.8782, 2.9846],
        [3.0159, 3.0000, 3.0150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): -0.008918454870581627 
model_pd.l_d.mean(): -25.225187301635742 
model_pd.lagr.mean(): -25.234106063842773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0251], device='cuda:0')), ('power', tensor([-25.2503], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:-0.008918454870581627
epoch£º648	 i:1 	 global-step:12961	 l-p:0.11358692497015
epoch£º648	 i:2 	 global-step:12962	 l-p:0.14453428983688354
epoch£º648	 i:3 	 global-step:12963	 l-p:0.12807713449001312
epoch£º648	 i:4 	 global-step:12964	 l-p:0.16404063999652863
epoch£º648	 i:5 	 global-step:12965	 l-p:0.2599259614944458
epoch£º648	 i:6 	 global-step:12966	 l-p:0.13685894012451172
epoch£º648	 i:7 	 global-step:12967	 l-p:0.19696378707885742
epoch£º648	 i:8 	 global-step:12968	 l-p:0.1174909770488739
epoch£º648	 i:9 	 global-step:12969	 l-p:0.29483509063720703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9913, 2.8562, 2.9611],
        [2.9913, 2.9810, 2.9909],
        [2.9913, 2.9834, 2.9910],
        [2.9913, 2.1261, 2.1361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12765075266361237 
model_pd.l_d.mean(): -24.955524444580078 
model_pd.lagr.mean(): -24.82787322998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0505], device='cuda:0')), ('power', tensor([-25.0061], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12765075266361237
epoch£º649	 i:1 	 global-step:12981	 l-p:0.12450012564659119
epoch£º649	 i:2 	 global-step:12982	 l-p:0.12320184707641602
epoch£º649	 i:3 	 global-step:12983	 l-p:0.1628052145242691
epoch£º649	 i:4 	 global-step:12984	 l-p:0.19071826338768005
epoch£º649	 i:5 	 global-step:12985	 l-p:0.13754990696907043
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13945452868938446
epoch£º649	 i:7 	 global-step:12987	 l-p:0.15303409099578857
epoch£º649	 i:8 	 global-step:12988	 l-p:-0.09753633290529251
epoch£º649	 i:9 	 global-step:12989	 l-p:0.1700078547000885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9736, 2.6085, 2.8003],
        [2.9736, 2.7970, 2.9256],
        [2.9736, 1.7476, 1.2896],
        [2.9736, 2.3368, 2.4948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.1272158920764923 
model_pd.l_d.mean(): -25.180526733398438 
model_pd.lagr.mean(): -25.05331039428711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0193], device='cuda:0')), ('power', tensor([-25.1999], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.1272158920764923
epoch£º650	 i:1 	 global-step:13001	 l-p:0.053045604377985
epoch£º650	 i:2 	 global-step:13002	 l-p:0.15788637101650238
epoch£º650	 i:3 	 global-step:13003	 l-p:0.9316306710243225
epoch£º650	 i:4 	 global-step:13004	 l-p:0.13521462678909302
epoch£º650	 i:5 	 global-step:13005	 l-p:0.13140808045864105
epoch£º650	 i:6 	 global-step:13006	 l-p:0.12973842024803162
epoch£º650	 i:7 	 global-step:13007	 l-p:0.1473834067583084
epoch£º650	 i:8 	 global-step:13008	 l-p:0.14369378983974457
epoch£º650	 i:9 	 global-step:13009	 l-p:0.1273251324892044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0280, 3.0280, 3.0280],
        [3.0280, 2.0765, 1.4236],
        [3.0280, 3.0280, 3.0280],
        [3.0280, 2.2600, 2.3454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.14975783228874207 
model_pd.l_d.mean(): -25.029983520507812 
model_pd.lagr.mean(): -24.880226135253906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0776], device='cuda:0')), ('power', tensor([-25.1076], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.14975783228874207
epoch£º651	 i:1 	 global-step:13021	 l-p:0.06105927750468254
epoch£º651	 i:2 	 global-step:13022	 l-p:0.22536872327327728
epoch£º651	 i:3 	 global-step:13023	 l-p:0.10126491636037827
epoch£º651	 i:4 	 global-step:13024	 l-p:0.11322750896215439
epoch£º651	 i:5 	 global-step:13025	 l-p:0.1456107348203659
epoch£º651	 i:6 	 global-step:13026	 l-p:0.05259781703352928
epoch£º651	 i:7 	 global-step:13027	 l-p:0.1545170694589615
epoch£º651	 i:8 	 global-step:13028	 l-p:0.16098301112651825
epoch£º651	 i:9 	 global-step:13029	 l-p:0.2882232666015625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0089, 2.5653, 2.7619],
        [3.0089, 2.9679, 3.0048],
        [3.0089, 3.0089, 3.0089],
        [3.0089, 3.0044, 3.0088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.2134282886981964 
model_pd.l_d.mean(): -25.066879272460938 
model_pd.lagr.mean(): -24.853450775146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0340], device='cuda:0')), ('power', tensor([-25.0328], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.2134282886981964
epoch£º652	 i:1 	 global-step:13041	 l-p:0.13914230465888977
epoch£º652	 i:2 	 global-step:13042	 l-p:0.1809021383523941
epoch£º652	 i:3 	 global-step:13043	 l-p:0.36614298820495605
epoch£º652	 i:4 	 global-step:13044	 l-p:0.22628213465213776
epoch£º652	 i:5 	 global-step:13045	 l-p:0.14144456386566162
epoch£º652	 i:6 	 global-step:13046	 l-p:0.1308729350566864
epoch£º652	 i:7 	 global-step:13047	 l-p:0.1310950368642807
epoch£º652	 i:8 	 global-step:13048	 l-p:-0.08330877870321274
epoch£º652	 i:9 	 global-step:13049	 l-p:0.14203816652297974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1286, 1.9589, 1.3267],
        [3.1286, 1.8653, 1.3019],
        [3.1286, 1.9638, 1.3305],
        [3.1286, 2.8780, 3.0392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.13333524763584137 
model_pd.l_d.mean(): -24.901716232299805 
model_pd.lagr.mean(): -24.768381118774414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0195], device='cuda:0')), ('power', tensor([-24.8822], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.13333524763584137
epoch£º653	 i:1 	 global-step:13061	 l-p:0.1137075200676918
epoch£º653	 i:2 	 global-step:13062	 l-p:0.1293625831604004
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12575878202915192
epoch£º653	 i:4 	 global-step:13064	 l-p:0.1391148567199707
epoch£º653	 i:5 	 global-step:13065	 l-p:0.17788554728031158
epoch£º653	 i:6 	 global-step:13066	 l-p:-0.03474779427051544
epoch£º653	 i:7 	 global-step:13067	 l-p:0.0691692978143692
epoch£º653	 i:8 	 global-step:13068	 l-p:-10.419858932495117
epoch£º653	 i:9 	 global-step:13069	 l-p:0.21457546949386597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0199, 1.8434, 1.2351],
        [3.0199, 2.3259, 2.4564],
        [3.0199, 3.0199, 3.0199],
        [3.0199, 2.2007, 2.2490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.051543302834033966 
model_pd.l_d.mean(): -24.82445526123047 
model_pd.lagr.mean(): -24.772911071777344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0881], device='cuda:0')), ('power', tensor([-24.9126], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.051543302834033966
epoch£º654	 i:1 	 global-step:13081	 l-p:0.17418284714221954
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14282657206058502
epoch£º654	 i:3 	 global-step:13083	 l-p:0.14674118161201477
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13519573211669922
epoch£º654	 i:5 	 global-step:13085	 l-p:0.14755795896053314
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12463302165269852
epoch£º654	 i:7 	 global-step:13087	 l-p:0.16314461827278137
epoch£º654	 i:8 	 global-step:13088	 l-p:0.14595521986484528
epoch£º654	 i:9 	 global-step:13089	 l-p:0.12372463196516037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[3.1067, 1.9985, 1.7020],
        [3.1067, 1.8856, 1.4207],
        [3.1067, 2.2497, 2.2608],
        [3.1067, 2.5794, 2.7659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.16913272440433502 
model_pd.l_d.mean(): -25.136228561401367 
model_pd.lagr.mean(): -24.96709632873535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0041], device='cuda:0')), ('power', tensor([-25.1321], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.16913272440433502
epoch£º655	 i:1 	 global-step:13101	 l-p:0.1899934709072113
epoch£º655	 i:2 	 global-step:13102	 l-p:0.13016213476657867
epoch£º655	 i:3 	 global-step:13103	 l-p:0.1503606140613556
epoch£º655	 i:4 	 global-step:13104	 l-p:0.13736271858215332
epoch£º655	 i:5 	 global-step:13105	 l-p:0.12253700196743011
epoch£º655	 i:6 	 global-step:13106	 l-p:0.12810318171977997
epoch£º655	 i:7 	 global-step:13107	 l-p:0.5816961526870728
epoch£º655	 i:8 	 global-step:13108	 l-p:0.14362213015556335
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12936842441558838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0131, 2.9398, 3.0024],
        [3.0131, 2.8948, 2.9890],
        [3.0131, 2.9607, 3.0070],
        [3.0131, 1.7562, 1.2239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.12268950790166855 
model_pd.l_d.mean(): -25.098949432373047 
model_pd.lagr.mean(): -24.976259231567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0105], device='cuda:0')), ('power', tensor([-25.0885], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.12268950790166855
epoch£º656	 i:1 	 global-step:13121	 l-p:0.13697850704193115
epoch£º656	 i:2 	 global-step:13122	 l-p:0.46263155341148376
epoch£º656	 i:3 	 global-step:13123	 l-p:0.14744339883327484
epoch£º656	 i:4 	 global-step:13124	 l-p:0.15330156683921814
epoch£º656	 i:5 	 global-step:13125	 l-p:0.2682323753833771
epoch£º656	 i:6 	 global-step:13126	 l-p:0.16675271093845367
epoch£º656	 i:7 	 global-step:13127	 l-p:0.0367267020046711
epoch£º656	 i:8 	 global-step:13128	 l-p:0.14133548736572266
epoch£º656	 i:9 	 global-step:13129	 l-p:0.12154774367809296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[2.9856, 2.1572, 2.2011],
        [2.9856, 1.9058, 1.6643],
        [2.9856, 1.7523, 1.1744],
        [2.9856, 1.7296, 1.1893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.15437589585781097 
model_pd.l_d.mean(): -25.1492977142334 
model_pd.lagr.mean(): -24.994922637939453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1177], device='cuda:0')), ('power', tensor([-25.2670], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.15437589585781097
epoch£º657	 i:1 	 global-step:13141	 l-p:0.2428387999534607
epoch£º657	 i:2 	 global-step:13142	 l-p:0.1585683971643448
epoch£º657	 i:3 	 global-step:13143	 l-p:0.1389349102973938
epoch£º657	 i:4 	 global-step:13144	 l-p:0.24047210812568665
epoch£º657	 i:5 	 global-step:13145	 l-p:-0.15480384230613708
epoch£º657	 i:6 	 global-step:13146	 l-p:1.0619986057281494
epoch£º657	 i:7 	 global-step:13147	 l-p:0.0983525887131691
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1402498334646225
epoch£º657	 i:9 	 global-step:13149	 l-p:0.1338610202074051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1170, 3.1168, 3.1170],
        [3.1170, 1.9911, 1.6678],
        [3.1170, 2.2110, 2.1749],
        [3.1170, 2.1579, 2.0636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.16653834283351898 
model_pd.l_d.mean(): -24.914417266845703 
model_pd.lagr.mean(): -24.747879028320312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0112], device='cuda:0')), ('power', tensor([-24.9032], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.16653834283351898
epoch£º658	 i:1 	 global-step:13161	 l-p:0.13512246310710907
epoch£º658	 i:2 	 global-step:13162	 l-p:0.13444606959819794
epoch£º658	 i:3 	 global-step:13163	 l-p:0.14755861461162567
epoch£º658	 i:4 	 global-step:13164	 l-p:0.1495283544063568
epoch£º658	 i:5 	 global-step:13165	 l-p:0.15062126517295837
epoch£º658	 i:6 	 global-step:13166	 l-p:0.13462601602077484
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12846288084983826
epoch£º658	 i:8 	 global-step:13168	 l-p:0.1458589881658554
epoch£º658	 i:9 	 global-step:13169	 l-p:0.0986969918012619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0523, 1.8065, 1.2203],
        [3.0523, 3.0512, 3.0523],
        [3.0523, 1.9911, 1.7714],
        [3.0523, 3.0491, 3.0522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12484129518270493 
model_pd.l_d.mean(): -25.045143127441406 
model_pd.lagr.mean(): -24.92030143737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0489], device='cuda:0')), ('power', tensor([-24.9963], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12484129518270493
epoch£º659	 i:1 	 global-step:13181	 l-p:-0.12450934946537018
epoch£º659	 i:2 	 global-step:13182	 l-p:0.15639039874076843
epoch£º659	 i:3 	 global-step:13183	 l-p:0.1278606504201889
epoch£º659	 i:4 	 global-step:13184	 l-p:0.10142780095338821
epoch£º659	 i:5 	 global-step:13185	 l-p:0.14118976891040802
epoch£º659	 i:6 	 global-step:13186	 l-p:0.08462671935558319
epoch£º659	 i:7 	 global-step:13187	 l-p:-0.06194122135639191
epoch£º659	 i:8 	 global-step:13188	 l-p:0.1681772619485855
epoch£º659	 i:9 	 global-step:13189	 l-p:0.14198510348796844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9582, 1.8440, 1.2323],
        [2.9582, 2.4845, 2.6814],
        [2.9582, 1.7286, 1.1548],
        [2.9582, 2.9579, 2.9582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14133867621421814 
model_pd.l_d.mean(): -24.86943817138672 
model_pd.lagr.mean(): -24.728099822998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2211], device='cuda:0')), ('power', tensor([-25.0905], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14133867621421814
epoch£º660	 i:1 	 global-step:13201	 l-p:0.15899747610092163
epoch£º660	 i:2 	 global-step:13202	 l-p:0.14703528583049774
epoch£º660	 i:3 	 global-step:13203	 l-p:0.1089489534497261
epoch£º660	 i:4 	 global-step:13204	 l-p:0.31915393471717834
epoch£º660	 i:5 	 global-step:13205	 l-p:0.10920175909996033
epoch£º660	 i:6 	 global-step:13206	 l-p:0.1742924302816391
epoch£º660	 i:7 	 global-step:13207	 l-p:0.12013429403305054
epoch£º660	 i:8 	 global-step:13208	 l-p:0.12161128968000412
epoch£º660	 i:9 	 global-step:13209	 l-p:0.14159420132637024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0839, 3.0808, 3.0839],
        [3.0839, 2.6939, 2.8879],
        [3.0839, 2.9891, 3.0674],
        [3.0839, 2.4781, 2.6451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13563264906406403 
model_pd.l_d.mean(): -24.91476058959961 
model_pd.lagr.mean(): -24.77912712097168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0376], device='cuda:0')), ('power', tensor([-24.9524], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13563264906406403
epoch£º661	 i:1 	 global-step:13221	 l-p:0.13093973696231842
epoch£º661	 i:2 	 global-step:13222	 l-p:0.24457284808158875
epoch£º661	 i:3 	 global-step:13223	 l-p:0.15839986503124237
epoch£º661	 i:4 	 global-step:13224	 l-p:0.3005636930465698
epoch£º661	 i:5 	 global-step:13225	 l-p:0.10327009856700897
epoch£º661	 i:6 	 global-step:13226	 l-p:0.13559365272521973
epoch£º661	 i:7 	 global-step:13227	 l-p:0.354693740606308
epoch£º661	 i:8 	 global-step:13228	 l-p:0.17066386342048645
epoch£º661	 i:9 	 global-step:13229	 l-p:0.08925192803144455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0230, 3.0230, 3.0230],
        [3.0230, 3.0230, 3.0230],
        [3.0230, 2.9456, 3.0113],
        [3.0230, 2.2517, 2.3374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13555637001991272 
model_pd.l_d.mean(): -25.142013549804688 
model_pd.lagr.mean(): -25.00645637512207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1078], device='cuda:0')), ('power', tensor([-25.0342], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13555637001991272
epoch£º662	 i:1 	 global-step:13241	 l-p:0.17531734704971313
epoch£º662	 i:2 	 global-step:13242	 l-p:0.1358262449502945
epoch£º662	 i:3 	 global-step:13243	 l-p:0.21473586559295654
epoch£º662	 i:4 	 global-step:13244	 l-p:-0.9846655130386353
epoch£º662	 i:5 	 global-step:13245	 l-p:0.1235627606511116
epoch£º662	 i:6 	 global-step:13246	 l-p:0.7168195843696594
epoch£º662	 i:7 	 global-step:13247	 l-p:0.11015138030052185
epoch£º662	 i:8 	 global-step:13248	 l-p:0.13635863363742828
epoch£º662	 i:9 	 global-step:13249	 l-p:0.17611342668533325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1029, 1.9358, 1.3070],
        [3.1029, 2.8806, 3.0309],
        [3.1029, 2.0243, 1.7772],
        [3.1029, 3.0580, 3.0981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12460672110319138 
model_pd.l_d.mean(): -25.031536102294922 
model_pd.lagr.mean(): -24.90692901611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0939], device='cuda:0')), ('power', tensor([-24.9377], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12460672110319138
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11063539236783981
epoch£º663	 i:2 	 global-step:13262	 l-p:0.14425325393676758
epoch£º663	 i:3 	 global-step:13263	 l-p:0.15553376078605652
epoch£º663	 i:4 	 global-step:13264	 l-p:0.19943183660507202
epoch£º663	 i:5 	 global-step:13265	 l-p:0.12660090625286102
epoch£º663	 i:6 	 global-step:13266	 l-p:0.1356152445077896
epoch£º663	 i:7 	 global-step:13267	 l-p:0.16127440333366394
epoch£º663	 i:8 	 global-step:13268	 l-p:0.18538706004619598
epoch£º663	 i:9 	 global-step:13269	 l-p:0.19213977456092834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0972, 3.0971, 3.0972],
        [3.0972, 1.9676, 1.6450],
        [3.0972, 3.0912, 3.0970],
        [3.0972, 2.3770, 2.4916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.16201485693454742 
model_pd.l_d.mean(): -25.008569717407227 
model_pd.lagr.mean(): -24.846555709838867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0047], device='cuda:0')), ('power', tensor([-25.0039], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.16201485693454742
epoch£º664	 i:1 	 global-step:13281	 l-p:0.1425727903842926
epoch£º664	 i:2 	 global-step:13282	 l-p:0.11974603682756424
epoch£º664	 i:3 	 global-step:13283	 l-p:0.14770273864269257
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12491908669471741
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12465136498212814
epoch£º664	 i:6 	 global-step:13286	 l-p:0.14163535833358765
epoch£º664	 i:7 	 global-step:13287	 l-p:0.08089368045330048
epoch£º664	 i:8 	 global-step:13288	 l-p:0.27788734436035156
epoch£º664	 i:9 	 global-step:13289	 l-p:0.3326520025730133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0642, 3.0563, 3.0639],
        [3.0642, 3.0561, 3.0639],
        [3.0642, 1.9501, 1.6550],
        [3.0642, 2.7886, 2.9593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.3506130874156952 
model_pd.l_d.mean(): -25.088815689086914 
model_pd.lagr.mean(): -24.738203048706055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0813], device='cuda:0')), ('power', tensor([-25.1701], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.3506130874156952
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13713228702545166
epoch£º665	 i:2 	 global-step:13302	 l-p:0.15701472759246826
epoch£º665	 i:3 	 global-step:13303	 l-p:0.19452975690364838
epoch£º665	 i:4 	 global-step:13304	 l-p:0.21182622015476227
epoch£º665	 i:5 	 global-step:13305	 l-p:0.12720006704330444
epoch£º665	 i:6 	 global-step:13306	 l-p:0.12716850638389587
epoch£º665	 i:7 	 global-step:13307	 l-p:0.13264749944210052
epoch£º665	 i:8 	 global-step:13308	 l-p:0.13689467310905457
epoch£º665	 i:9 	 global-step:13309	 l-p:0.12092512100934982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0489, 3.0489, 3.0489],
        [3.0489, 1.7876, 1.2200],
        [3.0489, 1.7916, 1.2160],
        [3.0489, 2.3635, 2.4992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.1258104294538498 
model_pd.l_d.mean(): -24.533124923706055 
model_pd.lagr.mean(): -24.40731430053711 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1238], device='cuda:0')), ('power', tensor([-24.6569], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.1258104294538498
epoch£º666	 i:1 	 global-step:13321	 l-p:0.2680441737174988
epoch£º666	 i:2 	 global-step:13322	 l-p:-0.00012836932728532702
epoch£º666	 i:3 	 global-step:13323	 l-p:0.13712096214294434
epoch£º666	 i:4 	 global-step:13324	 l-p:0.04537621885538101
epoch£º666	 i:5 	 global-step:13325	 l-p:0.1449676901102066
epoch£º666	 i:6 	 global-step:13326	 l-p:0.19717389345169067
epoch£º666	 i:7 	 global-step:13327	 l-p:0.08376191556453705
epoch£º666	 i:8 	 global-step:13328	 l-p:0.12870483100414276
epoch£º666	 i:9 	 global-step:13329	 l-p:0.1665113866329193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[3.0143, 2.1022, 1.4429],
        [3.0143, 2.0694, 2.0029],
        [3.0143, 1.9385, 1.3078],
        [3.0143, 1.8660, 1.5293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.4460345506668091 
model_pd.l_d.mean(): -25.080472946166992 
model_pd.lagr.mean(): -24.634437561035156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1364], device='cuda:0')), ('power', tensor([-25.2169], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.4460345506668091
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11412065476179123
epoch£º667	 i:2 	 global-step:13342	 l-p:0.1398671269416809
epoch£º667	 i:3 	 global-step:13343	 l-p:0.12720614671707153
epoch£º667	 i:4 	 global-step:13344	 l-p:0.16268806159496307
epoch£º667	 i:5 	 global-step:13345	 l-p:0.1711476445198059
epoch£º667	 i:6 	 global-step:13346	 l-p:0.09285536408424377
epoch£º667	 i:7 	 global-step:13347	 l-p:0.08374953269958496
epoch£º667	 i:8 	 global-step:13348	 l-p:0.1287420392036438
epoch£º667	 i:9 	 global-step:13349	 l-p:0.2433425486087799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0371, 2.9325, 3.0176],
        [3.0371, 3.0370, 3.0371],
        [3.0371, 2.1962, 2.2299],
        [3.0371, 1.7731, 1.2198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.10152220726013184 
model_pd.l_d.mean(): -24.843414306640625 
model_pd.lagr.mean(): -24.741891860961914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0619], device='cuda:0')), ('power', tensor([-24.9053], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.10152220726013184
epoch£º668	 i:1 	 global-step:13361	 l-p:0.13585807383060455
epoch£º668	 i:2 	 global-step:13362	 l-p:0.1762811690568924
epoch£º668	 i:3 	 global-step:13363	 l-p:0.14624330401420593
epoch£º668	 i:4 	 global-step:13364	 l-p:0.1738128960132599
epoch£º668	 i:5 	 global-step:13365	 l-p:0.38051319122314453
epoch£º668	 i:6 	 global-step:13366	 l-p:0.2935451567173004
epoch£º668	 i:7 	 global-step:13367	 l-p:0.1159437894821167
epoch£º668	 i:8 	 global-step:13368	 l-p:0.17134317755699158
epoch£º668	 i:9 	 global-step:13369	 l-p:0.14150598645210266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1132, 2.4721, 2.6262],
        [3.1132, 3.1129, 3.1132],
        [3.1132, 3.1132, 3.1132],
        [3.1132, 1.9430, 1.5612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.12890316545963287 
model_pd.l_d.mean(): -24.882322311401367 
model_pd.lagr.mean(): -24.753419876098633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1119], device='cuda:0')), ('power', tensor([-24.7704], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.12890316545963287
epoch£º669	 i:1 	 global-step:13381	 l-p:0.13874053955078125
epoch£º669	 i:2 	 global-step:13382	 l-p:0.154091015458107
epoch£º669	 i:3 	 global-step:13383	 l-p:0.157765731215477
epoch£º669	 i:4 	 global-step:13384	 l-p:0.13742569088935852
epoch£º669	 i:5 	 global-step:13385	 l-p:0.15602248907089233
epoch£º669	 i:6 	 global-step:13386	 l-p:0.1395271122455597
epoch£º669	 i:7 	 global-step:13387	 l-p:0.060698579996824265
epoch£º669	 i:8 	 global-step:13388	 l-p:0.13324223458766937
epoch£º669	 i:9 	 global-step:13389	 l-p:0.14615032076835632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1252, 3.1252, 3.1252],
        [3.1252, 2.8670, 3.0315],
        [3.1252, 3.1203, 3.1251],
        [3.1252, 3.1252, 3.1252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.17995281517505646 
model_pd.l_d.mean(): -25.122053146362305 
model_pd.lagr.mean(): -24.942100524902344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0215], device='cuda:0')), ('power', tensor([-25.1435], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.17995281517505646
epoch£º670	 i:1 	 global-step:13401	 l-p:0.10134485363960266
epoch£º670	 i:2 	 global-step:13402	 l-p:0.12795104086399078
epoch£º670	 i:3 	 global-step:13403	 l-p:0.11867453902959824
epoch£º670	 i:4 	 global-step:13404	 l-p:0.15876099467277527
epoch£º670	 i:5 	 global-step:13405	 l-p:0.13244174420833588
epoch£º670	 i:6 	 global-step:13406	 l-p:0.18148647248744965
epoch£º670	 i:7 	 global-step:13407	 l-p:0.2073378711938858
epoch£º670	 i:8 	 global-step:13408	 l-p:0.12483543157577515
epoch£º670	 i:9 	 global-step:13409	 l-p:0.1328434944152832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0729, 3.0361, 3.0695],
        [3.0729, 3.0606, 3.0724],
        [3.0729, 2.1773, 2.1600],
        [3.0729, 3.0602, 3.0723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.13679702579975128 
model_pd.l_d.mean(): -25.158180236816406 
model_pd.lagr.mean(): -25.02138328552246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0532], device='cuda:0')), ('power', tensor([-25.1050], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.13679702579975128
epoch£º671	 i:1 	 global-step:13421	 l-p:0.17699268460273743
epoch£º671	 i:2 	 global-step:13422	 l-p:0.24077865481376648
epoch£º671	 i:3 	 global-step:13423	 l-p:0.27442649006843567
epoch£º671	 i:4 	 global-step:13424	 l-p:0.15765544772148132
epoch£º671	 i:5 	 global-step:13425	 l-p:0.19119872152805328
epoch£º671	 i:6 	 global-step:13426	 l-p:0.12583288550376892
epoch£º671	 i:7 	 global-step:13427	 l-p:-0.03920027241110802
epoch£º671	 i:8 	 global-step:13428	 l-p:0.1318790316581726
epoch£º671	 i:9 	 global-step:13429	 l-p:0.13821938633918762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1534, 3.0802, 3.1427],
        [3.1534, 1.8850, 1.3328],
        [3.1534, 3.1534, 3.1534],
        [3.1534, 2.3299, 2.3719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.12916404008865356 
model_pd.l_d.mean(): -24.900480270385742 
model_pd.lagr.mean(): -24.771316528320312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0107], device='cuda:0')), ('power', tensor([-24.9112], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.12916404008865356
epoch£º672	 i:1 	 global-step:13441	 l-p:0.1298375129699707
epoch£º672	 i:2 	 global-step:13442	 l-p:0.12641483545303345
epoch£º672	 i:3 	 global-step:13443	 l-p:0.2036358267068863
epoch£º672	 i:4 	 global-step:13444	 l-p:0.1407933086156845
epoch£º672	 i:5 	 global-step:13445	 l-p:0.10831165313720703
epoch£º672	 i:6 	 global-step:13446	 l-p:0.160270094871521
epoch£º672	 i:7 	 global-step:13447	 l-p:0.12399087846279144
epoch£º672	 i:8 	 global-step:13448	 l-p:0.11546044796705246
epoch£º672	 i:9 	 global-step:13449	 l-p:-0.027955850586295128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0152, 3.0143, 3.0152],
        [3.0152, 3.0152, 3.0152],
        [3.0152, 1.7851, 1.1938],
        [3.0152, 2.9792, 3.0120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.133911594748497 
model_pd.l_d.mean(): -24.9943790435791 
model_pd.lagr.mean(): -24.8604679107666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0094], device='cuda:0')), ('power', tensor([-25.0037], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.133911594748497
epoch£º673	 i:1 	 global-step:13461	 l-p:0.14779072999954224
epoch£º673	 i:2 	 global-step:13462	 l-p:0.17890582978725433
epoch£º673	 i:3 	 global-step:13463	 l-p:0.2058231681585312
epoch£º673	 i:4 	 global-step:13464	 l-p:0.12229324132204056
epoch£º673	 i:5 	 global-step:13465	 l-p:0.16553033888339996
epoch£º673	 i:6 	 global-step:13466	 l-p:0.11514842510223389
epoch£º673	 i:7 	 global-step:13467	 l-p:0.08545716851949692
epoch£º673	 i:8 	 global-step:13468	 l-p:0.08878634870052338
epoch£º673	 i:9 	 global-step:13469	 l-p:0.07934476435184479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9870, 2.5570, 2.7556],
        [2.9870, 2.6189, 2.8123],
        [2.9870, 2.3857, 2.5595],
        [2.9870, 1.9132, 1.6890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.15774889290332794 
model_pd.l_d.mean(): -24.787935256958008 
model_pd.lagr.mean(): -24.630186080932617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1680], device='cuda:0')), ('power', tensor([-24.9559], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.15774889290332794
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13249072432518005
epoch£º674	 i:2 	 global-step:13482	 l-p:0.1701190173625946
epoch£º674	 i:3 	 global-step:13483	 l-p:0.12478536367416382
epoch£º674	 i:4 	 global-step:13484	 l-p:0.7225211262702942
epoch£º674	 i:5 	 global-step:13485	 l-p:0.1635390818119049
epoch£º674	 i:6 	 global-step:13486	 l-p:0.15293815732002258
epoch£º674	 i:7 	 global-step:13487	 l-p:0.1357065886259079
epoch£º674	 i:8 	 global-step:13488	 l-p:0.07252594083547592
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13073937594890594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1307, 2.4248, 2.5483],
        [3.1307, 2.1377, 2.0094],
        [3.1307, 2.7747, 2.9648],
        [3.1307, 2.8653, 3.0325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.13709045946598053 
model_pd.l_d.mean(): -24.912065505981445 
model_pd.lagr.mean(): -24.774974822998047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0437], device='cuda:0')), ('power', tensor([-24.8683], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.13709045946598053
epoch£º675	 i:1 	 global-step:13501	 l-p:0.1575322300195694
epoch£º675	 i:2 	 global-step:13502	 l-p:0.14104118943214417
epoch£º675	 i:3 	 global-step:13503	 l-p:0.15383149683475494
epoch£º675	 i:4 	 global-step:13504	 l-p:0.13437417149543762
epoch£º675	 i:5 	 global-step:13505	 l-p:0.13924334943294525
epoch£º675	 i:6 	 global-step:13506	 l-p:0.08847108483314514
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12437226623296738
epoch£º675	 i:8 	 global-step:13508	 l-p:0.1541760116815567
epoch£º675	 i:9 	 global-step:13509	 l-p:0.0966983363032341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0092, 3.0091, 3.0093],
        [3.0092, 1.8208, 1.4340],
        [3.0092, 2.9678, 3.0051],
        [3.0092, 2.9957, 3.0086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.06416133046150208 
model_pd.l_d.mean(): -25.00901222229004 
model_pd.lagr.mean(): -24.94485092163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0298], device='cuda:0')), ('power', tensor([-24.9792], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.06416133046150208
epoch£º676	 i:1 	 global-step:13521	 l-p:0.1605803519487381
epoch£º676	 i:2 	 global-step:13522	 l-p:0.18474403023719788
epoch£º676	 i:3 	 global-step:13523	 l-p:0.13335952162742615
epoch£º676	 i:4 	 global-step:13524	 l-p:0.15123777091503143
epoch£º676	 i:5 	 global-step:13525	 l-p:-0.12857404351234436
epoch£º676	 i:6 	 global-step:13526	 l-p:0.19230514764785767
epoch£º676	 i:7 	 global-step:13527	 l-p:0.10029172897338867
epoch£º676	 i:8 	 global-step:13528	 l-p:0.1245359554886818
epoch£º676	 i:9 	 global-step:13529	 l-p:0.13194182515144348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9520, 2.9520, 2.9520],
        [2.9520, 2.5755, 2.7707],
        [2.9520, 2.0798, 2.0945],
        [2.9520, 2.8953, 2.9451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.13318146765232086 
model_pd.l_d.mean(): -25.132320404052734 
model_pd.lagr.mean(): -24.9991397857666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0441], device='cuda:0')), ('power', tensor([-25.1764], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.13318146765232086
epoch£º677	 i:1 	 global-step:13541	 l-p:0.15830758213996887
epoch£º677	 i:2 	 global-step:13542	 l-p:0.11093877255916595
epoch£º677	 i:3 	 global-step:13543	 l-p:0.15213169157505035
epoch£º677	 i:4 	 global-step:13544	 l-p:0.19096504151821136
epoch£º677	 i:5 	 global-step:13545	 l-p:0.14195123314857483
epoch£º677	 i:6 	 global-step:13546	 l-p:0.3218061029911041
epoch£º677	 i:7 	 global-step:13547	 l-p:0.20683208107948303
epoch£º677	 i:8 	 global-step:13548	 l-p:0.12324094772338867
epoch£º677	 i:9 	 global-step:13549	 l-p:0.10766448825597763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9416, 2.8315, 2.9205],
        [2.9416, 2.9363, 2.9414],
        [2.9416, 2.8452, 2.9248],
        [2.9416, 2.8016, 2.9098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.19261755049228668 
model_pd.l_d.mean(): -24.889379501342773 
model_pd.lagr.mean(): -24.696762084960938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1756], device='cuda:0')), ('power', tensor([-25.0650], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.19261755049228668
epoch£º678	 i:1 	 global-step:13561	 l-p:0.14977121353149414
epoch£º678	 i:2 	 global-step:13562	 l-p:0.11617461591959
epoch£º678	 i:3 	 global-step:13563	 l-p:-0.017330264672636986
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12970787286758423
epoch£º678	 i:5 	 global-step:13565	 l-p:0.1283961832523346
epoch£º678	 i:6 	 global-step:13566	 l-p:0.12017621845006943
epoch£º678	 i:7 	 global-step:13567	 l-p:0.17836271226406097
epoch£º678	 i:8 	 global-step:13568	 l-p:0.13822804391384125
epoch£º678	 i:9 	 global-step:13569	 l-p:0.1228155791759491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0959, 3.0957, 3.0959],
        [3.0959, 1.8464, 1.2442],
        [3.0959, 3.0038, 3.0803],
        [3.0959, 2.9192, 3.0480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.08511996269226074 
model_pd.l_d.mean(): -25.013330459594727 
model_pd.lagr.mean(): -24.928211212158203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0168], device='cuda:0')), ('power', tensor([-24.9965], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.08511996269226074
epoch£º679	 i:1 	 global-step:13581	 l-p:0.14009369909763336
epoch£º679	 i:2 	 global-step:13582	 l-p:0.1564178168773651
epoch£º679	 i:3 	 global-step:13583	 l-p:0.16359791159629822
epoch£º679	 i:4 	 global-step:13584	 l-p:0.1900848150253296
epoch£º679	 i:5 	 global-step:13585	 l-p:0.10512621700763702
epoch£º679	 i:6 	 global-step:13586	 l-p:0.23547379672527313
epoch£º679	 i:7 	 global-step:13587	 l-p:0.196575328707695
epoch£º679	 i:8 	 global-step:13588	 l-p:0.1983940154314041
epoch£º679	 i:9 	 global-step:13589	 l-p:0.10887571424245834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0954, 3.0724, 3.0938],
        [3.0954, 3.0872, 3.0951],
        [3.0954, 2.9640, 3.0667],
        [3.0954, 1.9832, 1.3427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.15136487782001495 
model_pd.l_d.mean(): -25.251073837280273 
model_pd.lagr.mean(): -25.099708557128906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0577], device='cuda:0')), ('power', tensor([-25.1934], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.15136487782001495
epoch£º680	 i:1 	 global-step:13601	 l-p:0.1863860785961151
epoch£º680	 i:2 	 global-step:13602	 l-p:0.16554777324199677
epoch£º680	 i:3 	 global-step:13603	 l-p:0.13260111212730408
epoch£º680	 i:4 	 global-step:13604	 l-p:0.2284233272075653
epoch£º680	 i:5 	 global-step:13605	 l-p:0.0451398566365242
epoch£º680	 i:6 	 global-step:13606	 l-p:0.11737456172704697
epoch£º680	 i:7 	 global-step:13607	 l-p:0.14483478665351868
epoch£º680	 i:8 	 global-step:13608	 l-p:0.11284874379634857
epoch£º680	 i:9 	 global-step:13609	 l-p:0.10715944319963455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1151, 2.3394, 2.4226],
        [3.1151, 2.1301, 1.4659],
        [3.1151, 3.1121, 3.1150],
        [3.1151, 3.1119, 3.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.13762257993221283 
model_pd.l_d.mean(): -24.952383041381836 
model_pd.lagr.mean(): -24.814760208129883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0538], device='cuda:0')), ('power', tensor([-24.8986], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.13762257993221283
epoch£º681	 i:1 	 global-step:13621	 l-p:0.09289494901895523
epoch£º681	 i:2 	 global-step:13622	 l-p:0.13319195806980133
epoch£º681	 i:3 	 global-step:13623	 l-p:0.13389529287815094
epoch£º681	 i:4 	 global-step:13624	 l-p:0.13805893063545227
epoch£º681	 i:5 	 global-step:13625	 l-p:-0.07366304844617844
epoch£º681	 i:6 	 global-step:13626	 l-p:0.2020057588815689
epoch£º681	 i:7 	 global-step:13627	 l-p:0.15851916372776031
epoch£º681	 i:8 	 global-step:13628	 l-p:0.1684739887714386
epoch£º681	 i:9 	 global-step:13629	 l-p:0.09128693491220474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9741, 2.6134, 2.8063],
        [2.9741, 2.4915, 2.6899],
        [2.9741, 2.9659, 2.9738],
        [2.9741, 2.9223, 2.9682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.14721521735191345 
model_pd.l_d.mean(): -25.114501953125 
model_pd.lagr.mean(): -24.967287063598633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0509], device='cuda:0')), ('power', tensor([-25.1654], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.14721521735191345
epoch£º682	 i:1 	 global-step:13641	 l-p:0.11968041956424713
epoch£º682	 i:2 	 global-step:13642	 l-p:0.1236296221613884
epoch£º682	 i:3 	 global-step:13643	 l-p:0.14677943289279938
epoch£º682	 i:4 	 global-step:13644	 l-p:0.16085033118724823
epoch£º682	 i:5 	 global-step:13645	 l-p:0.1310393363237381
epoch£º682	 i:6 	 global-step:13646	 l-p:0.1426384150981903
epoch£º682	 i:7 	 global-step:13647	 l-p:0.14522823691368103
epoch£º682	 i:8 	 global-step:13648	 l-p:-0.10358041524887085
epoch£º682	 i:9 	 global-step:13649	 l-p:0.1802755445241928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9603, 2.5052, 2.7052],
        [2.9603, 2.8341, 2.9337],
        [2.9603, 2.9603, 2.9603],
        [2.9603, 2.3496, 2.5229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.15329162776470184 
model_pd.l_d.mean(): -25.296850204467773 
model_pd.lagr.mean(): -25.143558502197266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0605], device='cuda:0')), ('power', tensor([-25.3574], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.15329162776470184
epoch£º683	 i:1 	 global-step:13661	 l-p:0.24125555157661438
epoch£º683	 i:2 	 global-step:13662	 l-p:0.09715761244297028
epoch£º683	 i:3 	 global-step:13663	 l-p:0.10502509772777557
epoch£º683	 i:4 	 global-step:13664	 l-p:0.05267428606748581
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12132484465837479
epoch£º683	 i:6 	 global-step:13666	 l-p:0.24476371705532074
epoch£º683	 i:7 	 global-step:13667	 l-p:-0.03854099288582802
epoch£º683	 i:8 	 global-step:13668	 l-p:0.1405101716518402
epoch£º683	 i:9 	 global-step:13669	 l-p:0.14950227737426758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0829, 1.8140, 1.2822],
        [3.0829, 3.0829, 3.0829],
        [3.0829, 2.0066, 1.3617],
        [3.0829, 3.0754, 3.0826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.19212792813777924 
model_pd.l_d.mean(): -24.982280731201172 
model_pd.lagr.mean(): -24.79015350341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0032], device='cuda:0')), ('power', tensor([-24.9791], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.19212792813777924
epoch£º684	 i:1 	 global-step:13681	 l-p:0.15467004477977753
epoch£º684	 i:2 	 global-step:13682	 l-p:0.12258294969797134
epoch£º684	 i:3 	 global-step:13683	 l-p:0.12993793189525604
epoch£º684	 i:4 	 global-step:13684	 l-p:0.1357937902212143
epoch£º684	 i:5 	 global-step:13685	 l-p:0.13465185463428497
epoch£º684	 i:6 	 global-step:13686	 l-p:0.07947526127099991
epoch£º684	 i:7 	 global-step:13687	 l-p:0.23326370120048523
epoch£º684	 i:8 	 global-step:13688	 l-p:0.12032418698072433
epoch£º684	 i:9 	 global-step:13689	 l-p:0.15540854632854462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1048, 2.8187, 2.9931],
        [3.1048, 2.1159, 1.4535],
        [3.1048, 2.0166, 1.7677],
        [3.1048, 3.1046, 3.1048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.13890957832336426 
model_pd.l_d.mean(): -25.113929748535156 
model_pd.lagr.mean(): -24.975019454956055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0022], device='cuda:0')), ('power', tensor([-25.1117], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.13890957832336426
epoch£º685	 i:1 	 global-step:13701	 l-p:0.19993869960308075
epoch£º685	 i:2 	 global-step:13702	 l-p:0.15431281924247742
epoch£º685	 i:3 	 global-step:13703	 l-p:0.15033064782619476
epoch£º685	 i:4 	 global-step:13704	 l-p:0.08346965909004211
epoch£º685	 i:5 	 global-step:13705	 l-p:0.18117164075374603
epoch£º685	 i:6 	 global-step:13706	 l-p:0.17161479592323303
epoch£º685	 i:7 	 global-step:13707	 l-p:0.12277922034263611
epoch£º685	 i:8 	 global-step:13708	 l-p:0.1475333273410797
epoch£º685	 i:9 	 global-step:13709	 l-p:0.11555749922990799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1207, 3.1196, 3.1207],
        [3.1207, 3.1188, 3.1207],
        [3.1207, 3.0134, 3.1004],
        [3.1207, 2.0266, 1.3779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.14291027188301086 
model_pd.l_d.mean(): -24.5167236328125 
model_pd.lagr.mean(): -24.37381362915039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297], device='cuda:0')), ('power', tensor([-24.6464], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.14291027188301086
epoch£º686	 i:1 	 global-step:13721	 l-p:0.049891360104084015
epoch£º686	 i:2 	 global-step:13722	 l-p:0.14445559680461884
epoch£º686	 i:3 	 global-step:13723	 l-p:0.1370118111371994
epoch£º686	 i:4 	 global-step:13724	 l-p:0.1685047447681427
epoch£º686	 i:5 	 global-step:13725	 l-p:0.15834926068782806
epoch£º686	 i:6 	 global-step:13726	 l-p:0.15475714206695557
epoch£º686	 i:7 	 global-step:13727	 l-p:0.17247693240642548
epoch£º686	 i:8 	 global-step:13728	 l-p:0.1287750005722046
epoch£º686	 i:9 	 global-step:13729	 l-p:0.13447819650173187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[3.1390, 2.0184, 1.3708],
        [3.1390, 1.9025, 1.4282],
        [3.1390, 2.2723, 2.2832],
        [3.1390, 1.9438, 1.3110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): -0.3103106915950775 
model_pd.l_d.mean(): -24.767215728759766 
model_pd.lagr.mean(): -25.077526092529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0976], device='cuda:0')), ('power', tensor([-24.8648], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:-0.3103106915950775
epoch£º687	 i:1 	 global-step:13741	 l-p:0.15034063160419464
epoch£º687	 i:2 	 global-step:13742	 l-p:0.12836751341819763
epoch£º687	 i:3 	 global-step:13743	 l-p:0.13129176199436188
epoch£º687	 i:4 	 global-step:13744	 l-p:0.1390216201543808
epoch£º687	 i:5 	 global-step:13745	 l-p:0.09926198422908783
epoch£º687	 i:6 	 global-step:13746	 l-p:0.24196407198905945
epoch£º687	 i:7 	 global-step:13747	 l-p:0.2545199394226074
epoch£º687	 i:8 	 global-step:13748	 l-p:0.14364105463027954
epoch£º687	 i:9 	 global-step:13749	 l-p:-0.057399384677410126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0416, 3.0416, 3.0416],
        [3.0416, 1.8611, 1.4872],
        [3.0416, 2.3948, 2.5527],
        [3.0416, 3.0414, 3.0416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): -0.480349063873291 
model_pd.l_d.mean(): -24.706830978393555 
model_pd.lagr.mean(): -25.187179565429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1135], device='cuda:0')), ('power', tensor([-24.8203], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:-0.480349063873291
epoch£º688	 i:1 	 global-step:13761	 l-p:0.12398761510848999
epoch£º688	 i:2 	 global-step:13762	 l-p:0.14838282763957977
epoch£º688	 i:3 	 global-step:13763	 l-p:0.17344972491264343
epoch£º688	 i:4 	 global-step:13764	 l-p:0.15502387285232544
epoch£º688	 i:5 	 global-step:13765	 l-p:0.0634646937251091
epoch£º688	 i:6 	 global-step:13766	 l-p:0.8592079877853394
epoch£º688	 i:7 	 global-step:13767	 l-p:0.14583371579647064
epoch£º688	 i:8 	 global-step:13768	 l-p:0.13273151218891144
epoch£º688	 i:9 	 global-step:13769	 l-p:0.1537366807460785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9365, 2.6981, 2.8565],
        [2.9365, 2.9121, 2.9348],
        [2.9365, 2.9364, 2.9365],
        [2.9365, 2.9365, 2.9365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.2268701046705246 
model_pd.l_d.mean(): -25.1610164642334 
model_pd.lagr.mean(): -24.934146881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0429], device='cuda:0')), ('power', tensor([-25.2039], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.2268701046705246
epoch£º689	 i:1 	 global-step:13781	 l-p:0.16174769401550293
epoch£º689	 i:2 	 global-step:13782	 l-p:0.13088461756706238
epoch£º689	 i:3 	 global-step:13783	 l-p:0.14025317132472992
epoch£º689	 i:4 	 global-step:13784	 l-p:0.11816529929637909
epoch£º689	 i:5 	 global-step:13785	 l-p:0.11687522381544113
epoch£º689	 i:6 	 global-step:13786	 l-p:0.03534150496125221
epoch£º689	 i:7 	 global-step:13787	 l-p:0.189051553606987
epoch£º689	 i:8 	 global-step:13788	 l-p:0.15500794351100922
epoch£º689	 i:9 	 global-step:13789	 l-p:0.1265667825937271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0603, 3.0600, 3.0603],
        [3.0603, 2.1089, 1.4467],
        [3.0603, 3.0584, 3.0602],
        [3.0603, 3.0603, 3.0603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.1376652866601944 
model_pd.l_d.mean(): -25.148818969726562 
model_pd.lagr.mean(): -25.011154174804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0320], device='cuda:0')), ('power', tensor([-25.1809], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.1376652866601944
epoch£º690	 i:1 	 global-step:13801	 l-p:0.10571292787790298
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12104938179254532
epoch£º690	 i:3 	 global-step:13803	 l-p:0.2381434440612793
epoch£º690	 i:4 	 global-step:13804	 l-p:0.12386041134595871
epoch£º690	 i:5 	 global-step:13805	 l-p:0.2592097520828247
epoch£º690	 i:6 	 global-step:13806	 l-p:0.1579817235469818
epoch£º690	 i:7 	 global-step:13807	 l-p:0.24595431983470917
epoch£º690	 i:8 	 global-step:13808	 l-p:0.1328207105398178
epoch£º690	 i:9 	 global-step:13809	 l-p:0.12987031042575836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0817, 3.0816, 3.0817],
        [3.0817, 2.6884, 2.8847],
        [3.0817, 1.8085, 1.2333],
        [3.0817, 3.0742, 3.0814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.15738445520401 
model_pd.l_d.mean(): -24.759422302246094 
model_pd.lagr.mean(): -24.60203742980957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0649], device='cuda:0')), ('power', tensor([-24.8243], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.15738445520401
epoch£º691	 i:1 	 global-step:13821	 l-p:0.4539440870285034
epoch£º691	 i:2 	 global-step:13822	 l-p:0.1453283429145813
epoch£º691	 i:3 	 global-step:13823	 l-p:0.14891360700130463
epoch£º691	 i:4 	 global-step:13824	 l-p:0.16612620651721954
epoch£º691	 i:5 	 global-step:13825	 l-p:0.29696863889694214
epoch£º691	 i:6 	 global-step:13826	 l-p:0.17643685638904572
epoch£º691	 i:7 	 global-step:13827	 l-p:0.056239936500787735
epoch£º691	 i:8 	 global-step:13828	 l-p:0.10211770981550217
epoch£º691	 i:9 	 global-step:13829	 l-p:0.12114095687866211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0001, 2.9991, 3.0000],
        [3.0001, 2.9982, 3.0000],
        [3.0001, 2.7721, 2.9259],
        [3.0001, 3.0001, 3.0001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.22608250379562378 
model_pd.l_d.mean(): -25.174989700317383 
model_pd.lagr.mean(): -24.94890785217285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0581], device='cuda:0')), ('power', tensor([-25.1169], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.22608250379562378
epoch£º692	 i:1 	 global-step:13841	 l-p:0.08561479300260544
epoch£º692	 i:2 	 global-step:13842	 l-p:0.08944743871688843
epoch£º692	 i:3 	 global-step:13843	 l-p:0.1369522213935852
epoch£º692	 i:4 	 global-step:13844	 l-p:0.1874988079071045
epoch£º692	 i:5 	 global-step:13845	 l-p:0.30895861983299255
epoch£º692	 i:6 	 global-step:13846	 l-p:0.13834543526172638
epoch£º692	 i:7 	 global-step:13847	 l-p:0.11654576659202576
epoch£º692	 i:8 	 global-step:13848	 l-p:0.13473321497440338
epoch£º692	 i:9 	 global-step:13849	 l-p:0.10262452811002731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1115, 3.0991, 3.1109],
        [3.1115, 1.9436, 1.3097],
        [3.1115, 2.1210, 1.4572],
        [3.1115, 2.1502, 1.4820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.13559864461421967 
model_pd.l_d.mean(): -24.790056228637695 
model_pd.lagr.mean(): -24.654457092285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1743], device='cuda:0')), ('power', tensor([-24.9644], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.13559864461421967
epoch£º693	 i:1 	 global-step:13861	 l-p:0.11174162477254868
epoch£º693	 i:2 	 global-step:13862	 l-p:0.13789969682693481
epoch£º693	 i:3 	 global-step:13863	 l-p:0.14000508189201355
epoch£º693	 i:4 	 global-step:13864	 l-p:0.13079415261745453
epoch£º693	 i:5 	 global-step:13865	 l-p:0.1314939707517624
epoch£º693	 i:6 	 global-step:13866	 l-p:0.6079586148262024
epoch£º693	 i:7 	 global-step:13867	 l-p:0.1658121645450592
epoch£º693	 i:8 	 global-step:13868	 l-p:0.044986724853515625
epoch£º693	 i:9 	 global-step:13869	 l-p:0.417877197265625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0220, 2.7933, 2.9473],
        [3.0220, 1.8514, 1.4967],
        [3.0220, 3.0218, 3.0220],
        [3.0220, 3.0220, 3.0220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.20871321856975555 
model_pd.l_d.mean(): -24.589534759521484 
model_pd.lagr.mean(): -24.380821228027344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1148], device='cuda:0')), ('power', tensor([-24.7043], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.20871321856975555
epoch£º694	 i:1 	 global-step:13881	 l-p:0.35471105575561523
epoch£º694	 i:2 	 global-step:13882	 l-p:0.011931867338716984
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12635421752929688
epoch£º694	 i:4 	 global-step:13884	 l-p:0.12610682845115662
epoch£º694	 i:5 	 global-step:13885	 l-p:0.14110904932022095
epoch£º694	 i:6 	 global-step:13886	 l-p:0.1288144737482071
epoch£º694	 i:7 	 global-step:13887	 l-p:0.18050988018512726
epoch£º694	 i:8 	 global-step:13888	 l-p:0.12880392372608185
epoch£º694	 i:9 	 global-step:13889	 l-p:0.17830684781074524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1010, 2.0991, 1.9703],
        [3.1010, 1.8285, 1.2934],
        [3.1010, 1.8256, 1.2805],
        [3.1010, 3.1000, 3.1010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12313388288021088 
model_pd.l_d.mean(): -24.76824951171875 
model_pd.lagr.mean(): -24.64511489868164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0375], device='cuda:0')), ('power', tensor([-24.8057], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12313388288021088
epoch£º695	 i:1 	 global-step:13901	 l-p:0.14063622057437897
epoch£º695	 i:2 	 global-step:13902	 l-p:0.198882058262825
epoch£º695	 i:3 	 global-step:13903	 l-p:0.09597162157297134
epoch£º695	 i:4 	 global-step:13904	 l-p:0.17318926751613617
epoch£º695	 i:5 	 global-step:13905	 l-p:0.11672844737768173
epoch£º695	 i:6 	 global-step:13906	 l-p:0.12404082715511322
epoch£º695	 i:7 	 global-step:13907	 l-p:0.18426154553890228
epoch£º695	 i:8 	 global-step:13908	 l-p:0.12966953217983246
epoch£º695	 i:9 	 global-step:13909	 l-p:0.17349955439567566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1166, 3.0871, 3.1142],
        [3.1166, 1.8433, 1.3052],
        [3.1166, 2.9618, 3.0786],
        [3.1166, 2.1106, 1.4482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.14296256005764008 
model_pd.l_d.mean(): -25.17070770263672 
model_pd.lagr.mean(): -25.02774429321289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0515], device='cuda:0')), ('power', tensor([-25.1192], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.14296256005764008
epoch£º696	 i:1 	 global-step:13921	 l-p:0.05117062106728554
epoch£º696	 i:2 	 global-step:13922	 l-p:0.14229629933834076
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11467336118221283
epoch£º696	 i:4 	 global-step:13924	 l-p:0.13253732025623322
epoch£º696	 i:5 	 global-step:13925	 l-p:0.2464054822921753
epoch£º696	 i:6 	 global-step:13926	 l-p:0.1458945870399475
epoch£º696	 i:7 	 global-step:13927	 l-p:0.22246791422367096
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13447962701320648
epoch£º696	 i:9 	 global-step:13929	 l-p:0.19725190103054047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0953, 3.0953, 3.0953],
        [3.0953, 3.0952, 3.0953],
        [3.0953, 1.9182, 1.2893],
        [3.0953, 3.0924, 3.0953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.12609700858592987 
model_pd.l_d.mean(): -24.88674545288086 
model_pd.lagr.mean(): -24.760648727416992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0616], device='cuda:0')), ('power', tensor([-24.8251], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.12609700858592987
epoch£º697	 i:1 	 global-step:13941	 l-p:0.1479710191488266
epoch£º697	 i:2 	 global-step:13942	 l-p:0.09387912601232529
epoch£º697	 i:3 	 global-step:13943	 l-p:0.15365684032440186
epoch£º697	 i:4 	 global-step:13944	 l-p:0.17099815607070923
epoch£º697	 i:5 	 global-step:13945	 l-p:0.13361136615276337
epoch£º697	 i:6 	 global-step:13946	 l-p:0.13392317295074463
epoch£º697	 i:7 	 global-step:13947	 l-p:0.2918374240398407
epoch£º697	 i:8 	 global-step:13948	 l-p:0.13030605018138885
epoch£º697	 i:9 	 global-step:13949	 l-p:-0.2190645933151245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0424, 2.9168, 3.0160],
        [3.0424, 3.0341, 3.0421],
        [3.0424, 2.2617, 2.3484],
        [3.0424, 1.9626, 1.7359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.13805028796195984 
model_pd.l_d.mean(): -25.09203338623047 
model_pd.lagr.mean(): -24.953983306884766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0089], device='cuda:0')), ('power', tensor([-25.0832], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.13805028796195984
epoch£º698	 i:1 	 global-step:13961	 l-p:0.04479089006781578
epoch£º698	 i:2 	 global-step:13962	 l-p:0.23397964239120483
epoch£º698	 i:3 	 global-step:13963	 l-p:0.12704576551914215
epoch£º698	 i:4 	 global-step:13964	 l-p:0.12838716804981232
epoch£º698	 i:5 	 global-step:13965	 l-p:0.14225687086582184
epoch£º698	 i:6 	 global-step:13966	 l-p:0.17325033247470856
epoch£º698	 i:7 	 global-step:13967	 l-p:0.5576145648956299
epoch£º698	 i:8 	 global-step:13968	 l-p:0.40462347865104675
epoch£º698	 i:9 	 global-step:13969	 l-p:0.10702773183584213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0884, 3.0651, 3.0869],
        [3.0884, 2.8631, 3.0155],
        [3.0884, 1.9667, 1.6741],
        [3.0884, 3.0885, 3.0885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.17761707305908203 
model_pd.l_d.mean(): -25.061016082763672 
model_pd.lagr.mean(): -24.883399963378906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0169], device='cuda:0')), ('power', tensor([-25.0779], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.17761707305908203
epoch£º699	 i:1 	 global-step:13981	 l-p:0.11801135540008545
epoch£º699	 i:2 	 global-step:13982	 l-p:0.1304321438074112
epoch£º699	 i:3 	 global-step:13983	 l-p:0.18391211330890656
epoch£º699	 i:4 	 global-step:13984	 l-p:0.14093557000160217
epoch£º699	 i:5 	 global-step:13985	 l-p:0.15785744786262512
epoch£º699	 i:6 	 global-step:13986	 l-p:0.18816103041172028
epoch£º699	 i:7 	 global-step:13987	 l-p:0.13465340435504913
epoch£º699	 i:8 	 global-step:13988	 l-p:-0.004743270576000214
epoch£º699	 i:9 	 global-step:13989	 l-p:0.12398359179496765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1423, 3.1257, 3.1414],
        [3.1423, 2.1695, 2.0732],
        [3.1423, 2.3092, 2.3513],
        [3.1423, 2.9107, 3.0656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): -0.4339631497859955 
model_pd.l_d.mean(): -25.021669387817383 
model_pd.lagr.mean(): -25.45563316345215 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0814], device='cuda:0')), ('power', tensor([-24.9403], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:-0.4339631497859955
epoch£º700	 i:1 	 global-step:14001	 l-p:0.14336416125297546
epoch£º700	 i:2 	 global-step:14002	 l-p:0.11404430866241455
epoch£º700	 i:3 	 global-step:14003	 l-p:0.11661939322948456
epoch£º700	 i:4 	 global-step:14004	 l-p:0.1326853334903717
epoch£º700	 i:5 	 global-step:14005	 l-p:0.13985922932624817
epoch£º700	 i:6 	 global-step:14006	 l-p:0.15499593317508698
epoch£º700	 i:7 	 global-step:14007	 l-p:0.16625536978244781
epoch£º700	 i:8 	 global-step:14008	 l-p:0.17576633393764496
epoch£º700	 i:9 	 global-step:14009	 l-p:0.14824752509593964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0589, 3.0346, 3.0572],
        [3.0589, 2.7705, 2.9464],
        [3.0589, 3.0589, 3.0589],
        [3.0589, 2.3792, 2.5238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): -2.9107298851013184 
model_pd.l_d.mean(): -24.920679092407227 
model_pd.lagr.mean(): -27.831409454345703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0239], device='cuda:0')), ('power', tensor([-24.9446], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:-2.9107298851013184
epoch£º701	 i:1 	 global-step:14021	 l-p:0.14531394839286804
epoch£º701	 i:2 	 global-step:14022	 l-p:0.11950299143791199
epoch£º701	 i:3 	 global-step:14023	 l-p:0.1517302244901657
epoch£º701	 i:4 	 global-step:14024	 l-p:1.3367679119110107
epoch£º701	 i:5 	 global-step:14025	 l-p:0.19933123886585236
epoch£º701	 i:6 	 global-step:14026	 l-p:0.1345614641904831
epoch£º701	 i:7 	 global-step:14027	 l-p:0.026665987446904182
epoch£º701	 i:8 	 global-step:14028	 l-p:0.13062016665935516
epoch£º701	 i:9 	 global-step:14029	 l-p:0.1397758275270462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0503, 2.8240, 2.9771],
        [3.0503, 1.9187, 1.2884],
        [3.0503, 2.8314, 2.9812],
        [3.0503, 3.0503, 3.0503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.10932698845863342 
model_pd.l_d.mean(): -24.992321014404297 
model_pd.lagr.mean(): -24.882993698120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0328], device='cuda:0')), ('power', tensor([-25.0252], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.10932698845863342
epoch£º702	 i:1 	 global-step:14041	 l-p:0.15551668405532837
epoch£º702	 i:2 	 global-step:14042	 l-p:0.19514100253582
epoch£º702	 i:3 	 global-step:14043	 l-p:0.3813612163066864
epoch£º702	 i:4 	 global-step:14044	 l-p:0.1360054612159729
epoch£º702	 i:5 	 global-step:14045	 l-p:0.14179323613643646
epoch£º702	 i:6 	 global-step:14046	 l-p:0.18358798325061798
epoch£º702	 i:7 	 global-step:14047	 l-p:0.15897949039936066
epoch£º702	 i:8 	 global-step:14048	 l-p:-0.34514251351356506
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12120314687490463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0536, 3.0441, 3.0532],
        [3.0536, 1.9736, 1.3328],
        [3.0536, 3.0119, 3.0495],
        [3.0536, 1.9905, 1.3467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): -0.6992852091789246 
model_pd.l_d.mean(): -25.0614013671875 
model_pd.lagr.mean(): -25.76068687438965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0582], device='cuda:0')), ('power', tensor([-25.1196], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:-0.6992852091789246
epoch£º703	 i:1 	 global-step:14061	 l-p:0.4561556875705719
epoch£º703	 i:2 	 global-step:14062	 l-p:0.14568738639354706
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13498643040657043
epoch£º703	 i:4 	 global-step:14064	 l-p:0.1535816788673401
epoch£º703	 i:5 	 global-step:14065	 l-p:0.15254826843738556
epoch£º703	 i:6 	 global-step:14066	 l-p:0.2360461950302124
epoch£º703	 i:7 	 global-step:14067	 l-p:0.16785494983196259
epoch£º703	 i:8 	 global-step:14068	 l-p:0.12770161032676697
epoch£º703	 i:9 	 global-step:14069	 l-p:0.14791004359722137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0800, 3.0800, 3.0800],
        [3.0800, 2.9244, 3.0419],
        [3.0800, 2.7883, 2.9652],
        [3.0800, 1.8174, 1.3147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.1339036375284195 
model_pd.l_d.mean(): -25.253450393676758 
model_pd.lagr.mean(): -25.11954689025879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1305], device='cuda:0')), ('power', tensor([-25.1230], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.1339036375284195
epoch£º704	 i:1 	 global-step:14081	 l-p:0.3966515362262726
epoch£º704	 i:2 	 global-step:14082	 l-p:0.15001718699932098
epoch£º704	 i:3 	 global-step:14083	 l-p:0.1711103469133377
epoch£º704	 i:4 	 global-step:14084	 l-p:0.13549813628196716
epoch£º704	 i:5 	 global-step:14085	 l-p:0.2080269604921341
epoch£º704	 i:6 	 global-step:14086	 l-p:-0.00028675555950030684
epoch£º704	 i:7 	 global-step:14087	 l-p:0.14672167599201202
epoch£º704	 i:8 	 global-step:14088	 l-p:0.04884735867381096
epoch£º704	 i:9 	 global-step:14089	 l-p:0.12212526053190231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0522, 3.0522, 3.0522],
        [3.0522, 3.0374, 3.0514],
        [3.0522, 1.7833, 1.2697],
        [3.0522, 1.8701, 1.2501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.22734825313091278 
model_pd.l_d.mean(): -24.992408752441406 
model_pd.lagr.mean(): -24.765060424804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1223], device='cuda:0')), ('power', tensor([-25.1147], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.22734825313091278
epoch£º705	 i:1 	 global-step:14101	 l-p:0.14700306951999664
epoch£º705	 i:2 	 global-step:14102	 l-p:0.2719220817089081
epoch£º705	 i:3 	 global-step:14103	 l-p:0.13568158447742462
epoch£º705	 i:4 	 global-step:14104	 l-p:0.18364779651165009
epoch£º705	 i:5 	 global-step:14105	 l-p:0.1122535690665245
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12443915754556656
epoch£º705	 i:7 	 global-step:14107	 l-p:0.13294166326522827
epoch£º705	 i:8 	 global-step:14108	 l-p:0.19159728288650513
epoch£º705	 i:9 	 global-step:14109	 l-p:0.0782332792878151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1077, 2.8390, 3.0083],
        [3.1077, 1.9824, 1.3398],
        [3.1077, 2.9822, 3.0813],
        [3.1077, 2.0073, 1.7491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.1299530416727066 
model_pd.l_d.mean(): -25.064729690551758 
model_pd.lagr.mean(): -24.934776306152344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0056], device='cuda:0')), ('power', tensor([-25.0704], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.1299530416727066
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12455852329730988
epoch£º706	 i:2 	 global-step:14122	 l-p:0.14541469514369965
epoch£º706	 i:3 	 global-step:14123	 l-p:0.2242373824119568
epoch£º706	 i:4 	 global-step:14124	 l-p:0.24418790638446808
epoch£º706	 i:5 	 global-step:14125	 l-p:0.1527203470468521
epoch£º706	 i:6 	 global-step:14126	 l-p:0.0941103845834732
epoch£º706	 i:7 	 global-step:14127	 l-p:0.2116440087556839
epoch£º706	 i:8 	 global-step:14128	 l-p:0.1330268830060959
epoch£º706	 i:9 	 global-step:14129	 l-p:0.2259962409734726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0889, 3.0879, 3.0889],
        [3.0889, 3.0888, 3.0889],
        [3.0889, 3.0888, 3.0889],
        [3.0889, 2.6931, 2.8904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.15848128497600555 
model_pd.l_d.mean(): -25.217679977416992 
model_pd.lagr.mean(): -25.0591983795166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0466], device='cuda:0')), ('power', tensor([-25.1711], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.15848128497600555
epoch£º707	 i:1 	 global-step:14141	 l-p:0.19609904289245605
epoch£º707	 i:2 	 global-step:14142	 l-p:0.14530524611473083
epoch£º707	 i:3 	 global-step:14143	 l-p:0.17630530893802643
epoch£º707	 i:4 	 global-step:14144	 l-p:0.1234624907374382
epoch£º707	 i:5 	 global-step:14145	 l-p:0.13634973764419556
epoch£º707	 i:6 	 global-step:14146	 l-p:0.18488910794258118
epoch£º707	 i:7 	 global-step:14147	 l-p:0.1250154823064804
epoch£º707	 i:8 	 global-step:14148	 l-p:0.2415536791086197
epoch£º707	 i:9 	 global-step:14149	 l-p:0.08242276310920715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0920, 2.8312, 2.9979],
        [3.0920, 2.4123, 2.5567],
        [3.0920, 1.8361, 1.2334],
        [3.0920, 2.8281, 2.9959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.12849241495132446 
model_pd.l_d.mean(): -25.04461097717285 
model_pd.lagr.mean(): -24.916118621826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0425], device='cuda:0')), ('power', tensor([-25.0872], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.12849241495132446
epoch£º708	 i:1 	 global-step:14161	 l-p:0.14043889939785004
epoch£º708	 i:2 	 global-step:14162	 l-p:0.1373879462480545
epoch£º708	 i:3 	 global-step:14163	 l-p:0.19083377718925476
epoch£º708	 i:4 	 global-step:14164	 l-p:0.07969799637794495
epoch£º708	 i:5 	 global-step:14165	 l-p:0.23495544493198395
epoch£º708	 i:6 	 global-step:14166	 l-p:0.13454654812812805
epoch£º708	 i:7 	 global-step:14167	 l-p:0.13176943361759186
epoch£º708	 i:8 	 global-step:14168	 l-p:0.12455659359693527
epoch£º708	 i:9 	 global-step:14169	 l-p:0.13471710681915283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1286, 2.1989, 2.1549],
        [3.1286, 2.9996, 3.1009],
        [3.1286, 2.2096, 1.5317],
        [3.1286, 1.8592, 1.3386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.14391618967056274 
model_pd.l_d.mean(): -24.94735336303711 
model_pd.lagr.mean(): -24.803436279296875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0101], device='cuda:0')), ('power', tensor([-24.9373], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.14391618967056274
epoch£º709	 i:1 	 global-step:14181	 l-p:0.14185138046741486
epoch£º709	 i:2 	 global-step:14182	 l-p:0.1381826251745224
epoch£º709	 i:3 	 global-step:14183	 l-p:1.377812147140503
epoch£º709	 i:4 	 global-step:14184	 l-p:0.14696712791919708
epoch£º709	 i:5 	 global-step:14185	 l-p:-1.3051055669784546
epoch£º709	 i:6 	 global-step:14186	 l-p:0.11431657522916794
epoch£º709	 i:7 	 global-step:14187	 l-p:0.12782317399978638
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12359065562486649
epoch£º709	 i:9 	 global-step:14189	 l-p:0.1696879118680954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0017, 2.9437, 2.9945],
        [3.0017, 2.0781, 2.0497],
        [3.0017, 2.2920, 2.4257],
        [3.0017, 2.9081, 2.9858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.1204071193933487 
model_pd.l_d.mean(): -25.0340633392334 
model_pd.lagr.mean(): -24.91365623474121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0395], device='cuda:0')), ('power', tensor([-24.9945], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.1204071193933487
epoch£º710	 i:1 	 global-step:14201	 l-p:0.13012933731079102
epoch£º710	 i:2 	 global-step:14202	 l-p:0.138047456741333
epoch£º710	 i:3 	 global-step:14203	 l-p:0.03617582842707634
epoch£º710	 i:4 	 global-step:14204	 l-p:0.18105338513851166
epoch£º710	 i:5 	 global-step:14205	 l-p:0.10355930030345917
epoch£º710	 i:6 	 global-step:14206	 l-p:0.24798840284347534
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1612841635942459
epoch£º710	 i:8 	 global-step:14208	 l-p:0.1523030698299408
epoch£º710	 i:9 	 global-step:14209	 l-p:0.0739012062549591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0187, 2.9823, 3.0154],
        [3.0187, 1.8285, 1.2170],
        [3.0187, 1.7860, 1.1886],
        [3.0187, 3.0187, 3.0187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.13803140819072723 
model_pd.l_d.mean(): -24.778657913208008 
model_pd.lagr.mean(): -24.640626907348633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1044], device='cuda:0')), ('power', tensor([-24.8830], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.13803140819072723
epoch£º711	 i:1 	 global-step:14221	 l-p:0.15860025584697723
epoch£º711	 i:2 	 global-step:14222	 l-p:0.17437459528446198
epoch£º711	 i:3 	 global-step:14223	 l-p:0.142446368932724
epoch£º711	 i:4 	 global-step:14224	 l-p:0.13841494917869568
epoch£º711	 i:5 	 global-step:14225	 l-p:0.15269595384597778
epoch£º711	 i:6 	 global-step:14226	 l-p:-0.9632201194763184
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12612958252429962
epoch£º711	 i:8 	 global-step:14228	 l-p:-0.2779808044433594
epoch£º711	 i:9 	 global-step:14229	 l-p:0.12244658917188644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0407, 1.7783, 1.1923],
        [3.0407, 1.8577, 1.2397],
        [3.0407, 2.7787, 2.9462],
        [3.0407, 1.9429, 1.3068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.18801933526992798 
model_pd.l_d.mean(): -25.05503273010254 
model_pd.lagr.mean(): -24.867013931274414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1711], device='cuda:0')), ('power', tensor([-25.2261], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.18801933526992798
epoch£º712	 i:1 	 global-step:14241	 l-p:0.12067994475364685
epoch£º712	 i:2 	 global-step:14242	 l-p:0.14209698140621185
epoch£º712	 i:3 	 global-step:14243	 l-p:0.09575045108795166
epoch£º712	 i:4 	 global-step:14244	 l-p:0.1212519034743309
epoch£º712	 i:5 	 global-step:14245	 l-p:0.13337387144565582
epoch£º712	 i:6 	 global-step:14246	 l-p:0.15581375360488892
epoch£º712	 i:7 	 global-step:14247	 l-p:0.0945669561624527
epoch£º712	 i:8 	 global-step:14248	 l-p:0.11532094329595566
epoch£º712	 i:9 	 global-step:14249	 l-p:0.14018617570400238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9396, 2.9163, 2.9381],
        [2.9396, 1.8043, 1.5162],
        [2.9396, 2.9396, 2.9397],
        [2.9396, 1.6878, 1.2206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.12155625969171524 
model_pd.l_d.mean(): -25.225364685058594 
model_pd.lagr.mean(): -25.103809356689453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0392], device='cuda:0')), ('power', tensor([-25.1861], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.12155625969171524
epoch£º713	 i:1 	 global-step:14261	 l-p:0.08250050991773605
epoch£º713	 i:2 	 global-step:14262	 l-p:0.16388443112373352
epoch£º713	 i:3 	 global-step:14263	 l-p:0.16995024681091309
epoch£º713	 i:4 	 global-step:14264	 l-p:0.2082257866859436
epoch£º713	 i:5 	 global-step:14265	 l-p:0.16740237176418304
epoch£º713	 i:6 	 global-step:14266	 l-p:0.13503871858119965
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12211016565561295
epoch£º713	 i:8 	 global-step:14268	 l-p:0.288017600774765
epoch£º713	 i:9 	 global-step:14269	 l-p:0.16460448503494263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0408, 3.0408, 3.0408],
        [3.0408, 1.7772, 1.1911],
        [3.0408, 1.7693, 1.2575],
        [3.0408, 3.0270, 3.0401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15528973937034607 
model_pd.l_d.mean(): -25.133407592773438 
model_pd.lagr.mean(): -24.978116989135742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0248], device='cuda:0')), ('power', tensor([-25.1582], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15528973937034607
epoch£º714	 i:1 	 global-step:14281	 l-p:0.15426857769489288
epoch£º714	 i:2 	 global-step:14282	 l-p:0.10210147500038147
epoch£º714	 i:3 	 global-step:14283	 l-p:0.1363953948020935
epoch£º714	 i:4 	 global-step:14284	 l-p:0.179977148771286
epoch£º714	 i:5 	 global-step:14285	 l-p:0.19567960500717163
epoch£º714	 i:6 	 global-step:14286	 l-p:0.14547009766101837
epoch£º714	 i:7 	 global-step:14287	 l-p:0.135118767619133
epoch£º714	 i:8 	 global-step:14288	 l-p:0.11748853325843811
epoch£º714	 i:9 	 global-step:14289	 l-p:0.19555515050888062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1170, 3.0773, 3.1132],
        [3.1170, 2.1355, 1.4672],
        [3.1170, 1.9933, 1.7038],
        [3.1170, 3.1170, 3.1170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.0972529798746109 
model_pd.l_d.mean(): -24.999298095703125 
model_pd.lagr.mean(): -24.90204429626465 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0155], device='cuda:0')), ('power', tensor([-24.9838], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.0972529798746109
epoch£º715	 i:1 	 global-step:14301	 l-p:0.13869373500347137
epoch£º715	 i:2 	 global-step:14302	 l-p:0.1482836753129959
epoch£º715	 i:3 	 global-step:14303	 l-p:0.1334289163351059
epoch£º715	 i:4 	 global-step:14304	 l-p:0.14249855279922485
epoch£º715	 i:5 	 global-step:14305	 l-p:0.13303260505199432
epoch£º715	 i:6 	 global-step:14306	 l-p:0.126799076795578
epoch£º715	 i:7 	 global-step:14307	 l-p:0.05935695394873619
epoch£º715	 i:8 	 global-step:14308	 l-p:0.301509827375412
epoch£º715	 i:9 	 global-step:14309	 l-p:0.186110720038414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0038, 1.7291, 1.1687],
        [3.0038, 2.9954, 3.0034],
        [3.0038, 3.0038, 3.0038],
        [3.0038, 2.0077, 1.3585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 1.0575343370437622 
model_pd.l_d.mean(): -25.05054473876953 
model_pd.lagr.mean(): -23.993009567260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0839], device='cuda:0')), ('power', tensor([-25.1344], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:1.0575343370437622
epoch£º716	 i:1 	 global-step:14321	 l-p:0.13596400618553162
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14393147826194763
epoch£º716	 i:3 	 global-step:14323	 l-p:0.1349458545446396
epoch£º716	 i:4 	 global-step:14324	 l-p:0.18538042902946472
epoch£º716	 i:5 	 global-step:14325	 l-p:0.06401897966861725
epoch£º716	 i:6 	 global-step:14326	 l-p:0.13683004677295685
epoch£º716	 i:7 	 global-step:14327	 l-p:0.16697965562343597
epoch£º716	 i:8 	 global-step:14328	 l-p:2.881664276123047
epoch£º716	 i:9 	 global-step:14329	 l-p:0.12207138538360596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0134, 2.6462, 2.8414],
        [3.0134, 2.5309, 2.7313],
        [3.0134, 2.9961, 3.0124],
        [3.0134, 1.8569, 1.2373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.14884386956691742 
model_pd.l_d.mean(): -25.212682723999023 
model_pd.lagr.mean(): -25.063838958740234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0428], device='cuda:0')), ('power', tensor([-25.1699], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.14884386956691742
epoch£º717	 i:1 	 global-step:14341	 l-p:0.10748294740915298
epoch£º717	 i:2 	 global-step:14342	 l-p:0.16775444149971008
epoch£º717	 i:3 	 global-step:14343	 l-p:-0.04123084619641304
epoch£º717	 i:4 	 global-step:14344	 l-p:0.22930602729320526
epoch£º717	 i:5 	 global-step:14345	 l-p:0.48514246940612793
epoch£º717	 i:6 	 global-step:14346	 l-p:0.13781706988811493
epoch£º717	 i:7 	 global-step:14347	 l-p:0.13685351610183716
epoch£º717	 i:8 	 global-step:14348	 l-p:0.08426226675510406
epoch£º717	 i:9 	 global-step:14349	 l-p:0.11052686721086502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[3.1350, 2.2838, 2.3171],
        [3.1350, 2.4858, 2.6438],
        [3.1350, 2.1792, 1.5042],
        [3.1350, 2.6153, 2.8094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.12719620764255524 
model_pd.l_d.mean(): -25.1882381439209 
model_pd.lagr.mean(): -25.06104278564453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1003], device='cuda:0')), ('power', tensor([-25.0880], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.12719620764255524
epoch£º718	 i:1 	 global-step:14361	 l-p:0.13719509541988373
epoch£º718	 i:2 	 global-step:14362	 l-p:0.16760261356830597
epoch£º718	 i:3 	 global-step:14363	 l-p:0.11626381427049637
epoch£º718	 i:4 	 global-step:14364	 l-p:0.18865491449832916
epoch£º718	 i:5 	 global-step:14365	 l-p:0.13792863488197327
epoch£º718	 i:6 	 global-step:14366	 l-p:0.11970340460538864
epoch£º718	 i:7 	 global-step:14367	 l-p:0.14053769409656525
epoch£º718	 i:8 	 global-step:14368	 l-p:0.16746017336845398
epoch£º718	 i:9 	 global-step:14369	 l-p:0.26240718364715576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0652, 2.4958, 2.6820],
        [3.0652, 1.7904, 1.2064],
        [3.0652, 1.8825, 1.2582],
        [3.0652, 2.9904, 3.0543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.13954845070838928 
model_pd.l_d.mean(): -24.444318771362305 
model_pd.lagr.mean(): -24.30476951599121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1894], device='cuda:0')), ('power', tensor([-24.6338], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.13954845070838928
epoch£º719	 i:1 	 global-step:14381	 l-p:-1.2291226387023926
epoch£º719	 i:2 	 global-step:14382	 l-p:0.5620232820510864
epoch£º719	 i:3 	 global-step:14383	 l-p:0.1593645215034485
epoch£º719	 i:4 	 global-step:14384	 l-p:0.13658113777637482
epoch£º719	 i:5 	 global-step:14385	 l-p:0.14167335629463196
epoch£º719	 i:6 	 global-step:14386	 l-p:0.1331024020910263
epoch£º719	 i:7 	 global-step:14387	 l-p:0.14137941598892212
epoch£º719	 i:8 	 global-step:14388	 l-p:0.13209804892539978
epoch£º719	 i:9 	 global-step:14389	 l-p:0.1346687525510788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0493, 2.8042, 2.9655],
        [3.0493, 2.1906, 2.2238],
        [3.0493, 1.7776, 1.2711],
        [3.0493, 2.0091, 1.8467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.21832726895809174 
model_pd.l_d.mean(): -25.060428619384766 
model_pd.lagr.mean(): -24.84210205078125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0335], device='cuda:0')), ('power', tensor([-25.0940], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.21832726895809174
epoch£º720	 i:1 	 global-step:14401	 l-p:0.051703911274671555
epoch£º720	 i:2 	 global-step:14402	 l-p:0.2650619149208069
epoch£º720	 i:3 	 global-step:14403	 l-p:0.08342703431844711
epoch£º720	 i:4 	 global-step:14404	 l-p:0.1256290078163147
epoch£º720	 i:5 	 global-step:14405	 l-p:0.13475942611694336
epoch£º720	 i:6 	 global-step:14406	 l-p:0.14663171768188477
epoch£º720	 i:7 	 global-step:14407	 l-p:0.16918116807937622
epoch£º720	 i:8 	 global-step:14408	 l-p:0.1528276652097702
epoch£º720	 i:9 	 global-step:14409	 l-p:0.08124075084924698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[3.0045, 1.7241, 1.1810],
        [3.0045, 2.0442, 1.3877],
        [3.0045, 1.9195, 1.7001],
        [3.0045, 1.8233, 1.4693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11861982941627502 
model_pd.l_d.mean(): -24.832202911376953 
model_pd.lagr.mean(): -24.71358299255371 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1245], device='cuda:0')), ('power', tensor([-24.9567], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11861982941627502
epoch£º721	 i:1 	 global-step:14421	 l-p:0.32464537024497986
epoch£º721	 i:2 	 global-step:14422	 l-p:0.13070102035999298
epoch£º721	 i:3 	 global-step:14423	 l-p:0.14620579779148102
epoch£º721	 i:4 	 global-step:14424	 l-p:-0.05123576894402504
epoch£º721	 i:5 	 global-step:14425	 l-p:0.2330227792263031
epoch£º721	 i:6 	 global-step:14426	 l-p:0.1553518921136856
epoch£º721	 i:7 	 global-step:14427	 l-p:1.3524340391159058
epoch£º721	 i:8 	 global-step:14428	 l-p:0.12849511206150055
epoch£º721	 i:9 	 global-step:14429	 l-p:0.13451769948005676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1016, 3.1017, 3.1017],
        [3.1016, 3.0936, 3.1014],
        [3.1016, 3.1016, 3.1017],
        [3.1016, 2.9412, 3.0615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.1071278378367424 
model_pd.l_d.mean(): -24.932931900024414 
model_pd.lagr.mean(): -24.825803756713867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0229], device='cuda:0')), ('power', tensor([-24.9558], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.1071278378367424
epoch£º722	 i:1 	 global-step:14441	 l-p:0.17596793174743652
epoch£º722	 i:2 	 global-step:14442	 l-p:0.18157008290290833
epoch£º722	 i:3 	 global-step:14443	 l-p:0.1282845139503479
epoch£º722	 i:4 	 global-step:14444	 l-p:0.12095165252685547
epoch£º722	 i:5 	 global-step:14445	 l-p:0.12661918997764587
epoch£º722	 i:6 	 global-step:14446	 l-p:0.1445915699005127
epoch£º722	 i:7 	 global-step:14447	 l-p:0.12408612668514252
epoch£º722	 i:8 	 global-step:14448	 l-p:0.1485859900712967
epoch£º722	 i:9 	 global-step:14449	 l-p:0.1400328427553177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0839, 2.8604, 3.0125],
        [3.0839, 1.8084, 1.2177],
        [3.0839, 3.0833, 3.0839],
        [3.0839, 3.0839, 3.0839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.22275683283805847 
model_pd.l_d.mean(): -25.09193992614746 
model_pd.lagr.mean(): -24.869182586669922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0218], device='cuda:0')), ('power', tensor([-25.1137], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.22275683283805847
epoch£º723	 i:1 	 global-step:14461	 l-p:0.13846181333065033
epoch£º723	 i:2 	 global-step:14462	 l-p:0.15242010354995728
epoch£º723	 i:3 	 global-step:14463	 l-p:0.1893358826637268
epoch£º723	 i:4 	 global-step:14464	 l-p:0.1641875058412552
epoch£º723	 i:5 	 global-step:14465	 l-p:0.17153345048427582
epoch£º723	 i:6 	 global-step:14466	 l-p:0.0011467504082247615
epoch£º723	 i:7 	 global-step:14467	 l-p:0.1300783008337021
epoch£º723	 i:8 	 global-step:14468	 l-p:0.19318996369838715
epoch£º723	 i:9 	 global-step:14469	 l-p:0.010238351300358772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9995, 2.9906, 2.9992],
        [2.9995, 1.7453, 1.1602],
        [2.9995, 2.9604, 2.9958],
        [2.9995, 2.0558, 2.0125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.09503117203712463 
model_pd.l_d.mean(): -24.83946418762207 
model_pd.lagr.mean(): -24.74443244934082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0720], device='cuda:0')), ('power', tensor([-24.9115], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.09503117203712463
epoch£º724	 i:1 	 global-step:14481	 l-p:0.1631440669298172
epoch£º724	 i:2 	 global-step:14482	 l-p:0.14939145743846893
epoch£º724	 i:3 	 global-step:14483	 l-p:0.12844599783420563
epoch£º724	 i:4 	 global-step:14484	 l-p:0.10278104245662689
epoch£º724	 i:5 	 global-step:14485	 l-p:0.14299480617046356
epoch£º724	 i:6 	 global-step:14486	 l-p:-1.102912425994873
epoch£º724	 i:7 	 global-step:14487	 l-p:0.17412151396274567
epoch£º724	 i:8 	 global-step:14488	 l-p:0.1496150642633438
epoch£º724	 i:9 	 global-step:14489	 l-p:0.15198878943920135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1065, 3.0927, 3.1058],
        [3.1065, 1.8168, 1.2533],
        [3.1065, 1.8289, 1.3090],
        [3.1065, 2.6239, 2.8235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.12564648687839508 
model_pd.l_d.mean(): -25.01706314086914 
model_pd.lagr.mean(): -24.891416549682617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1037], device='cuda:0')), ('power', tensor([-24.9134], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.12564648687839508
epoch£º725	 i:1 	 global-step:14501	 l-p:0.11430048942565918
epoch£º725	 i:2 	 global-step:14502	 l-p:0.12594495713710785
epoch£º725	 i:3 	 global-step:14503	 l-p:0.17100609838962555
epoch£º725	 i:4 	 global-step:14504	 l-p:0.12011555582284927
epoch£º725	 i:5 	 global-step:14505	 l-p:0.22858020663261414
epoch£º725	 i:6 	 global-step:14506	 l-p:0.29492199420928955
epoch£º725	 i:7 	 global-step:14507	 l-p:0.12351037561893463
epoch£º725	 i:8 	 global-step:14508	 l-p:0.17612841725349426
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12102651596069336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[3.1330, 2.6802, 2.8806],
        [3.1330, 2.0251, 1.7645],
        [3.1330, 1.8760, 1.2592],
        [3.1330, 2.0357, 1.3813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.042352207005023956 
model_pd.l_d.mean(): -24.566125869750977 
model_pd.lagr.mean(): -24.523773193359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0738], device='cuda:0')), ('power', tensor([-24.6399], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.042352207005023956
epoch£º726	 i:1 	 global-step:14521	 l-p:0.1842876374721527
epoch£º726	 i:2 	 global-step:14522	 l-p:0.1296173334121704
epoch£º726	 i:3 	 global-step:14523	 l-p:0.1311226636171341
epoch£º726	 i:4 	 global-step:14524	 l-p:0.1414767950773239
epoch£º726	 i:5 	 global-step:14525	 l-p:0.12324000895023346
epoch£º726	 i:6 	 global-step:14526	 l-p:0.1283600926399231
epoch£º726	 i:7 	 global-step:14527	 l-p:0.13177251815795898
epoch£º726	 i:8 	 global-step:14528	 l-p:0.13783706724643707
epoch£º726	 i:9 	 global-step:14529	 l-p:0.1177612915635109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1143, 3.1143, 3.1143],
        [3.1143, 1.9717, 1.3285],
        [3.1143, 2.1574, 1.4842],
        [3.1143, 3.1143, 3.1143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.14235199987888336 
model_pd.l_d.mean(): -25.10933494567871 
model_pd.lagr.mean(): -24.966983795166016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0211], device='cuda:0')), ('power', tensor([-25.1305], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.14235199987888336
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12038084119558334
epoch£º727	 i:2 	 global-step:14542	 l-p:0.18918460607528687
epoch£º727	 i:3 	 global-step:14543	 l-p:0.034209102392196655
epoch£º727	 i:4 	 global-step:14544	 l-p:0.5643671751022339
epoch£º727	 i:5 	 global-step:14545	 l-p:0.166336327791214
epoch£º727	 i:6 	 global-step:14546	 l-p:0.10587956011295319
epoch£º727	 i:7 	 global-step:14547	 l-p:0.05998452380299568
epoch£º727	 i:8 	 global-step:14548	 l-p:0.14648936688899994
epoch£º727	 i:9 	 global-step:14549	 l-p:0.1564999371767044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0498, 3.0054, 3.0452],
        [3.0498, 3.0473, 3.0498],
        [3.0498, 1.7748, 1.2678],
        [3.0498, 2.1121, 1.4445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.22409026324748993 
model_pd.l_d.mean(): -25.034299850463867 
model_pd.lagr.mean(): -24.810209274291992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0029], device='cuda:0')), ('power', tensor([-25.0372], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.22409026324748993
epoch£º728	 i:1 	 global-step:14561	 l-p:0.1712162345647812
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12269832193851471
epoch£º728	 i:3 	 global-step:14563	 l-p:0.1317177414894104
epoch£º728	 i:4 	 global-step:14564	 l-p:0.19745774567127228
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13675455749034882
epoch£º728	 i:6 	 global-step:14566	 l-p:0.14383360743522644
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13741935789585114
epoch£º728	 i:8 	 global-step:14568	 l-p:0.24661952257156372
epoch£º728	 i:9 	 global-step:14569	 l-p:0.242192342877388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0868, 2.5697, 2.7666],
        [3.0868, 2.8279, 2.9945],
        [3.0868, 2.4352, 2.5963],
        [3.0868, 3.0868, 3.0868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.16312694549560547 
model_pd.l_d.mean(): -24.6949462890625 
model_pd.lagr.mean(): -24.531818389892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1968], device='cuda:0')), ('power', tensor([-24.8918], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.16312694549560547
epoch£º729	 i:1 	 global-step:14581	 l-p:0.2118234783411026
epoch£º729	 i:2 	 global-step:14582	 l-p:0.1442146599292755
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12963168323040009
epoch£º729	 i:4 	 global-step:14584	 l-p:0.12456270307302475
epoch£º729	 i:5 	 global-step:14585	 l-p:0.18003854155540466
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12593966722488403
epoch£º729	 i:7 	 global-step:14587	 l-p:0.21446460485458374
epoch£º729	 i:8 	 global-step:14588	 l-p:0.13367155194282532
epoch£º729	 i:9 	 global-step:14589	 l-p:0.08089414238929749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[3.1042, 1.8261, 1.2287],
        [3.1042, 1.9971, 1.3488],
        [3.1042, 1.8220, 1.2974],
        [3.1042, 1.8172, 1.2799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.19253289699554443 
model_pd.l_d.mean(): -24.72112274169922 
model_pd.lagr.mean(): -24.528589248657227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1638], device='cuda:0')), ('power', tensor([-24.8850], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.19253289699554443
epoch£º730	 i:1 	 global-step:14601	 l-p:0.12432126700878143
epoch£º730	 i:2 	 global-step:14602	 l-p:0.18204444646835327
epoch£º730	 i:3 	 global-step:14603	 l-p:0.15901179611682892
epoch£º730	 i:4 	 global-step:14604	 l-p:0.12324012815952301
epoch£º730	 i:5 	 global-step:14605	 l-p:0.12195806205272675
epoch£º730	 i:6 	 global-step:14606	 l-p:0.0953737422823906
epoch£º730	 i:7 	 global-step:14607	 l-p:0.13526391983032227
epoch£º730	 i:8 	 global-step:14608	 l-p:0.16076581180095673
epoch£º730	 i:9 	 global-step:14609	 l-p:-0.40168994665145874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0566, 2.0369, 1.3816],
        [3.0566, 2.3363, 2.4671],
        [3.0566, 1.9232, 1.6354],
        [3.0566, 3.0210, 3.0534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.11376725137233734 
model_pd.l_d.mean(): -25.00130271911621 
model_pd.lagr.mean(): -24.887535095214844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0691], device='cuda:0')), ('power', tensor([-25.0704], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.11376725137233734
epoch£º731	 i:1 	 global-step:14621	 l-p:0.06354124844074249
epoch£º731	 i:2 	 global-step:14622	 l-p:0.15058960020542145
epoch£º731	 i:3 	 global-step:14623	 l-p:0.11068113148212433
epoch£º731	 i:4 	 global-step:14624	 l-p:0.13424940407276154
epoch£º731	 i:5 	 global-step:14625	 l-p:0.052480801939964294
epoch£º731	 i:6 	 global-step:14626	 l-p:0.17748263478279114
epoch£º731	 i:7 	 global-step:14627	 l-p:0.2293587625026703
epoch£º731	 i:8 	 global-step:14628	 l-p:0.31437769532203674
epoch£º731	 i:9 	 global-step:14629	 l-p:0.027126410976052284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0598, 1.7964, 1.1993],
        [3.0598, 3.0522, 3.0595],
        [3.0598, 3.0598, 3.0598],
        [3.0598, 3.0479, 3.0593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.13871604204177856 
model_pd.l_d.mean(): -24.730758666992188 
model_pd.lagr.mean(): -24.592042922973633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0956], device='cuda:0')), ('power', tensor([-24.8263], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.13871604204177856
epoch£º732	 i:1 	 global-step:14641	 l-p:0.14773385226726532
epoch£º732	 i:2 	 global-step:14642	 l-p:0.11866459250450134
epoch£º732	 i:3 	 global-step:14643	 l-p:0.1684851348400116
epoch£º732	 i:4 	 global-step:14644	 l-p:0.12680433690547943
epoch£º732	 i:5 	 global-step:14645	 l-p:0.14915072917938232
epoch£º732	 i:6 	 global-step:14646	 l-p:0.7060644626617432
epoch£º732	 i:7 	 global-step:14647	 l-p:0.6985037922859192
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13496561348438263
epoch£º732	 i:9 	 global-step:14649	 l-p:0.11864274740219116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0920, 3.0920, 3.0920],
        [3.0920, 3.0529, 3.0883],
        [3.0920, 2.8200, 2.9914],
        [3.0920, 2.9826, 3.0713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.13141125440597534 
model_pd.l_d.mean(): -24.869932174682617 
model_pd.lagr.mean(): -24.738521575927734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0785], device='cuda:0')), ('power', tensor([-24.9484], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.13141125440597534
epoch£º733	 i:1 	 global-step:14661	 l-p:0.14744380116462708
epoch£º733	 i:2 	 global-step:14662	 l-p:0.15564055740833282
epoch£º733	 i:3 	 global-step:14663	 l-p:0.5124096870422363
epoch£º733	 i:4 	 global-step:14664	 l-p:0.4269843101501465
epoch£º733	 i:5 	 global-step:14665	 l-p:0.14744062721729279
epoch£º733	 i:6 	 global-step:14666	 l-p:0.21211543679237366
epoch£º733	 i:7 	 global-step:14667	 l-p:0.08702512830495834
epoch£º733	 i:8 	 global-step:14668	 l-p:0.1539362370967865
epoch£º733	 i:9 	 global-step:14669	 l-p:0.35030195116996765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0882, 3.0842, 3.0881],
        [3.0882, 1.9212, 1.2870],
        [3.0882, 2.0705, 1.4097],
        [3.0882, 1.8339, 1.3660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.14612828195095062 
model_pd.l_d.mean(): -25.020477294921875 
model_pd.lagr.mean(): -24.87434959411621 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0110], device='cuda:0')), ('power', tensor([-25.0095], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.14612828195095062
epoch£º734	 i:1 	 global-step:14681	 l-p:0.20233285427093506
epoch£º734	 i:2 	 global-step:14682	 l-p:0.15744620561599731
epoch£º734	 i:3 	 global-step:14683	 l-p:0.14841125905513763
epoch£º734	 i:4 	 global-step:14684	 l-p:0.3315031826496124
epoch£º734	 i:5 	 global-step:14685	 l-p:0.13077837228775024
epoch£º734	 i:6 	 global-step:14686	 l-p:0.19122089445590973
epoch£º734	 i:7 	 global-step:14687	 l-p:0.07792331278324127
epoch£º734	 i:8 	 global-step:14688	 l-p:0.13168549537658691
epoch£º734	 i:9 	 global-step:14689	 l-p:0.14781537652015686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0872, 2.0431, 1.8793],
        [3.0872, 2.7955, 2.9734],
        [3.0872, 1.8073, 1.2154],
        [3.0872, 1.7983, 1.2203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.13946880400180817 
model_pd.l_d.mean(): -24.909181594848633 
model_pd.lagr.mean(): -24.769712448120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0199], device='cuda:0')), ('power', tensor([-24.9290], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.13946880400180817
epoch£º735	 i:1 	 global-step:14701	 l-p:0.15376588702201843
epoch£º735	 i:2 	 global-step:14702	 l-p:-2.652101993560791
epoch£º735	 i:3 	 global-step:14703	 l-p:0.12880322337150574
epoch£º735	 i:4 	 global-step:14704	 l-p:0.1474917083978653
epoch£º735	 i:5 	 global-step:14705	 l-p:-0.5898430347442627
epoch£º735	 i:6 	 global-step:14706	 l-p:0.23566463589668274
epoch£º735	 i:7 	 global-step:14707	 l-p:0.13909600675106049
epoch£º735	 i:8 	 global-step:14708	 l-p:-0.05268194526433945
epoch£º735	 i:9 	 global-step:14709	 l-p:0.16828642785549164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0561, 2.0454, 1.3883],
        [3.0561, 2.8269, 2.9820],
        [3.0561, 1.9054, 1.5945],
        [3.0561, 1.9215, 1.6337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.15182887017726898 
model_pd.l_d.mean(): -25.145116806030273 
model_pd.lagr.mean(): -24.993288040161133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0979], device='cuda:0')), ('power', tensor([-25.2430], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.15182887017726898
epoch£º736	 i:1 	 global-step:14721	 l-p:0.21504348516464233
epoch£º736	 i:2 	 global-step:14722	 l-p:0.16539330780506134
epoch£º736	 i:3 	 global-step:14723	 l-p:0.13998374342918396
epoch£º736	 i:4 	 global-step:14724	 l-p:0.29694119095802307
epoch£º736	 i:5 	 global-step:14725	 l-p:0.12999582290649414
epoch£º736	 i:6 	 global-step:14726	 l-p:0.14746281504631042
epoch£º736	 i:7 	 global-step:14727	 l-p:0.1286056637763977
epoch£º736	 i:8 	 global-step:14728	 l-p:0.0905589684844017
epoch£º736	 i:9 	 global-step:14729	 l-p:0.18187011778354645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9771, 2.8649, 2.9556],
        [2.9771, 2.0324, 1.3750],
        [2.9771, 1.7017, 1.1382],
        [2.9771, 1.9866, 1.3378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.1296759843826294 
model_pd.l_d.mean(): -25.174327850341797 
model_pd.lagr.mean(): -25.04465103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0189], device='cuda:0')), ('power', tensor([-25.1554], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.1296759843826294
epoch£º737	 i:1 	 global-step:14741	 l-p:0.206934854388237
epoch£º737	 i:2 	 global-step:14742	 l-p:-0.2226799875497818
epoch£º737	 i:3 	 global-step:14743	 l-p:0.09897712618112564
epoch£º737	 i:4 	 global-step:14744	 l-p:0.15483345091342926
epoch£º737	 i:5 	 global-step:14745	 l-p:0.13158711791038513
epoch£º737	 i:6 	 global-step:14746	 l-p:0.04354066774249077
epoch£º737	 i:7 	 global-step:14747	 l-p:0.12234395742416382
epoch£º737	 i:8 	 global-step:14748	 l-p:0.12663353979587555
epoch£º737	 i:9 	 global-step:14749	 l-p:0.2895027995109558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0906, 3.0505, 3.0868],
        [3.0906, 3.0825, 3.0903],
        [3.0906, 3.0883, 3.0906],
        [3.0906, 3.0906, 3.0906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.20230762660503387 
model_pd.l_d.mean(): -24.969823837280273 
model_pd.lagr.mean(): -24.76751708984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0268], device='cuda:0')), ('power', tensor([-24.9967], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.20230762660503387
epoch£º738	 i:1 	 global-step:14761	 l-p:0.12013708055019379
epoch£º738	 i:2 	 global-step:14762	 l-p:0.1875428855419159
epoch£º738	 i:3 	 global-step:14763	 l-p:0.12925925850868225
epoch£º738	 i:4 	 global-step:14764	 l-p:0.15331421792507172
epoch£º738	 i:5 	 global-step:14765	 l-p:0.1323309987783432
epoch£º738	 i:6 	 global-step:14766	 l-p:0.09378329664468765
epoch£º738	 i:7 	 global-step:14767	 l-p:0.14672552049160004
epoch£º738	 i:8 	 global-step:14768	 l-p:0.12630890309810638
epoch£º738	 i:9 	 global-step:14769	 l-p:0.24534644186496735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0878, 3.0878, 3.0878],
        [3.0878, 3.0878, 3.0878],
        [3.0878, 1.8186, 1.2169],
        [3.0878, 2.5989, 2.7996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.13159021735191345 
model_pd.l_d.mean(): -24.82597541809082 
model_pd.lagr.mean(): -24.694385528564453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1137], device='cuda:0')), ('power', tensor([-24.9396], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.13159021735191345
epoch£º739	 i:1 	 global-step:14781	 l-p:0.12930212914943695
epoch£º739	 i:2 	 global-step:14782	 l-p:-0.42273977398872375
epoch£º739	 i:3 	 global-step:14783	 l-p:0.045430682599544525
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12963008880615234
epoch£º739	 i:5 	 global-step:14785	 l-p:0.15554289519786835
epoch£º739	 i:6 	 global-step:14786	 l-p:0.24265125393867493
epoch£º739	 i:7 	 global-step:14787	 l-p:0.14128513634204865
epoch£º739	 i:8 	 global-step:14788	 l-p:0.2750553786754608
epoch£º739	 i:9 	 global-step:14789	 l-p:0.04842715337872505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0026, 3.0026, 3.0026],
        [3.0026, 2.9657, 2.9992],
        [3.0026, 2.0795, 2.0622],
        [3.0026, 1.7511, 1.2993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.14125972986221313 
model_pd.l_d.mean(): -24.68747329711914 
model_pd.lagr.mean(): -24.546213150024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1568], device='cuda:0')), ('power', tensor([-24.8443], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.14125972986221313
epoch£º740	 i:1 	 global-step:14801	 l-p:0.36278071999549866
epoch£º740	 i:2 	 global-step:14802	 l-p:0.09998549520969391
epoch£º740	 i:3 	 global-step:14803	 l-p:-0.11050326377153397
epoch£º740	 i:4 	 global-step:14804	 l-p:0.14386209845542908
epoch£º740	 i:5 	 global-step:14805	 l-p:0.12369456142187119
epoch£º740	 i:6 	 global-step:14806	 l-p:0.15302720665931702
epoch£º740	 i:7 	 global-step:14807	 l-p:0.0780605748295784
epoch£º740	 i:8 	 global-step:14808	 l-p:0.11536359786987305
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15871255099773407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0453, 1.7568, 1.2230],
        [3.0453, 2.0807, 1.4168],
        [3.0453, 1.7552, 1.2133],
        [3.0453, 3.0453, 3.0453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11137984693050385 
model_pd.l_d.mean(): -25.03180694580078 
model_pd.lagr.mean(): -24.920427322387695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0714], device='cuda:0')), ('power', tensor([-25.1032], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11137984693050385
epoch£º741	 i:1 	 global-step:14821	 l-p:0.11319215595722198
epoch£º741	 i:2 	 global-step:14822	 l-p:-0.17779125273227692
epoch£º741	 i:3 	 global-step:14823	 l-p:0.1419139951467514
epoch£º741	 i:4 	 global-step:14824	 l-p:0.141697496175766
epoch£º741	 i:5 	 global-step:14825	 l-p:0.24523498117923737
epoch£º741	 i:6 	 global-step:14826	 l-p:0.13405779004096985
epoch£º741	 i:7 	 global-step:14827	 l-p:0.13081227242946625
epoch£º741	 i:8 	 global-step:14828	 l-p:0.16571325063705444
epoch£º741	 i:9 	 global-step:14829	 l-p:0.14433684945106506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1241, 2.1632, 1.4875],
        [3.1241, 2.6640, 2.8657],
        [3.1241, 3.1110, 3.1235],
        [3.1241, 2.1027, 1.9679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.05602622777223587 
model_pd.l_d.mean(): -24.81036376953125 
model_pd.lagr.mean(): -24.754337310791016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0946], device='cuda:0')), ('power', tensor([-24.9050], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.05602622777223587
epoch£º742	 i:1 	 global-step:14841	 l-p:0.15288175642490387
epoch£º742	 i:2 	 global-step:14842	 l-p:0.15968728065490723
epoch£º742	 i:3 	 global-step:14843	 l-p:0.10859844088554382
epoch£º742	 i:4 	 global-step:14844	 l-p:0.1541251838207245
epoch£º742	 i:5 	 global-step:14845	 l-p:0.13079378008842468
epoch£º742	 i:6 	 global-step:14846	 l-p:0.1320030689239502
epoch£º742	 i:7 	 global-step:14847	 l-p:0.22026881575584412
epoch£º742	 i:8 	 global-step:14848	 l-p:0.3050789535045624
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12593887746334076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0707, 3.0705, 3.0707],
        [3.0707, 2.8485, 3.0005],
        [3.0707, 1.8126, 1.2077],
        [3.0707, 1.7815, 1.2501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.3878317177295685 
model_pd.l_d.mean(): -24.838741302490234 
model_pd.lagr.mean(): -24.450908660888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0783], device='cuda:0')), ('power', tensor([-24.9171], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.3878317177295685
epoch£º743	 i:1 	 global-step:14861	 l-p:0.13218475878238678
epoch£º743	 i:2 	 global-step:14862	 l-p:-0.05366521701216698
epoch£º743	 i:3 	 global-step:14863	 l-p:0.21506650745868683
epoch£º743	 i:4 	 global-step:14864	 l-p:-1.524090051651001
epoch£º743	 i:5 	 global-step:14865	 l-p:0.12826906144618988
epoch£º743	 i:6 	 global-step:14866	 l-p:0.15390902757644653
epoch£º743	 i:7 	 global-step:14867	 l-p:0.1358126997947693
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12642338871955872
epoch£º743	 i:9 	 global-step:14869	 l-p:0.1475214660167694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0350, 2.7379, 2.9181],
        [3.0350, 2.5363, 2.7377],
        [3.0350, 2.7433, 2.9218],
        [3.0350, 2.9066, 3.0079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.1342606544494629 
model_pd.l_d.mean(): -25.21916961669922 
model_pd.lagr.mean(): -25.084909439086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0947], device='cuda:0')), ('power', tensor([-25.1244], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.1342606544494629
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13848000764846802
epoch£º744	 i:2 	 global-step:14882	 l-p:0.09477383643388748
epoch£º744	 i:3 	 global-step:14883	 l-p:0.10353212803602219
epoch£º744	 i:4 	 global-step:14884	 l-p:0.2190796285867691
epoch£º744	 i:5 	 global-step:14885	 l-p:0.06931614130735397
epoch£º744	 i:6 	 global-step:14886	 l-p:0.16394782066345215
epoch£º744	 i:7 	 global-step:14887	 l-p:0.1442704200744629
epoch£º744	 i:8 	 global-step:14888	 l-p:0.35362672805786133
epoch£º744	 i:9 	 global-step:14889	 l-p:0.26624202728271484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9365, 2.8258, 2.9156],
        [2.9365, 2.9364, 2.9365],
        [2.9365, 1.9897, 1.3372],
        [2.9365, 1.6556, 1.1188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.18483959138393402 
model_pd.l_d.mean(): -24.829517364501953 
model_pd.lagr.mean(): -24.644678115844727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1709], device='cuda:0')), ('power', tensor([-25.0005], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.18483959138393402
epoch£º745	 i:1 	 global-step:14901	 l-p:0.12907446920871735
epoch£º745	 i:2 	 global-step:14902	 l-p:0.21541817486286163
epoch£º745	 i:3 	 global-step:14903	 l-p:6.680092811584473
epoch£º745	 i:4 	 global-step:14904	 l-p:0.13831490278244019
epoch£º745	 i:5 	 global-step:14905	 l-p:0.11840882897377014
epoch£º745	 i:6 	 global-step:14906	 l-p:0.11946965754032135
epoch£º745	 i:7 	 global-step:14907	 l-p:-0.15294449031352997
epoch£º745	 i:8 	 global-step:14908	 l-p:0.15782403945922852
epoch£º745	 i:9 	 global-step:14909	 l-p:0.18699762225151062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1140, 3.0927, 3.1126],
        [3.1140, 3.0828, 3.1114],
        [3.1140, 1.9053, 1.2740],
        [3.1140, 3.1140, 3.1140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.19397465884685516 
model_pd.l_d.mean(): -24.652917861938477 
model_pd.lagr.mean(): -24.458942413330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1200], device='cuda:0')), ('power', tensor([-24.7729], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.19397465884685516
epoch£º746	 i:1 	 global-step:14921	 l-p:0.13757534325122833
epoch£º746	 i:2 	 global-step:14922	 l-p:0.11947628110647202
epoch£º746	 i:3 	 global-step:14923	 l-p:0.14030525088310242
epoch£º746	 i:4 	 global-step:14924	 l-p:-0.20127804577350616
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12485095858573914
epoch£º746	 i:6 	 global-step:14926	 l-p:0.14218692481517792
epoch£º746	 i:7 	 global-step:14927	 l-p:0.1409963071346283
epoch£º746	 i:8 	 global-step:14928	 l-p:0.1506035327911377
epoch£º746	 i:9 	 global-step:14929	 l-p:0.1170738935470581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1500, 2.9696, 3.1011],
        [3.1500, 3.0223, 3.1231],
        [3.1500, 1.9087, 1.2788],
        [3.1500, 1.9210, 1.2874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.1489161103963852 
model_pd.l_d.mean(): -24.91565704345703 
model_pd.lagr.mean(): -24.766740798950195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0655], device='cuda:0')), ('power', tensor([-24.9812], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.1489161103963852
epoch£º747	 i:1 	 global-step:14941	 l-p:0.14008523523807526
epoch£º747	 i:2 	 global-step:14942	 l-p:0.06624901294708252
epoch£º747	 i:3 	 global-step:14943	 l-p:0.15491117537021637
epoch£º747	 i:4 	 global-step:14944	 l-p:0.13023051619529724
epoch£º747	 i:5 	 global-step:14945	 l-p:0.2103303074836731
epoch£º747	 i:6 	 global-step:14946	 l-p:0.13954107463359833
epoch£º747	 i:7 	 global-step:14947	 l-p:0.17630641162395477
epoch£º747	 i:8 	 global-step:14948	 l-p:0.1303417682647705
epoch£º747	 i:9 	 global-step:14949	 l-p:0.13267016410827637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1250, 2.3590, 2.4629],
        [3.1250, 2.1631, 1.4870],
        [3.1250, 2.7346, 2.9334],
        [3.1250, 2.2728, 2.3145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.17729835212230682 
model_pd.l_d.mean(): -25.156492233276367 
model_pd.lagr.mean(): -24.97919464111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0493], device='cuda:0')), ('power', tensor([-25.1072], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.17729835212230682
epoch£º748	 i:1 	 global-step:14961	 l-p:0.13046802580356598
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1400933861732483
epoch£º748	 i:3 	 global-step:14963	 l-p:0.1626206487417221
epoch£º748	 i:4 	 global-step:14964	 l-p:0.13401128351688385
epoch£º748	 i:5 	 global-step:14965	 l-p:0.2207227349281311
epoch£º748	 i:6 	 global-step:14966	 l-p:0.2365751713514328
epoch£º748	 i:7 	 global-step:14967	 l-p:0.12967324256896973
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12402161955833435
epoch£º748	 i:9 	 global-step:14969	 l-p:0.21735331416130066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0515, 2.0599, 1.3989],
        [3.0515, 2.9759, 3.0405],
        [3.0515, 2.9618, 3.0369],
        [3.0515, 1.8192, 1.3979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): -0.00868741050362587 
model_pd.l_d.mean(): -24.917516708374023 
model_pd.lagr.mean(): -24.926204681396484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0871], device='cuda:0')), ('power', tensor([-25.0046], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:-0.00868741050362587
epoch£º749	 i:1 	 global-step:14981	 l-p:0.12949176132678986
epoch£º749	 i:2 	 global-step:14982	 l-p:0.13216108083724976
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13449709117412567
epoch£º749	 i:4 	 global-step:14984	 l-p:0.1975504457950592
epoch£º749	 i:5 	 global-step:14985	 l-p:0.05940483137965202
epoch£º749	 i:6 	 global-step:14986	 l-p:0.17445668578147888
epoch£º749	 i:7 	 global-step:14987	 l-p:0.15602761507034302
epoch£º749	 i:8 	 global-step:14988	 l-p:0.14554589986801147
epoch£º749	 i:9 	 global-step:14989	 l-p:0.07967600226402283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0394, 1.7616, 1.1767],
        [3.0394, 2.0184, 1.3644],
        [3.0394, 3.0253, 3.0387],
        [3.0394, 3.0219, 3.0384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.11908259987831116 
model_pd.l_d.mean(): -25.023866653442383 
model_pd.lagr.mean(): -24.904783248901367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0166], device='cuda:0')), ('power', tensor([-25.0405], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.11908259987831116
epoch£º750	 i:1 	 global-step:15001	 l-p:0.16183847188949585
epoch£º750	 i:2 	 global-step:15002	 l-p:0.1693287044763565
epoch£º750	 i:3 	 global-step:15003	 l-p:0.20795239508152008
epoch£º750	 i:4 	 global-step:15004	 l-p:-0.0931491032242775
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13500608503818512
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12759995460510254
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12078868597745895
epoch£º750	 i:8 	 global-step:15008	 l-p:0.020617736503481865
epoch£º750	 i:9 	 global-step:15009	 l-p:0.13122987747192383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0443, 2.6397, 2.8411],
        [3.0443, 1.7752, 1.1819],
        [3.0443, 3.0443, 3.0443],
        [3.0443, 2.3828, 2.5446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.2207062691450119 
model_pd.l_d.mean(): -25.179969787597656 
model_pd.lagr.mean(): -24.95926284790039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0451], device='cuda:0')), ('power', tensor([-25.2251], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.2207062691450119
epoch£º751	 i:1 	 global-step:15021	 l-p:0.1309574544429779
epoch£º751	 i:2 	 global-step:15022	 l-p:0.09072493016719818
epoch£º751	 i:3 	 global-step:15023	 l-p:0.15889164805412292
epoch£º751	 i:4 	 global-step:15024	 l-p:0.13949592411518097
epoch£º751	 i:5 	 global-step:15025	 l-p:0.07776561379432678
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14822740852832794
epoch£º751	 i:7 	 global-step:15027	 l-p:0.12945066392421722
epoch£º751	 i:8 	 global-step:15028	 l-p:0.1432831585407257
epoch£º751	 i:9 	 global-step:15029	 l-p:-0.012136297300457954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0102, 3.0102, 3.0102],
        [3.0102, 2.8736, 2.9802],
        [3.0102, 2.5454, 2.7496],
        [3.0102, 2.9963, 3.0095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.2638187110424042 
model_pd.l_d.mean(): -24.719364166259766 
model_pd.lagr.mean(): -24.45554542541504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1320], device='cuda:0')), ('power', tensor([-24.8514], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.2638187110424042
epoch£º752	 i:1 	 global-step:15041	 l-p:1.8660659790039062
epoch£º752	 i:2 	 global-step:15042	 l-p:0.03842543438076973
epoch£º752	 i:3 	 global-step:15043	 l-p:0.4024904668331146
epoch£º752	 i:4 	 global-step:15044	 l-p:0.1495121866464615
epoch£º752	 i:5 	 global-step:15045	 l-p:0.15129704773426056
epoch£º752	 i:6 	 global-step:15046	 l-p:0.10574094206094742
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13573424518108368
epoch£º752	 i:8 	 global-step:15048	 l-p:0.1316280961036682
epoch£º752	 i:9 	 global-step:15049	 l-p:0.11643838882446289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0715, 3.0653, 3.0713],
        [3.0715, 3.0713, 3.0715],
        [3.0715, 2.5844, 2.7866],
        [3.0715, 2.1375, 2.1087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.14080803096294403 
model_pd.l_d.mean(): -25.167491912841797 
model_pd.lagr.mean(): -25.026683807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0577], device='cuda:0')), ('power', tensor([-25.2252], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.14080803096294403
epoch£º753	 i:1 	 global-step:15061	 l-p:0.1463695466518402
epoch£º753	 i:2 	 global-step:15062	 l-p:0.07675390690565109
epoch£º753	 i:3 	 global-step:15063	 l-p:-0.013069047592580318
epoch£º753	 i:4 	 global-step:15064	 l-p:0.1443920135498047
epoch£º753	 i:5 	 global-step:15065	 l-p:0.23328757286071777
epoch£º753	 i:6 	 global-step:15066	 l-p:0.1617237776517868
epoch£º753	 i:7 	 global-step:15067	 l-p:0.12892252206802368
epoch£º753	 i:8 	 global-step:15068	 l-p:0.1251441389322281
epoch£º753	 i:9 	 global-step:15069	 l-p:0.13456997275352478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0095, 2.9919, 3.0085],
        [3.0095, 2.9963, 3.0088],
        [3.0095, 1.8401, 1.2201],
        [3.0095, 3.0090, 3.0095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.10213565826416016 
model_pd.l_d.mean(): -25.078161239624023 
model_pd.lagr.mean(): -24.976024627685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0101], device='cuda:0')), ('power', tensor([-25.0882], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.10213565826416016
epoch£º754	 i:1 	 global-step:15081	 l-p:3.1255815029144287
epoch£º754	 i:2 	 global-step:15082	 l-p:0.19586925208568573
epoch£º754	 i:3 	 global-step:15083	 l-p:0.48233842849731445
epoch£º754	 i:4 	 global-step:15084	 l-p:0.1658899486064911
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12221091240644455
epoch£º754	 i:6 	 global-step:15086	 l-p:0.12386228144168854
epoch£º754	 i:7 	 global-step:15087	 l-p:0.13696560263633728
epoch£º754	 i:8 	 global-step:15088	 l-p:0.13191363215446472
epoch£º754	 i:9 	 global-step:15089	 l-p:0.03669717535376549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0382, 2.4210, 2.5992],
        [3.0382, 2.8983, 3.0069],
        [3.0382, 1.7572, 1.1747],
        [3.0382, 2.0398, 1.9428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.057992227375507355 
model_pd.l_d.mean(): -24.900070190429688 
model_pd.lagr.mean(): -24.842077255249023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0337], device='cuda:0')), ('power', tensor([-24.9338], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.057992227375507355
epoch£º755	 i:1 	 global-step:15101	 l-p:0.4691427946090698
epoch£º755	 i:2 	 global-step:15102	 l-p:0.13169734179973602
epoch£º755	 i:3 	 global-step:15103	 l-p:0.00836306530982256
epoch£º755	 i:4 	 global-step:15104	 l-p:0.1456207036972046
epoch£º755	 i:5 	 global-step:15105	 l-p:0.16816802322864532
epoch£º755	 i:6 	 global-step:15106	 l-p:0.17271479964256287
epoch£º755	 i:7 	 global-step:15107	 l-p:0.12434981018304825
epoch£º755	 i:8 	 global-step:15108	 l-p:0.2792658805847168
epoch£º755	 i:9 	 global-step:15109	 l-p:0.1459467113018036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1030, 1.8082, 1.2245],
        [3.1030, 3.1030, 3.1030],
        [3.1030, 1.8994, 1.5145],
        [3.1030, 2.0800, 1.9487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.15690545737743378 
model_pd.l_d.mean(): -25.1032772064209 
model_pd.lagr.mean(): -24.94637107849121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0414], device='cuda:0')), ('power', tensor([-25.0619], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.15690545737743378
epoch£º756	 i:1 	 global-step:15121	 l-p:0.13915695250034332
epoch£º756	 i:2 	 global-step:15122	 l-p:0.1419837921857834
epoch£º756	 i:3 	 global-step:15123	 l-p:0.11882469058036804
epoch£º756	 i:4 	 global-step:15124	 l-p:0.15767481923103333
epoch£º756	 i:5 	 global-step:15125	 l-p:0.1161227747797966
epoch£º756	 i:6 	 global-step:15126	 l-p:0.034839943051338196
epoch£º756	 i:7 	 global-step:15127	 l-p:0.1392660290002823
epoch£º756	 i:8 	 global-step:15128	 l-p:-0.17136818170547485
epoch£º756	 i:9 	 global-step:15129	 l-p:0.16107803583145142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9649, 1.6881, 1.1957],
        [2.9649, 1.6916, 1.1235],
        [2.9649, 2.4741, 2.6782],
        [2.9649, 2.9649, 2.9649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.15967634320259094 
model_pd.l_d.mean(): -24.437864303588867 
model_pd.lagr.mean(): -24.278188705444336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1530], device='cuda:0')), ('power', tensor([-24.5909], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.15967634320259094
epoch£º757	 i:1 	 global-step:15141	 l-p:0.17166273295879364
epoch£º757	 i:2 	 global-step:15142	 l-p:0.15383177995681763
epoch£º757	 i:3 	 global-step:15143	 l-p:0.1275441199541092
epoch£º757	 i:4 	 global-step:15144	 l-p:0.16490474343299866
epoch£º757	 i:5 	 global-step:15145	 l-p:0.14228858053684235
epoch£º757	 i:6 	 global-step:15146	 l-p:0.05384369194507599
epoch£º757	 i:7 	 global-step:15147	 l-p:0.6029131412506104
epoch£º757	 i:8 	 global-step:15148	 l-p:0.009887458756566048
epoch£º757	 i:9 	 global-step:15149	 l-p:0.11394608020782471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0575, 2.3076, 2.4263],
        [3.0575, 2.4406, 2.6186],
        [3.0575, 2.8132, 2.9749],
        [3.0575, 2.0054, 1.8413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.1676006019115448 
model_pd.l_d.mean(): -25.165428161621094 
model_pd.lagr.mean(): -24.997827529907227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0443], device='cuda:0')), ('power', tensor([-25.1211], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.1676006019115448
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13561776280403137
epoch£º758	 i:2 	 global-step:15162	 l-p:0.14088024199008942
epoch£º758	 i:3 	 global-step:15163	 l-p:0.2054644376039505
epoch£º758	 i:4 	 global-step:15164	 l-p:0.15087266266345978
epoch£º758	 i:5 	 global-step:15165	 l-p:-0.0005448770243674517
epoch£º758	 i:6 	 global-step:15166	 l-p:0.10760287940502167
epoch£º758	 i:7 	 global-step:15167	 l-p:0.17146889865398407
epoch£º758	 i:8 	 global-step:15168	 l-p:0.1382146179676056
epoch£º758	 i:9 	 global-step:15169	 l-p:0.11138396710157394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1774, 3.1772, 3.1774],
        [3.1774, 3.1774, 3.1774],
        [3.1774, 1.8801, 1.2757],
        [3.1774, 3.1774, 3.1774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.12381398677825928 
model_pd.l_d.mean(): -24.771461486816406 
model_pd.lagr.mean(): -24.647647857666016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0675], device='cuda:0')), ('power', tensor([-24.7040], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.12381398677825928
epoch£º759	 i:1 	 global-step:15181	 l-p:0.6739764213562012
epoch£º759	 i:2 	 global-step:15182	 l-p:0.15894703567028046
epoch£º759	 i:3 	 global-step:15183	 l-p:0.13001655042171478
epoch£º759	 i:4 	 global-step:15184	 l-p:0.1091369241476059
epoch£º759	 i:5 	 global-step:15185	 l-p:0.14729629456996918
epoch£º759	 i:6 	 global-step:15186	 l-p:0.15302249789237976
epoch£º759	 i:7 	 global-step:15187	 l-p:0.13384422659873962
epoch£º759	 i:8 	 global-step:15188	 l-p:0.15546873211860657
epoch£º759	 i:9 	 global-step:15189	 l-p:0.07528676837682724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0380, 2.9299, 3.0179],
        [3.0380, 2.2888, 2.4090],
        [3.0380, 1.8413, 1.4770],
        [3.0380, 2.7999, 2.9592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.1451561152935028 
model_pd.l_d.mean(): -24.697338104248047 
model_pd.lagr.mean(): -24.552181243896484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1843], device='cuda:0')), ('power', tensor([-24.8816], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.1451561152935028
epoch£º760	 i:1 	 global-step:15201	 l-p:0.04725450277328491
epoch£º760	 i:2 	 global-step:15202	 l-p:0.17526286840438843
epoch£º760	 i:3 	 global-step:15203	 l-p:0.1412293016910553
epoch£º760	 i:4 	 global-step:15204	 l-p:1.5417622327804565
epoch£º760	 i:5 	 global-step:15205	 l-p:0.12447559088468552
epoch£º760	 i:6 	 global-step:15206	 l-p:0.158845454454422
epoch£º760	 i:7 	 global-step:15207	 l-p:0.1321803778409958
epoch£º760	 i:8 	 global-step:15208	 l-p:0.13888993859291077
epoch£º760	 i:9 	 global-step:15209	 l-p:0.08366499096155167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1127, 3.0589, 3.1064],
        [3.1127, 1.8243, 1.3021],
        [3.1127, 2.2472, 2.2809],
        [3.1127, 1.8485, 1.2347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.19201301038265228 
model_pd.l_d.mean(): -24.40961456298828 
model_pd.lagr.mean(): -24.217601776123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2265], device='cuda:0')), ('power', tensor([-24.6361], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.19201301038265228
epoch£º761	 i:1 	 global-step:15221	 l-p:0.18500125408172607
epoch£º761	 i:2 	 global-step:15222	 l-p:0.11562798172235489
epoch£º761	 i:3 	 global-step:15223	 l-p:0.12998899817466736
epoch£º761	 i:4 	 global-step:15224	 l-p:0.12407726049423218
epoch£º761	 i:5 	 global-step:15225	 l-p:0.12394940108060837
epoch£º761	 i:6 	 global-step:15226	 l-p:0.140500009059906
epoch£º761	 i:7 	 global-step:15227	 l-p:0.22616049647331238
epoch£º761	 i:8 	 global-step:15228	 l-p:0.13842029869556427
epoch£º761	 i:9 	 global-step:15229	 l-p:0.12789201736450195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0728, 2.2838, 2.3776],
        [3.0728, 2.2148, 2.2571],
        [3.0728, 1.8172, 1.3601],
        [3.0728, 3.0728, 3.0728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.13128353655338287 
model_pd.l_d.mean(): -25.20660972595215 
model_pd.lagr.mean(): -25.075326919555664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0157], device='cuda:0')), ('power', tensor([-25.2223], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.13128353655338287
epoch£º762	 i:1 	 global-step:15241	 l-p:0.167149618268013
epoch£º762	 i:2 	 global-step:15242	 l-p:0.2132173627614975
epoch£º762	 i:3 	 global-step:15243	 l-p:0.12318295985460281
epoch£º762	 i:4 	 global-step:15244	 l-p:0.13396191596984863
epoch£º762	 i:5 	 global-step:15245	 l-p:0.11195439845323563
epoch£º762	 i:6 	 global-step:15246	 l-p:0.09141708165407181
epoch£º762	 i:7 	 global-step:15247	 l-p:0.1686376929283142
epoch£º762	 i:8 	 global-step:15248	 l-p:0.05480891093611717
epoch£º762	 i:9 	 global-step:15249	 l-p:0.15535970032215118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0017, 1.8787, 1.6213],
        [3.0017, 2.9847, 3.0008],
        [3.0017, 2.8647, 2.9716],
        [3.0017, 1.9079, 1.2731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.11268147826194763 
model_pd.l_d.mean(): -24.700952529907227 
model_pd.lagr.mean(): -24.58827018737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0962], device='cuda:0')), ('power', tensor([-24.7972], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.11268147826194763
epoch£º763	 i:1 	 global-step:15261	 l-p:0.15851320326328278
epoch£º763	 i:2 	 global-step:15262	 l-p:0.23845748603343964
epoch£º763	 i:3 	 global-step:15263	 l-p:0.20365148782730103
epoch£º763	 i:4 	 global-step:15264	 l-p:0.13254988193511963
epoch£º763	 i:5 	 global-step:15265	 l-p:-0.0007407188531942666
epoch£º763	 i:6 	 global-step:15266	 l-p:0.16525451838970184
epoch£º763	 i:7 	 global-step:15267	 l-p:0.1415085643529892
epoch£º763	 i:8 	 global-step:15268	 l-p:0.12490770220756531
epoch£º763	 i:9 	 global-step:15269	 l-p:0.1102132499217987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1999, 2.9424, 3.1086],
        [3.1999, 3.1883, 3.1994],
        [3.1999, 1.9390, 1.4536],
        [3.1999, 2.3399, 2.3730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.16111727058887482 
model_pd.l_d.mean(): -25.145296096801758 
model_pd.lagr.mean(): -24.98417854309082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0754], device='cuda:0')), ('power', tensor([-25.0699], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.16111727058887482
epoch£º764	 i:1 	 global-step:15281	 l-p:0.12497403472661972
epoch£º764	 i:2 	 global-step:15282	 l-p:0.1296643167734146
epoch£º764	 i:3 	 global-step:15283	 l-p:0.15597932040691376
epoch£º764	 i:4 	 global-step:15284	 l-p:0.05878882110118866
epoch£º764	 i:5 	 global-step:15285	 l-p:0.17163017392158508
epoch£º764	 i:6 	 global-step:15286	 l-p:0.11536446213722229
epoch£º764	 i:7 	 global-step:15287	 l-p:0.1467640995979309
epoch£º764	 i:8 	 global-step:15288	 l-p:0.13728879392147064
epoch£º764	 i:9 	 global-step:15289	 l-p:0.1402701586484909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0872, 2.7191, 2.9158],
        [3.0872, 3.0682, 3.0861],
        [3.0872, 3.0868, 3.0872],
        [3.0872, 2.6644, 2.8671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.32076895236968994 
model_pd.l_d.mean(): -24.405017852783203 
model_pd.lagr.mean(): -24.08424949645996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1667], device='cuda:0')), ('power', tensor([-24.5718], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.32076895236968994
epoch£º765	 i:1 	 global-step:15301	 l-p:0.15294156968593597
epoch£º765	 i:2 	 global-step:15302	 l-p:0.17027407884597778
epoch£º765	 i:3 	 global-step:15303	 l-p:0.12214330583810806
epoch£º765	 i:4 	 global-step:15304	 l-p:0.16248208284378052
epoch£º765	 i:5 	 global-step:15305	 l-p:0.13688302040100098
epoch£º765	 i:6 	 global-step:15306	 l-p:0.08977798372507095
epoch£º765	 i:7 	 global-step:15307	 l-p:0.1362171471118927
epoch£º765	 i:8 	 global-step:15308	 l-p:0.022190583869814873
epoch£º765	 i:9 	 global-step:15309	 l-p:0.08688361197710037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0067, 1.9989, 1.3466],
        [3.0067, 1.9027, 1.6729],
        [3.0067, 2.8472, 2.9676],
        [3.0067, 1.8391, 1.2187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.13738107681274414 
model_pd.l_d.mean(): -25.12784767150879 
model_pd.lagr.mean(): -24.990467071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0439], device='cuda:0')), ('power', tensor([-25.1717], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.13738107681274414
epoch£º766	 i:1 	 global-step:15321	 l-p:0.14490079879760742
epoch£º766	 i:2 	 global-step:15322	 l-p:0.16531899571418762
epoch£º766	 i:3 	 global-step:15323	 l-p:1.136313557624817
epoch£º766	 i:4 	 global-step:15324	 l-p:0.16219063103199005
epoch£º766	 i:5 	 global-step:15325	 l-p:0.13733242452144623
epoch£º766	 i:6 	 global-step:15326	 l-p:0.09384429454803467
epoch£º766	 i:7 	 global-step:15327	 l-p:0.1263105869293213
epoch£º766	 i:8 	 global-step:15328	 l-p:0.022355251014232635
epoch£º766	 i:9 	 global-step:15329	 l-p:0.14432814717292786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0206, 2.8067, 2.9555],
        [3.0206, 3.0204, 3.0206],
        [3.0206, 3.0206, 3.0207],
        [3.0206, 1.9434, 1.3018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.12280555814504623 
model_pd.l_d.mean(): -25.198360443115234 
model_pd.lagr.mean(): -25.0755558013916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0360], device='cuda:0')), ('power', tensor([-25.1624], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.12280555814504623
epoch£º767	 i:1 	 global-step:15341	 l-p:0.04423995688557625
epoch£º767	 i:2 	 global-step:15342	 l-p:0.1247699111700058
epoch£º767	 i:3 	 global-step:15343	 l-p:-0.11975263059139252
epoch£º767	 i:4 	 global-step:15344	 l-p:0.14501789212226868
epoch£º767	 i:5 	 global-step:15345	 l-p:0.15181657671928406
epoch£º767	 i:6 	 global-step:15346	 l-p:0.20980361104011536
epoch£º767	 i:7 	 global-step:15347	 l-p:0.1805916577577591
epoch£º767	 i:8 	 global-step:15348	 l-p:0.1164330244064331
epoch£º767	 i:9 	 global-step:15349	 l-p:0.9876624941825867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0123, 1.8937, 1.6433],
        [3.0123, 2.1071, 1.4355],
        [3.0123, 2.8528, 2.9732],
        [3.0123, 1.7439, 1.1566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.16322015225887299 
model_pd.l_d.mean(): -24.82665252685547 
model_pd.lagr.mean(): -24.663433074951172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1360], device='cuda:0')), ('power', tensor([-24.9627], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.16322015225887299
epoch£º768	 i:1 	 global-step:15361	 l-p:0.13048548996448517
epoch£º768	 i:2 	 global-step:15362	 l-p:0.3183927536010742
epoch£º768	 i:3 	 global-step:15363	 l-p:-0.2557646632194519
epoch£º768	 i:4 	 global-step:15364	 l-p:0.2891405522823334
epoch£º768	 i:5 	 global-step:15365	 l-p:0.20785917341709137
epoch£º768	 i:6 	 global-step:15366	 l-p:0.12621177732944489
epoch£º768	 i:7 	 global-step:15367	 l-p:0.14734017848968506
epoch£º768	 i:8 	 global-step:15368	 l-p:0.12244948744773865
epoch£º768	 i:9 	 global-step:15369	 l-p:0.10123128443956375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1898, 1.9744, 1.5614],
        [3.1898, 1.9315, 1.2968],
        [3.1898, 1.9268, 1.2939],
        [3.1898, 3.1451, 3.1852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.12055158615112305 
model_pd.l_d.mean(): -24.955123901367188 
model_pd.lagr.mean(): -24.834571838378906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0134], device='cuda:0')), ('power', tensor([-24.9417], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.12055158615112305
epoch£º769	 i:1 	 global-step:15381	 l-p:0.13668303191661835
epoch£º769	 i:2 	 global-step:15382	 l-p:-3.911090850830078
epoch£º769	 i:3 	 global-step:15383	 l-p:0.12660612165927887
epoch£º769	 i:4 	 global-step:15384	 l-p:0.16649535298347473
epoch£º769	 i:5 	 global-step:15385	 l-p:0.14824669063091278
epoch£º769	 i:6 	 global-step:15386	 l-p:0.18123465776443481
epoch£º769	 i:7 	 global-step:15387	 l-p:0.1539592295885086
epoch£º769	 i:8 	 global-step:15388	 l-p:0.14194512367248535
epoch£º769	 i:9 	 global-step:15389	 l-p:0.12492493540048599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0727, 3.0725, 3.0727],
        [3.0727, 1.8095, 1.2037],
        [3.0727, 1.8248, 1.3834],
        [3.0727, 2.6687, 2.8704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): -0.4196859896183014 
model_pd.l_d.mean(): -24.31682586669922 
model_pd.lagr.mean(): -24.73651123046875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1995], device='cuda:0')), ('power', tensor([-24.5163], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:-0.4196859896183014
epoch£º770	 i:1 	 global-step:15401	 l-p:0.13961447775363922
epoch£º770	 i:2 	 global-step:15402	 l-p:0.21903790533542633
epoch£º770	 i:3 	 global-step:15403	 l-p:0.14202283322811127
epoch£º770	 i:4 	 global-step:15404	 l-p:0.18869434297084808
epoch£º770	 i:5 	 global-step:15405	 l-p:0.11935464292764664
epoch£º770	 i:6 	 global-step:15406	 l-p:0.0966690257191658
epoch£º770	 i:7 	 global-step:15407	 l-p:0.1531541794538498
epoch£º770	 i:8 	 global-step:15408	 l-p:0.10634279251098633
epoch£º770	 i:9 	 global-step:15409	 l-p:0.11666125059127808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9947, 2.9944, 2.9947],
        [2.9947, 2.8538, 2.9632],
        [2.9947, 2.9800, 2.9939],
        [2.9947, 1.7987, 1.4441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.1286441832780838 
model_pd.l_d.mean(): -25.149389266967773 
model_pd.lagr.mean(): -25.02074432373047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0581], device='cuda:0')), ('power', tensor([-25.0913], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.1286441832780838
epoch£º771	 i:1 	 global-step:15421	 l-p:0.11098235845565796
epoch£º771	 i:2 	 global-step:15422	 l-p:0.07105071097612381
epoch£º771	 i:3 	 global-step:15423	 l-p:0.1413356363773346
epoch£º771	 i:4 	 global-step:15424	 l-p:0.3259223997592926
epoch£º771	 i:5 	 global-step:15425	 l-p:0.14562641084194183
epoch£º771	 i:6 	 global-step:15426	 l-p:0.13926950097084045
epoch£º771	 i:7 	 global-step:15427	 l-p:0.1706131398677826
epoch£º771	 i:8 	 global-step:15428	 l-p:0.17650596797466278
epoch£º771	 i:9 	 global-step:15429	 l-p:0.15068383514881134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9961, 2.8600, 2.9664],
        [2.9961, 2.7501, 2.9130],
        [2.9961, 1.9700, 1.3223],
        [2.9961, 2.8551, 2.9646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.1867222934961319 
model_pd.l_d.mean(): -25.19481658935547 
model_pd.lagr.mean(): -25.008094787597656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0110], device='cuda:0')), ('power', tensor([-25.1838], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.1867222934961319
epoch£º772	 i:1 	 global-step:15441	 l-p:0.13793373107910156
epoch£º772	 i:2 	 global-step:15442	 l-p:0.02033277042210102
epoch£º772	 i:3 	 global-step:15443	 l-p:0.14511580765247345
epoch£º772	 i:4 	 global-step:15444	 l-p:0.15666444599628448
epoch£º772	 i:5 	 global-step:15445	 l-p:0.03606797009706497
epoch£º772	 i:6 	 global-step:15446	 l-p:0.114496149122715
epoch£º772	 i:7 	 global-step:15447	 l-p:0.15670718252658844
epoch£º772	 i:8 	 global-step:15448	 l-p:0.1001109704375267
epoch£º772	 i:9 	 global-step:15449	 l-p:0.14863015711307526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0298, 3.0298, 3.0298],
        [3.0298, 2.9068, 3.0048],
        [3.0298, 2.0265, 1.9287],
        [3.0298, 2.0512, 1.9819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.14423413574695587 
model_pd.l_d.mean(): -25.292646408081055 
model_pd.lagr.mean(): -25.148412704467773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0529], device='cuda:0')), ('power', tensor([-25.2398], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.14423413574695587
epoch£º773	 i:1 	 global-step:15461	 l-p:0.14297375082969666
epoch£º773	 i:2 	 global-step:15462	 l-p:0.0198520440608263
epoch£º773	 i:3 	 global-step:15463	 l-p:0.06665171682834625
epoch£º773	 i:4 	 global-step:15464	 l-p:0.16269639134407043
epoch£º773	 i:5 	 global-step:15465	 l-p:0.21044951677322388
epoch£º773	 i:6 	 global-step:15466	 l-p:0.0861511379480362
epoch£º773	 i:7 	 global-step:15467	 l-p:0.1523386687040329
epoch£º773	 i:8 	 global-step:15468	 l-p:0.1328846514225006
epoch£º773	 i:9 	 global-step:15469	 l-p:0.1405922770500183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9665, 2.6207, 2.8148],
        [2.9665, 2.8335, 2.9380],
        [2.9665, 1.8591, 1.6307],
        [2.9665, 2.9039, 2.9585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.1465165615081787 
model_pd.l_d.mean(): -25.254283905029297 
model_pd.lagr.mean(): -25.10776710510254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0537], device='cuda:0')), ('power', tensor([-25.3080], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.1465165615081787
epoch£º774	 i:1 	 global-step:15481	 l-p:0.16572393476963043
epoch£º774	 i:2 	 global-step:15482	 l-p:0.15364739298820496
epoch£º774	 i:3 	 global-step:15483	 l-p:-0.41341689229011536
epoch£º774	 i:4 	 global-step:15484	 l-p:0.15816842019557953
epoch£º774	 i:5 	 global-step:15485	 l-p:0.07115405797958374
epoch£º774	 i:6 	 global-step:15486	 l-p:0.15811696648597717
epoch£º774	 i:7 	 global-step:15487	 l-p:0.83689945936203
epoch£º774	 i:8 	 global-step:15488	 l-p:0.13217519223690033
epoch£º774	 i:9 	 global-step:15489	 l-p:0.12886622548103333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0494, 2.0088, 1.8652],
        [3.0494, 2.1515, 2.1637],
        [3.0494, 3.0494, 3.0495],
        [3.0494, 1.8762, 1.2478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.13756081461906433 
model_pd.l_d.mean(): -24.473613739013672 
model_pd.lagr.mean(): -24.3360538482666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2484], device='cuda:0')), ('power', tensor([-24.7220], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.13756081461906433
epoch£º775	 i:1 	 global-step:15501	 l-p:0.15382616221904755
epoch£º775	 i:2 	 global-step:15502	 l-p:-0.19876977801322937
epoch£º775	 i:3 	 global-step:15503	 l-p:0.16655513644218445
epoch£º775	 i:4 	 global-step:15504	 l-p:0.20674003660678864
epoch£º775	 i:5 	 global-step:15505	 l-p:0.08131488412618637
epoch£º775	 i:6 	 global-step:15506	 l-p:0.16930541396141052
epoch£º775	 i:7 	 global-step:15507	 l-p:0.1363021582365036
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13342981040477753
epoch£º775	 i:9 	 global-step:15509	 l-p:0.11935809999704361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[3.2194, 2.5647, 2.7257],
        [3.2194, 2.2889, 2.2587],
        [3.2194, 2.3313, 2.3419],
        [3.2194, 2.2114, 1.5260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.13654159009456635 
model_pd.l_d.mean(): -25.140220642089844 
model_pd.lagr.mean(): -25.003679275512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0304], device='cuda:0')), ('power', tensor([-25.1098], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.13654159009456635
epoch£º776	 i:1 	 global-step:15521	 l-p:0.1193147599697113
epoch£º776	 i:2 	 global-step:15522	 l-p:0.12068044394254684
epoch£º776	 i:3 	 global-step:15523	 l-p:0.1276988834142685
epoch£º776	 i:4 	 global-step:15524	 l-p:0.11956492066383362
epoch£º776	 i:5 	 global-step:15525	 l-p:0.16141870617866516
epoch£º776	 i:6 	 global-step:15526	 l-p:0.13132986426353455
epoch£º776	 i:7 	 global-step:15527	 l-p:0.1824813038110733
epoch£º776	 i:8 	 global-step:15528	 l-p:0.13745680451393127
epoch£º776	 i:9 	 global-step:15529	 l-p:-1.415327548980713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0566, 2.2026, 2.2529],
        [3.0566, 2.7698, 2.9475],
        [3.0566, 2.1295, 2.1142],
        [3.0566, 1.9042, 1.6060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.12104732543230057 
model_pd.l_d.mean(): -25.132675170898438 
model_pd.lagr.mean(): -25.011627197265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0023], device='cuda:0')), ('power', tensor([-25.1350], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.12104732543230057
epoch£º777	 i:1 	 global-step:15541	 l-p:0.1491604596376419
epoch£º777	 i:2 	 global-step:15542	 l-p:0.14613668620586395
epoch£º777	 i:3 	 global-step:15543	 l-p:0.15985524654388428
epoch£º777	 i:4 	 global-step:15544	 l-p:0.16996131837368011
epoch£º777	 i:5 	 global-step:15545	 l-p:0.014800348319113255
epoch£º777	 i:6 	 global-step:15546	 l-p:0.13643883168697357
epoch£º777	 i:7 	 global-step:15547	 l-p:0.1882590502500534
epoch£º777	 i:8 	 global-step:15548	 l-p:0.21824775636196136
epoch£º777	 i:9 	 global-step:15549	 l-p:0.15405888855457306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0335, 3.0305, 3.0334],
        [3.0335, 2.7688, 2.9389],
        [3.0335, 2.9792, 3.0272],
        [3.0335, 3.0120, 3.0321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): -0.5532482862472534 
model_pd.l_d.mean(): -24.557432174682617 
model_pd.lagr.mean(): -25.110679626464844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2152], device='cuda:0')), ('power', tensor([-24.7727], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:-0.5532482862472534
epoch£º778	 i:1 	 global-step:15561	 l-p:0.14742648601531982
epoch£º778	 i:2 	 global-step:15562	 l-p:-0.24410155415534973
epoch£º778	 i:3 	 global-step:15563	 l-p:0.14714251458644867
epoch£º778	 i:4 	 global-step:15564	 l-p:0.16043663024902344
epoch£º778	 i:5 	 global-step:15565	 l-p:0.13554978370666504
epoch£º778	 i:6 	 global-step:15566	 l-p:0.14267602562904358
epoch£º778	 i:7 	 global-step:15567	 l-p:0.13379865884780884
epoch£º778	 i:8 	 global-step:15568	 l-p:0.1018705889582634
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14685943722724915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1010, 2.5216, 2.7110],
        [3.1010, 1.8938, 1.5133],
        [3.1010, 3.1007, 3.1010],
        [3.1010, 2.8373, 3.0068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.3138602077960968 
model_pd.l_d.mean(): -25.031917572021484 
model_pd.lagr.mean(): -24.71805763244629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0257], device='cuda:0')), ('power', tensor([-25.0062], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.3138602077960968
epoch£º779	 i:1 	 global-step:15581	 l-p:0.1355564296245575
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13056835532188416
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12428880482912064
epoch£º779	 i:4 	 global-step:15584	 l-p:0.14867444336414337
epoch£º779	 i:5 	 global-step:15585	 l-p:0.009883670136332512
epoch£º779	 i:6 	 global-step:15586	 l-p:0.1332932859659195
epoch£º779	 i:7 	 global-step:15587	 l-p:0.0055519104935228825
epoch£º779	 i:8 	 global-step:15588	 l-p:0.15489931404590607
epoch£º779	 i:9 	 global-step:15589	 l-p:0.06844078749418259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9905, 2.9738, 2.9895],
        [2.9905, 2.9310, 2.9832],
        [2.9905, 1.8565, 1.2303],
        [2.9905, 1.8521, 1.2269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.1816820651292801 
model_pd.l_d.mean(): -24.494401931762695 
model_pd.lagr.mean(): -24.312719345092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2202], device='cuda:0')), ('power', tensor([-24.7146], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.1816820651292801
epoch£º780	 i:1 	 global-step:15601	 l-p:-0.2919621467590332
epoch£º780	 i:2 	 global-step:15602	 l-p:0.1529032438993454
epoch£º780	 i:3 	 global-step:15603	 l-p:0.10343534499406815
epoch£º780	 i:4 	 global-step:15604	 l-p:0.17070960998535156
epoch£º780	 i:5 	 global-step:15605	 l-p:0.10451152920722961
epoch£º780	 i:6 	 global-step:15606	 l-p:0.17176824808120728
epoch£º780	 i:7 	 global-step:15607	 l-p:0.32339709997177124
epoch£º780	 i:8 	 global-step:15608	 l-p:0.15423312783241272
epoch£º780	 i:9 	 global-step:15609	 l-p:0.13409419357776642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1243, 1.9812, 1.3317],
        [3.1243, 3.1243, 3.1244],
        [3.1243, 3.1053, 3.1232],
        [3.1243, 3.1159, 3.1240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.14815950393676758 
model_pd.l_d.mean(): -25.08906364440918 
model_pd.lagr.mean(): -24.94090461730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0206], device='cuda:0')), ('power', tensor([-25.1096], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.14815950393676758
epoch£º781	 i:1 	 global-step:15621	 l-p:0.17256473004817963
epoch£º781	 i:2 	 global-step:15622	 l-p:0.15632618963718414
epoch£º781	 i:3 	 global-step:15623	 l-p:0.14287808537483215
epoch£º781	 i:4 	 global-step:15624	 l-p:0.14844460785388947
epoch£º781	 i:5 	 global-step:15625	 l-p:0.21749503910541534
epoch£º781	 i:6 	 global-step:15626	 l-p:0.10207298398017883
epoch£º781	 i:7 	 global-step:15627	 l-p:0.14229653775691986
epoch£º781	 i:8 	 global-step:15628	 l-p:0.12176140397787094
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11404317617416382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1711, 3.1131, 3.1641],
        [3.1711, 1.9579, 1.3130],
        [3.1711, 2.9579, 3.1061],
        [3.1711, 2.9445, 3.0987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.15348418056964874 
model_pd.l_d.mean(): -24.87470245361328 
model_pd.lagr.mean(): -24.72121810913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0990], device='cuda:0')), ('power', tensor([-24.7757], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.15348418056964874
epoch£º782	 i:1 	 global-step:15641	 l-p:0.033340781927108765
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12329299747943878
epoch£º782	 i:3 	 global-step:15643	 l-p:0.13017812371253967
epoch£º782	 i:4 	 global-step:15644	 l-p:0.12466021627187729
epoch£º782	 i:5 	 global-step:15645	 l-p:-0.01352409366518259
epoch£º782	 i:6 	 global-step:15646	 l-p:0.1914222091436386
epoch£º782	 i:7 	 global-step:15647	 l-p:0.1374918669462204
epoch£º782	 i:8 	 global-step:15648	 l-p:0.09903499484062195
epoch£º782	 i:9 	 global-step:15649	 l-p:0.1637534648180008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0078, 1.7500, 1.1549],
        [3.0078, 2.0542, 2.0172],
        [3.0078, 1.7334, 1.1476],
        [3.0078, 2.8620, 2.9744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.16298066079616547 
model_pd.l_d.mean(): -25.172273635864258 
model_pd.lagr.mean(): -25.009292602539062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0106], device='cuda:0')), ('power', tensor([-25.1617], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.16298066079616547
epoch£º783	 i:1 	 global-step:15661	 l-p:0.12182724475860596
epoch£º783	 i:2 	 global-step:15662	 l-p:0.11909335851669312
epoch£º783	 i:3 	 global-step:15663	 l-p:0.09287925809621811
epoch£º783	 i:4 	 global-step:15664	 l-p:0.13155601918697357
epoch£º783	 i:5 	 global-step:15665	 l-p:0.07161670178174973
epoch£º783	 i:6 	 global-step:15666	 l-p:0.23013544082641602
epoch£º783	 i:7 	 global-step:15667	 l-p:0.13956427574157715
epoch£º783	 i:8 	 global-step:15668	 l-p:0.15368583798408508
epoch£º783	 i:9 	 global-step:15669	 l-p:0.15270398557186127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0968, 1.8293, 1.2172],
        [3.0968, 3.0968, 3.0968],
        [3.0968, 2.7469, 2.9410],
        [3.0968, 2.7169, 2.9163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.15151061117649078 
model_pd.l_d.mean(): -24.958011627197266 
model_pd.lagr.mean(): -24.806501388549805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0690], device='cuda:0')), ('power', tensor([-25.0270], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.15151061117649078
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13214720785617828
epoch£º784	 i:2 	 global-step:15682	 l-p:0.120292067527771
epoch£º784	 i:3 	 global-step:15683	 l-p:0.1484024077653885
epoch£º784	 i:4 	 global-step:15684	 l-p:0.14862212538719177
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13771076500415802
epoch£º784	 i:6 	 global-step:15686	 l-p:0.3089822232723236
epoch£º784	 i:7 	 global-step:15687	 l-p:0.326465904712677
epoch£º784	 i:8 	 global-step:15688	 l-p:0.22109924256801605
epoch£º784	 i:9 	 global-step:15689	 l-p:0.11434020847082138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1031, 3.1031, 3.1031],
        [3.1031, 3.0891, 3.1024],
        [3.1031, 1.8154, 1.3109],
        [3.1031, 2.4773, 2.6542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.12755239009857178 
model_pd.l_d.mean(): -24.873523712158203 
model_pd.lagr.mean(): -24.7459716796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0130], device='cuda:0')), ('power', tensor([-24.8605], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.12755239009857178
epoch£º785	 i:1 	 global-step:15701	 l-p:0.21663863956928253
epoch£º785	 i:2 	 global-step:15702	 l-p:0.13642831146717072
epoch£º785	 i:3 	 global-step:15703	 l-p:0.10234604775905609
epoch£º785	 i:4 	 global-step:15704	 l-p:0.16388309001922607
epoch£º785	 i:5 	 global-step:15705	 l-p:0.14468230307102203
epoch£º785	 i:6 	 global-step:15706	 l-p:0.150504007935524
epoch£º785	 i:7 	 global-step:15707	 l-p:0.1370180994272232
epoch£º785	 i:8 	 global-step:15708	 l-p:0.16314689815044403
epoch£º785	 i:9 	 global-step:15709	 l-p:0.13921785354614258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1219, 3.1219, 3.1219],
        [3.1219, 2.1494, 1.4720],
        [3.1219, 1.8721, 1.2470],
        [3.1219, 2.9077, 3.0566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.18752676248550415 
model_pd.l_d.mean(): -25.115680694580078 
model_pd.lagr.mean(): -24.92815399169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0777], device='cuda:0')), ('power', tensor([-25.0379], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.18752676248550415
epoch£º786	 i:1 	 global-step:15721	 l-p:0.13602879643440247
epoch£º786	 i:2 	 global-step:15722	 l-p:0.16170737147331238
epoch£º786	 i:3 	 global-step:15723	 l-p:0.13316740095615387
epoch£º786	 i:4 	 global-step:15724	 l-p:0.2512899339199066
epoch£º786	 i:5 	 global-step:15725	 l-p:0.17373263835906982
epoch£º786	 i:6 	 global-step:15726	 l-p:0.12028331309556961
epoch£º786	 i:7 	 global-step:15727	 l-p:0.12939585745334625
epoch£º786	 i:8 	 global-step:15728	 l-p:0.07622742652893066
epoch£º786	 i:9 	 global-step:15729	 l-p:0.13211499154567719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1152, 1.8415, 1.2270],
        [3.1152, 2.9700, 3.0820],
        [3.1152, 3.0569, 3.1081],
        [3.1152, 2.8474, 3.0185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.14960911870002747 
model_pd.l_d.mean(): -24.677309036254883 
model_pd.lagr.mean(): -24.527700424194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297], device='cuda:0')), ('power', tensor([-24.8070], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.14960911870002747
epoch£º787	 i:1 	 global-step:15741	 l-p:0.16902974247932434
epoch£º787	 i:2 	 global-step:15742	 l-p:0.13072550296783447
epoch£º787	 i:3 	 global-step:15743	 l-p:0.1249963566660881
epoch£º787	 i:4 	 global-step:15744	 l-p:0.1815590262413025
epoch£º787	 i:5 	 global-step:15745	 l-p:0.1391373574733734
epoch£º787	 i:6 	 global-step:15746	 l-p:0.11417413502931595
epoch£º787	 i:7 	 global-step:15747	 l-p:0.1672891229391098
epoch£º787	 i:8 	 global-step:15748	 l-p:0.14363965392112732
epoch£º787	 i:9 	 global-step:15749	 l-p:0.11928485333919525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1395, 2.1076, 1.9747],
        [3.1395, 3.1395, 3.1395],
        [3.1395, 2.0369, 1.8104],
        [3.1395, 1.9520, 1.3071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.12558725476264954 
model_pd.l_d.mean(): -25.09683609008789 
model_pd.lagr.mean(): -24.971248626708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0991], device='cuda:0')), ('power', tensor([-24.9978], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.12558725476264954
epoch£º788	 i:1 	 global-step:15761	 l-p:0.08587191998958588
epoch£º788	 i:2 	 global-step:15762	 l-p:0.14483316242694855
epoch£º788	 i:3 	 global-step:15763	 l-p:0.24224571883678436
epoch£º788	 i:4 	 global-step:15764	 l-p:0.13748735189437866
epoch£º788	 i:5 	 global-step:15765	 l-p:0.2021692395210266
epoch£º788	 i:6 	 global-step:15766	 l-p:0.15001490712165833
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12249738723039627
epoch£º788	 i:8 	 global-step:15768	 l-p:0.139230415225029
epoch£º788	 i:9 	 global-step:15769	 l-p:0.1324131041765213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0891, 2.9392, 3.0540],
        [3.0891, 2.1189, 2.0623],
        [3.0891, 3.0891, 3.0891],
        [3.0891, 3.0890, 3.0891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.2533697783946991 
model_pd.l_d.mean(): -25.01371955871582 
model_pd.lagr.mean(): -24.76034927368164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0151], device='cuda:0')), ('power', tensor([-24.9986], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.2533697783946991
epoch£º789	 i:1 	 global-step:15781	 l-p:-2.7455105781555176
epoch£º789	 i:2 	 global-step:15782	 l-p:0.20848269760608673
epoch£º789	 i:3 	 global-step:15783	 l-p:-0.8015505075454712
epoch£º789	 i:4 	 global-step:15784	 l-p:0.1431465893983841
epoch£º789	 i:5 	 global-step:15785	 l-p:0.1771324872970581
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11528726667165756
epoch£º789	 i:7 	 global-step:15787	 l-p:0.08691629022359848
epoch£º789	 i:8 	 global-step:15788	 l-p:0.132703498005867
epoch£º789	 i:9 	 global-step:15789	 l-p:0.16123774647712708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1120, 2.8677, 3.0299],
        [3.1120, 3.1120, 3.1120],
        [3.1120, 1.8464, 1.2283],
        [3.1120, 2.1430, 2.0862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.16784119606018066 
model_pd.l_d.mean(): -25.225444793701172 
model_pd.lagr.mean(): -25.05760383605957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0732], device='cuda:0')), ('power', tensor([-25.1522], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.16784119606018066
epoch£º790	 i:1 	 global-step:15801	 l-p:0.15795601904392242
epoch£º790	 i:2 	 global-step:15802	 l-p:0.13126297295093536
epoch£º790	 i:3 	 global-step:15803	 l-p:0.45016130805015564
epoch£º790	 i:4 	 global-step:15804	 l-p:0.27884823083877563
epoch£º790	 i:5 	 global-step:15805	 l-p:0.12939146161079407
epoch£º790	 i:6 	 global-step:15806	 l-p:0.1441645622253418
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12724383175373077
epoch£º790	 i:8 	 global-step:15808	 l-p:0.11801792681217194
epoch£º790	 i:9 	 global-step:15809	 l-p:0.17556023597717285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0403, 2.1358, 2.1481],
        [3.0403, 2.0982, 1.4262],
        [3.0403, 2.6673, 2.8668],
        [3.0403, 2.3007, 2.4324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.154710590839386 
model_pd.l_d.mean(): -25.08467674255371 
model_pd.lagr.mean(): -24.92996597290039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0714], device='cuda:0')), ('power', tensor([-25.0133], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.154710590839386
epoch£º791	 i:1 	 global-step:15821	 l-p:0.14748014509677887
epoch£º791	 i:2 	 global-step:15822	 l-p:0.31509649753570557
epoch£º791	 i:3 	 global-step:15823	 l-p:0.13313749432563782
epoch£º791	 i:4 	 global-step:15824	 l-p:0.10125603526830673
epoch£º791	 i:5 	 global-step:15825	 l-p:0.15636016428470612
epoch£º791	 i:6 	 global-step:15826	 l-p:0.10391822457313538
epoch£º791	 i:7 	 global-step:15827	 l-p:0.16923213005065918
epoch£º791	 i:8 	 global-step:15828	 l-p:0.13942286372184753
epoch£º791	 i:9 	 global-step:15829	 l-p:0.16524051129817963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0154, 1.7879, 1.3972],
        [3.0154, 1.7205, 1.1478],
        [3.0154, 2.7857, 2.9422],
        [3.0154, 2.5201, 2.7259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.14442004263401031 
model_pd.l_d.mean(): -25.180889129638672 
model_pd.lagr.mean(): -25.036468505859375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0287], device='cuda:0')), ('power', tensor([-25.1522], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.14442004263401031
epoch£º792	 i:1 	 global-step:15841	 l-p:-1.3566607236862183
epoch£º792	 i:2 	 global-step:15842	 l-p:0.15168829262256622
epoch£º792	 i:3 	 global-step:15843	 l-p:0.14412684738636017
epoch£º792	 i:4 	 global-step:15844	 l-p:0.12257508933544159
epoch£º792	 i:5 	 global-step:15845	 l-p:0.1282179355621338
epoch£º792	 i:6 	 global-step:15846	 l-p:-0.5999997854232788
epoch£º792	 i:7 	 global-step:15847	 l-p:0.16567866504192352
epoch£º792	 i:8 	 global-step:15848	 l-p:0.13717973232269287
epoch£º792	 i:9 	 global-step:15849	 l-p:0.22324077785015106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0093, 3.0092, 3.0093],
        [3.0093, 3.0008, 3.0090],
        [3.0093, 3.0092, 3.0093],
        [3.0093, 2.8562, 2.9732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13633973896503448 
model_pd.l_d.mean(): -25.160463333129883 
model_pd.lagr.mean(): -25.024124145507812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0719], device='cuda:0')), ('power', tensor([-25.0885], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13633973896503448
epoch£º793	 i:1 	 global-step:15861	 l-p:-0.16964423656463623
epoch£º793	 i:2 	 global-step:15862	 l-p:0.2316056340932846
epoch£º793	 i:3 	 global-step:15863	 l-p:1.2103338241577148
epoch£º793	 i:4 	 global-step:15864	 l-p:-0.04713542014360428
epoch£º793	 i:5 	 global-step:15865	 l-p:0.14661872386932373
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13137049973011017
epoch£º793	 i:7 	 global-step:15867	 l-p:0.12243009358644485
epoch£º793	 i:8 	 global-step:15868	 l-p:0.14441345632076263
epoch£º793	 i:9 	 global-step:15869	 l-p:0.13784299790859222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1315, 1.9798, 1.6846],
        [3.1315, 3.0630, 3.1222],
        [3.1315, 2.0241, 1.7943],
        [3.1315, 3.1315, 3.1315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.17767034471035004 
model_pd.l_d.mean(): -24.944368362426758 
model_pd.lagr.mean(): -24.766698837280273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0136], device='cuda:0')), ('power', tensor([-24.9580], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.17767034471035004
epoch£º794	 i:1 	 global-step:15881	 l-p:0.04726513847708702
epoch£º794	 i:2 	 global-step:15882	 l-p:0.14180202782154083
epoch£º794	 i:3 	 global-step:15883	 l-p:0.12416406720876694
epoch£º794	 i:4 	 global-step:15884	 l-p:0.14299868047237396
epoch£º794	 i:5 	 global-step:15885	 l-p:0.17713066935539246
epoch£º794	 i:6 	 global-step:15886	 l-p:0.1446242481470108
epoch£º794	 i:7 	 global-step:15887	 l-p:0.14168906211853027
epoch£º794	 i:8 	 global-step:15888	 l-p:0.11163537949323654
epoch£º794	 i:9 	 global-step:15889	 l-p:0.25876015424728394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0885, 3.0885, 3.0885],
        [3.0885, 1.7847, 1.2542],
        [3.0885, 2.9518, 3.0587],
        [3.0885, 2.9583, 3.0611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.16719704866409302 
model_pd.l_d.mean(): -24.975547790527344 
model_pd.lagr.mean(): -24.808351516723633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0288], device='cuda:0')), ('power', tensor([-24.9467], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.16719704866409302
epoch£º795	 i:1 	 global-step:15901	 l-p:0.13288432359695435
epoch£º795	 i:2 	 global-step:15902	 l-p:0.15046989917755127
epoch£º795	 i:3 	 global-step:15903	 l-p:-0.012596630491316319
epoch£º795	 i:4 	 global-step:15904	 l-p:0.14369092881679535
epoch£º795	 i:5 	 global-step:15905	 l-p:0.1933928281068802
epoch£º795	 i:6 	 global-step:15906	 l-p:0.3689173758029938
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12001267075538635
epoch£º795	 i:8 	 global-step:15908	 l-p:0.09376638382673264
epoch£º795	 i:9 	 global-step:15909	 l-p:0.09284547716379166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0437, 2.8777, 3.0021],
        [3.0437, 1.7825, 1.1778],
        [3.0437, 2.5836, 2.7903],
        [3.0437, 3.0436, 3.0437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.09335663914680481 
model_pd.l_d.mean(): -24.893558502197266 
model_pd.lagr.mean(): -24.800201416015625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0695], device='cuda:0')), ('power', tensor([-24.9631], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.09335663914680481
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12718887627124786
epoch£º796	 i:2 	 global-step:15922	 l-p:-0.01736324280500412
epoch£º796	 i:3 	 global-step:15923	 l-p:0.4420905113220215
epoch£º796	 i:4 	 global-step:15924	 l-p:0.13378354907035828
epoch£º796	 i:5 	 global-step:15925	 l-p:0.16038943827152252
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11115602403879166
epoch£º796	 i:7 	 global-step:15927	 l-p:0.1754431426525116
epoch£º796	 i:8 	 global-step:15928	 l-p:0.13687977194786072
epoch£º796	 i:9 	 global-step:15929	 l-p:0.14570510387420654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0945, 2.7947, 2.9768],
        [3.0945, 3.0428, 3.0887],
        [3.0945, 3.0926, 3.0945],
        [3.0945, 3.0945, 3.0945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.34973669052124023 
model_pd.l_d.mean(): -25.067142486572266 
model_pd.lagr.mean(): -24.717405319213867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0018], device='cuda:0')), ('power', tensor([-25.0654], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.34973669052124023
epoch£º797	 i:1 	 global-step:15941	 l-p:0.09923168271780014
epoch£º797	 i:2 	 global-step:15942	 l-p:0.14037543535232544
epoch£º797	 i:3 	 global-step:15943	 l-p:0.11493223160505295
epoch£º797	 i:4 	 global-step:15944	 l-p:0.4042361378669739
epoch£º797	 i:5 	 global-step:15945	 l-p:0.11926712095737457
epoch£º797	 i:6 	 global-step:15946	 l-p:0.26386556029319763
epoch£º797	 i:7 	 global-step:15947	 l-p:-0.09503726661205292
epoch£º797	 i:8 	 global-step:15948	 l-p:0.16055171191692352
epoch£º797	 i:9 	 global-step:15949	 l-p:0.1804196834564209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0816, 1.8722, 1.2431],
        [3.0816, 3.0673, 3.0809],
        [3.0816, 2.6164, 2.8226],
        [3.0816, 2.0356, 1.3751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.13821755349636078 
model_pd.l_d.mean(): -24.87767791748047 
model_pd.lagr.mean(): -24.739459991455078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0167], device='cuda:0')), ('power', tensor([-24.8944], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.13821755349636078
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11077695339918137
epoch£º798	 i:2 	 global-step:15962	 l-p:0.13541340827941895
epoch£º798	 i:3 	 global-step:15963	 l-p:0.1373218446969986
epoch£º798	 i:4 	 global-step:15964	 l-p:0.07560746371746063
epoch£º798	 i:5 	 global-step:15965	 l-p:0.0034193419851362705
epoch£º798	 i:6 	 global-step:15966	 l-p:-0.8583859205245972
epoch£º798	 i:7 	 global-step:15967	 l-p:0.18146266043186188
epoch£º798	 i:8 	 global-step:15968	 l-p:0.24163515865802765
epoch£º798	 i:9 	 global-step:15969	 l-p:0.15373638272285461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0217, 2.8845, 2.9918],
        [3.0217, 3.0177, 3.0216],
        [3.0217, 3.0217, 3.0217],
        [3.0217, 3.0134, 3.0214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.2473641186952591 
model_pd.l_d.mean(): -25.24586296081543 
model_pd.lagr.mean(): -24.998498916625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0861], device='cuda:0')), ('power', tensor([-25.1597], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.2473641186952591
epoch£º799	 i:1 	 global-step:15981	 l-p:0.15830011665821075
epoch£º799	 i:2 	 global-step:15982	 l-p:0.14122353494167328
epoch£º799	 i:3 	 global-step:15983	 l-p:0.2522684335708618
epoch£º799	 i:4 	 global-step:15984	 l-p:0.061916206032037735
epoch£º799	 i:5 	 global-step:15985	 l-p:0.21350696682929993
epoch£º799	 i:6 	 global-step:15986	 l-p:0.2831488847732544
epoch£º799	 i:7 	 global-step:15987	 l-p:0.07794275134801865
epoch£º799	 i:8 	 global-step:15988	 l-p:0.16125072538852692
epoch£º799	 i:9 	 global-step:15989	 l-p:0.15613433718681335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1947, 2.3419, 2.3909],
        [3.1947, 2.0739, 1.4064],
        [3.1947, 3.1706, 3.1930],
        [3.1947, 2.0389, 1.7318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.2648768424987793 
model_pd.l_d.mean(): -24.351892471313477 
model_pd.lagr.mean(): -24.08701515197754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1047], device='cuda:0')), ('power', tensor([-24.4566], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.2648768424987793
epoch£º800	 i:1 	 global-step:16001	 l-p:0.1266481578350067
epoch£º800	 i:2 	 global-step:16002	 l-p:0.13060615956783295
epoch£º800	 i:3 	 global-step:16003	 l-p:0.14564374089241028
epoch£º800	 i:4 	 global-step:16004	 l-p:0.12604685127735138
epoch£º800	 i:5 	 global-step:16005	 l-p:0.11129316687583923
epoch£º800	 i:6 	 global-step:16006	 l-p:0.1311325877904892
epoch£º800	 i:7 	 global-step:16007	 l-p:0.1182875782251358
epoch£º800	 i:8 	 global-step:16008	 l-p:0.14279057085514069
epoch£º800	 i:9 	 global-step:16009	 l-p:0.10694465041160583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[3.1738, 2.0534, 1.3895],
        [3.1738, 2.0211, 1.3625],
        [3.1738, 1.8663, 1.3217],
        [3.1738, 2.3011, 2.3350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.13379913568496704 
model_pd.l_d.mean(): -24.722597122192383 
model_pd.lagr.mean(): -24.58879852294922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0330], device='cuda:0')), ('power', tensor([-24.6896], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.13379913568496704
epoch£º801	 i:1 	 global-step:16021	 l-p:0.11271914839744568
epoch£º801	 i:2 	 global-step:16022	 l-p:0.1853388547897339
epoch£º801	 i:3 	 global-step:16023	 l-p:0.1668214052915573
epoch£º801	 i:4 	 global-step:16024	 l-p:0.14418573677539825
epoch£º801	 i:5 	 global-step:16025	 l-p:0.14043931663036346
epoch£º801	 i:6 	 global-step:16026	 l-p:0.09253406524658203
epoch£º801	 i:7 	 global-step:16027	 l-p:0.1660727709531784
epoch£º801	 i:8 	 global-step:16028	 l-p:0.14468052983283997
epoch£º801	 i:9 	 global-step:16029	 l-p:10.301308631896973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0820, 3.0712, 3.0815],
        [3.0820, 2.6720, 2.8761],
        [3.0820, 3.0820, 3.0820],
        [3.0820, 1.9928, 1.7934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 1.627524495124817 
model_pd.l_d.mean(): -24.58637046813965 
model_pd.lagr.mean(): -22.958845138549805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0738], device='cuda:0')), ('power', tensor([-24.6602], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:1.627524495124817
epoch£º802	 i:1 	 global-step:16041	 l-p:0.14607419073581696
epoch£º802	 i:2 	 global-step:16042	 l-p:0.530521035194397
epoch£º802	 i:3 	 global-step:16043	 l-p:0.18017959594726562
epoch£º802	 i:4 	 global-step:16044	 l-p:-3.271641254425049
epoch£º802	 i:5 	 global-step:16045	 l-p:0.13080482184886932
epoch£º802	 i:6 	 global-step:16046	 l-p:0.12358468770980835
epoch£º802	 i:7 	 global-step:16047	 l-p:0.11825253814458847
epoch£º802	 i:8 	 global-step:16048	 l-p:0.13139304518699646
epoch£º802	 i:9 	 global-step:16049	 l-p:0.13031762838363647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[3.0899, 1.8006, 1.3038],
        [3.0899, 1.9321, 1.6332],
        [3.0899, 2.0767, 1.9738],
        [3.0899, 1.9456, 1.6665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.562901496887207 
model_pd.l_d.mean(): -25.189416885375977 
model_pd.lagr.mean(): -24.626514434814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0053], device='cuda:0')), ('power', tensor([-25.1841], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.562901496887207
epoch£º803	 i:1 	 global-step:16061	 l-p:0.12253531813621521
epoch£º803	 i:2 	 global-step:16062	 l-p:0.329986035823822
epoch£º803	 i:3 	 global-step:16063	 l-p:0.13950224220752716
epoch£º803	 i:4 	 global-step:16064	 l-p:0.17342884838581085
epoch£º803	 i:5 	 global-step:16065	 l-p:0.14232073724269867
epoch£º803	 i:6 	 global-step:16066	 l-p:0.16465333104133606
epoch£º803	 i:7 	 global-step:16067	 l-p:0.1396523416042328
epoch£º803	 i:8 	 global-step:16068	 l-p:0.1444680541753769
epoch£º803	 i:9 	 global-step:16069	 l-p:-0.21521466970443726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0725, 3.0642, 3.0722],
        [3.0725, 2.9607, 3.0513],
        [3.0725, 2.0240, 1.8790],
        [3.0725, 1.7728, 1.1859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.1622087061405182 
model_pd.l_d.mean(): -24.81368637084961 
model_pd.lagr.mean(): -24.651477813720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1123], device='cuda:0')), ('power', tensor([-24.9260], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.1622087061405182
epoch£º804	 i:1 	 global-step:16081	 l-p:0.1411847323179245
epoch£º804	 i:2 	 global-step:16082	 l-p:0.1976279616355896
epoch£º804	 i:3 	 global-step:16083	 l-p:0.13697375357151031
epoch£º804	 i:4 	 global-step:16084	 l-p:-0.041521213948726654
epoch£º804	 i:5 	 global-step:16085	 l-p:-6.759399890899658
epoch£º804	 i:6 	 global-step:16086	 l-p:0.1436125487089157
epoch£º804	 i:7 	 global-step:16087	 l-p:-1.1132131814956665
epoch£º804	 i:8 	 global-step:16088	 l-p:0.15284055471420288
epoch£º804	 i:9 	 global-step:16089	 l-p:0.14047177135944366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0918, 3.0918, 3.0918],
        [3.0918, 3.0918, 3.0918],
        [3.0918, 2.0595, 1.3948],
        [3.0918, 2.6948, 2.8974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.11713625490665436 
model_pd.l_d.mean(): -24.674863815307617 
model_pd.lagr.mean(): -24.557727813720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0646], device='cuda:0')), ('power', tensor([-24.7394], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.11713625490665436
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12865979969501495
epoch£º805	 i:2 	 global-step:16102	 l-p:0.18515436351299286
epoch£º805	 i:3 	 global-step:16103	 l-p:0.41689857840538025
epoch£º805	 i:4 	 global-step:16104	 l-p:0.2452443689107895
epoch£º805	 i:5 	 global-step:16105	 l-p:0.2101811021566391
epoch£º805	 i:6 	 global-step:16106	 l-p:0.14261363446712494
epoch£º805	 i:7 	 global-step:16107	 l-p:0.1334138810634613
epoch£º805	 i:8 	 global-step:16108	 l-p:0.22827574610710144
epoch£º805	 i:9 	 global-step:16109	 l-p:0.1230771392583847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1135, 3.1134, 3.1136],
        [3.1135, 3.1116, 3.1135],
        [3.1135, 3.1136, 3.1135],
        [3.1135, 3.1058, 3.1133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.19053788483142853 
model_pd.l_d.mean(): -25.0176944732666 
model_pd.lagr.mean(): -24.82715606689453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0006], device='cuda:0')), ('power', tensor([-25.0183], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.19053788483142853
epoch£º806	 i:1 	 global-step:16121	 l-p:0.14533136785030365
epoch£º806	 i:2 	 global-step:16122	 l-p:0.18178333342075348
epoch£º806	 i:3 	 global-step:16123	 l-p:0.14175793528556824
epoch£º806	 i:4 	 global-step:16124	 l-p:0.08255049586296082
epoch£º806	 i:5 	 global-step:16125	 l-p:0.13174055516719818
epoch£º806	 i:6 	 global-step:16126	 l-p:0.25144991278648376
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12241658568382263
epoch£º806	 i:8 	 global-step:16128	 l-p:0.14035336673259735
epoch£º806	 i:9 	 global-step:16129	 l-p:0.12245666235685349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1140, 3.1121, 3.1140],
        [3.1140, 3.1125, 3.1140],
        [3.1140, 3.1100, 3.1139],
        [3.1140, 3.1140, 3.1140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.21594180166721344 
model_pd.l_d.mean(): -25.201831817626953 
model_pd.lagr.mean(): -24.985889434814453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0954], device='cuda:0')), ('power', tensor([-25.1065], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.21594180166721344
epoch£º807	 i:1 	 global-step:16141	 l-p:0.13400594890117645
epoch£º807	 i:2 	 global-step:16142	 l-p:0.3320181965827942
epoch£º807	 i:3 	 global-step:16143	 l-p:0.13666705787181854
epoch£º807	 i:4 	 global-step:16144	 l-p:0.09554348886013031
epoch£º807	 i:5 	 global-step:16145	 l-p:0.1426883488893509
epoch£º807	 i:6 	 global-step:16146	 l-p:0.16046668589115143
epoch£º807	 i:7 	 global-step:16147	 l-p:0.13789191842079163
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12002866715192795
epoch£º807	 i:9 	 global-step:16149	 l-p:0.16098624467849731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0764, 3.0597, 3.0755],
        [3.0764, 3.0355, 3.0725],
        [3.0764, 3.0764, 3.0764],
        [3.0764, 3.0635, 3.0758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.14178591966629028 
model_pd.l_d.mean(): -24.870296478271484 
model_pd.lagr.mean(): -24.7285099029541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0279], device='cuda:0')), ('power', tensor([-24.8424], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.14178591966629028
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12620089948177338
epoch£º808	 i:2 	 global-step:16162	 l-p:0.12130584567785263
epoch£º808	 i:3 	 global-step:16163	 l-p:0.15209068357944489
epoch£º808	 i:4 	 global-step:16164	 l-p:0.5272936820983887
epoch£º808	 i:5 	 global-step:16165	 l-p:0.1341271996498108
epoch£º808	 i:6 	 global-step:16166	 l-p:0.1558181494474411
epoch£º808	 i:7 	 global-step:16167	 l-p:0.08601053059101105
epoch£º808	 i:8 	 global-step:16168	 l-p:0.14199748635292053
epoch£º808	 i:9 	 global-step:16169	 l-p:0.10451764613389969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0052, 3.0052, 3.0052],
        [3.0052, 3.0037, 3.0051],
        [3.0052, 2.8082, 2.9493],
        [3.0052, 3.0048, 3.0052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.03027218207716942 
model_pd.l_d.mean(): -24.675920486450195 
model_pd.lagr.mean(): -24.645648956298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1657], device='cuda:0')), ('power', tensor([-24.8416], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.03027218207716942
epoch£º809	 i:1 	 global-step:16181	 l-p:0.4353550374507904
epoch£º809	 i:2 	 global-step:16182	 l-p:0.13418987393379211
epoch£º809	 i:3 	 global-step:16183	 l-p:0.004538402426987886
epoch£º809	 i:4 	 global-step:16184	 l-p:0.060316961258649826
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12789733707904816
epoch£º809	 i:6 	 global-step:16186	 l-p:0.1498430222272873
epoch£º809	 i:7 	 global-step:16187	 l-p:0.16056261956691742
epoch£º809	 i:8 	 global-step:16188	 l-p:0.13210006058216095
epoch£º809	 i:9 	 global-step:16189	 l-p:0.14292961359024048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0308, 2.3251, 2.4758],
        [3.0308, 1.7939, 1.3905],
        [3.0308, 2.1270, 1.4488],
        [3.0308, 3.0246, 3.0307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.1618380844593048 
model_pd.l_d.mean(): -24.870954513549805 
model_pd.lagr.mean(): -24.709115982055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0271], device='cuda:0')), ('power', tensor([-24.8980], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.1618380844593048
epoch£º810	 i:1 	 global-step:16201	 l-p:0.16236907243728638
epoch£º810	 i:2 	 global-step:16202	 l-p:-0.027145158499479294
epoch£º810	 i:3 	 global-step:16203	 l-p:0.08604925870895386
epoch£º810	 i:4 	 global-step:16204	 l-p:0.3169417381286621
epoch£º810	 i:5 	 global-step:16205	 l-p:0.19568374752998352
epoch£º810	 i:6 	 global-step:16206	 l-p:0.04289476200938225
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11545553803443909
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12846441566944122
epoch£º810	 i:9 	 global-step:16209	 l-p:-0.8014925718307495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0857, 1.9402, 1.6612],
        [3.0857, 1.7779, 1.1984],
        [3.0857, 2.7875, 2.9694],
        [3.0857, 2.5982, 2.8039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.6535289883613586 
model_pd.l_d.mean(): -24.76821517944336 
model_pd.lagr.mean(): -24.114686965942383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1450], device='cuda:0')), ('power', tensor([-24.9132], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.6535289883613586
epoch£º811	 i:1 	 global-step:16221	 l-p:0.1520499885082245
epoch£º811	 i:2 	 global-step:16222	 l-p:0.3251604735851288
epoch£º811	 i:3 	 global-step:16223	 l-p:0.1254752278327942
epoch£º811	 i:4 	 global-step:16224	 l-p:0.11101749539375305
epoch£º811	 i:5 	 global-step:16225	 l-p:0.14165304601192474
epoch£º811	 i:6 	 global-step:16226	 l-p:0.12543295323848724
epoch£º811	 i:7 	 global-step:16227	 l-p:0.17521601915359497
epoch£º811	 i:8 	 global-step:16228	 l-p:0.14195601642131805
epoch£º811	 i:9 	 global-step:16229	 l-p:0.11900689452886581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1302, 3.1110, 3.1291],
        [3.1302, 3.0616, 3.1209],
        [3.1302, 2.0142, 1.3569],
        [3.1302, 3.0493, 3.1179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.17269311845302582 
model_pd.l_d.mean(): -25.16066551208496 
model_pd.lagr.mean(): -24.987972259521484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0121], device='cuda:0')), ('power', tensor([-25.1485], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.17269311845302582
epoch£º812	 i:1 	 global-step:16241	 l-p:0.16588622331619263
epoch£º812	 i:2 	 global-step:16242	 l-p:0.15408587455749512
epoch£º812	 i:3 	 global-step:16243	 l-p:0.16438232362270355
epoch£º812	 i:4 	 global-step:16244	 l-p:0.14218153059482574
epoch£º812	 i:5 	 global-step:16245	 l-p:0.12847624719142914
epoch£º812	 i:6 	 global-step:16246	 l-p:0.10429015010595322
epoch£º812	 i:7 	 global-step:16247	 l-p:0.1267007738351822
epoch£º812	 i:8 	 global-step:16248	 l-p:0.08606574684381485
epoch£º812	 i:9 	 global-step:16249	 l-p:0.11822382360696793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1390, 2.7793, 2.9759],
        [3.1390, 2.1935, 2.1630],
        [3.1390, 2.2614, 2.2946],
        [3.1390, 3.1390, 3.1390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.13608670234680176 
model_pd.l_d.mean(): -24.846899032592773 
model_pd.lagr.mean(): -24.710811614990234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0143], device='cuda:0')), ('power', tensor([-24.8612], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.13608670234680176
epoch£º813	 i:1 	 global-step:16261	 l-p:0.15522150695323944
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12372376769781113
epoch£º813	 i:3 	 global-step:16263	 l-p:0.2053912729024887
epoch£º813	 i:4 	 global-step:16264	 l-p:0.15605208277702332
epoch£º813	 i:5 	 global-step:16265	 l-p:0.2236032485961914
epoch£º813	 i:6 	 global-step:16266	 l-p:0.17046678066253662
epoch£º813	 i:7 	 global-step:16267	 l-p:0.1388522833585739
epoch£º813	 i:8 	 global-step:16268	 l-p:-0.13849735260009766
epoch£º813	 i:9 	 global-step:16269	 l-p:0.2037670612335205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0773, 3.0773, 3.0773],
        [3.0773, 3.0773, 3.0773],
        [3.0773, 2.9353, 3.0455],
        [3.0773, 3.0772, 3.0773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.13563421368598938 
model_pd.l_d.mean(): -25.113964080810547 
model_pd.lagr.mean(): -24.978330612182617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0884], device='cuda:0')), ('power', tensor([-25.0255], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.13563421368598938
epoch£º814	 i:1 	 global-step:16281	 l-p:4.476860523223877
epoch£º814	 i:2 	 global-step:16282	 l-p:0.1810784935951233
epoch£º814	 i:3 	 global-step:16283	 l-p:-0.32322025299072266
epoch£º814	 i:4 	 global-step:16284	 l-p:0.1786218285560608
epoch£º814	 i:5 	 global-step:16285	 l-p:0.1532406210899353
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13622041046619415
epoch£º814	 i:7 	 global-step:16287	 l-p:0.18072178959846497
epoch£º814	 i:8 	 global-step:16288	 l-p:0.11817888915538788
epoch£º814	 i:9 	 global-step:16289	 l-p:0.14500950276851654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1075, 3.1075, 3.1075],
        [3.1075, 1.9920, 1.3385],
        [3.1075, 2.8295, 3.0047],
        [3.1075, 2.7608, 2.9551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.18853545188903809 
model_pd.l_d.mean(): -24.57587432861328 
model_pd.lagr.mean(): -24.387338638305664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1436], device='cuda:0')), ('power', tensor([-24.7194], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.18853545188903809
epoch£º815	 i:1 	 global-step:16301	 l-p:0.1388467252254486
epoch£º815	 i:2 	 global-step:16302	 l-p:0.13165824115276337
epoch£º815	 i:3 	 global-step:16303	 l-p:0.14796611666679382
epoch£º815	 i:4 	 global-step:16304	 l-p:0.29391154646873474
epoch£º815	 i:5 	 global-step:16305	 l-p:0.126265287399292
epoch£º815	 i:6 	 global-step:16306	 l-p:-68.67330932617188
epoch£º815	 i:7 	 global-step:16307	 l-p:0.12594163417816162
epoch£º815	 i:8 	 global-step:16308	 l-p:0.14558039605617523
epoch£º815	 i:9 	 global-step:16309	 l-p:-0.35568496584892273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0668, 2.7967, 2.9693],
        [3.0668, 2.5667, 2.7721],
        [3.0668, 3.0417, 3.0650],
        [3.0668, 2.9247, 3.0350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.1369062066078186 
model_pd.l_d.mean(): -25.166343688964844 
model_pd.lagr.mean(): -25.029438018798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1137], device='cuda:0')), ('power', tensor([-25.0527], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.1369062066078186
epoch£º816	 i:1 	 global-step:16321	 l-p:0.11478393524885178
epoch£º816	 i:2 	 global-step:16322	 l-p:0.13199329376220703
epoch£º816	 i:3 	 global-step:16323	 l-p:-0.7618672847747803
epoch£º816	 i:4 	 global-step:16324	 l-p:0.15512840449810028
epoch£º816	 i:5 	 global-step:16325	 l-p:0.4154670536518097
epoch£º816	 i:6 	 global-step:16326	 l-p:0.13409699499607086
epoch£º816	 i:7 	 global-step:16327	 l-p:0.1593737155199051
epoch£º816	 i:8 	 global-step:16328	 l-p:0.12651652097702026
epoch£º816	 i:9 	 global-step:16329	 l-p:0.08941762894392014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0472, 1.9426, 1.2977],
        [3.0472, 3.0468, 3.0472],
        [3.0472, 3.0438, 3.0471],
        [3.0472, 1.8144, 1.4168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.04728112742304802 
model_pd.l_d.mean(): -24.368566513061523 
model_pd.lagr.mean(): -24.321285247802734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1561], device='cuda:0')), ('power', tensor([-24.5247], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.04728112742304802
epoch£º817	 i:1 	 global-step:16341	 l-p:0.3312035799026489
epoch£º817	 i:2 	 global-step:16342	 l-p:0.16763988137245178
epoch£º817	 i:3 	 global-step:16343	 l-p:0.14341837167739868
epoch£º817	 i:4 	 global-step:16344	 l-p:1.9489927291870117
epoch£º817	 i:5 	 global-step:16345	 l-p:0.19966529309749603
epoch£º817	 i:6 	 global-step:16346	 l-p:0.13134454190731049
epoch£º817	 i:7 	 global-step:16347	 l-p:0.15204541385173798
epoch£º817	 i:8 	 global-step:16348	 l-p:0.10247518122196198
epoch£º817	 i:9 	 global-step:16349	 l-p:0.16873271763324738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1310, 2.0828, 1.4140],
        [3.1310, 2.0055, 1.3494],
        [3.1310, 3.1310, 3.1310],
        [3.1310, 2.8959, 3.0545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.16046565771102905 
model_pd.l_d.mean(): -24.618736267089844 
model_pd.lagr.mean(): -24.458271026611328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1465], device='cuda:0')), ('power', tensor([-24.7652], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.16046565771102905
epoch£º818	 i:1 	 global-step:16361	 l-p:0.13754552602767944
epoch£º818	 i:2 	 global-step:16362	 l-p:0.15195919573307037
epoch£º818	 i:3 	 global-step:16363	 l-p:0.10426434129476547
epoch£º818	 i:4 	 global-step:16364	 l-p:0.04605189338326454
epoch£º818	 i:5 	 global-step:16365	 l-p:0.13664127886295319
epoch£º818	 i:6 	 global-step:16366	 l-p:0.2021075040102005
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12031427025794983
epoch£º818	 i:8 	 global-step:16368	 l-p:0.12243905663490295
epoch£º818	 i:9 	 global-step:16369	 l-p:0.13641639053821564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1710, 2.7753, 2.9771],
        [3.1710, 1.9041, 1.4377],
        [3.1710, 3.1563, 3.1702],
        [3.1710, 1.9688, 1.3193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.1561574637889862 
model_pd.l_d.mean(): -25.214628219604492 
model_pd.lagr.mean(): -25.0584716796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0521], device='cuda:0')), ('power', tensor([-25.1626], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.1561574637889862
epoch£º819	 i:1 	 global-step:16381	 l-p:0.14827784895896912
epoch£º819	 i:2 	 global-step:16382	 l-p:0.14458699524402618
epoch£º819	 i:3 	 global-step:16383	 l-p:0.1532539278268814
epoch£º819	 i:4 	 global-step:16384	 l-p:0.1709640473127365
epoch£º819	 i:5 	 global-step:16385	 l-p:0.11104220896959305
epoch£º819	 i:6 	 global-step:16386	 l-p:0.051808957010507584
epoch£º819	 i:7 	 global-step:16387	 l-p:0.13267064094543457
epoch£º819	 i:8 	 global-step:16388	 l-p:0.13291257619857788
epoch£º819	 i:9 	 global-step:16389	 l-p:0.1871069073677063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1213, 1.8256, 1.3170],
        [3.1213, 3.1173, 3.1212],
        [3.1213, 1.9138, 1.5422],
        [3.1213, 3.1213, 3.1213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.14980514347553253 
model_pd.l_d.mean(): -24.731952667236328 
model_pd.lagr.mean(): -24.5821475982666 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0288], device='cuda:0')), ('power', tensor([-24.7608], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.14980514347553253
epoch£º820	 i:1 	 global-step:16401	 l-p:0.17749473452568054
epoch£º820	 i:2 	 global-step:16402	 l-p:0.14691492915153503
epoch£º820	 i:3 	 global-step:16403	 l-p:0.2441544234752655
epoch£º820	 i:4 	 global-step:16404	 l-p:0.12345278263092041
epoch£º820	 i:5 	 global-step:16405	 l-p:0.14683009684085846
epoch£º820	 i:6 	 global-step:16406	 l-p:0.13955722749233246
epoch£º820	 i:7 	 global-step:16407	 l-p:0.1268213391304016
epoch£º820	 i:8 	 global-step:16408	 l-p:0.14390286803245544
epoch£º820	 i:9 	 global-step:16409	 l-p:0.384956419467926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0880, 2.5507, 2.7517],
        [3.0880, 2.8095, 2.9850],
        [3.0880, 2.5024, 2.6940],
        [3.0880, 1.9721, 1.7377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.16695666313171387 
model_pd.l_d.mean(): -25.26743507385254 
model_pd.lagr.mean(): -25.100479125976562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0853], device='cuda:0')), ('power', tensor([-25.1822], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.16695666313171387
epoch£º821	 i:1 	 global-step:16421	 l-p:0.7718580961227417
epoch£º821	 i:2 	 global-step:16422	 l-p:0.31876513361930847
epoch£º821	 i:3 	 global-step:16423	 l-p:0.13931365311145782
epoch£º821	 i:4 	 global-step:16424	 l-p:0.14037254452705383
epoch£º821	 i:5 	 global-step:16425	 l-p:1.2019513845443726
epoch£º821	 i:6 	 global-step:16426	 l-p:0.15707853436470032
epoch£º821	 i:7 	 global-step:16427	 l-p:0.1842033416032791
epoch£º821	 i:8 	 global-step:16428	 l-p:0.105045847594738
epoch£º821	 i:9 	 global-step:16429	 l-p:0.1674915850162506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0975, 3.0744, 3.0960],
        [3.0975, 1.8246, 1.2110],
        [3.0975, 1.7999, 1.2013],
        [3.0975, 3.0965, 3.0975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.10705342143774033 
model_pd.l_d.mean(): -24.901243209838867 
model_pd.lagr.mean(): -24.794189453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0069], device='cuda:0')), ('power', tensor([-24.9082], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.10705342143774033
epoch£º822	 i:1 	 global-step:16441	 l-p:0.16261567175388336
epoch£º822	 i:2 	 global-step:16442	 l-p:0.13566115498542786
epoch£º822	 i:3 	 global-step:16443	 l-p:0.14316533505916595
epoch£º822	 i:4 	 global-step:16444	 l-p:0.23684407770633698
epoch£º822	 i:5 	 global-step:16445	 l-p:0.1246563121676445
epoch£º822	 i:6 	 global-step:16446	 l-p:0.18440566956996918
epoch£º822	 i:7 	 global-step:16447	 l-p:0.15133662521839142
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13383139669895172
epoch£º822	 i:9 	 global-step:16449	 l-p:0.1349828839302063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1460, 2.8633, 3.0400],
        [3.1460, 3.1460, 3.1460],
        [3.1460, 2.2686, 2.3025],
        [3.1460, 2.1194, 1.4448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.15526671707630157 
model_pd.l_d.mean(): -24.98194694519043 
model_pd.lagr.mean(): -24.82668113708496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1151], device='cuda:0')), ('power', tensor([-25.0970], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.15526671707630157
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14132970571517944
epoch£º823	 i:2 	 global-step:16462	 l-p:0.1669892817735672
epoch£º823	 i:3 	 global-step:16463	 l-p:0.1404470056295395
epoch£º823	 i:4 	 global-step:16464	 l-p:0.16383083164691925
epoch£º823	 i:5 	 global-step:16465	 l-p:0.15142212808132172
epoch£º823	 i:6 	 global-step:16466	 l-p:0.1910620480775833
epoch£º823	 i:7 	 global-step:16467	 l-p:0.13488225638866425
epoch£º823	 i:8 	 global-step:16468	 l-p:0.09719143062829971
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11633660644292831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1203, 1.9646, 1.6695],
        [3.1203, 3.0392, 3.1080],
        [3.1203, 2.7697, 2.9649],
        [3.1203, 3.1202, 3.1203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.1538788229227066 
model_pd.l_d.mean(): -24.97718048095703 
model_pd.lagr.mean(): -24.823301315307617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0128], device='cuda:0')), ('power', tensor([-24.9900], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.1538788229227066
epoch£º824	 i:1 	 global-step:16481	 l-p:0.19544480741024017
epoch£º824	 i:2 	 global-step:16482	 l-p:0.19483844935894012
epoch£º824	 i:3 	 global-step:16483	 l-p:0.11849498748779297
epoch£º824	 i:4 	 global-step:16484	 l-p:0.09464084357023239
epoch£º824	 i:5 	 global-step:16485	 l-p:0.19074653089046478
epoch£º824	 i:6 	 global-step:16486	 l-p:0.129189595580101
epoch£º824	 i:7 	 global-step:16487	 l-p:0.1339835673570633
epoch£º824	 i:8 	 global-step:16488	 l-p:0.13011287152767181
epoch£º824	 i:9 	 global-step:16489	 l-p:0.15573269128799438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1308, 2.1560, 1.4756],
        [3.1308, 3.1296, 3.1308],
        [3.1308, 3.1308, 3.1308],
        [3.1308, 3.0011, 3.1035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.13846801221370697 
model_pd.l_d.mean(): -24.75428009033203 
model_pd.lagr.mean(): -24.615812301635742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0152], device='cuda:0')), ('power', tensor([-24.7695], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.13846801221370697
epoch£º825	 i:1 	 global-step:16501	 l-p:0.11275911331176758
epoch£º825	 i:2 	 global-step:16502	 l-p:0.13272754848003387
epoch£º825	 i:3 	 global-step:16503	 l-p:0.2864392399787903
epoch£º825	 i:4 	 global-step:16504	 l-p:0.1551801860332489
epoch£º825	 i:5 	 global-step:16505	 l-p:0.11934791505336761
epoch£º825	 i:6 	 global-step:16506	 l-p:0.0868767723441124
epoch£º825	 i:7 	 global-step:16507	 l-p:0.3296624422073364
epoch£º825	 i:8 	 global-step:16508	 l-p:0.2101396918296814
epoch£º825	 i:9 	 global-step:16509	 l-p:0.17209331691265106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0918, 3.0807, 3.0913],
        [3.0918, 3.0610, 3.0894],
        [3.0918, 3.0835, 3.0915],
        [3.0918, 2.0514, 1.9184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.12674565613269806 
model_pd.l_d.mean(): -25.14314842224121 
model_pd.lagr.mean(): -25.016403198242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0498], device='cuda:0')), ('power', tensor([-25.0933], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.12674565613269806
epoch£º826	 i:1 	 global-step:16521	 l-p:0.2937285304069519
epoch£º826	 i:2 	 global-step:16522	 l-p:0.13014139235019684
epoch£º826	 i:3 	 global-step:16523	 l-p:0.12668880820274353
epoch£º826	 i:4 	 global-step:16524	 l-p:0.32064417004585266
epoch£º826	 i:5 	 global-step:16525	 l-p:0.12512829899787903
epoch£º826	 i:6 	 global-step:16526	 l-p:0.16324488818645477
epoch£º826	 i:7 	 global-step:16527	 l-p:0.13643182814121246
epoch£º826	 i:8 	 global-step:16528	 l-p:0.1391499638557434
epoch£º826	 i:9 	 global-step:16529	 l-p:0.20399194955825806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1149, 3.1125, 3.1149],
        [3.1149, 2.9844, 3.0874],
        [3.1149, 3.1149, 3.1149],
        [3.1149, 2.5852, 2.7869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.12229325622320175 
model_pd.l_d.mean(): -24.972753524780273 
model_pd.lagr.mean(): -24.850460052490234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0475], device='cuda:0')), ('power', tensor([-24.9252], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.12229325622320175
epoch£º827	 i:1 	 global-step:16541	 l-p:0.20793037116527557
epoch£º827	 i:2 	 global-step:16542	 l-p:0.22153876721858978
epoch£º827	 i:3 	 global-step:16543	 l-p:0.14714254438877106
epoch£º827	 i:4 	 global-step:16544	 l-p:0.14541195333003998
epoch£º827	 i:5 	 global-step:16545	 l-p:0.22403746843338013
epoch£º827	 i:6 	 global-step:16546	 l-p:0.14481021463871002
epoch£º827	 i:7 	 global-step:16547	 l-p:0.12459998577833176
epoch£º827	 i:8 	 global-step:16548	 l-p:0.09778021275997162
epoch£º827	 i:9 	 global-step:16549	 l-p:0.1161651462316513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1078, 2.7247, 2.9257],
        [3.1078, 2.3891, 2.5316],
        [3.1078, 2.9628, 3.0748],
        [3.1078, 2.9830, 3.0823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.11681447923183441 
model_pd.l_d.mean(): -25.018360137939453 
model_pd.lagr.mean(): -24.901546478271484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0489], device='cuda:0')), ('power', tensor([-24.9695], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.11681447923183441
epoch£º828	 i:1 	 global-step:16561	 l-p:0.15196220576763153
epoch£º828	 i:2 	 global-step:16562	 l-p:0.34559330344200134
epoch£º828	 i:3 	 global-step:16563	 l-p:0.18745793402194977
epoch£º828	 i:4 	 global-step:16564	 l-p:0.14645323157310486
epoch£º828	 i:5 	 global-step:16565	 l-p:0.15397344529628754
epoch£º828	 i:6 	 global-step:16566	 l-p:0.13936355710029602
epoch£º828	 i:7 	 global-step:16567	 l-p:0.14319072663784027
epoch£º828	 i:8 	 global-step:16568	 l-p:0.07881021499633789
epoch£º828	 i:9 	 global-step:16569	 l-p:0.06961147487163544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0439, 2.7953, 2.9599],
        [3.0439, 1.7625, 1.1660],
        [3.0439, 3.0434, 3.0439],
        [3.0439, 3.0420, 3.0439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.227238267660141 
model_pd.l_d.mean(): -24.743980407714844 
model_pd.lagr.mean(): -24.516742706298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0573], device='cuda:0')), ('power', tensor([-24.8013], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.227238267660141
epoch£º829	 i:1 	 global-step:16581	 l-p:0.07846477627754211
epoch£º829	 i:2 	 global-step:16582	 l-p:0.146180659532547
epoch£º829	 i:3 	 global-step:16583	 l-p:0.12606357038021088
epoch£º829	 i:4 	 global-step:16584	 l-p:0.14114533364772797
epoch£º829	 i:5 	 global-step:16585	 l-p:0.0539219044148922
epoch£º829	 i:6 	 global-step:16586	 l-p:0.17003419995307922
epoch£º829	 i:7 	 global-step:16587	 l-p:0.15465852618217468
epoch£º829	 i:8 	 global-step:16588	 l-p:0.1357429027557373
epoch£º829	 i:9 	 global-step:16589	 l-p:0.17764128744602203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0852, 3.0833, 3.0852],
        [3.0852, 2.2136, 2.2565],
        [3.0852, 1.9550, 1.7012],
        [3.0852, 3.0852, 3.0852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.13861754536628723 
model_pd.l_d.mean(): -25.129962921142578 
model_pd.lagr.mean(): -24.991344451904297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0182], device='cuda:0')), ('power', tensor([-25.1117], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.13861754536628723
epoch£º830	 i:1 	 global-step:16601	 l-p:0.1708577275276184
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13685330748558044
epoch£º830	 i:3 	 global-step:16603	 l-p:0.568791925907135
epoch£º830	 i:4 	 global-step:16604	 l-p:0.25244396924972534
epoch£º830	 i:5 	 global-step:16605	 l-p:0.1330597996711731
epoch£º830	 i:6 	 global-step:16606	 l-p:0.13514606654644012
epoch£º830	 i:7 	 global-step:16607	 l-p:0.13389453291893005
epoch£º830	 i:8 	 global-step:16608	 l-p:0.28013095259666443
epoch£º830	 i:9 	 global-step:16609	 l-p:0.08927244693040848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1052, 3.1051, 3.1052],
        [3.1052, 3.0969, 3.1049],
        [3.1052, 3.0598, 3.1005],
        [3.1052, 3.1052, 3.1052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13554824888706207 
model_pd.l_d.mean(): -24.904834747314453 
model_pd.lagr.mean(): -24.769287109375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0068], device='cuda:0')), ('power', tensor([-24.9116], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13554824888706207
epoch£º831	 i:1 	 global-step:16621	 l-p:0.1521947681903839
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13645240664482117
epoch£º831	 i:3 	 global-step:16623	 l-p:0.16662123799324036
epoch£º831	 i:4 	 global-step:16624	 l-p:0.13302215933799744
epoch£º831	 i:5 	 global-step:16625	 l-p:0.15314175188541412
epoch£º831	 i:6 	 global-step:16626	 l-p:0.14623133838176727
epoch£º831	 i:7 	 global-step:16627	 l-p:0.19595551490783691
epoch£º831	 i:8 	 global-step:16628	 l-p:0.17601540684700012
epoch£º831	 i:9 	 global-step:16629	 l-p:0.12323141098022461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1469, 3.1341, 3.1463],
        [3.1469, 2.2691, 2.3035],
        [3.1469, 1.8326, 1.2392],
        [3.1469, 2.2686, 2.3025]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.1369781345129013 
model_pd.l_d.mean(): -24.942365646362305 
model_pd.lagr.mean(): -24.805387496948242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0306], device='cuda:0')), ('power', tensor([-24.9118], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.1369781345129013
epoch£º832	 i:1 	 global-step:16641	 l-p:0.17317809164524078
epoch£º832	 i:2 	 global-step:16642	 l-p:0.17226821184158325
epoch£º832	 i:3 	 global-step:16643	 l-p:-0.026300562545657158
epoch£º832	 i:4 	 global-step:16644	 l-p:0.12509985268115997
epoch£º832	 i:5 	 global-step:16645	 l-p:0.12430846691131592
epoch£º832	 i:6 	 global-step:16646	 l-p:0.15036249160766602
epoch£º832	 i:7 	 global-step:16647	 l-p:0.1222783625125885
epoch£º832	 i:8 	 global-step:16648	 l-p:0.11426147073507309
epoch£º832	 i:9 	 global-step:16649	 l-p:0.16608434915542603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1608, 3.1608, 3.1608],
        [3.1608, 3.1392, 3.1594],
        [3.1608, 2.7883, 2.9872],
        [3.1608, 3.1492, 3.1603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.12222001701593399 
model_pd.l_d.mean(): -24.816377639770508 
model_pd.lagr.mean(): -24.69415855407715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0770], device='cuda:0')), ('power', tensor([-24.8934], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.12222001701593399
epoch£º833	 i:1 	 global-step:16661	 l-p:0.12113092094659805
epoch£º833	 i:2 	 global-step:16662	 l-p:0.058075837790966034
epoch£º833	 i:3 	 global-step:16663	 l-p:0.17224270105361938
epoch£º833	 i:4 	 global-step:16664	 l-p:0.14934760332107544
epoch£º833	 i:5 	 global-step:16665	 l-p:0.14751222729682922
epoch£º833	 i:6 	 global-step:16666	 l-p:0.14748556911945343
epoch£º833	 i:7 	 global-step:16667	 l-p:0.1511959433555603
epoch£º833	 i:8 	 global-step:16668	 l-p:0.1577548384666443
epoch£º833	 i:9 	 global-step:16669	 l-p:0.14727190136909485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1295, 3.1295, 3.1295],
        [3.1295, 2.1796, 1.4953],
        [3.1295, 3.1294, 3.1295],
        [3.1295, 2.5928, 2.7933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.1103803962469101 
model_pd.l_d.mean(): -24.77461814880371 
model_pd.lagr.mean(): -24.66423797607422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1322], device='cuda:0')), ('power', tensor([-24.9068], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.1103803962469101
epoch£º834	 i:1 	 global-step:16681	 l-p:0.11747872829437256
epoch£º834	 i:2 	 global-step:16682	 l-p:0.11119901388883591
epoch£º834	 i:3 	 global-step:16683	 l-p:0.1268836408853531
epoch£º834	 i:4 	 global-step:16684	 l-p:0.16885603964328766
epoch£º834	 i:5 	 global-step:16685	 l-p:0.02073131501674652
epoch£º834	 i:6 	 global-step:16686	 l-p:0.18107175827026367
epoch£º834	 i:7 	 global-step:16687	 l-p:0.49379706382751465
epoch£º834	 i:8 	 global-step:16688	 l-p:0.14782646298408508
epoch£º834	 i:9 	 global-step:16689	 l-p:0.08784280717372894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0504, 3.0500, 3.0504],
        [3.0504, 2.6390, 2.8442],
        [3.0504, 1.8279, 1.4475],
        [3.0504, 1.7396, 1.1884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.5678790807723999 
model_pd.l_d.mean(): -24.854555130004883 
model_pd.lagr.mean(): -24.28667640686035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0509], device='cuda:0')), ('power', tensor([-24.9054], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.5678790807723999
epoch£º835	 i:1 	 global-step:16701	 l-p:0.1337164044380188
epoch£º835	 i:2 	 global-step:16702	 l-p:0.015985621139407158
epoch£º835	 i:3 	 global-step:16703	 l-p:0.17112299799919128
epoch£º835	 i:4 	 global-step:16704	 l-p:0.1405566930770874
epoch£º835	 i:5 	 global-step:16705	 l-p:0.12613947689533234
epoch£º835	 i:6 	 global-step:16706	 l-p:0.1658073514699936
epoch£º835	 i:7 	 global-step:16707	 l-p:0.14651049673557281
epoch£º835	 i:8 	 global-step:16708	 l-p:0.14167501032352448
epoch£º835	 i:9 	 global-step:16709	 l-p:-0.1647426038980484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0715, 3.0314, 3.0677],
        [3.0715, 1.8497, 1.2244],
        [3.0715, 3.0684, 3.0714],
        [3.0715, 2.7193, 2.9155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.19480910897254944 
model_pd.l_d.mean(): -25.096235275268555 
model_pd.lagr.mean(): -24.901426315307617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0180], device='cuda:0')), ('power', tensor([-25.1143], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.19480910897254944
epoch£º836	 i:1 	 global-step:16721	 l-p:0.10548708587884903
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11609002202749252
epoch£º836	 i:3 	 global-step:16723	 l-p:-0.12570254504680634
epoch£º836	 i:4 	 global-step:16724	 l-p:0.5506108999252319
epoch£º836	 i:5 	 global-step:16725	 l-p:0.12987492978572845
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12541308999061584
epoch£º836	 i:7 	 global-step:16727	 l-p:0.13741154968738556
epoch£º836	 i:8 	 global-step:16728	 l-p:0.16705076396465302
epoch£º836	 i:9 	 global-step:16729	 l-p:0.18560835719108582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1034, 2.8734, 3.0300],
        [3.1034, 3.1026, 3.1034],
        [3.1034, 1.8148, 1.2069],
        [3.1034, 2.2715, 2.3453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.1597854197025299 
model_pd.l_d.mean(): -25.29439926147461 
model_pd.lagr.mean(): -25.134613037109375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0031], device='cuda:0')), ('power', tensor([-25.2913], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.1597854197025299
epoch£º837	 i:1 	 global-step:16741	 l-p:0.10195926576852798
epoch£º837	 i:2 	 global-step:16742	 l-p:0.2664312422275543
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12395276874303818
epoch£º837	 i:4 	 global-step:16744	 l-p:0.1188720315694809
epoch£º837	 i:5 	 global-step:16745	 l-p:0.11349169164896011
epoch£º837	 i:6 	 global-step:16746	 l-p:0.215788334608078
epoch£º837	 i:7 	 global-step:16747	 l-p:0.2818923890590668
epoch£º837	 i:8 	 global-step:16748	 l-p:0.12394677102565765
epoch£º837	 i:9 	 global-step:16749	 l-p:0.12447579950094223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1092, 3.1091, 3.1092],
        [3.1092, 2.1297, 1.4527],
        [3.1092, 2.5807, 2.7830],
        [3.1092, 2.6981, 2.9027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.26667529344558716 
model_pd.l_d.mean(): -24.815441131591797 
model_pd.lagr.mean(): -24.548765182495117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1855], device='cuda:0')), ('power', tensor([-25.0010], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.26667529344558716
epoch£º838	 i:1 	 global-step:16761	 l-p:0.21324661374092102
epoch£º838	 i:2 	 global-step:16762	 l-p:0.1212209090590477
epoch£º838	 i:3 	 global-step:16763	 l-p:0.12409231811761856
epoch£º838	 i:4 	 global-step:16764	 l-p:0.17467811703681946
epoch£º838	 i:5 	 global-step:16765	 l-p:0.2833327054977417
epoch£º838	 i:6 	 global-step:16766	 l-p:0.09900690615177155
epoch£º838	 i:7 	 global-step:16767	 l-p:0.16130155324935913
epoch£º838	 i:8 	 global-step:16768	 l-p:0.1297348588705063
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11705093085765839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0864, 3.0857, 3.0864],
        [3.0864, 3.0858, 3.0864],
        [3.0864, 3.0863, 3.0864],
        [3.0864, 3.0781, 3.0861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.0975818783044815 
model_pd.l_d.mean(): -24.9747371673584 
model_pd.lagr.mean(): -24.877155303955078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0244], device='cuda:0')), ('power', tensor([-24.9991], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.0975818783044815
epoch£º839	 i:1 	 global-step:16781	 l-p:0.15367843210697174
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13893887400627136
epoch£º839	 i:3 	 global-step:16783	 l-p:0.1590328961610794
epoch£º839	 i:4 	 global-step:16784	 l-p:0.1493733823299408
epoch£º839	 i:5 	 global-step:16785	 l-p:0.24510404467582703
epoch£º839	 i:6 	 global-step:16786	 l-p:0.1194465309381485
epoch£º839	 i:7 	 global-step:16787	 l-p:0.11967049539089203
epoch£º839	 i:8 	 global-step:16788	 l-p:0.07410234212875366
epoch£º839	 i:9 	 global-step:16789	 l-p:0.2288990318775177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0520, 3.0520, 3.0520],
        [3.0520, 1.7538, 1.1667],
        [3.0520, 2.6279, 2.8343],
        [3.0520, 3.0520, 3.0520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.21125759184360504 
model_pd.l_d.mean(): -24.68100357055664 
model_pd.lagr.mean(): -24.469745635986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1505], device='cuda:0')), ('power', tensor([-24.8315], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.21125759184360504
epoch£º840	 i:1 	 global-step:16801	 l-p:-0.04568152874708176
epoch£º840	 i:2 	 global-step:16802	 l-p:0.17560172080993652
epoch£º840	 i:3 	 global-step:16803	 l-p:0.10571719706058502
epoch£º840	 i:4 	 global-step:16804	 l-p:0.16311053931713104
epoch£º840	 i:5 	 global-step:16805	 l-p:0.1397506147623062
epoch£º840	 i:6 	 global-step:16806	 l-p:0.1298770159482956
epoch£º840	 i:7 	 global-step:16807	 l-p:0.13845781981945038
epoch£º840	 i:8 	 global-step:16808	 l-p:0.19028277695178986
epoch£º840	 i:9 	 global-step:16809	 l-p:-0.15685346722602844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0781, 1.8046, 1.3445],
        [3.0781, 3.0603, 3.0771],
        [3.0781, 3.0717, 3.0779],
        [3.0781, 1.7784, 1.1849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 3.6456522941589355 
model_pd.l_d.mean(): -25.109514236450195 
model_pd.lagr.mean(): -21.4638614654541 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0522], device='cuda:0')), ('power', tensor([-25.1617], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:3.6456522941589355
epoch£º841	 i:1 	 global-step:16821	 l-p:0.1663789451122284
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12262262403964996
epoch£º841	 i:3 	 global-step:16823	 l-p:0.16433730721473694
epoch£º841	 i:4 	 global-step:16824	 l-p:0.4375206232070923
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11343875527381897
epoch£º841	 i:6 	 global-step:16826	 l-p:0.14022375643253326
epoch£º841	 i:7 	 global-step:16827	 l-p:1.163832426071167
epoch£º841	 i:8 	 global-step:16828	 l-p:0.13142530620098114
epoch£º841	 i:9 	 global-step:16829	 l-p:0.11934289336204529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0877, 2.8124, 2.9871],
        [3.0877, 1.7747, 1.2305],
        [3.0877, 1.9703, 1.3198],
        [3.0877, 3.0877, 3.0877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.1994812935590744 
model_pd.l_d.mean(): -25.232799530029297 
model_pd.lagr.mean(): -25.03331756591797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0666], device='cuda:0')), ('power', tensor([-25.1662], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:0.1994812935590744
epoch£º842	 i:1 	 global-step:16841	 l-p:0.12116731703281403
epoch£º842	 i:2 	 global-step:16842	 l-p:0.14835400879383087
epoch£º842	 i:3 	 global-step:16843	 l-p:0.24059243500232697
epoch£º842	 i:4 	 global-step:16844	 l-p:0.12940484285354614
epoch£º842	 i:5 	 global-step:16845	 l-p:0.2313401699066162
epoch£º842	 i:6 	 global-step:16846	 l-p:0.22195588052272797
epoch£º842	 i:7 	 global-step:16847	 l-p:0.1585487574338913
epoch£º842	 i:8 	 global-step:16848	 l-p:0.16138221323490143
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11271167546510696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1255, 1.8168, 1.2187],
        [3.1255, 3.1255, 3.1255],
        [3.1255, 3.0782, 3.1205],
        [3.1255, 2.0022, 1.3459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.14764447510242462 
model_pd.l_d.mean(): -24.86722183227539 
model_pd.lagr.mean(): -24.71957778930664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0224], device='cuda:0')), ('power', tensor([-24.8896], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.14764447510242462
epoch£º843	 i:1 	 global-step:16861	 l-p:0.10912084579467773
epoch£º843	 i:2 	 global-step:16862	 l-p:0.15859565138816833
epoch£º843	 i:3 	 global-step:16863	 l-p:0.13118469715118408
epoch£º843	 i:4 	 global-step:16864	 l-p:0.14542779326438904
epoch£º843	 i:5 	 global-step:16865	 l-p:0.3910970985889435
epoch£º843	 i:6 	 global-step:16866	 l-p:0.13600191473960876
epoch£º843	 i:7 	 global-step:16867	 l-p:0.17956103384494781
epoch£º843	 i:8 	 global-step:16868	 l-p:0.13553959131240845
epoch£º843	 i:9 	 global-step:16869	 l-p:0.15958330035209656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0753, 1.7649, 1.1861],
        [3.0753, 3.0752, 3.0753],
        [3.0753, 2.1366, 2.1209],
        [3.0753, 2.6070, 2.8146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): -0.03393619507551193 
model_pd.l_d.mean(): -25.10515594482422 
model_pd.lagr.mean(): -25.13909149169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0400], device='cuda:0')), ('power', tensor([-25.0652], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:-0.03393619507551193
epoch£º844	 i:1 	 global-step:16881	 l-p:0.13555315136909485
epoch£º844	 i:2 	 global-step:16882	 l-p:0.1753806620836258
epoch£º844	 i:3 	 global-step:16883	 l-p:0.30714401602745056
epoch£º844	 i:4 	 global-step:16884	 l-p:0.14252261817455292
epoch£º844	 i:5 	 global-step:16885	 l-p:0.2714683413505554
epoch£º844	 i:6 	 global-step:16886	 l-p:0.09038509428501129
epoch£º844	 i:7 	 global-step:16887	 l-p:0.14683806896209717
epoch£º844	 i:8 	 global-step:16888	 l-p:0.12339115142822266
epoch£º844	 i:9 	 global-step:16889	 l-p:0.13506263494491577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0213, 3.0212, 3.0214],
        [3.0213, 2.9663, 3.0150],
        [3.0213, 2.3778, 2.5561],
        [3.0213, 1.9493, 1.7835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.13311515748500824 
model_pd.l_d.mean(): -25.08386993408203 
model_pd.lagr.mean(): -24.950754165649414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0479], device='cuda:0')), ('power', tensor([-25.1318], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.13311515748500824
epoch£º845	 i:1 	 global-step:16901	 l-p:0.6182405948638916
epoch£º845	 i:2 	 global-step:16902	 l-p:0.1295083910226822
epoch£º845	 i:3 	 global-step:16903	 l-p:0.16672290861606598
epoch£º845	 i:4 	 global-step:16904	 l-p:0.06955435872077942
epoch£º845	 i:5 	 global-step:16905	 l-p:0.1293206512928009
epoch£º845	 i:6 	 global-step:16906	 l-p:0.07871319353580475
epoch£º845	 i:7 	 global-step:16907	 l-p:0.14213940501213074
epoch£º845	 i:8 	 global-step:16908	 l-p:0.11655356734991074
epoch£º845	 i:9 	 global-step:16909	 l-p:0.15686087310314178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0119, 2.6559, 2.8536],
        [3.0119, 2.8860, 2.9862],
        [3.0119, 2.7687, 2.9314],
        [3.0119, 3.0119, 3.0119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.11813012510538101 
model_pd.l_d.mean(): -24.762855529785156 
model_pd.lagr.mean(): -24.644725799560547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1198], device='cuda:0')), ('power', tensor([-24.8826], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.11813012510538101
epoch£º846	 i:1 	 global-step:16921	 l-p:0.14543412625789642
epoch£º846	 i:2 	 global-step:16922	 l-p:0.17271342873573303
epoch£º846	 i:3 	 global-step:16923	 l-p:0.7288001179695129
epoch£º846	 i:4 	 global-step:16924	 l-p:0.13091127574443817
epoch£º846	 i:5 	 global-step:16925	 l-p:4.059864521026611
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11870229244232178
epoch£º846	 i:7 	 global-step:16927	 l-p:0.057086169719696045
epoch£º846	 i:8 	 global-step:16928	 l-p:0.1451866179704666
epoch£º846	 i:9 	 global-step:16929	 l-p:0.13161689043045044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0728, 2.9181, 3.0361],
        [3.0728, 3.0727, 3.0728],
        [3.0728, 2.9068, 3.0313],
        [3.0728, 2.7098, 2.9084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12587308883666992 
model_pd.l_d.mean(): -25.147441864013672 
model_pd.lagr.mean(): -25.021568298339844 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0589], device='cuda:0')), ('power', tensor([-25.0886], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12587308883666992
epoch£º847	 i:1 	 global-step:16941	 l-p:0.14720913767814636
epoch£º847	 i:2 	 global-step:16942	 l-p:0.1494923233985901
epoch£º847	 i:3 	 global-step:16943	 l-p:0.23108190298080444
epoch£º847	 i:4 	 global-step:16944	 l-p:0.1413261890411377
epoch£º847	 i:5 	 global-step:16945	 l-p:0.12102365493774414
epoch£º847	 i:6 	 global-step:16946	 l-p:0.16121593117713928
epoch£º847	 i:7 	 global-step:16947	 l-p:0.12565237283706665
epoch£º847	 i:8 	 global-step:16948	 l-p:0.09652393311262131
epoch£º847	 i:9 	 global-step:16949	 l-p:0.2292531579732895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0620, 2.8249, 2.9849],
        [3.0620, 2.3025, 2.4270],
        [3.0620, 3.0607, 3.0620],
        [3.0620, 1.7543, 1.2303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.15109269320964813 
model_pd.l_d.mean(): -25.182308197021484 
model_pd.lagr.mean(): -25.03121566772461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0563], device='cuda:0')), ('power', tensor([-25.1260], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.15109269320964813
epoch£º848	 i:1 	 global-step:16961	 l-p:0.1688690334558487
epoch£º848	 i:2 	 global-step:16962	 l-p:0.12617555260658264
epoch£º848	 i:3 	 global-step:16963	 l-p:0.15218237042427063
epoch£º848	 i:4 	 global-step:16964	 l-p:0.1427820324897766
epoch£º848	 i:5 	 global-step:16965	 l-p:0.13415823876857758
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13665008544921875
epoch£º848	 i:7 	 global-step:16967	 l-p:0.1507132351398468
epoch£º848	 i:8 	 global-step:16968	 l-p:0.201795756816864
epoch£º848	 i:9 	 global-step:16969	 l-p:0.014557132497429848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0420, 2.0997, 2.0832],
        [3.0420, 1.7907, 1.1798],
        [3.0420, 2.1746, 2.2257],
        [3.0420, 2.8880, 3.0057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12328046560287476 
model_pd.l_d.mean(): -24.53632926940918 
model_pd.lagr.mean(): -24.413049697875977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1446], device='cuda:0')), ('power', tensor([-24.6809], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12328046560287476
epoch£º849	 i:1 	 global-step:16981	 l-p:0.14734713733196259
epoch£º849	 i:2 	 global-step:16982	 l-p:0.13847361505031586
epoch£º849	 i:3 	 global-step:16983	 l-p:0.10957147181034088
epoch£º849	 i:4 	 global-step:16984	 l-p:0.046194352209568024
epoch£º849	 i:5 	 global-step:16985	 l-p:0.1548478603363037
epoch£º849	 i:6 	 global-step:16986	 l-p:0.15170320868492126
epoch£º849	 i:7 	 global-step:16987	 l-p:0.8328148722648621
epoch£º849	 i:8 	 global-step:16988	 l-p:0.03348945453763008
epoch£º849	 i:9 	 global-step:16989	 l-p:0.1715063452720642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[3.0212, 2.0146, 1.3543],
        [3.0212, 1.8081, 1.4480],
        [3.0212, 2.4792, 2.6822],
        [3.0212, 2.0260, 1.3636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.09913284331560135 
model_pd.l_d.mean(): -25.063783645629883 
model_pd.lagr.mean(): -24.964651107788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0911], device='cuda:0')), ('power', tensor([-25.1549], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.09913284331560135
epoch£º850	 i:1 	 global-step:17001	 l-p:0.1429661214351654
epoch£º850	 i:2 	 global-step:17002	 l-p:0.1190733015537262
epoch£º850	 i:3 	 global-step:17003	 l-p:0.14468638598918915
epoch£º850	 i:4 	 global-step:17004	 l-p:0.4039936363697052
epoch£º850	 i:5 	 global-step:17005	 l-p:0.14103201031684875
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11592662334442139
epoch£º850	 i:7 	 global-step:17007	 l-p:0.17607629299163818
epoch£º850	 i:8 	 global-step:17008	 l-p:-1.2057353258132935
epoch£º850	 i:9 	 global-step:17009	 l-p:0.2001204788684845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0604, 3.0604, 3.0604],
        [3.0604, 3.0597, 3.0604],
        [3.0604, 2.9004, 3.0215],
        [3.0604, 1.7460, 1.1904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.1361944079399109 
model_pd.l_d.mean(): -24.80109977722168 
model_pd.lagr.mean(): -24.664905548095703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1652], device='cuda:0')), ('power', tensor([-24.9663], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.1361944079399109
epoch£º851	 i:1 	 global-step:17021	 l-p:-0.03554195165634155
epoch£º851	 i:2 	 global-step:17022	 l-p:0.16257452964782715
epoch£º851	 i:3 	 global-step:17023	 l-p:0.21158061921596527
epoch£º851	 i:4 	 global-step:17024	 l-p:0.10317224264144897
epoch£º851	 i:5 	 global-step:17025	 l-p:-0.3409975469112396
epoch£º851	 i:6 	 global-step:17026	 l-p:0.17270268499851227
epoch£º851	 i:7 	 global-step:17027	 l-p:0.46307432651519775
epoch£º851	 i:8 	 global-step:17028	 l-p:0.128802090883255
epoch£º851	 i:9 	 global-step:17029	 l-p:0.11922220140695572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0968, 2.1867, 1.4996],
        [3.0968, 1.9621, 1.7057],
        [3.0968, 2.0498, 1.9126],
        [3.0968, 3.0967, 3.0968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.4677782654762268 
model_pd.l_d.mean(): -24.795494079589844 
model_pd.lagr.mean(): -24.327714920043945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0728], device='cuda:0')), ('power', tensor([-24.8683], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.4677782654762268
epoch£º852	 i:1 	 global-step:17041	 l-p:0.225900799036026
epoch£º852	 i:2 	 global-step:17042	 l-p:0.11746905744075775
epoch£º852	 i:3 	 global-step:17043	 l-p:0.11051584780216217
epoch£º852	 i:4 	 global-step:17044	 l-p:0.16326823830604553
epoch£º852	 i:5 	 global-step:17045	 l-p:0.24314357340335846
epoch£º852	 i:6 	 global-step:17046	 l-p:0.14520105719566345
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13528496026992798
epoch£º852	 i:8 	 global-step:17048	 l-p:0.1406758725643158
epoch£º852	 i:9 	 global-step:17049	 l-p:0.1362626701593399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1215, 1.9007, 1.2639],
        [3.1215, 3.0866, 3.1185],
        [3.1215, 2.5553, 2.7518],
        [3.1215, 2.6496, 2.8566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.13034813106060028 
model_pd.l_d.mean(): -24.707073211669922 
model_pd.lagr.mean(): -24.576725006103516 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0458], device='cuda:0')), ('power', tensor([-24.7528], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.13034813106060028
epoch£º853	 i:1 	 global-step:17061	 l-p:0.12316012382507324
epoch£º853	 i:2 	 global-step:17062	 l-p:0.215539813041687
epoch£º853	 i:3 	 global-step:17063	 l-p:0.27148571610450745
epoch£º853	 i:4 	 global-step:17064	 l-p:0.13440659642219543
epoch£º853	 i:5 	 global-step:17065	 l-p:0.13564324378967285
epoch£º853	 i:6 	 global-step:17066	 l-p:-0.4919499158859253
epoch£º853	 i:7 	 global-step:17067	 l-p:0.22384516894817352
epoch£º853	 i:8 	 global-step:17068	 l-p:0.13969212770462036
epoch£º853	 i:9 	 global-step:17069	 l-p:0.16424137353897095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0798, 1.7652, 1.2201],
        [3.0798, 3.0796, 3.0798],
        [3.0798, 2.0706, 1.4019],
        [3.0798, 3.0798, 3.0798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 2.97922945022583 
model_pd.l_d.mean(): -24.96172332763672 
model_pd.lagr.mean(): -21.982494354248047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1344], device='cuda:0')), ('power', tensor([-25.0961], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:2.97922945022583
epoch£º854	 i:1 	 global-step:17081	 l-p:0.16915050148963928
epoch£º854	 i:2 	 global-step:17082	 l-p:-0.19687537848949432
epoch£º854	 i:3 	 global-step:17083	 l-p:0.14535507559776306
epoch£º854	 i:4 	 global-step:17084	 l-p:0.1264306604862213
epoch£º854	 i:5 	 global-step:17085	 l-p:0.1624031811952591
epoch£º854	 i:6 	 global-step:17086	 l-p:0.17005854845046997
epoch£º854	 i:7 	 global-step:17087	 l-p:0.1220293939113617
epoch£º854	 i:8 	 global-step:17088	 l-p:0.17710889875888824
epoch£º854	 i:9 	 global-step:17089	 l-p:1.2876195907592773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0931, 1.8028, 1.1966],
        [3.0931, 1.9411, 1.2955],
        [3.0931, 3.0758, 3.0921],
        [3.0931, 2.1539, 2.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.1406307965517044 
model_pd.l_d.mean(): -25.14748191833496 
model_pd.lagr.mean(): -25.006851196289062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0014], device='cuda:0')), ('power', tensor([-25.1461], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.1406307965517044
epoch£º855	 i:1 	 global-step:17101	 l-p:0.1260373592376709
epoch£º855	 i:2 	 global-step:17102	 l-p:0.11905050277709961
epoch£º855	 i:3 	 global-step:17103	 l-p:0.3375784754753113
epoch£º855	 i:4 	 global-step:17104	 l-p:0.1163763478398323
epoch£º855	 i:5 	 global-step:17105	 l-p:0.13233362138271332
epoch£º855	 i:6 	 global-step:17106	 l-p:0.13305680453777313
epoch£º855	 i:7 	 global-step:17107	 l-p:0.17990925908088684
epoch£º855	 i:8 	 global-step:17108	 l-p:3.5403075218200684
epoch£º855	 i:9 	 global-step:17109	 l-p:-2.5896947383880615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0944, 2.3546, 2.4889],
        [3.0944, 3.0751, 3.0933],
        [3.0944, 1.7774, 1.2234],
        [3.0944, 3.0944, 3.0944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.13302670419216156 
model_pd.l_d.mean(): -24.831012725830078 
model_pd.lagr.mean(): -24.697986602783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0723], device='cuda:0')), ('power', tensor([-24.9033], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.13302670419216156
epoch£º856	 i:1 	 global-step:17121	 l-p:0.1320100724697113
epoch£º856	 i:2 	 global-step:17122	 l-p:0.5862509608268738
epoch£º856	 i:3 	 global-step:17123	 l-p:0.3263911008834839
epoch£º856	 i:4 	 global-step:17124	 l-p:0.14734821021556854
epoch£º856	 i:5 	 global-step:17125	 l-p:0.14211182296276093
epoch£º856	 i:6 	 global-step:17126	 l-p:0.24703700840473175
epoch£º856	 i:7 	 global-step:17127	 l-p:0.18346329033374786
epoch£º856	 i:8 	 global-step:17128	 l-p:0.10582086443901062
epoch£º856	 i:9 	 global-step:17129	 l-p:0.14019012451171875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1102, 1.8183, 1.2087],
        [3.1102, 2.3489, 2.4706],
        [3.1102, 3.0512, 3.1030],
        [3.1102, 1.8381, 1.2189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.10922913998365402 
model_pd.l_d.mean(): -24.648529052734375 
model_pd.lagr.mean(): -24.53929901123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1891], device='cuda:0')), ('power', tensor([-24.8376], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.10922913998365402
epoch£º857	 i:1 	 global-step:17141	 l-p:0.09150373190641403
epoch£º857	 i:2 	 global-step:17142	 l-p:0.14675621688365936
epoch£º857	 i:3 	 global-step:17143	 l-p:0.16301631927490234
epoch£º857	 i:4 	 global-step:17144	 l-p:0.14239457249641418
epoch£º857	 i:5 	 global-step:17145	 l-p:0.35865843296051025
epoch£º857	 i:6 	 global-step:17146	 l-p:0.20289449393749237
epoch£º857	 i:7 	 global-step:17147	 l-p:0.14020268619060516
epoch£º857	 i:8 	 global-step:17148	 l-p:0.12658260762691498
epoch£º857	 i:9 	 global-step:17149	 l-p:0.15278562903404236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1207, 3.1193, 3.1207],
        [3.1207, 2.5331, 2.7252],
        [3.1207, 3.1205, 3.1207],
        [3.1207, 3.0975, 3.1191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.14532485604286194 
model_pd.l_d.mean(): -25.08936309814453 
model_pd.lagr.mean(): -24.94403839111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0743], device='cuda:0')), ('power', tensor([-25.0151], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.14532485604286194
epoch£º858	 i:1 	 global-step:17161	 l-p:0.12303543090820312
epoch£º858	 i:2 	 global-step:17162	 l-p:0.12745462357997894
epoch£º858	 i:3 	 global-step:17163	 l-p:0.14232438802719116
epoch£º858	 i:4 	 global-step:17164	 l-p:0.4271887242794037
epoch£º858	 i:5 	 global-step:17165	 l-p:0.18947608768939972
epoch£º858	 i:6 	 global-step:17166	 l-p:0.10329072922468185
epoch£º858	 i:7 	 global-step:17167	 l-p:0.665155827999115
epoch£º858	 i:8 	 global-step:17168	 l-p:0.14296166598796844
epoch£º858	 i:9 	 global-step:17169	 l-p:0.1604083627462387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1034, 1.8863, 1.5113],
        [3.1034, 3.1032, 3.1034],
        [3.1034, 2.1464, 1.4655],
        [3.1034, 2.9415, 3.0637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.11682493984699249 
model_pd.l_d.mean(): -25.086462020874023 
model_pd.lagr.mean(): -24.969636917114258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0403], device='cuda:0')), ('power', tensor([-25.0462], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.11682493984699249
epoch£º859	 i:1 	 global-step:17181	 l-p:0.3339453935623169
epoch£º859	 i:2 	 global-step:17182	 l-p:0.22695790231227875
epoch£º859	 i:3 	 global-step:17183	 l-p:0.10908172279596329
epoch£º859	 i:4 	 global-step:17184	 l-p:0.16394443809986115
epoch£º859	 i:5 	 global-step:17185	 l-p:0.1374039202928543
epoch£º859	 i:6 	 global-step:17186	 l-p:0.16189174354076385
epoch£º859	 i:7 	 global-step:17187	 l-p:0.1412239372730255
epoch£º859	 i:8 	 global-step:17188	 l-p:0.13496974110603333
epoch£º859	 i:9 	 global-step:17189	 l-p:0.15515506267547607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1215, 2.9746, 3.0879],
        [3.1215, 3.0963, 3.1198],
        [3.1215, 2.5824, 2.7839],
        [3.1215, 3.0759, 3.1168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.1868823617696762 
model_pd.l_d.mean(): -25.034446716308594 
model_pd.lagr.mean(): -24.847564697265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0335], device='cuda:0')), ('power', tensor([-25.0009], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.1868823617696762
epoch£º860	 i:1 	 global-step:17201	 l-p:0.09290920943021774
epoch£º860	 i:2 	 global-step:17202	 l-p:0.1229187399148941
epoch£º860	 i:3 	 global-step:17203	 l-p:0.14423403143882751
epoch£º860	 i:4 	 global-step:17204	 l-p:0.1719764620065689
epoch£º860	 i:5 	 global-step:17205	 l-p:0.2684178948402405
epoch£º860	 i:6 	 global-step:17206	 l-p:0.12885814905166626
epoch£º860	 i:7 	 global-step:17207	 l-p:0.18348968029022217
epoch£º860	 i:8 	 global-step:17208	 l-p:0.1419498771429062
epoch£º860	 i:9 	 global-step:17209	 l-p:0.14145328104496002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1023, 2.0310, 1.8634],
        [3.1023, 1.9464, 1.2996],
        [3.1023, 2.8116, 2.9917],
        [3.1023, 3.0646, 3.0989]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.21611705422401428 
model_pd.l_d.mean(): -25.083219528198242 
model_pd.lagr.mean(): -24.867101669311523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0441], device='cuda:0')), ('power', tensor([-25.0391], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.21611705422401428
epoch£º861	 i:1 	 global-step:17221	 l-p:0.2354232519865036
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12261848151683807
epoch£º861	 i:3 	 global-step:17223	 l-p:0.13950207829475403
epoch£º861	 i:4 	 global-step:17224	 l-p:0.1368800401687622
epoch£º861	 i:5 	 global-step:17225	 l-p:-0.8917012810707092
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12145256996154785
epoch£º861	 i:7 	 global-step:17227	 l-p:-0.10981617867946625
epoch£º861	 i:8 	 global-step:17228	 l-p:0.12747715413570404
epoch£º861	 i:9 	 global-step:17229	 l-p:0.12583373486995697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0695, 1.7778, 1.2915],
        [3.0695, 2.3489, 2.4946],
        [3.0695, 3.0451, 3.0678],
        [3.0695, 3.0695, 3.0695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13499322533607483 
model_pd.l_d.mean(): -25.05782127380371 
model_pd.lagr.mean(): -24.922828674316406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0068], device='cuda:0')), ('power', tensor([-25.0646], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13499322533607483
epoch£º862	 i:1 	 global-step:17241	 l-p:0.13164468109607697
epoch£º862	 i:2 	 global-step:17242	 l-p:0.02898687683045864
epoch£º862	 i:3 	 global-step:17243	 l-p:0.44992196559906006
epoch£º862	 i:4 	 global-step:17244	 l-p:0.16087661683559418
epoch£º862	 i:5 	 global-step:17245	 l-p:0.24572306871414185
epoch£º862	 i:6 	 global-step:17246	 l-p:0.022779157385230064
epoch£º862	 i:7 	 global-step:17247	 l-p:0.19385531544685364
epoch£º862	 i:8 	 global-step:17248	 l-p:0.12358047813177109
epoch£º862	 i:9 	 global-step:17249	 l-p:0.12746666371822357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0739, 2.9483, 3.0483],
        [3.0739, 3.0097, 3.0657],
        [3.0739, 2.8068, 2.9788],
        [3.0739, 1.7749, 1.2742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.14200833439826965 
model_pd.l_d.mean(): -24.732440948486328 
model_pd.lagr.mean(): -24.59043312072754 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0467], device='cuda:0')), ('power', tensor([-24.7791], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.14200833439826965
epoch£º863	 i:1 	 global-step:17261	 l-p:0.1532333344221115
epoch£º863	 i:2 	 global-step:17262	 l-p:0.13278871774673462
epoch£º863	 i:3 	 global-step:17263	 l-p:0.02453211322426796
epoch£º863	 i:4 	 global-step:17264	 l-p:0.1619623899459839
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12420806288719177
epoch£º863	 i:6 	 global-step:17266	 l-p:-0.05338019132614136
epoch£º863	 i:7 	 global-step:17267	 l-p:0.2141910344362259
epoch£º863	 i:8 	 global-step:17268	 l-p:0.14257016777992249
epoch£º863	 i:9 	 global-step:17269	 l-p:0.02379557490348816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0790, 1.7900, 1.1861],
        [3.0790, 3.0789, 3.0790],
        [3.0790, 3.0790, 3.0790],
        [3.0790, 2.1173, 1.4404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.1510855108499527 
model_pd.l_d.mean(): -24.71629524230957 
model_pd.lagr.mean(): -24.565210342407227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0337], device='cuda:0')), ('power', tensor([-24.7500], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.1510855108499527
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12407325208187103
epoch£º864	 i:2 	 global-step:17282	 l-p:0.20723500847816467
epoch£º864	 i:3 	 global-step:17283	 l-p:-1.130545973777771
epoch£º864	 i:4 	 global-step:17284	 l-p:0.12892532348632812
epoch£º864	 i:5 	 global-step:17285	 l-p:-0.12942323088645935
epoch£º864	 i:6 	 global-step:17286	 l-p:0.11880258470773697
epoch£º864	 i:7 	 global-step:17287	 l-p:-2.203704833984375
epoch£º864	 i:8 	 global-step:17288	 l-p:0.14328926801681519
epoch£º864	 i:9 	 global-step:17289	 l-p:0.18388894200325012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0911, 3.0910, 3.0911],
        [3.0911, 3.0881, 3.0911],
        [3.0911, 3.0911, 3.0911],
        [3.0911, 3.0911, 3.0911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.12340496480464935 
model_pd.l_d.mean(): -24.97166633605957 
model_pd.lagr.mean(): -24.8482608795166 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0642], device='cuda:0')), ('power', tensor([-24.9075], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.12340496480464935
epoch£º865	 i:1 	 global-step:17301	 l-p:0.1547686755657196
epoch£º865	 i:2 	 global-step:17302	 l-p:0.13608597218990326
epoch£º865	 i:3 	 global-step:17303	 l-p:0.15184424817562103
epoch£º865	 i:4 	 global-step:17304	 l-p:0.14573872089385986
epoch£º865	 i:5 	 global-step:17305	 l-p:0.18711209297180176
epoch£º865	 i:6 	 global-step:17306	 l-p:-0.04591728001832962
epoch£º865	 i:7 	 global-step:17307	 l-p:0.012231497094035149
epoch£º865	 i:8 	 global-step:17308	 l-p:1.1177163124084473
epoch£º865	 i:9 	 global-step:17309	 l-p:0.146649569272995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0943, 1.9651, 1.3145],
        [3.0943, 2.1828, 1.4955],
        [3.0943, 2.8966, 3.0382],
        [3.0943, 3.0859, 3.0940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.1478816717863083 
model_pd.l_d.mean(): -24.72871971130371 
model_pd.lagr.mean(): -24.58083724975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0342], device='cuda:0')), ('power', tensor([-24.7629], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.1478816717863083
epoch£º866	 i:1 	 global-step:17321	 l-p:0.14424766600131989
epoch£º866	 i:2 	 global-step:17322	 l-p:0.2313157618045807
epoch£º866	 i:3 	 global-step:17323	 l-p:0.142694890499115
epoch£º866	 i:4 	 global-step:17324	 l-p:0.16086345911026
epoch£º866	 i:5 	 global-step:17325	 l-p:0.14983677864074707
epoch£º866	 i:6 	 global-step:17326	 l-p:0.156648650765419
epoch£º866	 i:7 	 global-step:17327	 l-p:0.20733287930488586
epoch£º866	 i:8 	 global-step:17328	 l-p:0.15664105117321014
epoch£º866	 i:9 	 global-step:17329	 l-p:0.11026060581207275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1432, 3.1432, 3.1432],
        [3.1432, 2.5276, 2.7121],
        [3.1432, 1.9741, 1.3217],
        [3.1432, 3.1328, 3.1428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.11994414776563644 
model_pd.l_d.mean(): -24.68073081970215 
model_pd.lagr.mean(): -24.560787200927734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0724], device='cuda:0')), ('power', tensor([-24.7531], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.11994414776563644
epoch£º867	 i:1 	 global-step:17341	 l-p:0.1544664204120636
epoch£º867	 i:2 	 global-step:17342	 l-p:0.19925975799560547
epoch£º867	 i:3 	 global-step:17343	 l-p:0.12251636385917664
epoch£º867	 i:4 	 global-step:17344	 l-p:0.05213310942053795
epoch£º867	 i:5 	 global-step:17345	 l-p:0.21014080941677094
epoch£º867	 i:6 	 global-step:17346	 l-p:0.11889483779668808
epoch£º867	 i:7 	 global-step:17347	 l-p:0.1401871144771576
epoch£º867	 i:8 	 global-step:17348	 l-p:0.13693687319755554
epoch£º867	 i:9 	 global-step:17349	 l-p:0.13986113667488098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[3.1372, 1.9674, 1.6584],
        [3.1372, 1.9402, 1.2943],
        [3.1372, 2.3759, 2.4974],
        [3.1372, 1.8971, 1.4861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.17410938441753387 
model_pd.l_d.mean(): -25.131187438964844 
model_pd.lagr.mean(): -24.95707893371582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0044], device='cuda:0')), ('power', tensor([-25.1356], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.17410938441753387
epoch£º868	 i:1 	 global-step:17361	 l-p:0.1150551587343216
epoch£º868	 i:2 	 global-step:17362	 l-p:0.13161803781986237
epoch£º868	 i:3 	 global-step:17363	 l-p:0.14948566257953644
epoch£º868	 i:4 	 global-step:17364	 l-p:0.1982400268316269
epoch£º868	 i:5 	 global-step:17365	 l-p:0.3107273280620575
epoch£º868	 i:6 	 global-step:17366	 l-p:0.4039418399333954
epoch£º868	 i:7 	 global-step:17367	 l-p:0.13790708780288696
epoch£º868	 i:8 	 global-step:17368	 l-p:0.10678715258836746
epoch£º868	 i:9 	 global-step:17369	 l-p:0.14353971183300018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1009, 2.1426, 1.4617],
        [3.1009, 3.0897, 3.1004],
        [3.1009, 1.7810, 1.2096],
        [3.1009, 2.5673, 2.7705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.12666885554790497 
model_pd.l_d.mean(): -24.79859733581543 
model_pd.lagr.mean(): -24.67192840576172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1845], device='cuda:0')), ('power', tensor([-24.9831], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.12666885554790497
epoch£º869	 i:1 	 global-step:17381	 l-p:0.2614487111568451
epoch£º869	 i:2 	 global-step:17382	 l-p:0.31898659467697144
epoch£º869	 i:3 	 global-step:17383	 l-p:0.1442641168832779
epoch£º869	 i:4 	 global-step:17384	 l-p:0.12224212288856506
epoch£º869	 i:5 	 global-step:17385	 l-p:0.1549953818321228
epoch£º869	 i:6 	 global-step:17386	 l-p:0.13358426094055176
epoch£º869	 i:7 	 global-step:17387	 l-p:0.273115336894989
epoch£º869	 i:8 	 global-step:17388	 l-p:0.12316060811281204
epoch£º869	 i:9 	 global-step:17389	 l-p:0.2023950219154358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[3.0909, 2.3249, 2.4460],
        [3.0909, 1.9478, 1.6828],
        [3.0909, 2.0683, 1.9639],
        [3.0909, 2.3450, 2.4775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.1265447437763214 
model_pd.l_d.mean(): -25.286298751831055 
model_pd.lagr.mean(): -25.159753799438477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0786], device='cuda:0')), ('power', tensor([-25.2077], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.1265447437763214
epoch£º870	 i:1 	 global-step:17401	 l-p:0.676627516746521
epoch£º870	 i:2 	 global-step:17402	 l-p:-0.21135658025741577
epoch£º870	 i:3 	 global-step:17403	 l-p:0.11523916572332382
epoch£º870	 i:4 	 global-step:17404	 l-p:0.1662420779466629
epoch£º870	 i:5 	 global-step:17405	 l-p:0.1331583559513092
epoch£º870	 i:6 	 global-step:17406	 l-p:0.12540388107299805
epoch£º870	 i:7 	 global-step:17407	 l-p:0.13724170625209808
epoch£º870	 i:8 	 global-step:17408	 l-p:0.11963111907243729
epoch£º870	 i:9 	 global-step:17409	 l-p:0.5005496144294739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0920, 2.0241, 1.8636],
        [3.0920, 3.0378, 3.0858],
        [3.0920, 1.7857, 1.1901],
        [3.0920, 2.2058, 2.2409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.33783793449401855 
model_pd.l_d.mean(): -25.03853988647461 
model_pd.lagr.mean(): -24.700702667236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1022], device='cuda:0')), ('power', tensor([-25.1408], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.33783793449401855
epoch£º871	 i:1 	 global-step:17421	 l-p:0.14137622714042664
epoch£º871	 i:2 	 global-step:17422	 l-p:0.13339191675186157
epoch£º871	 i:3 	 global-step:17423	 l-p:0.1253109723329544
epoch£º871	 i:4 	 global-step:17424	 l-p:-0.04514608159661293
epoch£º871	 i:5 	 global-step:17425	 l-p:0.10930978506803513
epoch£º871	 i:6 	 global-step:17426	 l-p:0.12562839686870575
epoch£º871	 i:7 	 global-step:17427	 l-p:0.06103551387786865
epoch£º871	 i:8 	 global-step:17428	 l-p:0.17666777968406677
epoch£º871	 i:9 	 global-step:17429	 l-p:0.3417724668979645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0689, 2.9372, 3.0412],
        [3.0689, 1.7984, 1.1871],
        [3.0689, 3.0689, 3.0689],
        [3.0689, 2.9682, 3.0514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.21516956388950348 
model_pd.l_d.mean(): -25.097639083862305 
model_pd.lagr.mean(): -24.882469177246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0594], device='cuda:0')), ('power', tensor([-25.1570], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:0.21516956388950348
epoch£º872	 i:1 	 global-step:17441	 l-p:0.1357199102640152
epoch£º872	 i:2 	 global-step:17442	 l-p:-0.5178598165512085
epoch£º872	 i:3 	 global-step:17443	 l-p:-0.1397615224123001
epoch£º872	 i:4 	 global-step:17444	 l-p:0.13420158624649048
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13256850838661194
epoch£º872	 i:6 	 global-step:17446	 l-p:0.14687353372573853
epoch£º872	 i:7 	 global-step:17447	 l-p:0.13840365409851074
epoch£º872	 i:8 	 global-step:17448	 l-p:0.15182194113731384
epoch£º872	 i:9 	 global-step:17449	 l-p:0.6205288171768188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0934, 2.6611, 2.8684],
        [3.0934, 3.0701, 3.0919],
        [3.0934, 2.0434, 1.9058],
        [3.0934, 2.2057, 2.2396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.10405272245407104 
model_pd.l_d.mean(): -24.6851806640625 
model_pd.lagr.mean(): -24.581127166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0877], device='cuda:0')), ('power', tensor([-24.7729], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.10405272245407104
epoch£º873	 i:1 	 global-step:17461	 l-p:0.11140290647745132
epoch£º873	 i:2 	 global-step:17462	 l-p:0.15007361769676208
epoch£º873	 i:3 	 global-step:17463	 l-p:0.1590021550655365
epoch£º873	 i:4 	 global-step:17464	 l-p:0.13113324344158173
epoch£º873	 i:5 	 global-step:17465	 l-p:0.14082787930965424
epoch£º873	 i:6 	 global-step:17466	 l-p:0.14198298752307892
epoch£º873	 i:7 	 global-step:17467	 l-p:0.048170652240514755
epoch£º873	 i:8 	 global-step:17468	 l-p:-0.31994307041168213
epoch£º873	 i:9 	 global-step:17469	 l-p:0.11523628234863281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0718, 3.0718, 3.0718],
        [3.0718, 3.0708, 3.0718],
        [3.0718, 3.0718, 3.0718],
        [3.0718, 1.9581, 1.3082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): -0.10532058775424957 
model_pd.l_d.mean(): -25.088777542114258 
model_pd.lagr.mean(): -25.1940975189209 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0747], device='cuda:0')), ('power', tensor([-25.0141], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:-0.10532058775424957
epoch£º874	 i:1 	 global-step:17481	 l-p:0.13185106217861176
epoch£º874	 i:2 	 global-step:17482	 l-p:0.11235283315181732
epoch£º874	 i:3 	 global-step:17483	 l-p:0.13139507174491882
epoch£º874	 i:4 	 global-step:17484	 l-p:0.26649245619773865
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12612076103687286
epoch£º874	 i:6 	 global-step:17486	 l-p:0.18002280592918396
epoch£º874	 i:7 	 global-step:17487	 l-p:0.09786946326494217
epoch£º874	 i:8 	 global-step:17488	 l-p:0.08182889968156815
epoch£º874	 i:9 	 global-step:17489	 l-p:-0.3276900351047516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0481, 2.9096, 3.0179],
        [3.0481, 2.4844, 2.6844],
        [3.0481, 3.0482, 3.0482],
        [3.0481, 1.7318, 1.1730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): -1.1668977737426758 
model_pd.l_d.mean(): -25.102949142456055 
model_pd.lagr.mean(): -26.269847869873047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0769], device='cuda:0')), ('power', tensor([-25.0261], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:-1.1668977737426758
epoch£º875	 i:1 	 global-step:17501	 l-p:0.14954471588134766
epoch£º875	 i:2 	 global-step:17502	 l-p:0.24603523313999176
epoch£º875	 i:3 	 global-step:17503	 l-p:0.16115702688694
epoch£º875	 i:4 	 global-step:17504	 l-p:0.04286366328597069
epoch£º875	 i:5 	 global-step:17505	 l-p:-0.09177106618881226
epoch£º875	 i:6 	 global-step:17506	 l-p:0.13276250660419464
epoch£º875	 i:7 	 global-step:17507	 l-p:0.09476333111524582
epoch£º875	 i:8 	 global-step:17508	 l-p:0.1479484587907791
epoch£º875	 i:9 	 global-step:17509	 l-p:0.17486442625522614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0948, 1.7817, 1.1920],
        [3.0948, 2.1776, 1.4905],
        [3.0948, 2.1447, 1.4629],
        [3.0948, 3.0947, 3.0948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.1485024392604828 
model_pd.l_d.mean(): -24.6845645904541 
model_pd.lagr.mean(): -24.536062240600586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0782], device='cuda:0')), ('power', tensor([-24.7627], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.1485024392604828
epoch£º876	 i:1 	 global-step:17521	 l-p:0.07739081978797913
epoch£º876	 i:2 	 global-step:17522	 l-p:0.43094402551651
epoch£º876	 i:3 	 global-step:17523	 l-p:0.1340368241071701
epoch£º876	 i:4 	 global-step:17524	 l-p:0.31614792346954346
epoch£º876	 i:5 	 global-step:17525	 l-p:0.15967120230197906
epoch£º876	 i:6 	 global-step:17526	 l-p:0.13013119995594025
epoch£º876	 i:7 	 global-step:17527	 l-p:0.13498495519161224
epoch£º876	 i:8 	 global-step:17528	 l-p:0.1995210349559784
epoch£º876	 i:9 	 global-step:17529	 l-p:0.1438649594783783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1134, 3.1088, 3.1132],
        [3.1134, 3.1134, 3.1134],
        [3.1134, 1.8193, 1.3300],
        [3.1134, 2.1967, 1.5072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.2119256556034088 
model_pd.l_d.mean(): -24.598215103149414 
model_pd.lagr.mean(): -24.386289596557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0381], device='cuda:0')), ('power', tensor([-24.6364], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.2119256556034088
epoch£º877	 i:1 	 global-step:17541	 l-p:0.1357468068599701
epoch£º877	 i:2 	 global-step:17542	 l-p:0.16473428905010223
epoch£º877	 i:3 	 global-step:17543	 l-p:0.2323233187198639
epoch£º877	 i:4 	 global-step:17544	 l-p:0.16435039043426514
epoch£º877	 i:5 	 global-step:17545	 l-p:0.1311844438314438
epoch£º877	 i:6 	 global-step:17546	 l-p:0.130840003490448
epoch£º877	 i:7 	 global-step:17547	 l-p:0.12960822880268097
epoch£º877	 i:8 	 global-step:17548	 l-p:0.12685751914978027
epoch£º877	 i:9 	 global-step:17549	 l-p:0.22151456773281097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1093, 3.1090, 3.1093],
        [3.1093, 2.0977, 1.4237],
        [3.1093, 2.1031, 1.4283],
        [3.1093, 3.1091, 3.1093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.13885708153247833 
model_pd.l_d.mean(): -25.081388473510742 
model_pd.lagr.mean(): -24.94253158569336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0771], device='cuda:0')), ('power', tensor([-25.0043], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.13885708153247833
epoch£º878	 i:1 	 global-step:17561	 l-p:0.19125136733055115
epoch£º878	 i:2 	 global-step:17562	 l-p:0.1584673970937729
epoch£º878	 i:3 	 global-step:17563	 l-p:0.231526181101799
epoch£º878	 i:4 	 global-step:17564	 l-p:0.11417055130004883
epoch£º878	 i:5 	 global-step:17565	 l-p:0.1298927515745163
epoch£º878	 i:6 	 global-step:17566	 l-p:0.1771479696035385
epoch£º878	 i:7 	 global-step:17567	 l-p:0.2434072494506836
epoch£º878	 i:8 	 global-step:17568	 l-p:0.13582450151443481
epoch£º878	 i:9 	 global-step:17569	 l-p:0.18766923248767853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1181, 3.1176, 3.1181],
        [3.1181, 2.8417, 3.0171],
        [3.1181, 1.7967, 1.2437],
        [3.1181, 3.0160, 3.1001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.22930514812469482 
model_pd.l_d.mean(): -25.090301513671875 
model_pd.lagr.mean(): -24.86099624633789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0339], device='cuda:0')), ('power', tensor([-25.0564], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.22930514812469482
epoch£º879	 i:1 	 global-step:17581	 l-p:0.1997338980436325
epoch£º879	 i:2 	 global-step:17582	 l-p:0.18702928721904755
epoch£º879	 i:3 	 global-step:17583	 l-p:0.14690284430980682
epoch£º879	 i:4 	 global-step:17584	 l-p:0.15299220383167267
epoch£º879	 i:5 	 global-step:17585	 l-p:0.13841547071933746
epoch£º879	 i:6 	 global-step:17586	 l-p:0.1000504344701767
epoch£º879	 i:7 	 global-step:17587	 l-p:-0.015841564163565636
epoch£º879	 i:8 	 global-step:17588	 l-p:0.12983666360378265
epoch£º879	 i:9 	 global-step:17589	 l-p:0.12213867902755737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1680, 1.9432, 1.5519],
        [3.1680, 2.8701, 3.0523],
        [3.1680, 2.1899, 2.1328],
        [3.1680, 3.1675, 3.1680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.166441410779953 
model_pd.l_d.mean(): -25.004566192626953 
model_pd.lagr.mean(): -24.838125228881836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1033], device='cuda:0')), ('power', tensor([-25.1079], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.166441410779953
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12103446573019028
epoch£º880	 i:2 	 global-step:17602	 l-p:0.12973563373088837
epoch£º880	 i:3 	 global-step:17603	 l-p:0.14533355832099915
epoch£º880	 i:4 	 global-step:17604	 l-p:0.1329256147146225
epoch£º880	 i:5 	 global-step:17605	 l-p:0.054450154304504395
epoch£º880	 i:6 	 global-step:17606	 l-p:0.16499124467372894
epoch£º880	 i:7 	 global-step:17607	 l-p:0.12697063386440277
epoch£º880	 i:8 	 global-step:17608	 l-p:0.19211383163928986
epoch£º880	 i:9 	 global-step:17609	 l-p:0.16443592309951782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1331, 3.1331, 3.1331],
        [3.1331, 2.7797, 2.9765],
        [3.1331, 2.1528, 2.0959],
        [3.1331, 2.1080, 1.4325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.12228943407535553 
model_pd.l_d.mean(): -24.61384391784668 
model_pd.lagr.mean(): -24.491554260253906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0016], device='cuda:0')), ('power', tensor([-24.6154], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.12228943407535553
epoch£º881	 i:1 	 global-step:17621	 l-p:0.16998107731342316
epoch£º881	 i:2 	 global-step:17622	 l-p:0.12543946504592896
epoch£º881	 i:3 	 global-step:17623	 l-p:0.2720252573490143
epoch£º881	 i:4 	 global-step:17624	 l-p:0.19706538319587708
epoch£º881	 i:5 	 global-step:17625	 l-p:0.14149586856365204
epoch£º881	 i:6 	 global-step:17626	 l-p:0.1180649921298027
epoch£º881	 i:7 	 global-step:17627	 l-p:0.1863214075565338
epoch£º881	 i:8 	 global-step:17628	 l-p:0.13058975338935852
epoch£º881	 i:9 	 global-step:17629	 l-p:0.12980037927627563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1016, 2.1589, 2.1429],
        [3.1016, 1.8308, 1.3821],
        [3.1016, 3.1016, 3.1016],
        [3.1016, 3.0896, 3.1011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.38901960849761963 
model_pd.l_d.mean(): -25.05729103088379 
model_pd.lagr.mean(): -24.668272018432617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0337], device='cuda:0')), ('power', tensor([-25.0910], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:0.38901960849761963
epoch£º882	 i:1 	 global-step:17641	 l-p:0.14299999177455902
epoch£º882	 i:2 	 global-step:17642	 l-p:0.15120993554592133
epoch£º882	 i:3 	 global-step:17643	 l-p:0.413013219833374
epoch£º882	 i:4 	 global-step:17644	 l-p:0.15746791660785675
epoch£º882	 i:5 	 global-step:17645	 l-p:0.13283969461917877
epoch£º882	 i:6 	 global-step:17646	 l-p:0.5901313424110413
epoch£º882	 i:7 	 global-step:17647	 l-p:0.13578352332115173
epoch£º882	 i:8 	 global-step:17648	 l-p:0.11790136247873306
epoch£º882	 i:9 	 global-step:17649	 l-p:0.17106547951698303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0675, 3.0592, 3.0672],
        [3.0675, 3.0660, 3.0675],
        [3.0675, 3.0285, 3.0639],
        [3.0675, 2.5039, 2.7038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.13348756730556488 
model_pd.l_d.mean(): -25.163854598999023 
model_pd.lagr.mean(): -25.030366897583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0076], device='cuda:0')), ('power', tensor([-25.1715], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.13348756730556488
epoch£º883	 i:1 	 global-step:17661	 l-p:0.11662444472312927
epoch£º883	 i:2 	 global-step:17662	 l-p:0.1319652497768402
epoch£º883	 i:3 	 global-step:17663	 l-p:0.2792544364929199
epoch£º883	 i:4 	 global-step:17664	 l-p:0.14367054402828217
epoch£º883	 i:5 	 global-step:17665	 l-p:0.14493012428283691
epoch£º883	 i:6 	 global-step:17666	 l-p:0.12988004088401794
epoch£º883	 i:7 	 global-step:17667	 l-p:-0.04783724248409271
epoch£º883	 i:8 	 global-step:17668	 l-p:0.11082986742258072
epoch£º883	 i:9 	 global-step:17669	 l-p:0.12394750118255615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0310, 3.0311, 3.0311],
        [3.0310, 1.7904, 1.3960],
        [3.0310, 3.0311, 3.0310],
        [3.0310, 1.7150, 1.1668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.07764238864183426 
model_pd.l_d.mean(): -24.90851402282715 
model_pd.lagr.mean(): -24.83087158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0657], device='cuda:0')), ('power', tensor([-24.9742], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.07764238864183426
epoch£º884	 i:1 	 global-step:17681	 l-p:0.22566457092761993
epoch£º884	 i:2 	 global-step:17682	 l-p:0.1453142762184143
epoch£º884	 i:3 	 global-step:17683	 l-p:0.13290220499038696
epoch£º884	 i:4 	 global-step:17684	 l-p:0.10365762561559677
epoch£º884	 i:5 	 global-step:17685	 l-p:0.051615431904792786
epoch£º884	 i:6 	 global-step:17686	 l-p:0.14713804423809052
epoch£º884	 i:7 	 global-step:17687	 l-p:0.15778286755084991
epoch£º884	 i:8 	 global-step:17688	 l-p:0.1251269280910492
epoch£º884	 i:9 	 global-step:17689	 l-p:0.13667435944080353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0469, 1.7334, 1.1594],
        [3.0469, 3.0469, 3.0470],
        [3.0469, 2.0238, 1.9243],
        [3.0469, 1.8303, 1.2063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.14336027204990387 
model_pd.l_d.mean(): -25.075180053710938 
model_pd.lagr.mean(): -24.931819915771484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0768], device='cuda:0')), ('power', tensor([-25.1520], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.14336027204990387
epoch£º885	 i:1 	 global-step:17701	 l-p:0.15099182724952698
epoch£º885	 i:2 	 global-step:17702	 l-p:0.01250598207116127
epoch£º885	 i:3 	 global-step:17703	 l-p:0.14581702649593353
epoch£º885	 i:4 	 global-step:17704	 l-p:0.13416987657546997
epoch£º885	 i:5 	 global-step:17705	 l-p:-0.016995253041386604
epoch£º885	 i:6 	 global-step:17706	 l-p:0.12033583223819733
epoch£º885	 i:7 	 global-step:17707	 l-p:1.000620722770691
epoch£º885	 i:8 	 global-step:17708	 l-p:0.12385977804660797
epoch£º885	 i:9 	 global-step:17709	 l-p:0.1464570164680481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0250, 3.0198, 3.0249],
        [3.0250, 2.4872, 2.6924],
        [3.0250, 3.0016, 3.0234],
        [3.0250, 2.7188, 2.9046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.15530019998550415 
model_pd.l_d.mean(): -24.731956481933594 
model_pd.lagr.mean(): -24.576656341552734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2134], device='cuda:0')), ('power', tensor([-24.9454], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.15530019998550415
epoch£º886	 i:1 	 global-step:17721	 l-p:0.06203623488545418
epoch£º886	 i:2 	 global-step:17722	 l-p:0.15751507878303528
epoch£º886	 i:3 	 global-step:17723	 l-p:0.1467316448688507
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11922408640384674
epoch£º886	 i:5 	 global-step:17725	 l-p:0.22255156934261322
epoch£º886	 i:6 	 global-step:17726	 l-p:0.13197952508926392
epoch£º886	 i:7 	 global-step:17727	 l-p:0.06423336267471313
epoch£º886	 i:8 	 global-step:17728	 l-p:0.15700550377368927
epoch£º886	 i:9 	 global-step:17729	 l-p:-2.6615898609161377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0370, 2.9672, 3.0275],
        [3.0370, 1.9152, 1.6889],
        [3.0370, 2.6486, 2.8526],
        [3.0370, 2.7321, 2.9174]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.09698741883039474 
model_pd.l_d.mean(): -25.25176239013672 
model_pd.lagr.mean(): -25.154775619506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0806], device='cuda:0')), ('power', tensor([-25.1711], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.09698741883039474
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11793872714042664
epoch£º887	 i:2 	 global-step:17742	 l-p:0.08720012754201889
epoch£º887	 i:3 	 global-step:17743	 l-p:0.19654220342636108
epoch£º887	 i:4 	 global-step:17744	 l-p:0.1530633270740509
epoch£º887	 i:5 	 global-step:17745	 l-p:0.11150281876325607
epoch£º887	 i:6 	 global-step:17746	 l-p:0.14066748321056366
epoch£º887	 i:7 	 global-step:17747	 l-p:0.18996527791023254
epoch£º887	 i:8 	 global-step:17748	 l-p:0.16266989707946777
epoch£º887	 i:9 	 global-step:17749	 l-p:0.6329589486122131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[3.1043, 2.0787, 1.4072],
        [3.1043, 2.1464, 2.1165],
        [3.1043, 2.0625, 1.3937],
        [3.1043, 2.1594, 2.1425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.11766137927770615 
model_pd.l_d.mean(): -24.675914764404297 
model_pd.lagr.mean(): -24.55825424194336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1446], device='cuda:0')), ('power', tensor([-24.8205], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.11766137927770615
epoch£º888	 i:1 	 global-step:17761	 l-p:0.15779191255569458
epoch£º888	 i:2 	 global-step:17762	 l-p:0.1245899572968483
epoch£º888	 i:3 	 global-step:17763	 l-p:0.16667592525482178
epoch£º888	 i:4 	 global-step:17764	 l-p:0.14328311383724213
epoch£º888	 i:5 	 global-step:17765	 l-p:0.21188007295131683
epoch£º888	 i:6 	 global-step:17766	 l-p:0.1722438782453537
epoch£º888	 i:7 	 global-step:17767	 l-p:0.2080083042383194
epoch£º888	 i:8 	 global-step:17768	 l-p:0.08888344466686249
epoch£º888	 i:9 	 global-step:17769	 l-p:0.14807403087615967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1552, 3.1552, 3.1553],
        [3.1552, 3.1552, 3.1553],
        [3.1552, 2.5678, 2.7608],
        [3.1552, 2.9575, 3.0991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.1483594924211502 
model_pd.l_d.mean(): -25.246292114257812 
model_pd.lagr.mean(): -25.097932815551758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0477], device='cuda:0')), ('power', tensor([-25.1986], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.1483594924211502
epoch£º889	 i:1 	 global-step:17781	 l-p:0.02837458625435829
epoch£º889	 i:2 	 global-step:17782	 l-p:0.12669141590595245
epoch£º889	 i:3 	 global-step:17783	 l-p:0.17803165316581726
epoch£º889	 i:4 	 global-step:17784	 l-p:0.14612071216106415
epoch£º889	 i:5 	 global-step:17785	 l-p:0.12512396275997162
epoch£º889	 i:6 	 global-step:17786	 l-p:0.1366172581911087
epoch£º889	 i:7 	 global-step:17787	 l-p:0.12387099862098694
epoch£º889	 i:8 	 global-step:17788	 l-p:0.1408056616783142
epoch£º889	 i:9 	 global-step:17789	 l-p:0.1557157337665558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1639, 3.1639, 3.1639],
        [3.1639, 3.1640, 3.1640],
        [3.1639, 2.3990, 2.5194],
        [3.1639, 2.3587, 2.4538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): -0.006811938248574734 
model_pd.l_d.mean(): -25.02213478088379 
model_pd.lagr.mean(): -25.028945922851562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0233], device='cuda:0')), ('power', tensor([-24.9988], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:-0.006811938248574734
epoch£º890	 i:1 	 global-step:17801	 l-p:0.1411038041114807
epoch£º890	 i:2 	 global-step:17802	 l-p:0.14365413784980774
epoch£º890	 i:3 	 global-step:17803	 l-p:0.15304496884346008
epoch£º890	 i:4 	 global-step:17804	 l-p:0.17067331075668335
epoch£º890	 i:5 	 global-step:17805	 l-p:0.1557963639497757
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13046622276306152
epoch£º890	 i:7 	 global-step:17807	 l-p:0.1293908804655075
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10100434720516205
epoch£º890	 i:9 	 global-step:17809	 l-p:0.1501217782497406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1550, 3.0908, 3.1468],
        [3.1550, 1.9819, 1.3267],
        [3.1550, 2.9683, 3.1042],
        [3.1550, 3.1478, 3.1548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.15310300886631012 
model_pd.l_d.mean(): -24.583179473876953 
model_pd.lagr.mean(): -24.430076599121094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1156], device='cuda:0')), ('power', tensor([-24.6988], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.15310300886631012
epoch£º891	 i:1 	 global-step:17821	 l-p:0.1571521759033203
epoch£º891	 i:2 	 global-step:17822	 l-p:0.12467698752880096
epoch£º891	 i:3 	 global-step:17823	 l-p:0.14432616531848907
epoch£º891	 i:4 	 global-step:17824	 l-p:0.16740767657756805
epoch£º891	 i:5 	 global-step:17825	 l-p:0.16886229813098907
epoch£º891	 i:6 	 global-step:17826	 l-p:0.19832743704319
epoch£º891	 i:7 	 global-step:17827	 l-p:0.10381077229976654
epoch£º891	 i:8 	 global-step:17828	 l-p:0.13970285654067993
epoch£º891	 i:9 	 global-step:17829	 l-p:0.3239046335220337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1059, 2.1881, 2.1975],
        [3.1059, 3.1059, 3.1060],
        [3.1059, 1.7970, 1.1959],
        [3.1059, 3.0925, 3.1053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.1266970932483673 
model_pd.l_d.mean(): -24.940343856811523 
model_pd.lagr.mean(): -24.81364631652832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1338], device='cuda:0')), ('power', tensor([-25.0741], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.1266970932483673
epoch£º892	 i:1 	 global-step:17841	 l-p:0.12102553248405457
epoch£º892	 i:2 	 global-step:17842	 l-p:0.14576123654842377
epoch£º892	 i:3 	 global-step:17843	 l-p:0.18338824808597565
epoch£º892	 i:4 	 global-step:17844	 l-p:0.15987521409988403
epoch£º892	 i:5 	 global-step:17845	 l-p:-0.56847083568573
epoch£º892	 i:6 	 global-step:17846	 l-p:0.21660950779914856
epoch£º892	 i:7 	 global-step:17847	 l-p:0.012126097455620766
epoch£º892	 i:8 	 global-step:17848	 l-p:0.136091411113739
epoch£º892	 i:9 	 global-step:17849	 l-p:0.13761956989765167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0779, 1.8868, 1.2500],
        [3.0779, 2.1118, 1.4338],
        [3.0779, 3.0371, 3.0741],
        [3.0779, 2.3322, 2.4678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): -0.1683352291584015 
model_pd.l_d.mean(): -24.703157424926758 
model_pd.lagr.mean(): -24.871492385864258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2041], device='cuda:0')), ('power', tensor([-24.9073], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:-0.1683352291584015
epoch£º893	 i:1 	 global-step:17861	 l-p:0.034335192292928696
epoch£º893	 i:2 	 global-step:17862	 l-p:0.13429294526576996
epoch£º893	 i:3 	 global-step:17863	 l-p:0.12580472230911255
epoch£º893	 i:4 	 global-step:17864	 l-p:0.16755180060863495
epoch£º893	 i:5 	 global-step:17865	 l-p:0.1219009980559349
epoch£º893	 i:6 	 global-step:17866	 l-p:0.1178073063492775
epoch£º893	 i:7 	 global-step:17867	 l-p:0.15640035271644592
epoch£º893	 i:8 	 global-step:17868	 l-p:-0.11824837327003479
epoch£º893	 i:9 	 global-step:17869	 l-p:0.2501431107521057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0302, 3.0287, 3.0302],
        [3.0302, 3.0122, 3.0292],
        [3.0302, 1.7130, 1.1766],
        [3.0302, 1.8809, 1.6180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.13212931156158447 
model_pd.l_d.mean(): -24.963130950927734 
model_pd.lagr.mean(): -24.83100128173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0767], device='cuda:0')), ('power', tensor([-25.0398], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.13212931156158447
epoch£º894	 i:1 	 global-step:17881	 l-p:0.29543155431747437
epoch£º894	 i:2 	 global-step:17882	 l-p:0.1381242871284485
epoch£º894	 i:3 	 global-step:17883	 l-p:0.13037388026714325
epoch£º894	 i:4 	 global-step:17884	 l-p:0.03865521028637886
epoch£º894	 i:5 	 global-step:17885	 l-p:0.10000989586114883
epoch£º894	 i:6 	 global-step:17886	 l-p:0.10352134704589844
epoch£º894	 i:7 	 global-step:17887	 l-p:0.2180509865283966
epoch£º894	 i:8 	 global-step:17888	 l-p:0.16235007345676422
epoch£º894	 i:9 	 global-step:17889	 l-p:0.2015802562236786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0252, 3.0252, 3.0252],
        [3.0252, 2.9860, 3.0216],
        [3.0252, 1.9996, 1.3398],
        [3.0252, 1.8757, 1.6130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): -0.1544228196144104 
model_pd.l_d.mean(): -24.533954620361328 
model_pd.lagr.mean(): -24.688377380371094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2041], device='cuda:0')), ('power', tensor([-24.7381], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:-0.1544228196144104
epoch£º895	 i:1 	 global-step:17901	 l-p:0.11491949111223221
epoch£º895	 i:2 	 global-step:17902	 l-p:-0.6984850168228149
epoch£º895	 i:3 	 global-step:17903	 l-p:0.1389346718788147
epoch£º895	 i:4 	 global-step:17904	 l-p:0.14250022172927856
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11690494418144226
epoch£º895	 i:6 	 global-step:17906	 l-p:0.13534675538539886
epoch£º895	 i:7 	 global-step:17907	 l-p:4.962738990783691
epoch£º895	 i:8 	 global-step:17908	 l-p:0.15432168543338776
epoch£º895	 i:9 	 global-step:17909	 l-p:-0.13813672959804535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0898, 2.1277, 1.4473],
        [3.0898, 2.9341, 3.0529],
        [3.0898, 3.0898, 3.0898],
        [3.0898, 1.9656, 1.7345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.1325712502002716 
model_pd.l_d.mean(): -25.02810287475586 
model_pd.lagr.mean(): -24.895530700683594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0513], device='cuda:0')), ('power', tensor([-25.0794], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.1325712502002716
epoch£º896	 i:1 	 global-step:17921	 l-p:-0.817596435546875
epoch£º896	 i:2 	 global-step:17922	 l-p:0.4508826732635498
epoch£º896	 i:3 	 global-step:17923	 l-p:0.17007610201835632
epoch£º896	 i:4 	 global-step:17924	 l-p:0.14461877942085266
epoch£º896	 i:5 	 global-step:17925	 l-p:0.1607189178466797
epoch£º896	 i:6 	 global-step:17926	 l-p:0.15797506272792816
epoch£º896	 i:7 	 global-step:17927	 l-p:0.1387927085161209
epoch£º896	 i:8 	 global-step:17928	 l-p:0.07766108214855194
epoch£º896	 i:9 	 global-step:17929	 l-p:0.19646114110946655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1449, 3.0065, 3.1147],
        [3.1449, 3.1314, 3.1443],
        [3.1449, 3.0016, 3.1128],
        [3.1449, 2.1875, 2.1566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.13590848445892334 
model_pd.l_d.mean(): -25.099811553955078 
model_pd.lagr.mean(): -24.963903427124023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0571], device='cuda:0')), ('power', tensor([-25.0427], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.13590848445892334
epoch£º897	 i:1 	 global-step:17941	 l-p:0.15952390432357788
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11467331647872925
epoch£º897	 i:3 	 global-step:17943	 l-p:0.15861739218235016
epoch£º897	 i:4 	 global-step:17944	 l-p:0.12619149684906006
epoch£º897	 i:5 	 global-step:17945	 l-p:0.12487327307462692
epoch£º897	 i:6 	 global-step:17946	 l-p:0.1443541944026947
epoch£º897	 i:7 	 global-step:17947	 l-p:-0.021885156631469727
epoch£º897	 i:8 	 global-step:17948	 l-p:0.14800159633159637
epoch£º897	 i:9 	 global-step:17949	 l-p:0.15796926617622375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1680, 2.2255, 2.2081],
        [3.1680, 3.1660, 3.1680],
        [3.1680, 3.0298, 3.1378],
        [3.1680, 2.2561, 1.5580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.12574683129787445 
model_pd.l_d.mean(): -24.733448028564453 
model_pd.lagr.mean(): -24.60770034790039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0221], device='cuda:0')), ('power', tensor([-24.7556], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.12574683129787445
epoch£º898	 i:1 	 global-step:17961	 l-p:0.13241668045520782
epoch£º898	 i:2 	 global-step:17962	 l-p:0.16707687079906464
epoch£º898	 i:3 	 global-step:17963	 l-p:0.14078350365161896
epoch£º898	 i:4 	 global-step:17964	 l-p:0.20516861975193024
epoch£º898	 i:5 	 global-step:17965	 l-p:0.06787639111280441
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10051295906305313
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14059549570083618
epoch£º898	 i:8 	 global-step:17968	 l-p:0.18205195665359497
epoch£º898	 i:9 	 global-step:17969	 l-p:0.1020258367061615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1455, 1.8450, 1.2270],
        [3.1455, 1.8657, 1.4019],
        [3.1455, 2.2024, 2.1858],
        [3.1455, 3.1424, 3.1455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.1376420259475708 
model_pd.l_d.mean(): -24.897310256958008 
model_pd.lagr.mean(): -24.759668350219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0353], device='cuda:0')), ('power', tensor([-24.8620], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.1376420259475708
epoch£º899	 i:1 	 global-step:17981	 l-p:0.14419521391391754
epoch£º899	 i:2 	 global-step:17982	 l-p:0.19588814675807953
epoch£º899	 i:3 	 global-step:17983	 l-p:0.12094415724277496
epoch£º899	 i:4 	 global-step:17984	 l-p:0.1218828335404396
epoch£º899	 i:5 	 global-step:17985	 l-p:0.15467149019241333
epoch£º899	 i:6 	 global-step:17986	 l-p:0.1417623609304428
epoch£º899	 i:7 	 global-step:17987	 l-p:0.2769281268119812
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12648575007915497
epoch£º899	 i:9 	 global-step:17989	 l-p:0.12817855179309845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0882, 1.9600, 1.7236],
        [3.0882, 2.7309, 2.9293],
        [3.0882, 3.0881, 3.0882],
        [3.0882, 2.7554, 2.9483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.1471453756093979 
model_pd.l_d.mean(): -25.195545196533203 
model_pd.lagr.mean(): -25.048398971557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0029], device='cuda:0')), ('power', tensor([-25.1984], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.1471453756093979
epoch£º900	 i:1 	 global-step:18001	 l-p:0.2802444100379944
epoch£º900	 i:2 	 global-step:18002	 l-p:0.1480454057455063
epoch£º900	 i:3 	 global-step:18003	 l-p:0.15521399676799774
epoch£º900	 i:4 	 global-step:18004	 l-p:0.08269944787025452
epoch£º900	 i:5 	 global-step:18005	 l-p:0.11297965794801712
epoch£º900	 i:6 	 global-step:18006	 l-p:0.08643753826618195
epoch£º900	 i:7 	 global-step:18007	 l-p:0.12615440785884857
epoch£º900	 i:8 	 global-step:18008	 l-p:-0.36297744512557983
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11937715113162994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0251, 2.8925, 2.9972],
        [3.0251, 3.0121, 3.0245],
        [3.0251, 1.8716, 1.2368],
        [3.0251, 2.1499, 2.2014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.18673373758792877 
model_pd.l_d.mean(): -25.097858428955078 
model_pd.lagr.mean(): -24.91112518310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0850], device='cuda:0')), ('power', tensor([-25.1829], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.18673373758792877
epoch£º901	 i:1 	 global-step:18021	 l-p:0.1435364931821823
epoch£º901	 i:2 	 global-step:18022	 l-p:0.08139103651046753
epoch£º901	 i:3 	 global-step:18023	 l-p:0.0627162754535675
epoch£º901	 i:4 	 global-step:18024	 l-p:0.13560055196285248
epoch£º901	 i:5 	 global-step:18025	 l-p:0.11039633303880692
epoch£º901	 i:6 	 global-step:18026	 l-p:0.15977627038955688
epoch£º901	 i:7 	 global-step:18027	 l-p:18.293746948242188
epoch£º901	 i:8 	 global-step:18028	 l-p:0.15503773093223572
epoch£º901	 i:9 	 global-step:18029	 l-p:0.21918082237243652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0491, 3.0398, 3.0488],
        [3.0491, 2.1561, 2.1917],
        [3.0491, 1.7453, 1.1552],
        [3.0491, 2.8162, 2.9748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.13387465476989746 
model_pd.l_d.mean(): -24.765962600708008 
model_pd.lagr.mean(): -24.63208770751953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0527], device='cuda:0')), ('power', tensor([-24.8187], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.13387465476989746
epoch£º902	 i:1 	 global-step:18041	 l-p:0.1433926373720169
epoch£º902	 i:2 	 global-step:18042	 l-p:-0.7135708332061768
epoch£º902	 i:3 	 global-step:18043	 l-p:0.14950551092624664
epoch£º902	 i:4 	 global-step:18044	 l-p:0.2508707642555237
epoch£º902	 i:5 	 global-step:18045	 l-p:0.14611509442329407
epoch£º902	 i:6 	 global-step:18046	 l-p:-0.008607936091721058
epoch£º902	 i:7 	 global-step:18047	 l-p:0.1794978231191635
epoch£º902	 i:8 	 global-step:18048	 l-p:-0.018267039209604263
epoch£º902	 i:9 	 global-step:18049	 l-p:0.10616912692785263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0986, 2.9504, 3.0647],
        [3.0986, 2.0716, 1.4006],
        [3.0986, 2.4524, 2.6310],
        [3.0986, 2.6045, 2.8130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.45974135398864746 
model_pd.l_d.mean(): -25.021663665771484 
model_pd.lagr.mean(): -24.561922073364258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1036], device='cuda:0')), ('power', tensor([-25.1253], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.45974135398864746
epoch£º903	 i:1 	 global-step:18061	 l-p:0.11877725273370743
epoch£º903	 i:2 	 global-step:18062	 l-p:0.1629151850938797
epoch£º903	 i:3 	 global-step:18063	 l-p:0.16442561149597168
epoch£º903	 i:4 	 global-step:18064	 l-p:0.11855493485927582
epoch£º903	 i:5 	 global-step:18065	 l-p:0.1430554986000061
epoch£º903	 i:6 	 global-step:18066	 l-p:0.13082461059093475
epoch£º903	 i:7 	 global-step:18067	 l-p:0.17087499797344208
epoch£º903	 i:8 	 global-step:18068	 l-p:0.12409447133541107
epoch£º903	 i:9 	 global-step:18069	 l-p:0.14230060577392578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1697, 3.0921, 3.1584],
        [3.1697, 2.5392, 2.7206],
        [3.1697, 3.1697, 3.1697],
        [3.1697, 3.1569, 3.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.11173100024461746 
model_pd.l_d.mean(): -24.625659942626953 
model_pd.lagr.mean(): -24.51392936706543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0519], device='cuda:0')), ('power', tensor([-24.6776], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.11173100024461746
epoch£º904	 i:1 	 global-step:18081	 l-p:0.13393090665340424
epoch£º904	 i:2 	 global-step:18082	 l-p:0.13648854196071625
epoch£º904	 i:3 	 global-step:18083	 l-p:0.1377790868282318
epoch£º904	 i:4 	 global-step:18084	 l-p:0.12599186599254608
epoch£º904	 i:5 	 global-step:18085	 l-p:0.20611251890659332
epoch£º904	 i:6 	 global-step:18086	 l-p:0.19761693477630615
epoch£º904	 i:7 	 global-step:18087	 l-p:0.1185968816280365
epoch£º904	 i:8 	 global-step:18088	 l-p:0.2481696605682373
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12527434527873993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1197, 3.1173, 3.1197],
        [3.1197, 2.7627, 2.9608],
        [3.1197, 1.8742, 1.4671],
        [3.1197, 2.1339, 2.0760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.13505741953849792 
model_pd.l_d.mean(): -25.060592651367188 
model_pd.lagr.mean(): -24.925535202026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0859], device='cuda:0')), ('power', tensor([-24.9747], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.13505741953849792
epoch£º905	 i:1 	 global-step:18101	 l-p:0.12446726858615875
epoch£º905	 i:2 	 global-step:18102	 l-p:0.09832403063774109
epoch£º905	 i:3 	 global-step:18103	 l-p:0.4802279472351074
epoch£º905	 i:4 	 global-step:18104	 l-p:0.18828918039798737
epoch£º905	 i:5 	 global-step:18105	 l-p:0.14470380544662476
epoch£º905	 i:6 	 global-step:18106	 l-p:0.2646075487136841
epoch£º905	 i:7 	 global-step:18107	 l-p:0.12802177667617798
epoch£º905	 i:8 	 global-step:18108	 l-p:0.12607887387275696
epoch£º905	 i:9 	 global-step:18109	 l-p:0.14465875923633575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1048, 1.7850, 1.2520],
        [3.1048, 1.7803, 1.2316],
        [3.1048, 1.9324, 1.2860],
        [3.1048, 3.1037, 3.1048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.38964447379112244 
model_pd.l_d.mean(): -24.946754455566406 
model_pd.lagr.mean(): -24.557109832763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0064], device='cuda:0')), ('power', tensor([-24.9404], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.38964447379112244
epoch£º906	 i:1 	 global-step:18121	 l-p:0.18335594236850739
epoch£º906	 i:2 	 global-step:18122	 l-p:0.13181467354297638
epoch£º906	 i:3 	 global-step:18123	 l-p:0.3469732701778412
epoch£º906	 i:4 	 global-step:18124	 l-p:0.10396537184715271
epoch£º906	 i:5 	 global-step:18125	 l-p:0.14594024419784546
epoch£º906	 i:6 	 global-step:18126	 l-p:0.13616517186164856
epoch£º906	 i:7 	 global-step:18127	 l-p:0.30513450503349304
epoch£º906	 i:8 	 global-step:18128	 l-p:0.11225628852844238
epoch£º906	 i:9 	 global-step:18129	 l-p:0.1511579304933548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0982, 3.0142, 3.0853],
        [3.0982, 2.6831, 2.8902],
        [3.0982, 2.8472, 3.0134],
        [3.0982, 1.7823, 1.2598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.5522819757461548 
model_pd.l_d.mean(): -24.735143661499023 
model_pd.lagr.mean(): -24.182861328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0051], device='cuda:0')), ('power', tensor([-24.7403], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.5522819757461548
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12988808751106262
epoch£º907	 i:2 	 global-step:18142	 l-p:-0.3045256435871124
epoch£º907	 i:3 	 global-step:18143	 l-p:0.12882742285728455
epoch£º907	 i:4 	 global-step:18144	 l-p:0.1852070391178131
epoch£º907	 i:5 	 global-step:18145	 l-p:1.914354920387268
epoch£º907	 i:6 	 global-step:18146	 l-p:0.13686983287334442
epoch£º907	 i:7 	 global-step:18147	 l-p:0.10225799679756165
epoch£º907	 i:8 	 global-step:18148	 l-p:0.149363175034523
epoch£º907	 i:9 	 global-step:18149	 l-p:0.12869830429553986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[3.0711, 2.0275, 1.3633],
        [3.0711, 2.4052, 2.5781],
        [3.0711, 1.7471, 1.1884],
        [3.0711, 1.7476, 1.1972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.013450541533529758 
model_pd.l_d.mean(): -24.647125244140625 
model_pd.lagr.mean(): -24.63367462158203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1791], device='cuda:0')), ('power', tensor([-24.8262], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.013450541533529758
epoch£º908	 i:1 	 global-step:18161	 l-p:0.3271961212158203
epoch£º908	 i:2 	 global-step:18162	 l-p:0.06520351767539978
epoch£º908	 i:3 	 global-step:18163	 l-p:0.13333997130393982
epoch£º908	 i:4 	 global-step:18164	 l-p:0.17661911249160767
epoch£º908	 i:5 	 global-step:18165	 l-p:0.004781680181622505
epoch£º908	 i:6 	 global-step:18166	 l-p:0.1671060174703598
epoch£º908	 i:7 	 global-step:18167	 l-p:0.13395896553993225
epoch£º908	 i:8 	 global-step:18168	 l-p:0.1441749781370163
epoch£º908	 i:9 	 global-step:18169	 l-p:0.20967967808246613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0662, 3.0237, 3.0621],
        [3.0662, 3.0662, 3.0662],
        [3.0662, 1.7705, 1.1688],
        [3.0662, 3.0662, 3.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.139642134308815 
model_pd.l_d.mean(): -25.049930572509766 
model_pd.lagr.mean(): -24.910287857055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0077], device='cuda:0')), ('power', tensor([-25.0577], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.139642134308815
epoch£º909	 i:1 	 global-step:18181	 l-p:0.23050183057785034
epoch£º909	 i:2 	 global-step:18182	 l-p:0.12094851583242416
epoch£º909	 i:3 	 global-step:18183	 l-p:0.20009185373783112
epoch£º909	 i:4 	 global-step:18184	 l-p:0.13413186371326447
epoch£º909	 i:5 	 global-step:18185	 l-p:0.06476739048957825
epoch£º909	 i:6 	 global-step:18186	 l-p:0.18159474432468414
epoch£º909	 i:7 	 global-step:18187	 l-p:0.6895272135734558
epoch£º909	 i:8 	 global-step:18188	 l-p:0.04251248389482498
epoch£º909	 i:9 	 global-step:18189	 l-p:0.12978368997573853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0795, 3.0795, 3.0795],
        [3.0795, 3.0784, 3.0795],
        [3.0795, 3.0795, 3.0795],
        [3.0795, 1.7600, 1.2294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.1522912234067917 
model_pd.l_d.mean(): -24.70893669128418 
model_pd.lagr.mean(): -24.5566463470459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1347], device='cuda:0')), ('power', tensor([-24.8437], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.1522912234067917
epoch£º910	 i:1 	 global-step:18201	 l-p:-0.1075446680188179
epoch£º910	 i:2 	 global-step:18202	 l-p:0.18193857371807098
epoch£º910	 i:3 	 global-step:18203	 l-p:0.13548962771892548
epoch£º910	 i:4 	 global-step:18204	 l-p:0.14328597486019135
epoch£º910	 i:5 	 global-step:18205	 l-p:0.23308174312114716
epoch£º910	 i:6 	 global-step:18206	 l-p:0.10594504326581955
epoch£º910	 i:7 	 global-step:18207	 l-p:0.11259669065475464
epoch£º910	 i:8 	 global-step:18208	 l-p:0.15302674472332
epoch£º910	 i:9 	 global-step:18209	 l-p:0.21174976229667664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1369, 2.4249, 2.5763],
        [3.1369, 3.1368, 3.1369],
        [3.1369, 1.8223, 1.2148],
        [3.1369, 2.2963, 2.3706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.16693462431430817 
model_pd.l_d.mean(): -25.14790153503418 
model_pd.lagr.mean(): -24.980966567993164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0463], device='cuda:0')), ('power', tensor([-25.1016], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.16693462431430817
epoch£º911	 i:1 	 global-step:18221	 l-p:0.15471702814102173
epoch£º911	 i:2 	 global-step:18222	 l-p:0.11949654668569565
epoch£º911	 i:3 	 global-step:18223	 l-p:0.12405192852020264
epoch£º911	 i:4 	 global-step:18224	 l-p:0.12402255833148956
epoch£º911	 i:5 	 global-step:18225	 l-p:0.13899894058704376
epoch£º911	 i:6 	 global-step:18226	 l-p:0.21847155690193176
epoch£º911	 i:7 	 global-step:18227	 l-p:0.12403666228055954
epoch£º911	 i:8 	 global-step:18228	 l-p:0.1478053480386734
epoch£º911	 i:9 	 global-step:18229	 l-p:0.11300969868898392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1330, 1.9729, 1.3185],
        [3.1330, 2.8944, 3.0553],
        [3.1330, 1.8456, 1.3751],
        [3.1330, 3.1175, 3.1322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.16008321940898895 
model_pd.l_d.mean(): -24.711694717407227 
model_pd.lagr.mean(): -24.551610946655273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1732], device='cuda:0')), ('power', tensor([-24.8849], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.16008321940898895
epoch£º912	 i:1 	 global-step:18241	 l-p:0.21224834024906158
epoch£º912	 i:2 	 global-step:18242	 l-p:0.1892361342906952
epoch£º912	 i:3 	 global-step:18243	 l-p:0.14918923377990723
epoch£º912	 i:4 	 global-step:18244	 l-p:0.09883634001016617
epoch£º912	 i:5 	 global-step:18245	 l-p:0.08919328451156616
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13470597565174103
epoch£º912	 i:7 	 global-step:18247	 l-p:0.16492608189582825
epoch£º912	 i:8 	 global-step:18248	 l-p:0.13634251058101654
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12658706307411194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1425, 3.1423, 3.1425],
        [3.1425, 1.8547, 1.3827],
        [3.1425, 3.1411, 3.1424],
        [3.1425, 1.8124, 1.2479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.09975776821374893 
model_pd.l_d.mean(): -24.90962028503418 
model_pd.lagr.mean(): -24.80986213684082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0385], device='cuda:0')), ('power', tensor([-24.9481], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.09975776821374893
epoch£º913	 i:1 	 global-step:18261	 l-p:0.10564478486776352
epoch£º913	 i:2 	 global-step:18262	 l-p:0.12398391216993332
epoch£º913	 i:3 	 global-step:18263	 l-p:0.13181127607822418
epoch£º913	 i:4 	 global-step:18264	 l-p:0.25517332553863525
epoch£º913	 i:5 	 global-step:18265	 l-p:0.13916783034801483
epoch£º913	 i:6 	 global-step:18266	 l-p:0.14701291918754578
epoch£º913	 i:7 	 global-step:18267	 l-p:0.1979297548532486
epoch£º913	 i:8 	 global-step:18268	 l-p:0.13909532129764557
epoch£º913	 i:9 	 global-step:18269	 l-p:-0.622081995010376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0791, 2.0214, 1.8832],
        [3.0791, 3.0789, 3.0791],
        [3.0791, 2.9228, 3.0421],
        [3.0791, 1.8996, 1.5932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.13103654980659485 
model_pd.l_d.mean(): -24.94504165649414 
model_pd.lagr.mean(): -24.81400489807129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0876], device='cuda:0')), ('power', tensor([-25.0326], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.13103654980659485
epoch£º914	 i:1 	 global-step:18281	 l-p:0.16854219138622284
epoch£º914	 i:2 	 global-step:18282	 l-p:0.16550332307815552
epoch£º914	 i:3 	 global-step:18283	 l-p:0.13568149507045746
epoch£º914	 i:4 	 global-step:18284	 l-p:0.06811820715665817
epoch£º914	 i:5 	 global-step:18285	 l-p:0.15344753861427307
epoch£º914	 i:6 	 global-step:18286	 l-p:0.1591743677854538
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12801681458950043
epoch£º914	 i:8 	 global-step:18288	 l-p:0.2431144267320633
epoch£º914	 i:9 	 global-step:18289	 l-p:0.15594138205051422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0622, 2.8243, 2.9852],
        [3.0622, 3.0557, 3.0620],
        [3.0622, 3.0622, 3.0622],
        [3.0622, 3.0486, 3.0615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.4427693784236908 
model_pd.l_d.mean(): -25.198453903198242 
model_pd.lagr.mean(): -24.75568389892578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0459], device='cuda:0')), ('power', tensor([-25.2444], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.4427693784236908
epoch£º915	 i:1 	 global-step:18301	 l-p:0.13024185597896576
epoch£º915	 i:2 	 global-step:18302	 l-p:0.20732669532299042
epoch£º915	 i:3 	 global-step:18303	 l-p:0.13761095702648163
epoch£º915	 i:4 	 global-step:18304	 l-p:0.1142287403345108
epoch£º915	 i:5 	 global-step:18305	 l-p:0.1440165489912033
epoch£º915	 i:6 	 global-step:18306	 l-p:0.15937212109565735
epoch£º915	 i:7 	 global-step:18307	 l-p:-0.024279411882162094
epoch£º915	 i:8 	 global-step:18308	 l-p:0.10656020790338516
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12549327313899994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0884, 2.1755, 1.4863],
        [3.0884, 3.0884, 3.0884],
        [3.0884, 3.0103, 3.0770],
        [3.0884, 1.7698, 1.2442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13421379029750824 
model_pd.l_d.mean(): -24.90347671508789 
model_pd.lagr.mean(): -24.769262313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0024], device='cuda:0')), ('power', tensor([-24.9010], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13421379029750824
epoch£º916	 i:1 	 global-step:18321	 l-p:-0.10061769932508469
epoch£º916	 i:2 	 global-step:18322	 l-p:-0.26577016711235046
epoch£º916	 i:3 	 global-step:18323	 l-p:0.17517395317554474
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12307882308959961
epoch£º916	 i:5 	 global-step:18325	 l-p:0.1731974333524704
epoch£º916	 i:6 	 global-step:18326	 l-p:0.16785430908203125
epoch£º916	 i:7 	 global-step:18327	 l-p:0.1313633769750595
epoch£º916	 i:8 	 global-step:18328	 l-p:0.12263553589582443
epoch£º916	 i:9 	 global-step:18329	 l-p:0.19418348371982574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1276, 2.9953, 3.0997],
        [3.1276, 3.1276, 3.1276],
        [3.1276, 2.0797, 1.4069],
        [3.1276, 2.9087, 3.0609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12695005536079407 
model_pd.l_d.mean(): -25.17479705810547 
model_pd.lagr.mean(): -25.047847747802734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1246], device='cuda:0')), ('power', tensor([-25.0502], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12695005536079407
epoch£º917	 i:1 	 global-step:18341	 l-p:0.19425010681152344
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12009872496128082
epoch£º917	 i:3 	 global-step:18343	 l-p:0.21753835678100586
epoch£º917	 i:4 	 global-step:18344	 l-p:0.12995152175426483
epoch£º917	 i:5 	 global-step:18345	 l-p:0.11867164075374603
epoch£º917	 i:6 	 global-step:18346	 l-p:0.1792612075805664
epoch£º917	 i:7 	 global-step:18347	 l-p:0.14134317636489868
epoch£º917	 i:8 	 global-step:18348	 l-p:0.09112651646137238
epoch£º917	 i:9 	 global-step:18349	 l-p:0.2009427547454834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1334, 3.1334, 3.1334],
        [3.1334, 3.0919, 3.1294],
        [3.1334, 3.1334, 3.1334],
        [3.1334, 1.8162, 1.2115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.16815254092216492 
model_pd.l_d.mean(): -24.309436798095703 
model_pd.lagr.mean(): -24.141284942626953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1788], device='cuda:0')), ('power', tensor([-24.4882], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.16815254092216492
epoch£º918	 i:1 	 global-step:18361	 l-p:0.18260227143764496
epoch£º918	 i:2 	 global-step:18362	 l-p:0.11212564259767532
epoch£º918	 i:3 	 global-step:18363	 l-p:0.15066756308078766
epoch£º918	 i:4 	 global-step:18364	 l-p:0.14542259275913239
epoch£º918	 i:5 	 global-step:18365	 l-p:0.13098767399787903
epoch£º918	 i:6 	 global-step:18366	 l-p:0.12950481474399567
epoch£º918	 i:7 	 global-step:18367	 l-p:0.1418544203042984
epoch£º918	 i:8 	 global-step:18368	 l-p:0.14483720064163208
epoch£º918	 i:9 	 global-step:18369	 l-p:0.11608646810054779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[3.1396, 2.1724, 1.4851],
        [3.1396, 1.8704, 1.2393],
        [3.1396, 1.9418, 1.2932],
        [3.1396, 2.4156, 2.5615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.13393788039684296 
model_pd.l_d.mean(): -24.70935821533203 
model_pd.lagr.mean(): -24.575420379638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0518], device='cuda:0')), ('power', tensor([-24.7612], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.13393788039684296
epoch£º919	 i:1 	 global-step:18381	 l-p:0.1441403329372406
epoch£º919	 i:2 	 global-step:18382	 l-p:0.14384202659130096
epoch£º919	 i:3 	 global-step:18383	 l-p:0.11244344711303711
epoch£º919	 i:4 	 global-step:18384	 l-p:0.14804527163505554
epoch£º919	 i:5 	 global-step:18385	 l-p:0.12113017588853836
epoch£º919	 i:6 	 global-step:18386	 l-p:0.10234133154153824
epoch£º919	 i:7 	 global-step:18387	 l-p:0.11974108219146729
epoch£º919	 i:8 	 global-step:18388	 l-p:0.08591929078102112
epoch£º919	 i:9 	 global-step:18389	 l-p:0.0500500425696373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0092, 1.6903, 1.1517],
        [3.0092, 3.0091, 3.0092],
        [3.0092, 2.2573, 2.3945],
        [3.0092, 2.0550, 2.0384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.14400357007980347 
model_pd.l_d.mean(): -24.900489807128906 
model_pd.lagr.mean(): -24.756486892700195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1141], device='cuda:0')), ('power', tensor([-25.0146], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.14400357007980347
epoch£º920	 i:1 	 global-step:18401	 l-p:0.0980428084731102
epoch£º920	 i:2 	 global-step:18402	 l-p:0.15469707548618317
epoch£º920	 i:3 	 global-step:18403	 l-p:0.04323337599635124
epoch£º920	 i:4 	 global-step:18404	 l-p:0.12470702826976776
epoch£º920	 i:5 	 global-step:18405	 l-p:1.3760079145431519
epoch£º920	 i:6 	 global-step:18406	 l-p:0.1432352364063263
epoch£º920	 i:7 	 global-step:18407	 l-p:0.10842601209878922
epoch£º920	 i:8 	 global-step:18408	 l-p:0.18709717690944672
epoch£º920	 i:9 	 global-step:18409	 l-p:0.09856054931879044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0196, 3.0196, 3.0196],
        [3.0196, 1.7259, 1.1344],
        [3.0196, 2.7782, 2.9409],
        [3.0196, 3.0196, 3.0196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.14545294642448425 
model_pd.l_d.mean(): -24.636186599731445 
model_pd.lagr.mean(): -24.490734100341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1768], device='cuda:0')), ('power', tensor([-24.8130], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.14545294642448425
epoch£º921	 i:1 	 global-step:18421	 l-p:0.11175111681222916
epoch£º921	 i:2 	 global-step:18422	 l-p:0.16224035620689392
epoch£º921	 i:3 	 global-step:18423	 l-p:0.03328004106879234
epoch£º921	 i:4 	 global-step:18424	 l-p:0.1798480749130249
epoch£º921	 i:5 	 global-step:18425	 l-p:0.2552652955055237
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12827453017234802
epoch£º921	 i:7 	 global-step:18427	 l-p:0.12597963213920593
epoch£º921	 i:8 	 global-step:18428	 l-p:-0.04894717037677765
epoch£º921	 i:9 	 global-step:18429	 l-p:0.14430780708789825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0844, 3.0844, 3.0844],
        [3.0844, 2.4178, 2.5908],
        [3.0844, 2.0554, 1.3861],
        [3.0844, 1.9215, 1.2766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.14480051398277283 
model_pd.l_d.mean(): -25.059932708740234 
model_pd.lagr.mean(): -24.915132522583008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0039], device='cuda:0')), ('power', tensor([-25.0638], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.14480051398277283
epoch£º922	 i:1 	 global-step:18441	 l-p:-0.4679320454597473
epoch£º922	 i:2 	 global-step:18442	 l-p:0.13310670852661133
epoch£º922	 i:3 	 global-step:18443	 l-p:0.10532115399837494
epoch£º922	 i:4 	 global-step:18444	 l-p:-0.1991589069366455
epoch£º922	 i:5 	 global-step:18445	 l-p:0.16771146655082703
epoch£º922	 i:6 	 global-step:18446	 l-p:0.1600249856710434
epoch£º922	 i:7 	 global-step:18447	 l-p:0.14923939108848572
epoch£º922	 i:8 	 global-step:18448	 l-p:0.4433076083660126
epoch£º922	 i:9 	 global-step:18449	 l-p:0.16268593072891235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1164, 1.7910, 1.2474],
        [3.1164, 2.2425, 2.2929],
        [3.1164, 2.6131, 2.8214],
        [3.1164, 3.1091, 3.1162]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.2277025580406189 
model_pd.l_d.mean(): -25.039705276489258 
model_pd.lagr.mean(): -24.812002182006836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0387], device='cuda:0')), ('power', tensor([-25.0010], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:0.2277025580406189
epoch£º923	 i:1 	 global-step:18461	 l-p:0.15038228034973145
epoch£º923	 i:2 	 global-step:18462	 l-p:0.15826816856861115
epoch£º923	 i:3 	 global-step:18463	 l-p:0.12693241238594055
epoch£º923	 i:4 	 global-step:18464	 l-p:0.1767660677433014
epoch£º923	 i:5 	 global-step:18465	 l-p:0.1270037740468979
epoch£º923	 i:6 	 global-step:18466	 l-p:0.10052622854709625
epoch£º923	 i:7 	 global-step:18467	 l-p:0.13722869753837585
epoch£º923	 i:8 	 global-step:18468	 l-p:0.17158202826976776
epoch£º923	 i:9 	 global-step:18469	 l-p:0.14619144797325134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1415, 2.4136, 2.5577],
        [3.1415, 1.8119, 1.2536],
        [3.1415, 2.9545, 3.0908],
        [3.1415, 2.6355, 2.8431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.23509852588176727 
model_pd.l_d.mean(): -24.962482452392578 
model_pd.lagr.mean(): -24.727384567260742 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0323], device='cuda:0')), ('power', tensor([-24.9948], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.23509852588176727
epoch£º924	 i:1 	 global-step:18481	 l-p:0.10170549899339676
epoch£º924	 i:2 	 global-step:18482	 l-p:0.13390570878982544
epoch£º924	 i:3 	 global-step:18483	 l-p:0.12708406150341034
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12328546494245529
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12634341418743134
epoch£º924	 i:6 	 global-step:18486	 l-p:0.15733098983764648
epoch£º924	 i:7 	 global-step:18487	 l-p:0.161599263548851
epoch£º924	 i:8 	 global-step:18488	 l-p:0.13987047970294952
epoch£º924	 i:9 	 global-step:18489	 l-p:0.22231586277484894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1114, 3.1111, 3.1114],
        [3.1114, 1.8589, 1.2288],
        [3.1114, 1.7901, 1.1963],
        [3.1114, 1.7862, 1.2435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.15137724578380585 
model_pd.l_d.mean(): -25.08390235900879 
model_pd.lagr.mean(): -24.932525634765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0122], device='cuda:0')), ('power', tensor([-25.0961], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.15137724578380585
epoch£º925	 i:1 	 global-step:18501	 l-p:0.25085020065307617
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12457965314388275
epoch£º925	 i:3 	 global-step:18503	 l-p:0.14711974561214447
epoch£º925	 i:4 	 global-step:18504	 l-p:0.10905109345912933
epoch£º925	 i:5 	 global-step:18505	 l-p:1.124672770500183
epoch£º925	 i:6 	 global-step:18506	 l-p:0.13189959526062012
epoch£º925	 i:7 	 global-step:18507	 l-p:0.12557080388069153
epoch£º925	 i:8 	 global-step:18508	 l-p:0.13243989646434784
epoch£º925	 i:9 	 global-step:18509	 l-p:0.25436416268348694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0756, 3.0755, 3.0756],
        [3.0756, 1.9470, 1.7157],
        [3.0756, 1.9216, 1.6535],
        [3.0756, 1.7899, 1.1796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.1329547017812729 
model_pd.l_d.mean(): -24.866764068603516 
model_pd.lagr.mean(): -24.733808517456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0079], device='cuda:0')), ('power', tensor([-24.8588], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.1329547017812729
epoch£º926	 i:1 	 global-step:18521	 l-p:0.1319459229707718
epoch£º926	 i:2 	 global-step:18522	 l-p:0.0065318201668560505
epoch£º926	 i:3 	 global-step:18523	 l-p:0.2736177444458008
epoch£º926	 i:4 	 global-step:18524	 l-p:0.16633468866348267
epoch£º926	 i:5 	 global-step:18525	 l-p:-0.07616967707872391
epoch£º926	 i:6 	 global-step:18526	 l-p:-0.27738338708877563
epoch£º926	 i:7 	 global-step:18527	 l-p:0.11164304614067078
epoch£º926	 i:8 	 global-step:18528	 l-p:0.15995292365550995
epoch£º926	 i:9 	 global-step:18529	 l-p:0.1715371310710907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1162, 3.1143, 3.1162],
        [3.1162, 2.8787, 3.0394],
        [3.1162, 3.1040, 3.1157],
        [3.1162, 2.8378, 3.0144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.1263602375984192 
model_pd.l_d.mean(): -24.40254020690918 
model_pd.lagr.mean(): -24.276180267333984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0810], device='cuda:0')), ('power', tensor([-24.4835], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.1263602375984192
epoch£º927	 i:1 	 global-step:18541	 l-p:0.23038668930530548
epoch£º927	 i:2 	 global-step:18542	 l-p:0.12902718782424927
epoch£º927	 i:3 	 global-step:18543	 l-p:0.1287030130624771
epoch£º927	 i:4 	 global-step:18544	 l-p:0.40893644094467163
epoch£º927	 i:5 	 global-step:18545	 l-p:0.1780962496995926
epoch£º927	 i:6 	 global-step:18546	 l-p:0.15348102152347565
epoch£º927	 i:7 	 global-step:18547	 l-p:0.1558019071817398
epoch£º927	 i:8 	 global-step:18548	 l-p:0.25765350461006165
epoch£º927	 i:9 	 global-step:18549	 l-p:0.11948484182357788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1159, 3.1159, 3.1159],
        [3.1159, 2.7633, 2.9610],
        [3.1159, 3.1158, 3.1159],
        [3.1159, 3.1106, 3.1157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.20029316842556 
model_pd.l_d.mean(): -25.018091201782227 
model_pd.lagr.mean(): -24.817798614501953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0453], device='cuda:0')), ('power', tensor([-24.9728], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.20029316842556
epoch£º928	 i:1 	 global-step:18561	 l-p:0.15243519842624664
epoch£º928	 i:2 	 global-step:18562	 l-p:0.1471424102783203
epoch£º928	 i:3 	 global-step:18563	 l-p:0.11787616461515427
epoch£º928	 i:4 	 global-step:18564	 l-p:0.152296781539917
epoch£º928	 i:5 	 global-step:18565	 l-p:0.09608442336320877
epoch£º928	 i:6 	 global-step:18566	 l-p:0.20802241563796997
epoch£º928	 i:7 	 global-step:18567	 l-p:0.17151889204978943
epoch£º928	 i:8 	 global-step:18568	 l-p:0.13117654621601105
epoch£º928	 i:9 	 global-step:18569	 l-p:0.1514158844947815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1599, 3.1599, 3.1599],
        [3.1599, 3.0090, 3.1249],
        [3.1599, 1.9615, 1.6204],
        [3.1599, 3.1599, 3.1599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.12167850881814957 
model_pd.l_d.mean(): -25.085268020629883 
model_pd.lagr.mean(): -24.96358871459961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0707], device='cuda:0')), ('power', tensor([-25.0145], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.12167850881814957
epoch£º929	 i:1 	 global-step:18581	 l-p:0.170744851231575
epoch£º929	 i:2 	 global-step:18582	 l-p:0.1473415195941925
epoch£º929	 i:3 	 global-step:18583	 l-p:0.1449202001094818
epoch£º929	 i:4 	 global-step:18584	 l-p:0.17811904847621918
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13294464349746704
epoch£º929	 i:6 	 global-step:18586	 l-p:0.1383676826953888
epoch£º929	 i:7 	 global-step:18587	 l-p:0.0695764347910881
epoch£º929	 i:8 	 global-step:18588	 l-p:0.16478444635868073
epoch£º929	 i:9 	 global-step:18589	 l-p:0.12685127556324005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1420, 3.1174, 3.1403],
        [3.1420, 1.8254, 1.2167],
        [3.1420, 3.0815, 3.1346],
        [3.1420, 3.1386, 3.1419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.14006216824054718 
model_pd.l_d.mean(): -25.04532814025879 
model_pd.lagr.mean(): -24.90526580810547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0378], device='cuda:0')), ('power', tensor([-25.0075], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.14006216824054718
epoch£º930	 i:1 	 global-step:18601	 l-p:0.08697456121444702
epoch£º930	 i:2 	 global-step:18602	 l-p:0.12784019112586975
epoch£º930	 i:3 	 global-step:18603	 l-p:0.13785450160503387
epoch£º930	 i:4 	 global-step:18604	 l-p:0.14300870895385742
epoch£º930	 i:5 	 global-step:18605	 l-p:0.13381323218345642
epoch£º930	 i:6 	 global-step:18606	 l-p:0.13416408002376556
epoch£º930	 i:7 	 global-step:18607	 l-p:0.15886901319026947
epoch£º930	 i:8 	 global-step:18608	 l-p:-0.13767535984516144
epoch£º930	 i:9 	 global-step:18609	 l-p:0.019224051386117935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0646, 3.0581, 3.0644],
        [3.0646, 1.9182, 1.6625],
        [3.0646, 3.0646, 3.0646],
        [3.0646, 2.7052, 2.9047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.22644780576229095 
model_pd.l_d.mean(): -24.964540481567383 
model_pd.lagr.mean(): -24.73809242248535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0010], device='cuda:0')), ('power', tensor([-24.9635], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.22644780576229095
epoch£º931	 i:1 	 global-step:18621	 l-p:0.5529236793518066
epoch£º931	 i:2 	 global-step:18622	 l-p:0.07661387324333191
epoch£º931	 i:3 	 global-step:18623	 l-p:0.09781801700592041
epoch£º931	 i:4 	 global-step:18624	 l-p:0.12190236151218414
epoch£º931	 i:5 	 global-step:18625	 l-p:0.12971574068069458
epoch£º931	 i:6 	 global-step:18626	 l-p:-0.05308341979980469
epoch£º931	 i:7 	 global-step:18627	 l-p:0.13947759568691254
epoch£º931	 i:8 	 global-step:18628	 l-p:0.147894486784935
epoch£º931	 i:9 	 global-step:18629	 l-p:0.1106162816286087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0674, 1.7527, 1.1640],
        [3.0674, 2.7972, 2.9712],
        [3.0674, 2.2439, 2.3356],
        [3.0674, 3.0256, 3.0633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.1434149295091629 
model_pd.l_d.mean(): -24.603681564331055 
model_pd.lagr.mean(): -24.46026611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1817], device='cuda:0')), ('power', tensor([-24.7854], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.1434149295091629
epoch£º932	 i:1 	 global-step:18641	 l-p:0.09189275652170181
epoch£º932	 i:2 	 global-step:18642	 l-p:0.07603113353252411
epoch£º932	 i:3 	 global-step:18643	 l-p:0.17450131475925446
epoch£º932	 i:4 	 global-step:18644	 l-p:0.13853198289871216
epoch£º932	 i:5 	 global-step:18645	 l-p:0.7916824817657471
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12591342628002167
epoch£º932	 i:7 	 global-step:18647	 l-p:0.15930917859077454
epoch£º932	 i:8 	 global-step:18648	 l-p:0.1382308453321457
epoch£º932	 i:9 	 global-step:18649	 l-p:0.1643703430891037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0795, 1.7686, 1.1722],
        [3.0795, 2.1256, 2.1069],
        [3.0795, 1.8587, 1.4969],
        [3.0795, 3.0796, 3.0795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): -0.00521509163081646 
model_pd.l_d.mean(): -25.130529403686523 
model_pd.lagr.mean(): -25.135744094848633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0182], device='cuda:0')), ('power', tensor([-25.1487], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:-0.00521509163081646
epoch£º933	 i:1 	 global-step:18661	 l-p:0.22331970930099487
epoch£º933	 i:2 	 global-step:18662	 l-p:0.13044068217277527
epoch£º933	 i:3 	 global-step:18663	 l-p:0.12582264840602875
epoch£º933	 i:4 	 global-step:18664	 l-p:0.22008109092712402
epoch£º933	 i:5 	 global-step:18665	 l-p:0.5814787745475769
epoch£º933	 i:6 	 global-step:18666	 l-p:0.11666613072156906
epoch£º933	 i:7 	 global-step:18667	 l-p:0.13323567807674408
epoch£º933	 i:8 	 global-step:18668	 l-p:0.323311448097229
epoch£º933	 i:9 	 global-step:18669	 l-p:0.1376723349094391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1192, 3.1192, 3.1192],
        [3.1192, 3.1191, 3.1192],
        [3.1192, 2.5970, 2.8040],
        [3.1192, 2.7397, 2.9425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.1398577243089676 
model_pd.l_d.mean(): -24.869401931762695 
model_pd.lagr.mean(): -24.729543685913086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0226], device='cuda:0')), ('power', tensor([-24.8920], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.1398577243089676
epoch£º934	 i:1 	 global-step:18681	 l-p:0.12895917892456055
epoch£º934	 i:2 	 global-step:18682	 l-p:0.11397956311702728
epoch£º934	 i:3 	 global-step:18683	 l-p:0.1175444945693016
epoch£º934	 i:4 	 global-step:18684	 l-p:0.17140361666679382
epoch£º934	 i:5 	 global-step:18685	 l-p:0.313019722700119
epoch£º934	 i:6 	 global-step:18686	 l-p:0.23409919440746307
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11165887862443924
epoch£º934	 i:8 	 global-step:18688	 l-p:0.16840903460979462
epoch£º934	 i:9 	 global-step:18689	 l-p:0.20705151557922363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1348, 1.8040, 1.2469],
        [3.1348, 3.0910, 3.1304],
        [3.1348, 3.1113, 3.1332],
        [3.1348, 2.6277, 2.8358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.18877296149730682 
model_pd.l_d.mean(): -25.063335418701172 
model_pd.lagr.mean(): -24.874563217163086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0111], device='cuda:0')), ('power', tensor([-25.0745], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:0.18877296149730682
epoch£º935	 i:1 	 global-step:18701	 l-p:0.11778577417135239
epoch£º935	 i:2 	 global-step:18702	 l-p:0.117291159927845
epoch£º935	 i:3 	 global-step:18703	 l-p:0.12280672043561935
epoch£º935	 i:4 	 global-step:18704	 l-p:0.1503508985042572
epoch£º935	 i:5 	 global-step:18705	 l-p:0.1293831169605255
epoch£º935	 i:6 	 global-step:18706	 l-p:0.16818515956401825
epoch£º935	 i:7 	 global-step:18707	 l-p:0.16519123315811157
epoch£º935	 i:8 	 global-step:18708	 l-p:0.21666495501995087
epoch£º935	 i:9 	 global-step:18709	 l-p:0.11438058316707611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1312, 2.7729, 2.9717],
        [3.1312, 3.0298, 3.1135],
        [3.1312, 2.0547, 1.8923],
        [3.1312, 1.9919, 1.3334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.1630052775144577 
model_pd.l_d.mean(): -24.79153060913086 
model_pd.lagr.mean(): -24.628524780273438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0080], device='cuda:0')), ('power', tensor([-24.7995], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.1630052775144577
epoch£º936	 i:1 	 global-step:18721	 l-p:0.16158874332904816
epoch£º936	 i:2 	 global-step:18722	 l-p:0.13894693553447723
epoch£º936	 i:3 	 global-step:18723	 l-p:0.1307462900876999
epoch£º936	 i:4 	 global-step:18724	 l-p:0.1265433430671692
epoch£º936	 i:5 	 global-step:18725	 l-p:0.2591170072555542
epoch£º936	 i:6 	 global-step:18726	 l-p:0.16185133159160614
epoch£º936	 i:7 	 global-step:18727	 l-p:0.11969791352748871
epoch£º936	 i:8 	 global-step:18728	 l-p:0.12145400792360306
epoch£º936	 i:9 	 global-step:18729	 l-p:0.21792665123939514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1177, 2.1384, 2.0922],
        [3.1177, 1.8941, 1.2547],
        [3.1177, 2.2230, 2.2577],
        [3.1177, 3.1166, 3.1177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.1930782049894333 
model_pd.l_d.mean(): -25.157270431518555 
model_pd.lagr.mean(): -24.964191436767578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0456], device='cuda:0')), ('power', tensor([-25.1117], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.1930782049894333
epoch£º937	 i:1 	 global-step:18741	 l-p:0.20884333550930023
epoch£º937	 i:2 	 global-step:18742	 l-p:0.13320821523666382
epoch£º937	 i:3 	 global-step:18743	 l-p:0.11703203618526459
epoch£º937	 i:4 	 global-step:18744	 l-p:0.23199160397052765
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12461592257022858
epoch£º937	 i:6 	 global-step:18746	 l-p:0.23531576991081238
epoch£º937	 i:7 	 global-step:18747	 l-p:0.16335849463939667
epoch£º937	 i:8 	 global-step:18748	 l-p:0.11890756338834763
epoch£º937	 i:9 	 global-step:18749	 l-p:0.10127437114715576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1168, 3.1167, 3.1168],
        [3.1168, 3.0241, 3.1016],
        [3.1168, 1.9388, 1.6349],
        [3.1168, 2.6877, 2.8965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.12575094401836395 
model_pd.l_d.mean(): -24.678192138671875 
model_pd.lagr.mean(): -24.552440643310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0066], device='cuda:0')), ('power', tensor([-24.6848], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.12575094401836395
epoch£º938	 i:1 	 global-step:18761	 l-p:0.13265153765678406
epoch£º938	 i:2 	 global-step:18762	 l-p:0.1325896829366684
epoch£º938	 i:3 	 global-step:18763	 l-p:1.6621121168136597
epoch£º938	 i:4 	 global-step:18764	 l-p:0.14261078834533691
epoch£º938	 i:5 	 global-step:18765	 l-p:0.1425553262233734
epoch£º938	 i:6 	 global-step:18766	 l-p:-0.13400624692440033
epoch£º938	 i:7 	 global-step:18767	 l-p:0.23359335958957672
epoch£º938	 i:8 	 global-step:18768	 l-p:-0.004148516338318586
epoch£º938	 i:9 	 global-step:18769	 l-p:0.15590599179267883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[3.0906, 2.0292, 1.3638],
        [3.0906, 1.7608, 1.1963],
        [3.0906, 1.9564, 1.7190],
        [3.0906, 1.8015, 1.1881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.15574917197227478 
model_pd.l_d.mean(): -24.95094108581543 
model_pd.lagr.mean(): -24.79519271850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0634], device='cuda:0')), ('power', tensor([-25.0144], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.15574917197227478
epoch£º939	 i:1 	 global-step:18781	 l-p:0.18968501687049866
epoch£º939	 i:2 	 global-step:18782	 l-p:0.12474335730075836
epoch£º939	 i:3 	 global-step:18783	 l-p:0.14437228441238403
epoch£º939	 i:4 	 global-step:18784	 l-p:0.5103526711463928
epoch£º939	 i:5 	 global-step:18785	 l-p:0.15254820883274078
epoch£º939	 i:6 	 global-step:18786	 l-p:0.2967798113822937
epoch£º939	 i:7 	 global-step:18787	 l-p:0.14549754559993744
epoch£º939	 i:8 	 global-step:18788	 l-p:0.13640229403972626
epoch£º939	 i:9 	 global-step:18789	 l-p:0.22429105639457703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1083, 3.1050, 3.1082],
        [3.1083, 1.7768, 1.2088],
        [3.1083, 3.0644, 3.1039],
        [3.1083, 2.8684, 3.0302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.12711657583713531 
model_pd.l_d.mean(): -24.750049591064453 
model_pd.lagr.mean(): -24.62293243408203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0992], device='cuda:0')), ('power', tensor([-24.8493], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.12711657583713531
epoch£º940	 i:1 	 global-step:18801	 l-p:0.1485932171344757
epoch£º940	 i:2 	 global-step:18802	 l-p:0.1377062052488327
epoch£º940	 i:3 	 global-step:18803	 l-p:-0.36042335629463196
epoch£º940	 i:4 	 global-step:18804	 l-p:0.14007213711738586
epoch£º940	 i:5 	 global-step:18805	 l-p:0.09616098552942276
epoch£º940	 i:6 	 global-step:18806	 l-p:0.14767974615097046
epoch£º940	 i:7 	 global-step:18807	 l-p:0.10722808539867401
epoch£º940	 i:8 	 global-step:18808	 l-p:-0.4151463210582733
epoch£º940	 i:9 	 global-step:18809	 l-p:0.37311288714408875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0496, 1.7403, 1.1497],
        [3.0496, 3.0491, 3.0496],
        [3.0496, 2.0184, 1.9180],
        [3.0496, 1.7397, 1.2420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.10314089804887772 
model_pd.l_d.mean(): -24.821191787719727 
model_pd.lagr.mean(): -24.718050003051758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1194], device='cuda:0')), ('power', tensor([-24.9406], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.10314089804887772
epoch£º941	 i:1 	 global-step:18821	 l-p:0.13898968696594238
epoch£º941	 i:2 	 global-step:18822	 l-p:0.14444704353809357
epoch£º941	 i:3 	 global-step:18823	 l-p:0.3233906924724579
epoch£º941	 i:4 	 global-step:18824	 l-p:0.1503976583480835
epoch£º941	 i:5 	 global-step:18825	 l-p:0.1682216376066208
epoch£º941	 i:6 	 global-step:18826	 l-p:0.09391770511865616
epoch£º941	 i:7 	 global-step:18827	 l-p:0.12596602737903595
epoch£º941	 i:8 	 global-step:18828	 l-p:0.001258315984159708
epoch£º941	 i:9 	 global-step:18829	 l-p:0.31592708826065063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0339, 2.3440, 2.5111],
        [3.0339, 2.0764, 1.4007],
        [3.0339, 1.7104, 1.1726],
        [3.0339, 3.0182, 3.0331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12636104226112366 
model_pd.l_d.mean(): -24.988981246948242 
model_pd.lagr.mean(): -24.862619400024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0174], device='cuda:0')), ('power', tensor([-24.9716], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12636104226112366
epoch£º942	 i:1 	 global-step:18841	 l-p:0.16173215210437775
epoch£º942	 i:2 	 global-step:18842	 l-p:-0.2197677344083786
epoch£º942	 i:3 	 global-step:18843	 l-p:0.16327987611293793
epoch£º942	 i:4 	 global-step:18844	 l-p:0.1432153731584549
epoch£º942	 i:5 	 global-step:18845	 l-p:0.09364806115627289
epoch£º942	 i:6 	 global-step:18846	 l-p:0.07555728405714035
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12978413701057434
epoch£º942	 i:8 	 global-step:18848	 l-p:0.15477429330348969
epoch£º942	 i:9 	 global-step:18849	 l-p:0.21880538761615753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0393, 3.0393, 3.0393],
        [3.0393, 2.0069, 1.9062],
        [3.0393, 3.0392, 3.0393],
        [3.0393, 3.0393, 3.0393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12686187028884888 
model_pd.l_d.mean(): -25.01626205444336 
model_pd.lagr.mean(): -24.889400482177734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0499], device='cuda:0')), ('power', tensor([-24.9664], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12686187028884888
epoch£º943	 i:1 	 global-step:18861	 l-p:0.14337731897830963
epoch£º943	 i:2 	 global-step:18862	 l-p:0.1429603397846222
epoch£º943	 i:3 	 global-step:18863	 l-p:-0.7458729147911072
epoch£º943	 i:4 	 global-step:18864	 l-p:0.11242125183343887
epoch£º943	 i:5 	 global-step:18865	 l-p:0.25898921489715576
epoch£º943	 i:6 	 global-step:18866	 l-p:0.14203117787837982
epoch£º943	 i:7 	 global-step:18867	 l-p:0.13598771393299103
epoch£º943	 i:8 	 global-step:18868	 l-p:-0.2804357707500458
epoch£º943	 i:9 	 global-step:18869	 l-p:0.11985103040933609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[3.0681, 1.7658, 1.2832],
        [3.0681, 1.7867, 1.1749],
        [3.0681, 2.1411, 2.1509],
        [3.0681, 2.4234, 2.6061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.06784643232822418 
model_pd.l_d.mean(): -25.281394958496094 
model_pd.lagr.mean(): -25.21354866027832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0695], device='cuda:0')), ('power', tensor([-25.2119], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.06784643232822418
epoch£º944	 i:1 	 global-step:18881	 l-p:0.0003135681035928428
epoch£º944	 i:2 	 global-step:18882	 l-p:0.20833009481430054
epoch£º944	 i:3 	 global-step:18883	 l-p:0.3350777328014374
epoch£º944	 i:4 	 global-step:18884	 l-p:0.1147293820977211
epoch£º944	 i:5 	 global-step:18885	 l-p:0.15014535188674927
epoch£º944	 i:6 	 global-step:18886	 l-p:0.13386371731758118
epoch£º944	 i:7 	 global-step:18887	 l-p:0.1318494826555252
epoch£º944	 i:8 	 global-step:18888	 l-p:0.0662754625082016
epoch£º944	 i:9 	 global-step:18889	 l-p:0.1263846606016159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1480, 1.8311, 1.2187],
        [3.1480, 3.1356, 3.1474],
        [3.1480, 2.8412, 3.0272],
        [3.1480, 3.1479, 3.1480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.14583735167980194 
model_pd.l_d.mean(): -24.724735260009766 
model_pd.lagr.mean(): -24.57889747619629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0513], device='cuda:0')), ('power', tensor([-24.6734], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.14583735167980194
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12203717976808548
epoch£º945	 i:2 	 global-step:18902	 l-p:0.1189209446310997
epoch£º945	 i:3 	 global-step:18903	 l-p:0.08973349630832672
epoch£º945	 i:4 	 global-step:18904	 l-p:0.1754346489906311
epoch£º945	 i:5 	 global-step:18905	 l-p:0.14903971552848816
epoch£º945	 i:6 	 global-step:18906	 l-p:0.14546096324920654
epoch£º945	 i:7 	 global-step:18907	 l-p:0.28596532344818115
epoch£º945	 i:8 	 global-step:18908	 l-p:0.13078580796718597
epoch£º945	 i:9 	 global-step:18909	 l-p:0.16656643152236938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1158, 1.8016, 1.1957],
        [3.1158, 1.8727, 1.2380],
        [3.1158, 3.1157, 3.1158],
        [3.1158, 2.1918, 1.4997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.14552763104438782 
model_pd.l_d.mean(): -24.760196685791016 
model_pd.lagr.mean(): -24.614669799804688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0790], device='cuda:0')), ('power', tensor([-24.8392], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.14552763104438782
epoch£º946	 i:1 	 global-step:18921	 l-p:0.1194283738732338
epoch£º946	 i:2 	 global-step:18922	 l-p:0.17476166784763336
epoch£º946	 i:3 	 global-step:18923	 l-p:-0.07187986373901367
epoch£º946	 i:4 	 global-step:18924	 l-p:-0.2521600127220154
epoch£º946	 i:5 	 global-step:18925	 l-p:0.136220782995224
epoch£º946	 i:6 	 global-step:18926	 l-p:0.1413811445236206
epoch£º946	 i:7 	 global-step:18927	 l-p:0.10626716166734695
epoch£º946	 i:8 	 global-step:18928	 l-p:0.13514992594718933
epoch£º946	 i:9 	 global-step:18929	 l-p:0.1172109842300415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0817, 3.0409, 3.0778],
        [3.0817, 3.0755, 3.0815],
        [3.0817, 3.0817, 3.0817],
        [3.0817, 3.0817, 3.0817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.14148391783237457 
model_pd.l_d.mean(): -25.235803604125977 
model_pd.lagr.mean(): -25.09432029724121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0225], device='cuda:0')), ('power', tensor([-25.2133], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.14148391783237457
epoch£º947	 i:1 	 global-step:18941	 l-p:0.011592215858399868
epoch£º947	 i:2 	 global-step:18942	 l-p:0.13026057183742523
epoch£º947	 i:3 	 global-step:18943	 l-p:0.11702599376440048
epoch£º947	 i:4 	 global-step:18944	 l-p:0.27854591608047485
epoch£º947	 i:5 	 global-step:18945	 l-p:0.1391727179288864
epoch£º947	 i:6 	 global-step:18946	 l-p:0.03683656454086304
epoch£º947	 i:7 	 global-step:18947	 l-p:0.1590987741947174
epoch£º947	 i:8 	 global-step:18948	 l-p:-0.17818620800971985
epoch£º947	 i:9 	 global-step:18949	 l-p:0.14081135392189026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0919, 1.8787, 1.2417],
        [3.0919, 2.8576, 3.0171],
        [3.0919, 3.0916, 3.0919],
        [3.0919, 2.6481, 2.8584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11205903440713882 
model_pd.l_d.mean(): -24.61069107055664 
model_pd.lagr.mean(): -24.498632431030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1346], device='cuda:0')), ('power', tensor([-24.7453], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11205903440713882
epoch£º948	 i:1 	 global-step:18961	 l-p:0.16275697946548462
epoch£º948	 i:2 	 global-step:18962	 l-p:0.11543656885623932
epoch£º948	 i:3 	 global-step:18963	 l-p:-0.3588966727256775
epoch£º948	 i:4 	 global-step:18964	 l-p:0.14181111752986908
epoch£º948	 i:5 	 global-step:18965	 l-p:0.15144315361976624
epoch£º948	 i:6 	 global-step:18966	 l-p:0.15237048268318176
epoch£º948	 i:7 	 global-step:18967	 l-p:-0.24998310208320618
epoch£º948	 i:8 	 global-step:18968	 l-p:0.14269967377185822
epoch£º948	 i:9 	 global-step:18969	 l-p:0.20776265859603882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0729, 3.0696, 3.0728],
        [3.0729, 2.6882, 2.8927],
        [3.0729, 2.8312, 2.9940],
        [3.0729, 2.9043, 3.0308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.011145018972456455 
model_pd.l_d.mean(): -25.112218856811523 
model_pd.lagr.mean(): -25.10107421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0961], device='cuda:0')), ('power', tensor([-25.0161], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.011145018972456455
epoch£º949	 i:1 	 global-step:18981	 l-p:0.18145304918289185
epoch£º949	 i:2 	 global-step:18982	 l-p:0.2576325833797455
epoch£º949	 i:3 	 global-step:18983	 l-p:0.1424940526485443
epoch£º949	 i:4 	 global-step:18984	 l-p:0.12259706109762192
epoch£º949	 i:5 	 global-step:18985	 l-p:0.14975190162658691
epoch£º949	 i:6 	 global-step:18986	 l-p:-0.28247228264808655
epoch£º949	 i:7 	 global-step:18987	 l-p:0.14695234596729279
epoch£º949	 i:8 	 global-step:18988	 l-p:0.1459926813840866
epoch£º949	 i:9 	 global-step:18989	 l-p:0.28645652532577515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1182, 3.0947, 3.1166],
        [3.1182, 1.9385, 1.6343],
        [3.1182, 1.8312, 1.3709],
        [3.1182, 3.0197, 3.1014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.25498199462890625 
model_pd.l_d.mean(): -24.835142135620117 
model_pd.lagr.mean(): -24.58016014099121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0616], device='cuda:0')), ('power', tensor([-24.8967], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.25498199462890625
epoch£º950	 i:1 	 global-step:19001	 l-p:0.109992615878582
epoch£º950	 i:2 	 global-step:19002	 l-p:0.1417529433965683
epoch£º950	 i:3 	 global-step:19003	 l-p:0.17167873680591583
epoch£º950	 i:4 	 global-step:19004	 l-p:0.10822287946939468
epoch£º950	 i:5 	 global-step:19005	 l-p:0.13154685497283936
epoch£º950	 i:6 	 global-step:19006	 l-p:0.14081071317195892
epoch£º950	 i:7 	 global-step:19007	 l-p:0.1720278114080429
epoch£º950	 i:8 	 global-step:19008	 l-p:0.19039829075336456
epoch£º950	 i:9 	 global-step:19009	 l-p:0.14640842378139496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1410, 2.5454, 2.7401],
        [3.1410, 3.1410, 3.1410],
        [3.1410, 3.1410, 3.1410],
        [3.1410, 1.9618, 1.6562]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.14220847189426422 
model_pd.l_d.mean(): -24.547740936279297 
model_pd.lagr.mean(): -24.405532836914062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0639], device='cuda:0')), ('power', tensor([-24.6117], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.14220847189426422
epoch£º951	 i:1 	 global-step:19021	 l-p:0.11576016992330551
epoch£º951	 i:2 	 global-step:19022	 l-p:0.1599339097738266
epoch£º951	 i:3 	 global-step:19023	 l-p:0.1802612990140915
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12155139446258545
epoch£º951	 i:5 	 global-step:19025	 l-p:0.10707133263349533
epoch£º951	 i:6 	 global-step:19026	 l-p:0.13028137385845184
epoch£º951	 i:7 	 global-step:19027	 l-p:0.13726289570331573
epoch£º951	 i:8 	 global-step:19028	 l-p:0.1877288818359375
epoch£º951	 i:9 	 global-step:19029	 l-p:0.2876865267753601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1266, 3.1085, 3.1255],
        [3.1266, 1.8080, 1.2021],
        [3.1266, 2.9994, 3.1006],
        [3.1266, 3.0564, 3.1170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.21662645041942596 
model_pd.l_d.mean(): -25.00977897644043 
model_pd.lagr.mean(): -24.79315185546875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0055], device='cuda:0')), ('power', tensor([-25.0043], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.21662645041942596
epoch£º952	 i:1 	 global-step:19041	 l-p:0.28497371077537537
epoch£º952	 i:2 	 global-step:19042	 l-p:0.14774151146411896
epoch£º952	 i:3 	 global-step:19043	 l-p:0.13205738365650177
epoch£º952	 i:4 	 global-step:19044	 l-p:0.16721242666244507
epoch£º952	 i:5 	 global-step:19045	 l-p:0.08883833885192871
epoch£º952	 i:6 	 global-step:19046	 l-p:0.10838008671998978
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12668989598751068
epoch£º952	 i:8 	 global-step:19048	 l-p:0.1159265786409378
epoch£º952	 i:9 	 global-step:19049	 l-p:0.14949536323547363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1271, 1.8982, 1.5231],
        [3.1271, 3.1266, 3.1271],
        [3.1271, 3.1268, 3.1271],
        [3.1271, 3.1157, 3.1266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.23350860178470612 
model_pd.l_d.mean(): -25.072811126708984 
model_pd.lagr.mean(): -24.83930206298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0329], device='cuda:0')), ('power', tensor([-25.1057], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.23350860178470612
epoch£º953	 i:1 	 global-step:19061	 l-p:0.12925371527671814
epoch£º953	 i:2 	 global-step:19062	 l-p:0.29803359508514404
epoch£º953	 i:3 	 global-step:19063	 l-p:0.13755014538764954
epoch£º953	 i:4 	 global-step:19064	 l-p:0.1441008746623993
epoch£º953	 i:5 	 global-step:19065	 l-p:0.13060742616653442
epoch£º953	 i:6 	 global-step:19066	 l-p:-0.5718047618865967
epoch£º953	 i:7 	 global-step:19067	 l-p:0.18222758173942566
epoch£º953	 i:8 	 global-step:19068	 l-p:0.17496812343597412
epoch£º953	 i:9 	 global-step:19069	 l-p:0.14723463356494904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0764, 2.8658, 3.0144],
        [3.0764, 2.1118, 1.4310],
        [3.0764, 3.0676, 3.0761],
        [3.0764, 2.4312, 2.6140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.24578070640563965 
model_pd.l_d.mean(): -24.860300064086914 
model_pd.lagr.mean(): -24.614519119262695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1574], device='cuda:0')), ('power', tensor([-25.0177], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.24578070640563965
epoch£º954	 i:1 	 global-step:19081	 l-p:0.032626885920763016
epoch£º954	 i:2 	 global-step:19082	 l-p:0.13997717201709747
epoch£º954	 i:3 	 global-step:19083	 l-p:0.15936875343322754
epoch£º954	 i:4 	 global-step:19084	 l-p:0.05978146567940712
epoch£º954	 i:5 	 global-step:19085	 l-p:0.12145020812749863
epoch£º954	 i:6 	 global-step:19086	 l-p:0.16193942725658417
epoch£º954	 i:7 	 global-step:19087	 l-p:0.054520782083272934
epoch£º954	 i:8 	 global-step:19088	 l-p:0.15684272348880768
epoch£º954	 i:9 	 global-step:19089	 l-p:0.13107234239578247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0673, 3.0674, 3.0673],
        [3.0673, 2.1024, 1.4229],
        [3.0673, 2.8208, 2.9858],
        [3.0673, 2.9404, 3.0415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.16793131828308105 
model_pd.l_d.mean(): -24.989118576049805 
model_pd.lagr.mean(): -24.82118797302246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1048], device='cuda:0')), ('power', tensor([-25.0940], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.16793131828308105
epoch£º955	 i:1 	 global-step:19101	 l-p:-0.027516087517142296
epoch£º955	 i:2 	 global-step:19102	 l-p:0.1875247210264206
epoch£º955	 i:3 	 global-step:19103	 l-p:0.1644810140132904
epoch£º955	 i:4 	 global-step:19104	 l-p:0.13449975848197937
epoch£º955	 i:5 	 global-step:19105	 l-p:0.3324365019798279
epoch£º955	 i:6 	 global-step:19106	 l-p:0.1447756141424179
epoch£º955	 i:7 	 global-step:19107	 l-p:0.060507409274578094
epoch£º955	 i:8 	 global-step:19108	 l-p:0.1381726711988449
epoch£º955	 i:9 	 global-step:19109	 l-p:0.17457765340805054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0705, 1.8300, 1.2038],
        [3.0705, 3.0685, 3.0705],
        [3.0705, 3.0672, 3.0704],
        [3.0705, 1.8889, 1.5881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.14932124316692352 
model_pd.l_d.mean(): -25.291406631469727 
model_pd.lagr.mean(): -25.142086029052734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0296], device='cuda:0')), ('power', tensor([-25.2618], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.14932124316692352
epoch£º956	 i:1 	 global-step:19121	 l-p:0.14761154353618622
epoch£º956	 i:2 	 global-step:19122	 l-p:0.0769297406077385
epoch£º956	 i:3 	 global-step:19123	 l-p:0.14190754294395447
epoch£º956	 i:4 	 global-step:19124	 l-p:0.060855474323034286
epoch£º956	 i:5 	 global-step:19125	 l-p:-1.6913390159606934
epoch£º956	 i:6 	 global-step:19126	 l-p:0.14020514488220215
epoch£º956	 i:7 	 global-step:19127	 l-p:0.2366042137145996
epoch£º956	 i:8 	 global-step:19128	 l-p:0.1306288093328476
epoch£º956	 i:9 	 global-step:19129	 l-p:0.07934572547674179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0749, 3.0738, 3.0749],
        [3.0749, 2.4761, 2.6722],
        [3.0749, 3.0097, 3.0665],
        [3.0749, 3.0045, 3.0654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.07246233522891998 
model_pd.l_d.mean(): -25.162553787231445 
model_pd.lagr.mean(): -25.090091705322266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0004], device='cuda:0')), ('power', tensor([-25.1622], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.07246233522891998
epoch£º957	 i:1 	 global-step:19141	 l-p:0.13355566561222076
epoch£º957	 i:2 	 global-step:19142	 l-p:0.13934965431690216
epoch£º957	 i:3 	 global-step:19143	 l-p:0.09362855553627014
epoch£º957	 i:4 	 global-step:19144	 l-p:-0.1739700585603714
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11691530793905258
epoch£º957	 i:6 	 global-step:19146	 l-p:0.14747418463230133
epoch£º957	 i:7 	 global-step:19147	 l-p:-0.7789536714553833
epoch£º957	 i:8 	 global-step:19148	 l-p:0.18218116462230682
epoch£º957	 i:9 	 global-step:19149	 l-p:0.1921170949935913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0963, 3.0260, 3.0868],
        [3.0963, 2.8756, 3.0290],
        [3.0963, 2.8501, 3.0148],
        [3.0963, 1.8055, 1.1901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.19441482424736023 
model_pd.l_d.mean(): -25.167682647705078 
model_pd.lagr.mean(): -24.973268508911133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0734], device='cuda:0')), ('power', tensor([-25.0943], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:0.19441482424736023
epoch£º958	 i:1 	 global-step:19161	 l-p:0.1414710432291031
epoch£º958	 i:2 	 global-step:19162	 l-p:0.11655696481466293
epoch£º958	 i:3 	 global-step:19163	 l-p:0.31780806183815
epoch£º958	 i:4 	 global-step:19164	 l-p:0.13490696251392365
epoch£º958	 i:5 	 global-step:19165	 l-p:0.49219173192977905
epoch£º958	 i:6 	 global-step:19166	 l-p:0.15874147415161133
epoch£º958	 i:7 	 global-step:19167	 l-p:0.13893285393714905
epoch£º958	 i:8 	 global-step:19168	 l-p:0.10665997862815857
epoch£º958	 i:9 	 global-step:19169	 l-p:0.12700653076171875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1127, 1.7827, 1.1944],
        [3.1127, 3.1127, 3.1127],
        [3.1127, 1.7841, 1.2465],
        [3.1127, 3.1042, 3.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.18624083697795868 
model_pd.l_d.mean(): -25.098997116088867 
model_pd.lagr.mean(): -24.912755966186523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0167], device='cuda:0')), ('power', tensor([-25.0823], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.18624083697795868
epoch£º959	 i:1 	 global-step:19181	 l-p:0.11982230842113495
epoch£º959	 i:2 	 global-step:19182	 l-p:0.1442444622516632
epoch£º959	 i:3 	 global-step:19183	 l-p:0.13441866636276245
epoch£º959	 i:4 	 global-step:19184	 l-p:1.6699742078781128
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12905818223953247
epoch£º959	 i:6 	 global-step:19186	 l-p:0.14248037338256836
epoch£º959	 i:7 	 global-step:19187	 l-p:0.1306460201740265
epoch£º959	 i:8 	 global-step:19188	 l-p:-0.02696368470788002
epoch£º959	 i:9 	 global-step:19189	 l-p:0.6702843904495239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0609, 3.0607, 3.0610],
        [3.0609, 3.0503, 3.0605],
        [3.0609, 2.0273, 1.3606],
        [3.0609, 2.0195, 1.3542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.07274579256772995 
model_pd.l_d.mean(): -24.734310150146484 
model_pd.lagr.mean(): -24.661563873291016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0766], device='cuda:0')), ('power', tensor([-24.8109], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.07274579256772995
epoch£º960	 i:1 	 global-step:19201	 l-p:0.16602656245231628
epoch£º960	 i:2 	 global-step:19202	 l-p:0.1344529092311859
epoch£º960	 i:3 	 global-step:19203	 l-p:0.2857188284397125
epoch£º960	 i:4 	 global-step:19204	 l-p:0.1434689164161682
epoch£º960	 i:5 	 global-step:19205	 l-p:0.2160421758890152
epoch£º960	 i:6 	 global-step:19206	 l-p:0.08197349309921265
epoch£º960	 i:7 	 global-step:19207	 l-p:0.07458929717540741
epoch£º960	 i:8 	 global-step:19208	 l-p:0.6022077202796936
epoch£º960	 i:9 	 global-step:19209	 l-p:0.10645943135023117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0778, 1.7625, 1.1662],
        [3.0778, 2.8462, 3.0047],
        [3.0778, 1.7544, 1.1669],
        [3.0778, 3.0632, 3.0771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.13062752783298492 
model_pd.l_d.mean(): -24.795461654663086 
model_pd.lagr.mean(): -24.66483497619629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2222], device='cuda:0')), ('power', tensor([-25.0177], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.13062752783298492
epoch£º961	 i:1 	 global-step:19221	 l-p:0.1244506910443306
epoch£º961	 i:2 	 global-step:19222	 l-p:0.13624894618988037
epoch£º961	 i:3 	 global-step:19223	 l-p:0.1404084414243698
epoch£º961	 i:4 	 global-step:19224	 l-p:0.14785565435886383
epoch£º961	 i:5 	 global-step:19225	 l-p:-0.12006256729364395
epoch£º961	 i:6 	 global-step:19226	 l-p:0.1779816448688507
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13915614783763885
epoch£º961	 i:8 	 global-step:19228	 l-p:0.21436935663223267
epoch£º961	 i:9 	 global-step:19229	 l-p:0.31442874670028687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0777, 2.9280, 3.0434],
        [3.0777, 3.0777, 3.0777],
        [3.0777, 3.0735, 3.0776],
        [3.0777, 2.7559, 2.9470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.19410406053066254 
model_pd.l_d.mean(): -25.196327209472656 
model_pd.lagr.mean(): -25.00222396850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0742], device='cuda:0')), ('power', tensor([-25.1221], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.19410406053066254
epoch£º962	 i:1 	 global-step:19241	 l-p:0.20506423711776733
epoch£º962	 i:2 	 global-step:19242	 l-p:0.13015224039554596
epoch£º962	 i:3 	 global-step:19243	 l-p:0.13911470770835876
epoch£º962	 i:4 	 global-step:19244	 l-p:0.8065414428710938
epoch£º962	 i:5 	 global-step:19245	 l-p:0.17604395747184753
epoch£º962	 i:6 	 global-step:19246	 l-p:0.5203580260276794
epoch£º962	 i:7 	 global-step:19247	 l-p:0.24866977334022522
epoch£º962	 i:8 	 global-step:19248	 l-p:0.13487330079078674
epoch£º962	 i:9 	 global-step:19249	 l-p:0.15525659918785095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1220, 3.1147, 3.1218],
        [3.1220, 3.1174, 3.1219],
        [3.1220, 2.9525, 3.0795],
        [3.1220, 2.9888, 3.0939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.16271749138832092 
model_pd.l_d.mean(): -24.943391799926758 
model_pd.lagr.mean(): -24.78067398071289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0586], device='cuda:0')), ('power', tensor([-25.0020], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.16271749138832092
epoch£º963	 i:1 	 global-step:19261	 l-p:0.13132134079933167
epoch£º963	 i:2 	 global-step:19262	 l-p:0.12236320227384567
epoch£º963	 i:3 	 global-step:19263	 l-p:0.34477657079696655
epoch£º963	 i:4 	 global-step:19264	 l-p:0.23557183146476746
epoch£º963	 i:5 	 global-step:19265	 l-p:0.14227750897407532
epoch£º963	 i:6 	 global-step:19266	 l-p:0.0988338366150856
epoch£º963	 i:7 	 global-step:19267	 l-p:0.1414574235677719
epoch£º963	 i:8 	 global-step:19268	 l-p:0.1808585673570633
epoch£º963	 i:9 	 global-step:19269	 l-p:0.10936924070119858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1277, 3.1230, 3.1276],
        [3.1277, 3.1268, 3.1277],
        [3.1277, 2.2067, 1.5118],
        [3.1277, 1.7916, 1.2133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.13630574941635132 
model_pd.l_d.mean(): -24.38145637512207 
model_pd.lagr.mean(): -24.24515151977539 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1385], device='cuda:0')), ('power', tensor([-24.5200], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.13630574941635132
epoch£º964	 i:1 	 global-step:19281	 l-p:0.13938762247562408
epoch£º964	 i:2 	 global-step:19282	 l-p:0.14344695210456848
epoch£º964	 i:3 	 global-step:19283	 l-p:0.13874584436416626
epoch£º964	 i:4 	 global-step:19284	 l-p:0.04216651991009712
epoch£º964	 i:5 	 global-step:19285	 l-p:0.1435278207063675
epoch£º964	 i:6 	 global-step:19286	 l-p:0.13104204833507538
epoch£º964	 i:7 	 global-step:19287	 l-p:0.10747941583395004
epoch£º964	 i:8 	 global-step:19288	 l-p:33.42275619506836
epoch£º964	 i:9 	 global-step:19289	 l-p:0.34339797496795654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[3.0506, 1.7224, 1.1833],
        [3.0506, 1.9846, 1.8459],
        [3.0506, 1.7219, 1.1805],
        [3.0506, 2.0918, 2.0748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.15256325900554657 
model_pd.l_d.mean(): -25.102764129638672 
model_pd.lagr.mean(): -24.9502010345459 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0030], device='cuda:0')), ('power', tensor([-25.1058], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.15256325900554657
epoch£º965	 i:1 	 global-step:19301	 l-p:0.64849454164505
epoch£º965	 i:2 	 global-step:19302	 l-p:0.12615202367305756
epoch£º965	 i:3 	 global-step:19303	 l-p:-0.0009903144091367722
epoch£º965	 i:4 	 global-step:19304	 l-p:0.13502389192581177
epoch£º965	 i:5 	 global-step:19305	 l-p:0.19965384900569916
epoch£º965	 i:6 	 global-step:19306	 l-p:0.1311662644147873
epoch£º965	 i:7 	 global-step:19307	 l-p:0.14881962537765503
epoch£º965	 i:8 	 global-step:19308	 l-p:0.1374276727437973
epoch£º965	 i:9 	 global-step:19309	 l-p:0.2043478786945343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0439, 1.8882, 1.2475],
        [3.0439, 3.0424, 3.0439],
        [3.0439, 1.8681, 1.2315],
        [3.0439, 2.1126, 2.1225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.1631183922290802 
model_pd.l_d.mean(): -24.745262145996094 
model_pd.lagr.mean(): -24.582143783569336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1435], device='cuda:0')), ('power', tensor([-24.8887], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.1631183922290802
epoch£º966	 i:1 	 global-step:19321	 l-p:0.11103742569684982
epoch£º966	 i:2 	 global-step:19322	 l-p:-0.05015205219388008
epoch£º966	 i:3 	 global-step:19323	 l-p:0.1479865461587906
epoch£º966	 i:4 	 global-step:19324	 l-p:0.1369422823190689
epoch£º966	 i:5 	 global-step:19325	 l-p:0.13883446156978607
epoch£º966	 i:6 	 global-step:19326	 l-p:0.13728168606758118
epoch£º966	 i:7 	 global-step:19327	 l-p:0.06058434396982193
epoch£º966	 i:8 	 global-step:19328	 l-p:0.14425045251846313
epoch£º966	 i:9 	 global-step:19329	 l-p:0.08808187395334244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0725, 3.0720, 3.0725],
        [3.0725, 3.0725, 3.0725],
        [3.0725, 3.0724, 3.0725],
        [3.0725, 1.7841, 1.3307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.1680365949869156 
model_pd.l_d.mean(): -25.292888641357422 
model_pd.lagr.mean(): -25.12485122680664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0549], device='cuda:0')), ('power', tensor([-25.3478], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.1680365949869156
epoch£º967	 i:1 	 global-step:19341	 l-p:0.14619764685630798
epoch£º967	 i:2 	 global-step:19342	 l-p:0.1255868673324585
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12661483883857727
epoch£º967	 i:4 	 global-step:19344	 l-p:0.07109379023313522
epoch£º967	 i:5 	 global-step:19345	 l-p:0.030697640031576157
epoch£º967	 i:6 	 global-step:19346	 l-p:0.18406258523464203
epoch£º967	 i:7 	 global-step:19347	 l-p:0.46909841895103455
epoch£º967	 i:8 	 global-step:19348	 l-p:0.14265093207359314
epoch£º967	 i:9 	 global-step:19349	 l-p:0.1921815127134323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0857, 1.8149, 1.1936],
        [3.0857, 1.7685, 1.1707],
        [3.0857, 2.8141, 2.9890],
        [3.0857, 1.7916, 1.3280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): -0.24720999598503113 
model_pd.l_d.mean(): -25.04259490966797 
model_pd.lagr.mean(): -25.289804458618164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1161], device='cuda:0')), ('power', tensor([-25.1587], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:-0.24720999598503113
epoch£º968	 i:1 	 global-step:19361	 l-p:0.16136817634105682
epoch£º968	 i:2 	 global-step:19362	 l-p:0.1664067953824997
epoch£º968	 i:3 	 global-step:19363	 l-p:0.15876504778862
epoch£º968	 i:4 	 global-step:19364	 l-p:0.26609039306640625
epoch£º968	 i:5 	 global-step:19365	 l-p:0.1438247710466385
epoch£º968	 i:6 	 global-step:19366	 l-p:0.07125360518693924
epoch£º968	 i:7 	 global-step:19367	 l-p:0.14324013888835907
epoch£º968	 i:8 	 global-step:19368	 l-p:0.13771849870681763
epoch£º968	 i:9 	 global-step:19369	 l-p:0.13042065501213074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1535, 1.8333, 1.3163],
        [3.1535, 1.9630, 1.3082],
        [3.1535, 1.8289, 1.2183],
        [3.1535, 2.7556, 2.9616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.14309345185756683 
model_pd.l_d.mean(): -25.20047378540039 
model_pd.lagr.mean(): -25.05738067626953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1215], device='cuda:0')), ('power', tensor([-25.0789], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.14309345185756683
epoch£º969	 i:1 	 global-step:19381	 l-p:0.12147583067417145
epoch£º969	 i:2 	 global-step:19382	 l-p:0.2584972679615021
epoch£º969	 i:3 	 global-step:19383	 l-p:0.07082442939281464
epoch£º969	 i:4 	 global-step:19384	 l-p:0.17116506397724152
epoch£º969	 i:5 	 global-step:19385	 l-p:0.14968176186084747
epoch£º969	 i:6 	 global-step:19386	 l-p:0.11847926676273346
epoch£º969	 i:7 	 global-step:19387	 l-p:0.14939913153648376
epoch£º969	 i:8 	 global-step:19388	 l-p:0.14125095307826996
epoch£º969	 i:9 	 global-step:19389	 l-p:0.17060844600200653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1238, 3.1236, 3.1239],
        [3.1238, 2.2953, 2.3848],
        [3.1238, 2.6170, 2.8268],
        [3.1238, 2.0360, 1.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.15084797143936157 
model_pd.l_d.mean(): -25.132844924926758 
model_pd.lagr.mean(): -24.981996536254883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0276], device='cuda:0')), ('power', tensor([-25.1053], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.15084797143936157
epoch£º970	 i:1 	 global-step:19401	 l-p:0.3953084349632263
epoch£º970	 i:2 	 global-step:19402	 l-p:0.1572793871164322
epoch£º970	 i:3 	 global-step:19403	 l-p:0.14881600439548492
epoch£º970	 i:4 	 global-step:19404	 l-p:0.13441883027553558
epoch£º970	 i:5 	 global-step:19405	 l-p:0.18437659740447998
epoch£º970	 i:6 	 global-step:19406	 l-p:0.12396281957626343
epoch£º970	 i:7 	 global-step:19407	 l-p:0.02374196983873844
epoch£º970	 i:8 	 global-step:19408	 l-p:0.3824271261692047
epoch£º970	 i:9 	 global-step:19409	 l-p:0.11166436225175858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0644, 2.7679, 2.9515],
        [3.0644, 2.7831, 2.9616],
        [3.0644, 3.0619, 3.0643],
        [3.0644, 1.8078, 1.4049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.2290048599243164 
model_pd.l_d.mean(): -25.19692039489746 
model_pd.lagr.mean(): -24.967914581298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0247], device='cuda:0')), ('power', tensor([-25.1722], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.2290048599243164
epoch£º971	 i:1 	 global-step:19421	 l-p:0.1408190280199051
epoch£º971	 i:2 	 global-step:19422	 l-p:0.12814535200595856
epoch£º971	 i:3 	 global-step:19423	 l-p:0.14736086130142212
epoch£º971	 i:4 	 global-step:19424	 l-p:0.09962379187345505
epoch£º971	 i:5 	 global-step:19425	 l-p:0.14552541077136993
epoch£º971	 i:6 	 global-step:19426	 l-p:0.15635085105895996
epoch£º971	 i:7 	 global-step:19427	 l-p:-0.039201028645038605
epoch£º971	 i:8 	 global-step:19428	 l-p:0.14110472798347473
epoch£º971	 i:9 	 global-step:19429	 l-p:0.11065006256103516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0398, 2.8281, 2.9775],
        [3.0398, 3.0350, 3.0397],
        [3.0398, 2.0797, 2.0626],
        [3.0398, 3.0313, 3.0395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.14894835650920868 
model_pd.l_d.mean(): -24.879383087158203 
model_pd.lagr.mean(): -24.73043441772461 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1191], device='cuda:0')), ('power', tensor([-24.9985], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.14894835650920868
epoch£º972	 i:1 	 global-step:19441	 l-p:0.2502380907535553
epoch£º972	 i:2 	 global-step:19442	 l-p:0.14503462612628937
epoch£º972	 i:3 	 global-step:19443	 l-p:0.15253308415412903
epoch£º972	 i:4 	 global-step:19444	 l-p:0.1601153314113617
epoch£º972	 i:5 	 global-step:19445	 l-p:0.07840289175510406
epoch£º972	 i:6 	 global-step:19446	 l-p:0.15510308742523193
epoch£º972	 i:7 	 global-step:19447	 l-p:-0.33058586716651917
epoch£º972	 i:8 	 global-step:19448	 l-p:0.11128488928079605
epoch£º972	 i:9 	 global-step:19449	 l-p:0.08973324298858643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0446, 3.0036, 3.0407],
        [3.0446, 2.2839, 2.4195],
        [3.0446, 2.6841, 2.8850],
        [3.0446, 1.7920, 1.1740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.16395770013332367 
model_pd.l_d.mean(): -24.927555084228516 
model_pd.lagr.mean(): -24.76359748840332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0133], device='cuda:0')), ('power', tensor([-24.9408], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.16395770013332367
epoch£º973	 i:1 	 global-step:19461	 l-p:0.23454688489437103
epoch£º973	 i:2 	 global-step:19462	 l-p:-0.4496919512748718
epoch£º973	 i:3 	 global-step:19463	 l-p:0.13685011863708496
epoch£º973	 i:4 	 global-step:19464	 l-p:0.16207797825336456
epoch£º973	 i:5 	 global-step:19465	 l-p:0.12360046058893204
epoch£º973	 i:6 	 global-step:19466	 l-p:0.13354459404945374
epoch£º973	 i:7 	 global-step:19467	 l-p:0.15895316004753113
epoch£º973	 i:8 	 global-step:19468	 l-p:0.11534380912780762
epoch£º973	 i:9 	 global-step:19469	 l-p:0.274468332529068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0481, 2.0078, 1.9022],
        [3.0481, 2.8827, 3.0076],
        [3.0481, 2.2669, 2.3908],
        [3.0481, 1.8453, 1.2136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.14356723427772522 
model_pd.l_d.mean(): -24.664514541625977 
model_pd.lagr.mean(): -24.520946502685547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0879], device='cuda:0')), ('power', tensor([-24.7524], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.14356723427772522
epoch£º974	 i:1 	 global-step:19481	 l-p:0.1247919425368309
epoch£º974	 i:2 	 global-step:19482	 l-p:0.07120173424482346
epoch£º974	 i:3 	 global-step:19483	 l-p:0.018453530967235565
epoch£º974	 i:4 	 global-step:19484	 l-p:0.16750308871269226
epoch£º974	 i:5 	 global-step:19485	 l-p:1.1727155447006226
epoch£º974	 i:6 	 global-step:19486	 l-p:0.13924387097358704
epoch£º974	 i:7 	 global-step:19487	 l-p:0.1020810604095459
epoch£º974	 i:8 	 global-step:19488	 l-p:0.1457590013742447
epoch£º974	 i:9 	 global-step:19489	 l-p:0.15441428124904633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0350, 3.0350, 3.0350],
        [3.0350, 3.0350, 3.0350],
        [3.0350, 1.7989, 1.4300],
        [3.0350, 2.6011, 2.8124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.09119098633527756 
model_pd.l_d.mean(): -24.724292755126953 
model_pd.lagr.mean(): -24.633102416992188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0294], device='cuda:0')), ('power', tensor([-24.7537], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.09119098633527756
epoch£º975	 i:1 	 global-step:19501	 l-p:0.18093474209308624
epoch£º975	 i:2 	 global-step:19502	 l-p:0.2468908280134201
epoch£º975	 i:3 	 global-step:19503	 l-p:0.13933052122592926
epoch£º975	 i:4 	 global-step:19504	 l-p:-0.02243698015809059
epoch£º975	 i:5 	 global-step:19505	 l-p:0.14854711294174194
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10338934510946274
epoch£º975	 i:7 	 global-step:19507	 l-p:0.2718978822231293
epoch£º975	 i:8 	 global-step:19508	 l-p:0.1716008335351944
epoch£º975	 i:9 	 global-step:19509	 l-p:0.14220942556858063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0957, 2.0858, 2.0135],
        [3.0957, 3.0169, 3.0842],
        [3.0957, 1.8938, 1.2522],
        [3.0957, 3.0957, 3.0957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.15728874504566193 
model_pd.l_d.mean(): -25.126176834106445 
model_pd.lagr.mean(): -24.968887329101562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0390], device='cuda:0')), ('power', tensor([-25.1652], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.15728874504566193
epoch£º976	 i:1 	 global-step:19521	 l-p:0.18429817259311676
epoch£º976	 i:2 	 global-step:19522	 l-p:0.12865090370178223
epoch£º976	 i:3 	 global-step:19523	 l-p:0.17079593241214752
epoch£º976	 i:4 	 global-step:19524	 l-p:0.11040551215410233
epoch£º976	 i:5 	 global-step:19525	 l-p:0.12755651772022247
epoch£º976	 i:6 	 global-step:19526	 l-p:0.7949252128601074
epoch£º976	 i:7 	 global-step:19527	 l-p:0.24798975884914398
epoch£º976	 i:8 	 global-step:19528	 l-p:0.24235789477825165
epoch£º976	 i:9 	 global-step:19529	 l-p:0.13182789087295532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1351, 3.1351, 3.1351],
        [3.1351, 3.1350, 3.1351],
        [3.1351, 2.8320, 3.0174],
        [3.1351, 2.1799, 2.1631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.15315589308738708 
model_pd.l_d.mean(): -25.15957260131836 
model_pd.lagr.mean(): -25.00641632080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0109], device='cuda:0')), ('power', tensor([-25.1705], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.15315589308738708
epoch£º977	 i:1 	 global-step:19541	 l-p:0.1371496170759201
epoch£º977	 i:2 	 global-step:19542	 l-p:0.14544185996055603
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12018410116434097
epoch£º977	 i:4 	 global-step:19544	 l-p:0.14142072200775146
epoch£º977	 i:5 	 global-step:19545	 l-p:0.33680984377861023
epoch£º977	 i:6 	 global-step:19546	 l-p:0.46258530020713806
epoch£º977	 i:7 	 global-step:19547	 l-p:0.1362646073102951
epoch£º977	 i:8 	 global-step:19548	 l-p:0.17700020968914032
epoch£º977	 i:9 	 global-step:19549	 l-p:0.12664638459682465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1001, 3.0981, 3.1000],
        [3.1001, 2.8451, 3.0136],
        [3.1001, 3.1001, 3.1001],
        [3.1001, 1.8938, 1.5594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.1219324842095375 
model_pd.l_d.mean(): -25.014644622802734 
model_pd.lagr.mean(): -24.892711639404297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0341], device='cuda:0')), ('power', tensor([-25.0488], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.1219324842095375
epoch£º978	 i:1 	 global-step:19561	 l-p:0.15119735896587372
epoch£º978	 i:2 	 global-step:19562	 l-p:0.16396182775497437
epoch£º978	 i:3 	 global-step:19563	 l-p:-0.04260389134287834
epoch£º978	 i:4 	 global-step:19564	 l-p:0.13099125027656555
epoch£º978	 i:5 	 global-step:19565	 l-p:0.13613633811473846
epoch£º978	 i:6 	 global-step:19566	 l-p:0.1379261612892151
epoch£º978	 i:7 	 global-step:19567	 l-p:-0.6717804670333862
epoch£º978	 i:8 	 global-step:19568	 l-p:0.25085702538490295
epoch£º978	 i:9 	 global-step:19569	 l-p:0.061180680990219116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[3.0389, 2.0940, 1.4135],
        [3.0389, 1.7652, 1.3407],
        [3.0389, 1.7172, 1.2045],
        [3.0389, 1.9525, 1.7908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.15904006361961365 
model_pd.l_d.mean(): -24.89972686767578 
model_pd.lagr.mean(): -24.740686416625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1746], device='cuda:0')), ('power', tensor([-25.0744], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.15904006361961365
epoch£º979	 i:1 	 global-step:19581	 l-p:0.24242763221263885
epoch£º979	 i:2 	 global-step:19582	 l-p:0.16380949318408966
epoch£º979	 i:3 	 global-step:19583	 l-p:0.1320859044790268
epoch£º979	 i:4 	 global-step:19584	 l-p:0.09116040170192719
epoch£º979	 i:5 	 global-step:19585	 l-p:0.3039546012878418
epoch£º979	 i:6 	 global-step:19586	 l-p:1.388763666152954
epoch£º979	 i:7 	 global-step:19587	 l-p:0.14377973973751068
epoch£º979	 i:8 	 global-step:19588	 l-p:-0.018255453556776047
epoch£º979	 i:9 	 global-step:19589	 l-p:0.11163119971752167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0930, 3.0930, 3.0930],
        [3.0930, 3.0842, 3.0927],
        [3.0930, 2.9524, 3.0623],
        [3.0930, 3.0919, 3.0930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.16201141476631165 
model_pd.l_d.mean(): -25.122655868530273 
model_pd.lagr.mean(): -24.960643768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0284], device='cuda:0')), ('power', tensor([-25.0943], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.16201141476631165
epoch£º980	 i:1 	 global-step:19601	 l-p:0.13838976621627808
epoch£º980	 i:2 	 global-step:19602	 l-p:0.13340188562870026
epoch£º980	 i:3 	 global-step:19603	 l-p:-0.12152130156755447
epoch£º980	 i:4 	 global-step:19604	 l-p:0.17887337505817413
epoch£º980	 i:5 	 global-step:19605	 l-p:0.1500406116247177
epoch£º980	 i:6 	 global-step:19606	 l-p:0.11604858934879303
epoch£º980	 i:7 	 global-step:19607	 l-p:-2.276414394378662
epoch£º980	 i:8 	 global-step:19608	 l-p:0.12855783104896545
epoch£º980	 i:9 	 global-step:19609	 l-p:0.19083468616008759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1020, 2.7021, 2.9093],
        [3.1020, 3.1020, 3.1020],
        [3.1020, 3.0955, 3.1018],
        [3.1020, 1.7757, 1.2538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.13451483845710754 
model_pd.l_d.mean(): -24.835172653198242 
model_pd.lagr.mean(): -24.70065689086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0522], device='cuda:0')), ('power', tensor([-24.8873], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.13451483845710754
epoch£º981	 i:1 	 global-step:19621	 l-p:0.1347421258687973
epoch£º981	 i:2 	 global-step:19622	 l-p:0.1360764503479004
epoch£º981	 i:3 	 global-step:19623	 l-p:0.12998875975608826
epoch£º981	 i:4 	 global-step:19624	 l-p:0.018357934430241585
epoch£º981	 i:5 	 global-step:19625	 l-p:0.03681610897183418
epoch£º981	 i:6 	 global-step:19626	 l-p:0.14280155301094055
epoch£º981	 i:7 	 global-step:19627	 l-p:0.14812389016151428
epoch£º981	 i:8 	 global-step:19628	 l-p:0.1667856127023697
epoch£º981	 i:9 	 global-step:19629	 l-p:0.5298185348510742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0690, 2.1365, 2.1463],
        [3.0690, 1.9445, 1.2922],
        [3.0690, 3.0690, 3.0690],
        [3.0690, 2.7953, 2.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.07574120163917542 
model_pd.l_d.mean(): -25.10569953918457 
model_pd.lagr.mean(): -25.029958724975586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0334], device='cuda:0')), ('power', tensor([-25.1391], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.07574120163917542
epoch£º982	 i:1 	 global-step:19641	 l-p:0.15524505078792572
epoch£º982	 i:2 	 global-step:19642	 l-p:0.14221276342868805
epoch£º982	 i:3 	 global-step:19643	 l-p:0.15103541314601898
epoch£º982	 i:4 	 global-step:19644	 l-p:0.06382257491350174
epoch£º982	 i:5 	 global-step:19645	 l-p:-6.777568817138672
epoch£º982	 i:6 	 global-step:19646	 l-p:0.13251341879367828
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12706124782562256
epoch£º982	 i:8 	 global-step:19648	 l-p:0.21044403314590454
epoch£º982	 i:9 	 global-step:19649	 l-p:0.15520496666431427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0518, 2.4783, 2.6818],
        [3.0518, 2.6178, 2.8291],
        [3.0518, 2.7778, 2.9540],
        [3.0518, 3.0430, 3.0515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.41134780645370483 
model_pd.l_d.mean(): -25.183197021484375 
model_pd.lagr.mean(): -24.771848678588867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0427], device='cuda:0')), ('power', tensor([-25.2259], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:0.41134780645370483
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11982458084821701
epoch£º983	 i:2 	 global-step:19662	 l-p:0.012344426475465298
epoch£º983	 i:3 	 global-step:19663	 l-p:0.09575704485177994
epoch£º983	 i:4 	 global-step:19664	 l-p:0.13753627240657806
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11643648892641068
epoch£º983	 i:6 	 global-step:19666	 l-p:0.11208651959896088
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14952559769153595
epoch£º983	 i:8 	 global-step:19668	 l-p:0.181144118309021
epoch£º983	 i:9 	 global-step:19669	 l-p:0.156206876039505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0405, 1.7139, 1.1396],
        [3.0405, 3.0222, 3.0395],
        [3.0405, 3.0402, 3.0405],
        [3.0405, 3.0253, 3.0397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.2380019873380661 
model_pd.l_d.mean(): -24.95657730102539 
model_pd.lagr.mean(): -24.71857452392578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0714], device='cuda:0')), ('power', tensor([-25.0280], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.2380019873380661
epoch£º984	 i:1 	 global-step:19681	 l-p:0.17206259071826935
epoch£º984	 i:2 	 global-step:19682	 l-p:0.0974278524518013
epoch£º984	 i:3 	 global-step:19683	 l-p:0.14263443648815155
epoch£º984	 i:4 	 global-step:19684	 l-p:0.0858956053853035
epoch£º984	 i:5 	 global-step:19685	 l-p:-0.35020309686660767
epoch£º984	 i:6 	 global-step:19686	 l-p:0.23140886425971985
epoch£º984	 i:7 	 global-step:19687	 l-p:0.08052709698677063
epoch£º984	 i:8 	 global-step:19688	 l-p:0.12766718864440918
epoch£º984	 i:9 	 global-step:19689	 l-p:0.1380268782377243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1118, 2.1507, 1.4630],
        [3.1118, 2.5404, 2.7429],
        [3.1118, 1.9055, 1.2612],
        [3.1118, 3.1118, 3.1118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.2710445821285248 
model_pd.l_d.mean(): -25.057832717895508 
model_pd.lagr.mean(): -24.786788940429688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0184], device='cuda:0')), ('power', tensor([-25.0762], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.2710445821285248
epoch£º985	 i:1 	 global-step:19701	 l-p:0.25825047492980957
epoch£º985	 i:2 	 global-step:19702	 l-p:0.19350183010101318
epoch£º985	 i:3 	 global-step:19703	 l-p:0.15809579193592072
epoch£º985	 i:4 	 global-step:19704	 l-p:0.11996825784444809
epoch£º985	 i:5 	 global-step:19705	 l-p:0.14921388030052185
epoch£º985	 i:6 	 global-step:19706	 l-p:0.12508590519428253
epoch£º985	 i:7 	 global-step:19707	 l-p:0.022336682304739952
epoch£º985	 i:8 	 global-step:19708	 l-p:0.13868655264377594
epoch£º985	 i:9 	 global-step:19709	 l-p:0.1292269080877304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1697, 3.1697, 3.1697],
        [3.1697, 2.8800, 3.0610],
        [3.1697, 3.1567, 3.1691],
        [3.1697, 2.7502, 2.9588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.13254354894161224 
model_pd.l_d.mean(): -25.05510711669922 
model_pd.lagr.mean(): -24.922563552856445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0174], device='cuda:0')), ('power', tensor([-25.0377], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.13254354894161224
epoch£º986	 i:1 	 global-step:19721	 l-p:0.12967051565647125
epoch£º986	 i:2 	 global-step:19722	 l-p:0.15095092356204987
epoch£º986	 i:3 	 global-step:19723	 l-p:0.19532832503318787
epoch£º986	 i:4 	 global-step:19724	 l-p:0.14054596424102783
epoch£º986	 i:5 	 global-step:19725	 l-p:0.14167073369026184
epoch£º986	 i:6 	 global-step:19726	 l-p:0.15459953248500824
epoch£º986	 i:7 	 global-step:19727	 l-p:0.13343533873558044
epoch£º986	 i:8 	 global-step:19728	 l-p:-0.03255050629377365
epoch£º986	 i:9 	 global-step:19729	 l-p:0.026357121765613556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0729, 3.0729, 3.0729],
        [3.0729, 3.0729, 3.0729],
        [3.0729, 2.9226, 3.0385],
        [3.0729, 2.7992, 2.9751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11686914414167404 
model_pd.l_d.mean(): -24.612810134887695 
model_pd.lagr.mean(): -24.495941162109375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0049], device='cuda:0')), ('power', tensor([-24.6079], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11686914414167404
epoch£º987	 i:1 	 global-step:19741	 l-p:0.08003062754869461
epoch£º987	 i:2 	 global-step:19742	 l-p:-0.49445104598999023
epoch£º987	 i:3 	 global-step:19743	 l-p:0.2688373625278473
epoch£º987	 i:4 	 global-step:19744	 l-p:0.15585695207118988
epoch£º987	 i:5 	 global-step:19745	 l-p:0.10614317655563354
epoch£º987	 i:6 	 global-step:19746	 l-p:0.13960881531238556
epoch£º987	 i:7 	 global-step:19747	 l-p:0.1563812494277954
epoch£º987	 i:8 	 global-step:19748	 l-p:0.15387879312038422
epoch£º987	 i:9 	 global-step:19749	 l-p:0.14917631447315216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0559, 1.7910, 1.3807],
        [3.0559, 1.9134, 1.6760],
        [3.0559, 3.0547, 3.0559],
        [3.0559, 3.0559, 3.0559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.16324982047080994 
model_pd.l_d.mean(): -25.151824951171875 
model_pd.lagr.mean(): -24.988574981689453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0456], device='cuda:0')), ('power', tensor([-25.1062], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.16324982047080994
epoch£º988	 i:1 	 global-step:19761	 l-p:-0.01956489495933056
epoch£º988	 i:2 	 global-step:19762	 l-p:0.14000163972377777
epoch£º988	 i:3 	 global-step:19763	 l-p:0.12355485558509827
epoch£º988	 i:4 	 global-step:19764	 l-p:0.1701384335756302
epoch£º988	 i:5 	 global-step:19765	 l-p:0.11691192537546158
epoch£º988	 i:6 	 global-step:19766	 l-p:-0.23877669870853424
epoch£º988	 i:7 	 global-step:19767	 l-p:0.14716586470603943
epoch£º988	 i:8 	 global-step:19768	 l-p:0.11072275787591934
epoch£º988	 i:9 	 global-step:19769	 l-p:0.22007767856121063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0580, 2.3182, 2.4654],
        [3.0580, 3.0486, 3.0576],
        [3.0580, 2.6355, 2.8459],
        [3.0580, 3.0580, 3.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.1321071833372116 
model_pd.l_d.mean(): -25.14738655090332 
model_pd.lagr.mean(): -25.01527976989746 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0637], device='cuda:0')), ('power', tensor([-25.0837], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.1321071833372116
epoch£º989	 i:1 	 global-step:19781	 l-p:0.10794880986213684
epoch£º989	 i:2 	 global-step:19782	 l-p:0.1544131487607956
epoch£º989	 i:3 	 global-step:19783	 l-p:0.3470916748046875
epoch£º989	 i:4 	 global-step:19784	 l-p:-0.002061386127024889
epoch£º989	 i:5 	 global-step:19785	 l-p:0.14721304178237915
epoch£º989	 i:6 	 global-step:19786	 l-p:0.13385172188282013
epoch£º989	 i:7 	 global-step:19787	 l-p:0.13888199627399445
epoch£º989	 i:8 	 global-step:19788	 l-p:-0.041340168565511703
epoch£º989	 i:9 	 global-step:19789	 l-p:0.12665748596191406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[3.0946, 2.0580, 1.3853],
        [3.0946, 2.0264, 1.8867],
        [3.0946, 1.7687, 1.1736],
        [3.0946, 2.0760, 1.4002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): -0.10159492492675781 
model_pd.l_d.mean(): -24.605628967285156 
model_pd.lagr.mean(): -24.707223892211914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1910], device='cuda:0')), ('power', tensor([-24.7966], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:-0.10159492492675781
epoch£º990	 i:1 	 global-step:19801	 l-p:2.578399896621704
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12278095632791519
epoch£º990	 i:3 	 global-step:19803	 l-p:0.18565452098846436
epoch£º990	 i:4 	 global-step:19804	 l-p:0.10888107866048813
epoch£º990	 i:5 	 global-step:19805	 l-p:0.144723579287529
epoch£º990	 i:6 	 global-step:19806	 l-p:0.1488022357225418
epoch£º990	 i:7 	 global-step:19807	 l-p:0.13718928396701813
epoch£º990	 i:8 	 global-step:19808	 l-p:0.0033508490305393934
epoch£º990	 i:9 	 global-step:19809	 l-p:0.16967415809631348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0855, 2.9308, 3.0494],
        [3.0855, 1.9229, 1.6549],
        [3.0855, 2.8331, 3.0007],
        [3.0855, 3.0733, 3.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.039516348391771317 
model_pd.l_d.mean(): -25.210124969482422 
model_pd.lagr.mean(): -25.170608520507812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0796], device='cuda:0')), ('power', tensor([-25.1305], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.039516348391771317
epoch£º991	 i:1 	 global-step:19821	 l-p:-0.1278262436389923
epoch£º991	 i:2 	 global-step:19822	 l-p:0.13164152204990387
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15660437941551208
epoch£º991	 i:4 	 global-step:19824	 l-p:0.121178537607193
epoch£º991	 i:5 	 global-step:19825	 l-p:0.22164838016033173
epoch£º991	 i:6 	 global-step:19826	 l-p:0.1342829465866089
epoch£º991	 i:7 	 global-step:19827	 l-p:0.15485015511512756
epoch£º991	 i:8 	 global-step:19828	 l-p:0.1118592619895935
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10281285643577576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0699, 3.0695, 3.0699],
        [3.0699, 3.0256, 3.0655],
        [3.0699, 1.8681, 1.2310],
        [3.0699, 1.7513, 1.1563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.05210723355412483 
model_pd.l_d.mean(): -25.158601760864258 
model_pd.lagr.mean(): -25.106494903564453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0141], device='cuda:0')), ('power', tensor([-25.1445], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.05210723355412483
epoch£º992	 i:1 	 global-step:19841	 l-p:0.6510828733444214
epoch£º992	 i:2 	 global-step:19842	 l-p:0.06698063015937805
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10741958022117615
epoch£º992	 i:4 	 global-step:19844	 l-p:0.15312592685222626
epoch£º992	 i:5 	 global-step:19845	 l-p:0.1949443817138672
epoch£º992	 i:6 	 global-step:19846	 l-p:0.16488748788833618
epoch£º992	 i:7 	 global-step:19847	 l-p:0.13398493826389313
epoch£º992	 i:8 	 global-step:19848	 l-p:0.1916472166776657
epoch£º992	 i:9 	 global-step:19849	 l-p:0.09818530827760696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0659, 1.9058, 1.2607],
        [3.0659, 1.8301, 1.4617],
        [3.0659, 3.0658, 3.0659],
        [3.0659, 3.0333, 3.0633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.19905638694763184 
model_pd.l_d.mean(): -25.163532257080078 
model_pd.lagr.mean(): -24.964475631713867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0887], device='cuda:0')), ('power', tensor([-25.2522], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.19905638694763184
epoch£º993	 i:1 	 global-step:19861	 l-p:0.14911100268363953
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12215948104858398
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15515372157096863
epoch£º993	 i:4 	 global-step:19864	 l-p:0.05306016653776169
epoch£º993	 i:5 	 global-step:19865	 l-p:0.18976901471614838
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12029590457677841
epoch£º993	 i:7 	 global-step:19867	 l-p:0.12364103645086288
epoch£º993	 i:8 	 global-step:19868	 l-p:0.13889388740062714
epoch£º993	 i:9 	 global-step:19869	 l-p:-0.07144033163785934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0498, 3.0353, 3.0491],
        [3.0498, 1.8434, 1.2113],
        [3.0498, 1.7177, 1.1783],
        [3.0498, 3.0493, 3.0499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13623900711536407 
model_pd.l_d.mean(): -25.07479476928711 
model_pd.lagr.mean(): -24.938556671142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0590], device='cuda:0')), ('power', tensor([-25.1338], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13623900711536407
epoch£º994	 i:1 	 global-step:19881	 l-p:0.0841236487030983
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11127841472625732
epoch£º994	 i:3 	 global-step:19883	 l-p:0.04641904681921005
epoch£º994	 i:4 	 global-step:19884	 l-p:0.31212350726127625
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13679124414920807
epoch£º994	 i:6 	 global-step:19886	 l-p:-0.09198438376188278
epoch£º994	 i:7 	 global-step:19887	 l-p:0.15849781036376953
epoch£º994	 i:8 	 global-step:19888	 l-p:0.16343750059604645
epoch£º994	 i:9 	 global-step:19889	 l-p:0.14629410207271576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0336, 2.6484, 2.8544],
        [3.0336, 1.7059, 1.1341],
        [3.0336, 2.3570, 2.5330],
        [3.0336, 3.0273, 3.0334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.15332292020320892 
model_pd.l_d.mean(): -25.21598243713379 
model_pd.lagr.mean(): -25.062660217285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0336], device='cuda:0')), ('power', tensor([-25.2496], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.15332292020320892
epoch£º995	 i:1 	 global-step:19901	 l-p:0.0762099027633667
epoch£º995	 i:2 	 global-step:19902	 l-p:0.13009819388389587
epoch£º995	 i:3 	 global-step:19903	 l-p:0.21794791519641876
epoch£º995	 i:4 	 global-step:19904	 l-p:0.16123400628566742
epoch£º995	 i:5 	 global-step:19905	 l-p:0.16995760798454285
epoch£º995	 i:6 	 global-step:19906	 l-p:0.12435783445835114
epoch£º995	 i:7 	 global-step:19907	 l-p:0.042148735374212265
epoch£º995	 i:8 	 global-step:19908	 l-p:0.06159166991710663
epoch£º995	 i:9 	 global-step:19909	 l-p:0.11863577365875244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0112, 2.2277, 2.3538],
        [3.0112, 2.0032, 1.3375],
        [3.0112, 1.7804, 1.4254],
        [3.0112, 3.0092, 3.0112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): -0.04463450238108635 
model_pd.l_d.mean(): -24.45537567138672 
model_pd.lagr.mean(): -24.500009536743164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2301], device='cuda:0')), ('power', tensor([-24.6855], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:-0.04463450238108635
epoch£º996	 i:1 	 global-step:19921	 l-p:0.17690423130989075
epoch£º996	 i:2 	 global-step:19922	 l-p:-0.06891780346632004
epoch£º996	 i:3 	 global-step:19923	 l-p:0.1319403499364853
epoch£º996	 i:4 	 global-step:19924	 l-p:0.15838958323001862
epoch£º996	 i:5 	 global-step:19925	 l-p:0.13339471817016602
epoch£º996	 i:6 	 global-step:19926	 l-p:0.1328643262386322
epoch£º996	 i:7 	 global-step:19927	 l-p:0.14452548325061798
epoch£º996	 i:8 	 global-step:19928	 l-p:0.13374100625514984
epoch£º996	 i:9 	 global-step:19929	 l-p:0.1130705252289772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0582, 2.8145, 2.9787],
        [3.0582, 3.0582, 3.0582],
        [3.0582, 1.9145, 1.6768],
        [3.0582, 1.8777, 1.2381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.14326681196689606 
model_pd.l_d.mean(): -24.915203094482422 
model_pd.lagr.mean(): -24.771936416625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1615], device='cuda:0')), ('power', tensor([-25.0767], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.14326681196689606
epoch£º997	 i:1 	 global-step:19941	 l-p:0.11174515634775162
epoch£º997	 i:2 	 global-step:19942	 l-p:-0.009088551625609398
epoch£º997	 i:3 	 global-step:19943	 l-p:0.11216869205236435
epoch£º997	 i:4 	 global-step:19944	 l-p:0.14030201733112335
epoch£º997	 i:5 	 global-step:19945	 l-p:-0.1275865137577057
epoch£º997	 i:6 	 global-step:19946	 l-p:0.3177211582660675
epoch£º997	 i:7 	 global-step:19947	 l-p:0.13719186186790466
epoch£º997	 i:8 	 global-step:19948	 l-p:0.11019919812679291
epoch£º997	 i:9 	 global-step:19949	 l-p:0.1632818877696991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0447, 1.7203, 1.1383],
        [3.0447, 3.0398, 3.0445],
        [3.0447, 3.0447, 3.0447],
        [3.0447, 2.9293, 3.0229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.143317312002182 
model_pd.l_d.mean(): -25.275299072265625 
model_pd.lagr.mean(): -25.131980895996094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0844], device='cuda:0')), ('power', tensor([-25.1909], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.143317312002182
epoch£º998	 i:1 	 global-step:19961	 l-p:0.004502539522945881
epoch£º998	 i:2 	 global-step:19962	 l-p:0.16319099068641663
epoch£º998	 i:3 	 global-step:19963	 l-p:0.18655987083911896
epoch£º998	 i:4 	 global-step:19964	 l-p:0.2380398064851761
epoch£º998	 i:5 	 global-step:19965	 l-p:0.14137715101242065
epoch£º998	 i:6 	 global-step:19966	 l-p:0.15551167726516724
epoch£º998	 i:7 	 global-step:19967	 l-p:-0.008436269126832485
epoch£º998	 i:8 	 global-step:19968	 l-p:0.6547671556472778
epoch£º998	 i:9 	 global-step:19969	 l-p:0.14302407205104828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0532, 3.0525, 3.0532],
        [3.0532, 2.3645, 2.5355],
        [3.0532, 3.0447, 3.0529],
        [3.0532, 3.0400, 3.0526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.14162345230579376 
model_pd.l_d.mean(): -24.185523986816406 
model_pd.lagr.mean(): -24.043901443481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1900], device='cuda:0')), ('power', tensor([-24.3755], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.14162345230579376
epoch£º999	 i:1 	 global-step:19981	 l-p:0.06002501770853996
epoch£º999	 i:2 	 global-step:19982	 l-p:0.10571037977933884
epoch£º999	 i:3 	 global-step:19983	 l-p:0.11780229210853577
epoch£º999	 i:4 	 global-step:19984	 l-p:0.19006939232349396
epoch£º999	 i:5 	 global-step:19985	 l-p:0.13912029564380646
epoch£º999	 i:6 	 global-step:19986	 l-p:0.20067618787288666
epoch£º999	 i:7 	 global-step:19987	 l-p:0.22137828171253204
epoch£º999	 i:8 	 global-step:19988	 l-p:-0.2915366590023041
epoch£º999	 i:9 	 global-step:19989	 l-p:0.1110321506857872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1198, 2.8462, 3.0220],
        [3.1198, 1.7790, 1.2177],
        [3.1198, 2.8287, 3.0106],
        [3.1198, 1.9082, 1.2629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.1216922327876091 
model_pd.l_d.mean(): -25.04652976989746 
model_pd.lagr.mean(): -24.924837112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0386], device='cuda:0')), ('power', tensor([-25.0079], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.1216922327876091
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12534956634044647
epoch£º1000	 i:2 	 global-step:20002	 l-p:0.2619958519935608
epoch£º1000	 i:3 	 global-step:20003	 l-p:0.2619639039039612
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.12881629168987274
epoch£º1000	 i:5 	 global-step:20005	 l-p:0.2191738486289978
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.13134855031967163
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.19217579066753387
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.15585409104824066
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.13487577438354492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1284, 3.1251, 3.1283],
        [3.1284, 2.1995, 2.2113],
        [3.1284, 2.5865, 2.7939],
        [3.1284, 3.1283, 3.1284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.2025991827249527 
model_pd.l_d.mean(): -24.64188575744629 
model_pd.lagr.mean(): -24.439287185668945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0340], device='cuda:0')), ('power', tensor([-24.6079], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:0.2025991827249527
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.13964961469173431
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.0997575893998146
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.15999636054039001
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.13380667567253113
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.21586237847805023
epoch£º1001	 i:6 	 global-step:20026	 l-p:0.2221519947052002
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.1466846466064453
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.15773339569568634
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.14806735515594482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1322, 3.0676, 3.1240],
        [3.1322, 2.3521, 2.4753],
        [3.1322, 2.1599, 1.4705],
        [3.1322, 2.6992, 2.9099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.1507837027311325 
model_pd.l_d.mean(): -25.042537689208984 
model_pd.lagr.mean(): -24.891754150390625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0252], device='cuda:0')), ('power', tensor([-25.0174], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.1507837027311325
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.1533544957637787
epoch£º1002	 i:2 	 global-step:20042	 l-p:0.22883689403533936
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.1068996712565422
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.15286299586296082
epoch£º1002	 i:5 	 global-step:20045	 l-p:0.24072040617465973
epoch£º1002	 i:6 	 global-step:20046	 l-p:0.2853744924068451
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.15850836038589478
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.1308034509420395
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.13203677535057068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1246, 1.7944, 1.1927],
        [3.1246, 3.1246, 3.1246],
        [3.1246, 3.1246, 3.1246],
        [3.1246, 2.9964, 3.0984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.11330490559339523 
model_pd.l_d.mean(): -24.778789520263672 
model_pd.lagr.mean(): -24.665485382080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1063], device='cuda:0')), ('power', tensor([-24.8851], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.11330490559339523
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.13816307485103607
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.14429758489131927
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.13149896264076233
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.12336847186088562
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.18976376950740814
epoch£º1003	 i:6 	 global-step:20066	 l-p:0.24683965742588043
epoch£º1003	 i:7 	 global-step:20067	 l-p:0.6432214379310608
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.1329120248556137
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.16981124877929688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1151, 1.8654, 1.4733],
        [3.1151, 1.8784, 1.2395],
        [3.1151, 2.2215, 2.2652],
        [3.1151, 3.1144, 3.1151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.20053789019584656 
model_pd.l_d.mean(): -25.038305282592773 
model_pd.lagr.mean(): -24.837766647338867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0896], device='cuda:0')), ('power', tensor([-25.1279], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.20053789019584656
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.13599249720573425
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.12447678297758102
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.12713216245174408
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.1349204033613205
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.11982765048742294
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.15989139676094055
epoch£º1004	 i:7 	 global-step:20087	 l-p:0.18591290712356567
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.1162966638803482
epoch£º1004	 i:9 	 global-step:20089	 l-p:0.4071114659309387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1066, 2.7458, 2.9468],
        [3.1066, 2.2029, 2.2384],
        [3.1066, 3.1025, 3.1065],
        [3.1066, 2.5336, 2.7366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.1284627914428711 
model_pd.l_d.mean(): -25.042760848999023 
model_pd.lagr.mean(): -24.91429901123047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0622], device='cuda:0')), ('power', tensor([-24.9806], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.1284627914428711
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11231814324855804
epoch£º1005	 i:2 	 global-step:20102	 l-p:-1.0005502700805664
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.14198468625545502
epoch£º1005	 i:4 	 global-step:20104	 l-p:1.323007345199585
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.13458946347236633
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.15235546231269836
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.18072104454040527
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.16779030859470367
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.10098495334386826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[3.1048, 2.0793, 1.4026],
        [3.1048, 1.7764, 1.1788],
        [3.1048, 1.9743, 1.3160],
        [3.1048, 1.9342, 1.2834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.41434386372566223 
model_pd.l_d.mean(): -24.637414932250977 
model_pd.lagr.mean(): -24.22307014465332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2277], device='cuda:0')), ('power', tensor([-24.8651], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.41434386372566223
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.22835299372673035
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.688000500202179
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.08151198923587799
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.25672367215156555
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.1432800143957138
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.14933514595031738
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.14163435995578766
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.13503478467464447
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.1136026382446289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1340, 2.9645, 3.0917],
        [3.1340, 3.1320, 3.1340],
        [3.1340, 3.1340, 3.1340],
        [3.1340, 3.0685, 3.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.21532726287841797 
model_pd.l_d.mean(): -24.861534118652344 
model_pd.lagr.mean(): -24.64620590209961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0751], device='cuda:0')), ('power', tensor([-24.9366], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:0.21532726287841797
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.1490296572446823
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.1293170303106308
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11485869437456131
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.14235307276248932
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.11322830617427826
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.18187472224235535
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.13750703632831573
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13581687211990356
epoch£º1007	 i:9 	 global-step:20149	 l-p:1.0983216762542725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1091, 2.4351, 2.6099],
        [3.1091, 3.0065, 3.0912],
        [3.1091, 1.7730, 1.1848],
        [3.1091, 1.7747, 1.2394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.20091387629508972 
model_pd.l_d.mean(): -25.192821502685547 
model_pd.lagr.mean(): -24.991907119750977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0420], device='cuda:0')), ('power', tensor([-25.2348], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.20091387629508972
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.21042902767658234
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.08632224798202515
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.12702886760234833
epoch£º1008	 i:4 	 global-step:20164	 l-p:0.2696797251701355
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.147206649184227
epoch£º1008	 i:6 	 global-step:20166	 l-p:0.5337187051773071
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.1338605135679245
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.14410340785980225
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.12562081217765808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1157, 3.1072, 3.1154],
        [3.1157, 3.1155, 3.1157],
        [3.1157, 2.4730, 2.6584],
        [3.1157, 2.7288, 2.9344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.15341892838478088 
model_pd.l_d.mean(): -25.181896209716797 
model_pd.lagr.mean(): -25.02847671508789 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0103], device='cuda:0')), ('power', tensor([-25.1716], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.15341892838478088
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.1537146270275116
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.14111748337745667
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.5159034132957458
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.14098548889160156
epoch£º1009	 i:5 	 global-step:20185	 l-p:-0.8739219307899475
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.13508418202400208
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12490899115800858
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.8909961581230164
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.15019546449184418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0962, 1.7734, 1.1727],
        [3.0962, 3.0958, 3.0962],
        [3.0962, 3.0962, 3.0962],
        [3.0962, 3.0306, 3.0877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12414728105068207 
model_pd.l_d.mean(): -24.93047332763672 
model_pd.lagr.mean(): -24.806325912475586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0441], device='cuda:0')), ('power', tensor([-24.9746], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12414728105068207
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.14798426628112793
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.039832837879657745
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.15686629712581635
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.16963353753089905
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.1288605034351349
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.102269247174263
epoch£º1010	 i:7 	 global-step:20207	 l-p:1.4503716230392456
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.01260173786431551
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.31426823139190674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0592, 1.8778, 1.2378],
        [3.0592, 1.8981, 1.2539],
        [3.0592, 1.8913, 1.2485],
        [3.0592, 3.0577, 3.0592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.17608687281608582 
model_pd.l_d.mean(): -24.407575607299805 
model_pd.lagr.mean(): -24.231489181518555 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2686], device='cuda:0')), ('power', tensor([-24.6761], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.17608687281608582
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.19533011317253113
epoch£º1011	 i:2 	 global-step:20222	 l-p:-0.3802749216556549
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.07263423502445221
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.23381541669368744
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.06127506121993065
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.1252976357936859
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.049453869462013245
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.1246422752737999
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.14547696709632874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0881, 2.5773, 2.7888],
        [3.0881, 3.0744, 3.0874],
        [3.0881, 1.8199, 1.4047],
        [3.0881, 2.7164, 2.9197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.131123349070549 
model_pd.l_d.mean(): -25.204742431640625 
model_pd.lagr.mean(): -25.073619842529297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0185], device='cuda:0')), ('power', tensor([-25.1862], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.131123349070549
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.13043902814388275
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.14291898906230927
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.14914605021476746
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.03740796819329262
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.1718335896730423
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.12408939003944397
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.14684328436851501
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.00426045386120677
epoch£º1012	 i:9 	 global-step:20249	 l-p:0.10879969596862793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0871, 1.7878, 1.1749],
        [3.0871, 3.0871, 3.0871],
        [3.0871, 1.8978, 1.5943],
        [3.0871, 1.8405, 1.4569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): -0.07495849579572678 
model_pd.l_d.mean(): -25.145626068115234 
model_pd.lagr.mean(): -25.220584869384766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0308], device='cuda:0')), ('power', tensor([-25.1764], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:-0.07495849579572678
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.18223056197166443
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.1264084279537201
epoch£º1013	 i:3 	 global-step:20263	 l-p:-0.07803330570459366
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.17474566400051117
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.09790021926164627
epoch£º1013	 i:6 	 global-step:20266	 l-p:0.17631785571575165
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.13694064319133759
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.13991549611091614
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.36750662326812744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1200, 2.1941, 1.4988],
        [3.1200, 3.1189, 3.1199],
        [3.1200, 3.1199, 3.1200],
        [3.1200, 1.8824, 1.5073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.2768917381763458 
model_pd.l_d.mean(): -24.85356330871582 
model_pd.lagr.mean(): -24.576671600341797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0259], device='cuda:0')), ('power', tensor([-24.8795], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.2768917381763458
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.18863245844841003
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.08884080499410629
epoch£º1014	 i:3 	 global-step:20283	 l-p:0.26114675402641296
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.12185979634523392
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.14204873144626617
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.11769676953554153
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.1580900102853775
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.14201326668262482
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.1322900503873825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1514, 1.8258, 1.2118],
        [3.1514, 3.1338, 3.1504],
        [3.1514, 3.1458, 3.1513],
        [3.1514, 2.1827, 1.4900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.1386350840330124 
model_pd.l_d.mean(): -24.99268913269043 
model_pd.lagr.mean(): -24.854053497314453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0864], device='cuda:0')), ('power', tensor([-25.0791], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.1386350840330124
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.1745680570602417
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.10999735444784164
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.18766269087791443
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.17214462161064148
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.2152741253376007
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.14426325261592865
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.12186679244041443
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.07637433707714081
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.11495785415172577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1523, 3.1512, 3.1523],
        [3.1523, 3.1523, 3.1523],
        [3.1523, 2.9819, 3.1096],
        [3.1523, 3.1476, 3.1522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.1326787918806076 
model_pd.l_d.mean(): -24.560218811035156 
model_pd.lagr.mean(): -24.427539825439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0038], device='cuda:0')), ('power', tensor([-24.5564], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.1326787918806076
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.14761318266391754
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.14150680601596832
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.18430587649345398
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.13203057646751404
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.2018747627735138
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.14482685923576355
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.1338733434677124
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.14510971307754517
epoch£º1016	 i:9 	 global-step:20329	 l-p:0.1253066062927246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1382, 3.1381, 3.1382],
        [3.1382, 2.8524, 3.0325],
        [3.1382, 2.3087, 2.4005],
        [3.1382, 3.1382, 3.1382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.10893608629703522 
model_pd.l_d.mean(): -24.901309967041016 
model_pd.lagr.mean(): -24.792373657226562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0191], device='cuda:0')), ('power', tensor([-24.9204], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.10893608629703522
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.12679219245910645
epoch£º1017	 i:2 	 global-step:20342	 l-p:0.15137502551078796
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.1470971554517746
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.14322839677333832
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.2722196578979492
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.17229069769382477
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.1681170016527176
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.14104008674621582
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.3031259775161743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1200, 3.1063, 3.1193],
        [3.1200, 2.8654, 3.0339],
        [3.1200, 1.8723, 1.2350],
        [3.1200, 2.0533, 1.9167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.11869731545448303 
model_pd.l_d.mean(): -24.73868751525879 
model_pd.lagr.mean(): -24.6199893951416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0258], device='cuda:0')), ('power', tensor([-24.7645], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.11869731545448303
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.1603107750415802
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.24264734983444214
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.1377330720424652
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.12918083369731903
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.6485462188720703
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.17672893404960632
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.4087517559528351
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.1213161051273346
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.17973239719867706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1137, 3.0821, 3.1112],
        [3.1137, 2.1135, 2.0555],
        [3.1137, 1.9007, 1.2566],
        [3.1137, 2.1676, 1.4763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.13927878439426422 
model_pd.l_d.mean(): -25.071060180664062 
model_pd.lagr.mean(): -24.931781768798828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0480], device='cuda:0')), ('power', tensor([-25.0230], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.13927878439426422
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.14958152174949646
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.14564953744411469
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.1539342999458313
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.12253068387508392
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.13052739202976227
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.4689706563949585
epoch£º1019	 i:7 	 global-step:20387	 l-p:-0.24038712680339813
epoch£º1019	 i:8 	 global-step:20388	 l-p:-1.016963005065918
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.17162904143333435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1052, 1.7816, 1.2728],
        [3.1052, 2.8230, 3.0021],
        [3.1052, 1.8671, 1.4936],
        [3.1052, 1.7930, 1.1821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.19201543927192688 
model_pd.l_d.mean(): -25.083179473876953 
model_pd.lagr.mean(): -24.891164779663086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0243], device='cuda:0')), ('power', tensor([-25.1075], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.19201543927192688
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.13236171007156372
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.1539541631937027
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.15638861060142517
epoch£º1020	 i:4 	 global-step:20404	 l-p:0.7190679907798767
epoch£º1020	 i:5 	 global-step:20405	 l-p:1.5212434530258179
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.14445993304252625
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.15363213419914246
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.11654947698116302
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.12806284427642822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1131, 1.9470, 1.6751],
        [3.1131, 3.1131, 3.1131],
        [3.1131, 1.8066, 1.3286],
        [3.1131, 2.1517, 2.1342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.12951181828975677 
model_pd.l_d.mean(): -25.193708419799805 
model_pd.lagr.mean(): -25.064197540283203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([9.4461e-06], device='cuda:0')), ('power', tensor([-25.1937], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.12951181828975677
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.12411139905452728
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.7775881290435791
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.18103136122226715
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.12620040774345398
epoch£º1021	 i:5 	 global-step:20425	 l-p:-0.31799912452697754
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.15522997081279755
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.6776168942451477
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.17470456659793854
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.13393382728099823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1024, 3.1021, 3.1024],
        [3.1024, 1.7790, 1.2706],
        [3.1024, 2.4116, 2.5804],
        [3.1024, 3.1024, 3.1024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.13380956649780273 
model_pd.l_d.mean(): -25.131671905517578 
model_pd.lagr.mean(): -24.997861862182617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0366], device='cuda:0')), ('power', tensor([-25.0951], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.13380956649780273
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.17458288371562958
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.1319964975118637
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.12685707211494446
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.09236107021570206
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.17581386864185333
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.1438651829957962
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.08406972885131836
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.1629076898097992
epoch£º1022	 i:9 	 global-step:20449	 l-p:-0.1616288721561432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0852, 3.0832, 3.0851],
        [3.0852, 1.7901, 1.3340],
        [3.0852, 3.0852, 3.0852],
        [3.0852, 2.5319, 2.7390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.1402992308139801 
model_pd.l_d.mean(): -25.26235008239746 
model_pd.lagr.mean(): -25.122051239013672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1251], device='cuda:0')), ('power', tensor([-25.1373], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.1402992308139801
epoch£º1023	 i:1 	 global-step:20461	 l-p:0.28001347184181213
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.10942834615707397
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.16163264214992523
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.15158770978450775
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.21790458261966705
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.12275227904319763
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.12098398059606552
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.07748513668775558
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.06004353240132332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0685, 3.0685, 3.0685],
        [3.0685, 1.8250, 1.1973],
        [3.0685, 1.8089, 1.1858],
        [3.0685, 3.0685, 3.0685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.15705440938472748 
model_pd.l_d.mean(): -24.815858840942383 
model_pd.lagr.mean(): -24.658803939819336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1284], device='cuda:0')), ('power', tensor([-24.9443], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.15705440938472748
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.8551310300827026
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.07360270619392395
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.15747441351413727
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12936639785766602
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.18378281593322754
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.15152962505817413
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.09377837926149368
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.06019676849246025
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.14746975898742676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0774, 3.0733, 3.0773],
        [3.0774, 3.0618, 3.0766],
        [3.0774, 3.0774, 3.0774],
        [3.0774, 3.0750, 3.0774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.02250366099178791 
model_pd.l_d.mean(): -25.105928421020508 
model_pd.lagr.mean(): -25.083425521850586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0881], device='cuda:0')), ('power', tensor([-25.0178], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.02250366099178791
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.14805974066257477
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13716444373130798
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.12323782593011856
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.1361417919397354
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.14322392642498016
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.07641369849443436
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.12040123343467712
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.2318364977836609
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.575919508934021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[3.0721, 2.1244, 1.4385],
        [3.0721, 1.8089, 1.4041],
        [3.0721, 1.8658, 1.2283],
        [3.0721, 1.7439, 1.2271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.12340053915977478 
model_pd.l_d.mean(): -24.90042495727539 
model_pd.lagr.mean(): -24.77702522277832 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1388], device='cuda:0')), ('power', tensor([-25.0392], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.12340053915977478
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.13102863729000092
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.1510912925004959
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.14219245314598083
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.15060515701770782
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.07755139470100403
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.2059909999370575
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.7176212668418884
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.1361244022846222
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.06830500066280365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0723, 1.8595, 1.5261],
        [3.0723, 1.7510, 1.1550],
        [3.0723, 2.7924, 2.9709],
        [3.0723, 2.1064, 2.0874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.12261729687452316 
model_pd.l_d.mean(): -24.942703247070312 
model_pd.lagr.mean(): -24.820085525512695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1617], device='cuda:0')), ('power', tensor([-25.1044], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.12261729687452316
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.1367163360118866
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.08612002432346344
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.13561347126960754
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.213340625166893
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.1560627669095993
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.5533561706542969
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.18272137641906738
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.028844323009252548
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.07238695025444031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0865, 3.0845, 3.0865],
        [3.0865, 3.0865, 3.0865],
        [3.0865, 3.0733, 3.0859],
        [3.0865, 3.0217, 3.0783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.15358512103557587 
model_pd.l_d.mean(): -25.194503784179688 
model_pd.lagr.mean(): -25.040918350219727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1385], device='cuda:0')), ('power', tensor([-25.0560], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.15358512103557587
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.1555941104888916
epoch£º1028	 i:2 	 global-step:20562	 l-p:-0.004081215709447861
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.2062816619873047
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.1434471756219864
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.13835464417934418
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.13666869699954987
epoch£º1028	 i:7 	 global-step:20567	 l-p:-1.4366623163223267
epoch£º1028	 i:8 	 global-step:20568	 l-p:0.39169418811798096
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.16537898778915405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1146, 1.7869, 1.2709],
        [3.1146, 3.1146, 3.1146],
        [3.1146, 3.1146, 3.1146],
        [3.1146, 2.3905, 2.5447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.1322358399629593 
model_pd.l_d.mean(): -25.178699493408203 
model_pd.lagr.mean(): -25.046463012695312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0824], device='cuda:0')), ('power', tensor([-25.0963], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.1322358399629593
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.1269766390323639
epoch£º1029	 i:2 	 global-step:20582	 l-p:0.4227506220340729
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.1431785523891449
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.17967139184474945
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.12084086239337921
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.17436906695365906
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.12932369112968445
epoch£º1029	 i:8 	 global-step:20588	 l-p:0.21386098861694336
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.16609302163124084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1310, 3.0931, 3.1276],
        [3.1310, 3.0185, 3.1100],
        [3.1310, 1.9555, 1.3003],
        [3.1310, 2.3265, 2.4359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.12609730660915375 
model_pd.l_d.mean(): -24.945173263549805 
model_pd.lagr.mean(): -24.819076538085938 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0104], device='cuda:0')), ('power', tensor([-24.9556], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.12609730660915375
epoch£º1030	 i:1 	 global-step:20601	 l-p:0.3168158531188965
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.13147152960300446
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.21725860238075256
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.0785243809223175
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.13377606868743896
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12236569821834564
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.15067492425441742
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.16377627849578857
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.13805125653743744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1412, 2.0927, 1.4137],
        [3.1412, 1.8227, 1.2057],
        [3.1412, 2.1030, 2.0010],
        [3.1412, 3.0757, 3.1328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.11947749555110931 
model_pd.l_d.mean(): -24.86135482788086 
model_pd.lagr.mean(): -24.74187660217285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0310], device='cuda:0')), ('power', tensor([-24.8923], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.11947749555110931
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.12371261417865753
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.14095710217952728
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.15458303689956665
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.1235835924744606
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.35200637578964233
epoch£º1031	 i:6 	 global-step:20626	 l-p:0.2815347909927368
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.12179046124219894
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.15009555220603943
epoch£º1031	 i:9 	 global-step:20629	 l-p:0.28176355361938477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1176, 2.2631, 2.3388],
        [3.1176, 2.8815, 3.0423],
        [3.1176, 3.1164, 3.1176],
        [3.1176, 2.6573, 2.8697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.1421125829219818 
model_pd.l_d.mean(): -24.927064895629883 
model_pd.lagr.mean(): -24.78495216369629 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0159], device='cuda:0')), ('power', tensor([-24.9112], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.1421125829219818
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.11021358519792557
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17726680636405945
epoch£º1032	 i:3 	 global-step:20643	 l-p:0.2234872430562973
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.12516093254089355
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.1413150429725647
epoch£º1032	 i:6 	 global-step:20646	 l-p:0.5294356942176819
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.13840997219085693
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.19656163454055786
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.1347077190876007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1171, 3.1170, 3.1171],
        [3.1171, 1.9271, 1.6216],
        [3.1171, 2.3596, 2.4971],
        [3.1171, 3.1136, 3.1170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.266826331615448 
model_pd.l_d.mean(): -25.119775772094727 
model_pd.lagr.mean(): -24.852949142456055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0235], device='cuda:0')), ('power', tensor([-25.1433], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:0.266826331615448
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.13630713522434235
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.20318886637687683
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.1370546668767929
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.13453751802444458
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.14236828684806824
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12389551848173141
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12638984620571136
epoch£º1033	 i:8 	 global-step:20668	 l-p:-0.25986647605895996
epoch£º1033	 i:9 	 global-step:20669	 l-p:-0.2963145077228546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0985, 1.7608, 1.2192],
        [3.0985, 3.0658, 3.0958],
        [3.0985, 2.9284, 3.0560],
        [3.0985, 3.0900, 3.0982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.16570283472537994 
model_pd.l_d.mean(): -25.1668758392334 
model_pd.lagr.mean(): -25.00117301940918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1199], device='cuda:0')), ('power', tensor([-25.0470], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.16570283472537994
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.14220768213272095
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.18498744070529938
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.14294259250164032
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.1646154373884201
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.054266881197690964
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.26871877908706665
epoch£º1034	 i:7 	 global-step:20687	 l-p:-0.06840202957391739
epoch£º1034	 i:8 	 global-step:20688	 l-p:-0.016812531277537346
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.10135059058666229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0935, 3.0549, 3.0900],
        [3.0935, 3.0675, 3.0917],
        [3.0935, 1.7694, 1.2626],
        [3.0935, 3.0381, 3.0871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.15742795169353485 
model_pd.l_d.mean(): -24.783748626708984 
model_pd.lagr.mean(): -24.626319885253906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0372], device='cuda:0')), ('power', tensor([-24.8209], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.15742795169353485
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.18388274312019348
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.15212318301200867
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.10421986132860184
epoch£º1035	 i:4 	 global-step:20704	 l-p:-0.7575908303260803
epoch£º1035	 i:5 	 global-step:20705	 l-p:0.5163174867630005
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.1410268247127533
epoch£º1035	 i:7 	 global-step:20707	 l-p:-2.8855319023132324
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.21587981283664703
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11490427702665329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1159, 3.0450, 3.1063],
        [3.1159, 2.1888, 1.4936],
        [3.1159, 2.4662, 2.6500],
        [3.1159, 2.5347, 2.7366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.34264281392097473 
model_pd.l_d.mean(): -24.693256378173828 
model_pd.lagr.mean(): -24.35061264038086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1161], device='cuda:0')), ('power', tensor([-24.8093], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:0.34264281392097473
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.14068277180194855
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.1490398794412613
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.07122226804494858
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.30988985300064087
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.14744485914707184
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.17477023601531982
epoch£º1036	 i:7 	 global-step:20727	 l-p:0.2141657918691635
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.16532188653945923
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.13382576406002045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1358, 3.0745, 3.1283],
        [3.1358, 2.5360, 2.7333],
        [3.1358, 1.9498, 1.6487],
        [3.1358, 3.1236, 3.1353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.2285984605550766 
model_pd.l_d.mean(): -24.497262954711914 
model_pd.lagr.mean(): -24.268665313720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0859], device='cuda:0')), ('power', tensor([-24.5832], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:0.2285984605550766
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.13237635791301727
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.14126776158809662
epoch£º1037	 i:3 	 global-step:20743	 l-p:0.20534616708755493
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.15713058412075043
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.0750405490398407
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.1335066556930542
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.18793857097625732
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12348783016204834
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.13260067999362946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1557, 2.2370, 2.2582],
        [3.1557, 1.8517, 1.2236],
        [3.1557, 2.2625, 2.3059],
        [3.1557, 3.1557, 3.1557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.13672460615634918 
model_pd.l_d.mean(): -24.807601928710938 
model_pd.lagr.mean(): -24.67087745666504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0595], device='cuda:0')), ('power', tensor([-24.7481], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.13672460615634918
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.13446763157844543
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.11724440008401871
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.14199945330619812
epoch£º1038	 i:4 	 global-step:20764	 l-p:0.20410746335983276
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.2196197211742401
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.12656599283218384
epoch£º1038	 i:7 	 global-step:20767	 l-p:0.19429658353328705
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.14975784718990326
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.08636333048343658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1368, 2.8547, 3.0337],
        [3.1368, 2.5900, 2.7972],
        [3.1368, 2.9712, 3.0962],
        [3.1368, 1.7968, 1.2019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.21930594742298126 
model_pd.l_d.mean(): -25.165403366088867 
model_pd.lagr.mean(): -24.94609832763672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0498], device='cuda:0')), ('power', tensor([-25.2152], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:0.21930594742298126
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.11962274461984634
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.11929401010274887
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.11373811215162277
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.16429303586483002
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.1252679079771042
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.14020422101020813
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.12172378599643707
epoch£º1039	 i:8 	 global-step:20788	 l-p:0.4707358777523041
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.12127744406461716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1185, 3.1185, 3.1185],
        [3.1185, 3.0142, 3.1001],
        [3.1185, 1.8102, 1.3311],
        [3.1185, 3.1097, 3.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.2571432292461395 
model_pd.l_d.mean(): -24.96116828918457 
model_pd.lagr.mean(): -24.704025268554688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1157], device='cuda:0')), ('power', tensor([-25.0769], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.2571432292461395
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.1335570514202118
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.1315351128578186
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.14431849122047424
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.7577243447303772
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.490681529045105
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.124801404774189
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.15243369340896606
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.23775260150432587
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.11679268628358841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1063, 3.1062, 3.1063],
        [3.1063, 2.1714, 2.1809],
        [3.1063, 3.0938, 3.1058],
        [3.1063, 3.0354, 3.0967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11863129585981369 
model_pd.l_d.mean(): -24.472034454345703 
model_pd.lagr.mean(): -24.353403091430664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0952], device='cuda:0')), ('power', tensor([-24.5673], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11863129585981369
epoch£º1041	 i:1 	 global-step:20821	 l-p:-0.40166857838630676
epoch£º1041	 i:2 	 global-step:20822	 l-p:3.07694149017334
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.16720470786094666
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.14073871076107025
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.17213459312915802
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.13237471878528595
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.15154366195201874
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.4006546437740326
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.10745273530483246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1065, 3.1064, 3.1065],
        [3.1065, 2.5322, 2.7358],
        [3.1065, 3.1060, 3.1065],
        [3.1065, 1.7681, 1.1815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.13049928843975067 
model_pd.l_d.mean(): -25.043354034423828 
model_pd.lagr.mean(): -24.91285514831543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0033], device='cuda:0')), ('power', tensor([-25.0467], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.13049928843975067
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.47845980525016785
epoch£º1042	 i:2 	 global-step:20842	 l-p:-0.5366146564483643
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.10273297876119614
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.13521575927734375
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.5235939621925354
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.1758219450712204
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.13202343881130219
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.14608334004878998
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.13504871726036072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1179, 3.1177, 3.1179],
        [3.1179, 2.8470, 3.0220],
        [3.1179, 2.1180, 1.4344],
        [3.1179, 2.2941, 2.3919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.14505597949028015 
model_pd.l_d.mean(): -25.136171340942383 
model_pd.lagr.mean(): -24.99111557006836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0148], device='cuda:0')), ('power', tensor([-25.1509], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.14505597949028015
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11184510588645935
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.14508171379566193
epoch£º1043	 i:3 	 global-step:20863	 l-p:0.20037391781806946
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.13804574310779572
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.5769944787025452
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.19733622670173645
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.14702214300632477
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11309249699115753
epoch£º1043	 i:9 	 global-step:20869	 l-p:-0.30037441849708557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0958, 3.0926, 3.0957],
        [3.0958, 3.0954, 3.0958],
        [3.0958, 2.9246, 3.0529],
        [3.0958, 1.8069, 1.1868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.1595483422279358 
model_pd.l_d.mean(): -24.99357795715332 
model_pd.lagr.mean(): -24.834030151367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0055], device='cuda:0')), ('power', tensor([-24.9991], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.1595483422279358
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.14164133369922638
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.1946178376674652
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.13343404233455658
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.12812167406082153
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.13504953682422638
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.36955228447914124
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.06867477297782898
epoch£º1044	 i:8 	 global-step:20888	 l-p:-0.01983838900923729
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.12295253574848175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0740, 2.0485, 1.3757],
        [3.0740, 2.7883, 2.9689],
        [3.0740, 3.0563, 3.0730],
        [3.0740, 1.7930, 1.3625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.14165936410427094 
model_pd.l_d.mean(): -24.593441009521484 
model_pd.lagr.mean(): -24.4517822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1199], device='cuda:0')), ('power', tensor([-24.7133], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.14165936410427094
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.49269136786460876
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.06657972186803818
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.17233917117118835
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11406075209379196
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.03603515028953552
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.17045842111110687
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.14296269416809082
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.1746494323015213
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.02215992845594883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0925, 1.9963, 1.8256],
        [3.0925, 3.0906, 3.0925],
        [3.0925, 1.7630, 1.1679],
        [3.0925, 3.0926, 3.0926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.2083628624677658 
model_pd.l_d.mean(): -25.00106430053711 
model_pd.lagr.mean(): -24.792701721191406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0250], device='cuda:0')), ('power', tensor([-25.0260], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:0.2083628624677658
epoch£º1046	 i:1 	 global-step:20921	 l-p:-1.2921702861785889
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.16247864067554474
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.14770165085792542
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.819665789604187
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.09430276602506638
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.2609573304653168
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.13248418271541595
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.1390434056520462
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.12720312178134918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1381, 2.6587, 2.8711],
        [3.1381, 3.0590, 3.1266],
        [3.1381, 1.9519, 1.6505],
        [3.1381, 2.9261, 3.0757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.07539729028940201 
model_pd.l_d.mean(): -24.857357025146484 
model_pd.lagr.mean(): -24.781959533691406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0165], device='cuda:0')), ('power', tensor([-24.8409], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.07539729028940201
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.16295009851455688
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.129924014210701
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.12278006225824356
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.14756560325622559
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.15265391767024994
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.18725010752677917
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.13665838539600372
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.18676528334617615
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.25091639161109924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1385, 2.0069, 1.3422],
        [3.1385, 2.5365, 2.7333],
        [3.1385, 3.1385, 3.1385],
        [3.1385, 1.8186, 1.3174]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.21346381306648254 
model_pd.l_d.mean(): -24.85183334350586 
model_pd.lagr.mean(): -24.638368606567383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0801], device='cuda:0')), ('power', tensor([-24.9319], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.21346381306648254
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.16415485739707947
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.13959632813930511
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.12496742606163025
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.11624160408973694
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.1642838567495346
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.10740775614976883
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.14626440405845642
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.22701281309127808
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.13606169819831848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1316, 2.8987, 3.0581],
        [3.1316, 1.7972, 1.1953],
        [3.1316, 2.1668, 1.4755],
        [3.1316, 2.4822, 2.6659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.12315314263105392 
model_pd.l_d.mean(): -24.69877052307129 
model_pd.lagr.mean(): -24.57561683654785 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0525], device='cuda:0')), ('power', tensor([-24.7513], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.12315314263105392
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.2597874104976654
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.24837657809257507
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.1386404037475586
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.33052578568458557
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.13449017703533173
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.12899956107139587
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.12275747954845428
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.13155266642570496
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.13900397717952728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1126, 2.0717, 1.9693],
        [3.1126, 2.9749, 3.0831],
        [3.1126, 3.1084, 3.1125],
        [3.1126, 2.9998, 3.0916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.1959965080022812 
model_pd.l_d.mean(): -25.248994827270508 
model_pd.lagr.mean(): -25.052997589111328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0304], device='cuda:0')), ('power', tensor([-25.2186], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:0.1959965080022812
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.10757045447826385
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11009453237056732
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.14583826065063477
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.12227614969015121
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.17115065455436707
epoch£º1050	 i:6 	 global-step:21006	 l-p:-0.20243768393993378
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.0003492260002531111
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.11966866999864578
epoch£º1050	 i:9 	 global-step:21009	 l-p:-0.036551542580127716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0948, 1.8817, 1.5470],
        [3.0948, 3.0916, 3.0947],
        [3.0948, 1.8596, 1.4943],
        [3.0948, 1.9519, 1.7176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.13097114861011505 
model_pd.l_d.mean(): -24.55180549621582 
model_pd.lagr.mean(): -24.420833587646484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1147], device='cuda:0')), ('power', tensor([-24.6666], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.13097114861011505
epoch£º1051	 i:1 	 global-step:21021	 l-p:0.03938714787364006
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.1433917135000229
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.17216628789901733
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.1466493159532547
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.14456559717655182
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.12717662751674652
epoch£º1051	 i:7 	 global-step:21027	 l-p:-0.6926397681236267
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.12891419231891632
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.14402930438518524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0941, 3.0941, 3.0941],
        [3.0941, 3.0942, 3.0941],
        [3.0941, 2.6454, 2.8581],
        [3.0941, 2.1970, 2.2409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.16123808920383453 
model_pd.l_d.mean(): -25.117591857910156 
model_pd.lagr.mean(): -24.95635414123535 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0449], device='cuda:0')), ('power', tensor([-25.0727], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.16123808920383453
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.1286855787038803
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.14001552760601044
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.1307540088891983
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.1735367476940155
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.0008516955422237515
epoch£º1052	 i:6 	 global-step:21046	 l-p:0.11706801503896713
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.17607083916664124
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.13565239310264587
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.41604697704315186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[3.0717, 1.7319, 1.1715],
        [3.0717, 2.0342, 1.3638],
        [3.0717, 2.1044, 2.0854],
        [3.0717, 2.3782, 2.5480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.13768896460533142 
model_pd.l_d.mean(): -25.092086791992188 
model_pd.lagr.mean(): -24.954397201538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0272], device='cuda:0')), ('power', tensor([-25.1193], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.13768896460533142
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.05064772441983223
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.1424146592617035
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.21419315040111542
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.05301554501056671
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.39018529653549194
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.056195735931396484
epoch£º1053	 i:7 	 global-step:21067	 l-p:0.21606133878231049
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.14852619171142578
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.12169766426086426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0924, 3.0835, 3.0920],
        [3.0924, 3.0924, 3.0924],
        [3.0924, 3.0914, 3.0923],
        [3.0924, 3.0799, 3.0918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.12109901010990143 
model_pd.l_d.mean(): -24.108400344848633 
model_pd.lagr.mean(): -23.987300872802734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2118], device='cuda:0')), ('power', tensor([-24.3202], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.12109901010990143
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.12050056457519531
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.11248361319303513
epoch£º1054	 i:3 	 global-step:21083	 l-p:-0.09765564650297165
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.16535359621047974
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.2988474667072296
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.13663174211978912
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.13508714735507965
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.14676876366138458
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.07592026889324188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0770, 3.0597, 3.0760],
        [3.0770, 2.9341, 3.0456],
        [3.0770, 1.7953, 1.3644],
        [3.0770, 2.5223, 2.7300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.1381627768278122 
model_pd.l_d.mean(): -24.677522659301758 
model_pd.lagr.mean(): -24.53936004638672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1787], device='cuda:0')), ('power', tensor([-24.8562], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.1381627768278122
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.09934698790311813
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.1419627070426941
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.1531669795513153
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.2522702217102051
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13315972685813904
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.035937484353780746
epoch£º1055	 i:7 	 global-step:21107	 l-p:0.4365540146827698
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.15005332231521606
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.1397649049758911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0722, 3.0722, 3.0722],
        [3.0722, 3.0707, 3.0722],
        [3.0722, 3.0499, 3.0708],
        [3.0722, 3.0721, 3.0722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.15998220443725586 
model_pd.l_d.mean(): -25.269269943237305 
model_pd.lagr.mean(): -25.10928726196289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0968], device='cuda:0')), ('power', tensor([-25.1724], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.15998220443725586
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12862834334373474
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.16278307139873505
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.06703362613916397
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.12404660880565643
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.13889817893505096
epoch£º1056	 i:6 	 global-step:21126	 l-p:-0.030040249228477478
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.14723892509937286
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.4557476043701172
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.16593095660209656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0560, 2.8653, 3.0044],
        [3.0560, 1.8732, 1.2334],
        [3.0560, 3.0560, 3.0560],
        [3.0560, 2.7724, 2.9524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.1512393206357956 
model_pd.l_d.mean(): -25.061981201171875 
model_pd.lagr.mean(): -24.910741806030273 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0040], device='cuda:0')), ('power', tensor([-25.0660], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.1512393206357956
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.03417065367102623
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.08487783372402191
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.06881728768348694
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.2893528938293457
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.11440254002809525
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.07321350276470184
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.16256782412528992
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.14506211876869202
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.1334238350391388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0861, 3.0307, 3.0798],
        [3.0861, 2.2600, 2.3583],
        [3.0861, 2.0998, 2.0607],
        [3.0861, 2.9147, 3.0431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.1407478153705597 
model_pd.l_d.mean(): -25.17498779296875 
model_pd.lagr.mean(): -25.03424072265625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0427], device='cuda:0')), ('power', tensor([-25.1323], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.1407478153705597
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10962273180484772
epoch£º1058	 i:2 	 global-step:21162	 l-p:0.264411062002182
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.13089582324028015
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.15397971868515015
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.16514372825622559
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.04677746817469597
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.09786008298397064
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.13156206905841827
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.16575004160404205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0705, 3.0598, 3.0700],
        [3.0705, 2.4239, 2.6107],
        [3.0705, 3.0150, 3.0641],
        [3.0705, 3.0046, 3.0620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12607219815254211 
model_pd.l_d.mean(): -25.10369300842285 
model_pd.lagr.mean(): -24.97762107849121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0077], device='cuda:0')), ('power', tensor([-25.0960], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12607219815254211
epoch£º1059	 i:1 	 global-step:21181	 l-p:0.24379819631576538
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.02299503982067108
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.16772235929965973
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.18707455694675446
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.14353826642036438
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.14715388417243958
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.08314994722604752
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.20129993557929993
epoch£º1059	 i:9 	 global-step:21189	 l-p:-0.09016667306423187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0587, 3.0498, 3.0583],
        [3.0587, 1.7524, 1.2832],
        [3.0587, 1.7248, 1.1466],
        [3.0587, 3.0571, 3.0587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.13665854930877686 
model_pd.l_d.mean(): -24.85267448425293 
model_pd.lagr.mean(): -24.71601676940918 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0178], device='cuda:0')), ('power', tensor([-24.8349], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.13665854930877686
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.15683339536190033
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.10961481183767319
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.15898185968399048
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.07923604547977448
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.3487478792667389
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.2127120941877365
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.10863009840250015
epoch£º1060	 i:8 	 global-step:21208	 l-p:-0.6035792231559753
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.1409226804971695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0739, 3.0739, 3.0739],
        [3.0739, 2.4215, 2.6064],
        [3.0739, 2.1016, 1.4190],
        [3.0739, 2.0312, 1.9295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.13079188764095306 
model_pd.l_d.mean(): -24.84465789794922 
model_pd.lagr.mean(): -24.713865280151367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1250], device='cuda:0')), ('power', tensor([-24.9696], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.13079188764095306
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.07817229628562927
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.14264290034770966
epoch£º1061	 i:3 	 global-step:21223	 l-p:0.167437344789505
epoch£º1061	 i:4 	 global-step:21224	 l-p:-0.050052184611558914
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.18360061943531036
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.11116151511669159
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12558533251285553
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.15725375711917877
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.13769830763339996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1072, 3.1059, 3.1072],
        [3.1072, 2.9694, 3.0776],
        [3.1072, 2.5248, 2.7272],
        [3.1072, 2.6456, 2.8586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.16961927711963654 
model_pd.l_d.mean(): -24.990577697753906 
model_pd.lagr.mean(): -24.820959091186523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0462], device='cuda:0')), ('power', tensor([-25.0368], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.16961927711963654
epoch£º1062	 i:1 	 global-step:21241	 l-p:-0.8662497997283936
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.696480393409729
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.15314631164073944
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.09384139627218246
epoch£º1062	 i:5 	 global-step:21245	 l-p:0.2741793692111969
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.1320304423570633
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.12171075493097305
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.16509079933166504
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.16366706788539886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1255, 3.1246, 3.1255],
        [3.1255, 2.1902, 2.1996],
        [3.1255, 3.1255, 3.1255],
        [3.1255, 2.0559, 1.9190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.13179326057434082 
model_pd.l_d.mean(): -25.061975479125977 
model_pd.lagr.mean(): -24.9301815032959 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0755], device='cuda:0')), ('power', tensor([-24.9865], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.13179326057434082
epoch£º1063	 i:1 	 global-step:21261	 l-p:0.33640196919441223
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.13798676431179047
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.13758325576782227
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.10381202399730682
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.16272756457328796
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.4163860082626343
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.16128486394882202
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.11300015449523926
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.13720783591270447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1187, 3.1180, 3.1187],
        [3.1187, 1.9225, 1.6105],
        [3.1187, 2.3554, 2.4909],
        [3.1187, 2.8821, 3.0432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.1556044965982437 
model_pd.l_d.mean(): -25.074094772338867 
model_pd.lagr.mean(): -24.918489456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0672], device='cuda:0')), ('power', tensor([-25.0069], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.1556044965982437
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.5208690166473389
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.2880094349384308
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.1295420080423355
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.12481720745563507
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.11976047605276108
epoch£º1064	 i:6 	 global-step:21286	 l-p:1.0354816913604736
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.15303710103034973
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.17204810678958893
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.13424275815486908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1077, 3.1077, 3.1077],
        [3.1077, 3.1077, 3.1077],
        [3.1077, 1.8030, 1.1859],
        [3.1077, 3.1057, 3.1077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.1329990178346634 
model_pd.l_d.mean(): -24.85115623474121 
model_pd.lagr.mean(): -24.718156814575195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0517], device='cuda:0')), ('power', tensor([-24.9029], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.1329990178346634
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.16064254939556122
epoch£º1065	 i:2 	 global-step:21302	 l-p:-0.13689640164375305
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.11795852333307266
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.15040837228298187
epoch£º1065	 i:5 	 global-step:21305	 l-p:-0.6231398582458496
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.12715420126914978
epoch£º1065	 i:7 	 global-step:21307	 l-p:0.5371071696281433
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12092595547437668
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.12975122034549713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1078, 2.1355, 1.4482],
        [3.1078, 1.7636, 1.1982],
        [3.1078, 2.7495, 2.9504],
        [3.1078, 1.7654, 1.2143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.14847016334533691 
model_pd.l_d.mean(): -25.0600643157959 
model_pd.lagr.mean(): -24.91159439086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0286], device='cuda:0')), ('power', tensor([-25.0315], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.14847016334533691
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.1135178655385971
epoch£º1066	 i:2 	 global-step:21322	 l-p:0.9039555788040161
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.1080654039978981
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.15489564836025238
epoch£º1066	 i:5 	 global-step:21325	 l-p:-0.20747753977775574
epoch£º1066	 i:6 	 global-step:21326	 l-p:0.1600852906703949
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.12410430610179901
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.1434326022863388
epoch£º1066	 i:9 	 global-step:21329	 l-p:-0.03476107120513916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1002, 3.1002, 3.1002],
        [3.1002, 3.0967, 3.1001],
        [3.1002, 3.0849, 3.0994],
        [3.1002, 2.0990, 1.4176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.13645610213279724 
model_pd.l_d.mean(): -25.061185836791992 
model_pd.lagr.mean(): -24.92473030090332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0179], device='cuda:0')), ('power', tensor([-25.0791], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.13645610213279724
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.13529491424560547
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.1448482722043991
epoch£º1067	 i:3 	 global-step:21343	 l-p:-0.1641896367073059
epoch£º1067	 i:4 	 global-step:21344	 l-p:0.19887202978134155
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.1425953507423401
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.14701232314109802
epoch£º1067	 i:7 	 global-step:21347	 l-p:-0.03287845104932785
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.10930344462394714
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.1501012146472931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0811, 3.0811, 3.0811],
        [3.0811, 3.0796, 3.0811],
        [3.0811, 3.0809, 3.0811],
        [3.0811, 3.0811, 3.0811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.15345056354999542 
model_pd.l_d.mean(): -24.726167678833008 
model_pd.lagr.mean(): -24.572717666625977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0376], device='cuda:0')), ('power', tensor([-24.7638], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.15345056354999542
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.15683677792549133
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.12881138920783997
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.1453511118888855
epoch£º1068	 i:4 	 global-step:21364	 l-p:0.1188533678650856
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.11179452389478683
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.29753774404525757
epoch£º1068	 i:7 	 global-step:21367	 l-p:-0.22724707424640656
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.1611609011888504
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.02091522142291069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0641, 2.1662, 2.2123],
        [3.0641, 3.0242, 3.0604],
        [3.0641, 3.0024, 3.0565],
        [3.0641, 2.6201, 2.8332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.0821818932890892 
model_pd.l_d.mean(): -25.196760177612305 
model_pd.lagr.mean(): -25.114578247070312 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0402], device='cuda:0')), ('power', tensor([-25.1566], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.0821818932890892
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.1433994323015213
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.1459045708179474
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.23183967173099518
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.13257434964179993
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.07461244612932205
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.1210952177643776
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.37012138962745667
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.1574258953332901
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.13751700520515442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0794, 1.9402, 1.2868],
        [3.0794, 3.0753, 3.0793],
        [3.0794, 2.9663, 3.0584],
        [3.0794, 1.8934, 1.2493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.40644413232803345 
model_pd.l_d.mean(): -24.708110809326172 
model_pd.lagr.mean(): -24.301666259765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1419], device='cuda:0')), ('power', tensor([-24.8500], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:0.40644413232803345
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.13146042823791504
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.04692760854959488
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.14782868325710297
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.050288133323192596
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.06722601503133774
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.1732279360294342
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.143625870347023
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.14750930666923523
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11389902979135513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0844, 3.0844, 3.0844],
        [3.0844, 3.0308, 3.0784],
        [3.0844, 2.5800, 2.7930],
        [3.0844, 2.1949, 2.2468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.02909429930150509 
model_pd.l_d.mean(): -24.914260864257812 
model_pd.lagr.mean(): -24.88516616821289 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0441], device='cuda:0')), ('power', tensor([-24.9583], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.02909429930150509
epoch£º1071	 i:1 	 global-step:21421	 l-p:0.2674044072628021
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.13697391748428345
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.06569599360227585
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.13056580722332
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.16833874583244324
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.18310077488422394
epoch£º1071	 i:7 	 global-step:21427	 l-p:-0.09168117493391037
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.1692473590373993
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.1294652819633484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0952, 3.0952, 3.0952],
        [3.0952, 2.1282, 1.4415],
        [3.0952, 3.0927, 3.0952],
        [3.0952, 1.8409, 1.4508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.2022748589515686 
model_pd.l_d.mean(): -24.765377044677734 
model_pd.lagr.mean(): -24.56310272216797 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1663], device='cuda:0')), ('power', tensor([-24.9317], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.2022748589515686
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.17175772786140442
epoch£º1072	 i:2 	 global-step:21442	 l-p:-0.07697597146034241
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.9801712036132812
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.10534700006246567
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.15230602025985718
epoch£º1072	 i:6 	 global-step:21446	 l-p:12.104558944702148
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.11650913208723068
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12242496013641357
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.14121299982070923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1123, 3.0473, 3.1040],
        [3.1123, 3.0924, 3.1111],
        [3.1123, 1.8302, 1.2030],
        [3.1123, 3.0654, 3.1075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.16321803629398346 
model_pd.l_d.mean(): -24.776973724365234 
model_pd.lagr.mean(): -24.61375617980957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1219], device='cuda:0')), ('power', tensor([-24.8989], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.16321803629398346
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.6026989817619324
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.11812711507081985
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.10821899026632309
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11117066442966461
epoch£º1073	 i:5 	 global-step:21465	 l-p:1.1406886577606201
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.1487405002117157
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.13166211545467377
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.18066082894802094
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.14268063008785248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1109, 3.1109, 3.1109],
        [3.1109, 1.9688, 1.7370],
        [3.1109, 2.8235, 3.0046],
        [3.1109, 1.9628, 1.7224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.84385746717453 
model_pd.l_d.mean(): -24.843355178833008 
model_pd.lagr.mean(): -23.99949836730957 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0586], device='cuda:0')), ('power', tensor([-24.9019], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:0.84385746717453
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12216180562973022
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.1304941475391388
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.13109879195690155
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.14055414497852325
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.1873241364955902
epoch£º1074	 i:6 	 global-step:21486	 l-p:-0.028768768534064293
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.1457175612449646
epoch£º1074	 i:8 	 global-step:21488	 l-p:0.24064289033412933
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.16672074794769287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0938, 2.7875, 2.9750],
        [3.0938, 3.0826, 3.0933],
        [3.0938, 3.0928, 3.0938],
        [3.0938, 2.2669, 2.3653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.16200149059295654 
model_pd.l_d.mean(): -25.189790725708008 
model_pd.lagr.mean(): -25.027790069580078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0701], device='cuda:0')), ('power', tensor([-25.1197], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.16200149059295654
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.13807977735996246
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.1311882585287094
epoch£º1075	 i:3 	 global-step:21503	 l-p:-0.050241172313690186
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.041028764098882675
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.1295391470193863
epoch£º1075	 i:6 	 global-step:21506	 l-p:-0.5772451162338257
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.14198708534240723
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.23682710528373718
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12570889294147491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0996, 3.0431, 3.0931],
        [3.0996, 2.0393, 1.3681],
        [3.0996, 2.0606, 1.3856],
        [3.0996, 2.9950, 3.0812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.2346360981464386 
model_pd.l_d.mean(): -25.030710220336914 
model_pd.lagr.mean(): -24.79607391357422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0547], device='cuda:0')), ('power', tensor([-25.0854], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.2346360981464386
epoch£º1076	 i:1 	 global-step:21521	 l-p:4.002184867858887
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.1296701282262802
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.1268770694732666
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.13034787774085999
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10467025637626648
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.13565589487552643
epoch£º1076	 i:7 	 global-step:21527	 l-p:-1.6233463287353516
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.1682327389717102
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.13284873962402344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1001, 2.0185, 1.8690],
        [3.1001, 2.1213, 2.0906],
        [3.1001, 3.0853, 3.0993],
        [3.1001, 3.0981, 3.1000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.14494255185127258 
model_pd.l_d.mean(): -24.916202545166016 
model_pd.lagr.mean(): -24.771259307861328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0537], device='cuda:0')), ('power', tensor([-24.8625], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.14494255185127258
epoch£º1077	 i:1 	 global-step:21541	 l-p:-0.021944036707282066
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.126654714345932
epoch£º1077	 i:3 	 global-step:21543	 l-p:-0.0759020522236824
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.192501038312912
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.030895976349711418
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.1268347203731537
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.13546276092529297
epoch£º1077	 i:8 	 global-step:21548	 l-p:0.2171361744403839
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12647271156311035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0869, 1.9420, 1.7077],
        [3.0869, 2.4527, 2.6433],
        [3.0869, 2.1775, 2.2129],
        [3.0869, 3.0870, 3.0869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): -0.010642223060131073 
model_pd.l_d.mean(): -24.672481536865234 
model_pd.lagr.mean(): -24.683124542236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1221], device='cuda:0')), ('power', tensor([-24.7946], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:-0.010642223060131073
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.14134877920150757
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.1281941831111908
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.204167902469635
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12241746485233307
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.17670753598213196
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.04416431486606598
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.05973321944475174
epoch£º1078	 i:8 	 global-step:21568	 l-p:0.3163214921951294
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.1312093287706375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0870, 2.9692, 3.0644],
        [3.0870, 1.7827, 1.1701],
        [3.0870, 3.0732, 3.0863],
        [3.0870, 1.8893, 1.5797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.16483333706855774 
model_pd.l_d.mean(): -24.89723014831543 
model_pd.lagr.mean(): -24.732397079467773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0453], device='cuda:0')), ('power', tensor([-24.9425], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.16483333706855774
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.176915243268013
epoch£º1079	 i:2 	 global-step:21582	 l-p:0.2383793592453003
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.13634520769119263
epoch£º1079	 i:4 	 global-step:21584	 l-p:-0.37208032608032227
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.13721182942390442
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.13466429710388184
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.5093616843223572
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.12857405841350555
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.13918890058994293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1120, 3.0282, 3.0993],
        [3.1120, 2.8003, 2.9893],
        [3.1120, 2.0615, 1.3866],
        [3.1120, 1.7699, 1.1842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11099080741405487 
model_pd.l_d.mean(): -24.964855194091797 
model_pd.lagr.mean(): -24.853864669799805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0319], device='cuda:0')), ('power', tensor([-24.9967], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11099080741405487
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.11281190812587738
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.1498398333787918
epoch£º1080	 i:3 	 global-step:21603	 l-p:-12.174845695495605
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.1529487520456314
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.7747738361358643
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.13534745573997498
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.19666381180286407
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.15359751880168915
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.13490070402622223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1126, 3.1122, 3.1126],
        [3.1126, 3.1126, 3.1126],
        [3.1126, 3.1084, 3.1125],
        [3.1126, 3.0876, 3.1109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.11227782815694809 
model_pd.l_d.mean(): -24.614398956298828 
model_pd.lagr.mean(): -24.502120971679688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0630], device='cuda:0')), ('power', tensor([-24.6774], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.11227782815694809
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.16988427937030792
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.11849793791770935
epoch£º1081	 i:3 	 global-step:21623	 l-p:-0.8698443174362183
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.15055447816848755
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.1849699318408966
epoch£º1081	 i:6 	 global-step:21626	 l-p:1.1938462257385254
epoch£º1081	 i:7 	 global-step:21627	 l-p:-0.1328633725643158
epoch£º1081	 i:8 	 global-step:21628	 l-p:0.1856631636619568
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14336760342121124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1101, 3.1101, 3.1101],
        [3.1101, 2.7212, 2.9279],
        [3.1101, 1.7712, 1.2352],
        [3.1101, 1.7823, 1.1781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.09107780456542969 
model_pd.l_d.mean(): -24.679100036621094 
model_pd.lagr.mean(): -24.588022232055664 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1130], device='cuda:0')), ('power', tensor([-24.7921], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.09107780456542969
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.33402252197265625
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.13342465460300446
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.1321878284215927
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.16364675760269165
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.3101399838924408
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.31852442026138306
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.1370609849691391
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.1400248408317566
epoch£º1082	 i:9 	 global-step:21649	 l-p:0.1613045036792755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1320, 3.1234, 3.1317],
        [3.1320, 2.2248, 2.2598],
        [3.1320, 2.1029, 1.4213],
        [3.1320, 1.8136, 1.1970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.15245424211025238 
model_pd.l_d.mean(): -24.975587844848633 
model_pd.lagr.mean(): -24.82313346862793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0867], device='cuda:0')), ('power', tensor([-25.0623], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.15245424211025238
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.13016214966773987
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.16841094195842743
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.1279449313879013
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.14073458313941956
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.0825965404510498
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.1518593430519104
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.13323074579238892
epoch£º1083	 i:8 	 global-step:21668	 l-p:0.3330838680267334
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.1441543996334076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1371, 2.1216, 1.4369],
        [3.1371, 3.0954, 3.1331],
        [3.1371, 1.8695, 1.4552],
        [3.1371, 1.8181, 1.2005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.11504329741001129 
model_pd.l_d.mean(): -24.977659225463867 
model_pd.lagr.mean(): -24.86261558532715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0785], device='cuda:0')), ('power', tensor([-25.0562], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.11504329741001129
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.13104474544525146
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.12162256240844727
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.13267144560813904
epoch£º1084	 i:4 	 global-step:21684	 l-p:0.3727383315563202
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.09782159328460693
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.3835202753543854
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.3016447424888611
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.15021777153015137
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.16690021753311157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1153, 2.7426, 2.9464],
        [3.1153, 1.7899, 1.2846],
        [3.1153, 2.2810, 2.3735],
        [3.1153, 3.1073, 3.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.14097711443901062 
model_pd.l_d.mean(): -24.76593780517578 
model_pd.lagr.mean(): -24.62495994567871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0310], device='cuda:0')), ('power', tensor([-24.7969], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.14097711443901062
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.12293069809675217
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.13827183842658997
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13141269981861115
epoch£º1085	 i:4 	 global-step:21704	 l-p:-1.8948452472686768
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.15660950541496277
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.20806489884853363
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.02244161069393158
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.06812345236539841
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.1679074466228485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0892, 2.8086, 2.9875],
        [3.0892, 1.9196, 1.2702],
        [3.0892, 3.0892, 3.0892],
        [3.0892, 3.0889, 3.0892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.14347127079963684 
model_pd.l_d.mean(): -24.797008514404297 
model_pd.lagr.mean(): -24.65353775024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0854], device='cuda:0')), ('power', tensor([-24.7116], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.14347127079963684
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.13323423266410828
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.19103240966796875
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.022661613300442696
epoch£º1086	 i:4 	 global-step:21724	 l-p:-0.07563649117946625
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.15131644904613495
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.1626196950674057
epoch£º1086	 i:7 	 global-step:21727	 l-p:0.1155211329460144
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.12580841779708862
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.14989331364631653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0913, 3.0909, 3.0913],
        [3.0913, 2.9918, 3.0744],
        [3.0913, 3.0348, 3.0847],
        [3.0913, 1.7635, 1.1645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.141361802816391 
model_pd.l_d.mean(): -24.733076095581055 
model_pd.lagr.mean(): -24.59171485900879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0908], device='cuda:0')), ('power', tensor([-24.8239], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.141361802816391
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.1286107450723648
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.19987128674983978
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.1337178647518158
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.10426490753889084
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.10013522207736969
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.6041416525840759
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.1388760507106781
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.001148176146671176
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.1595650613307953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0688, 3.0370, 3.0663],
        [3.0688, 1.8319, 1.2012],
        [3.0688, 2.0267, 1.3568],
        [3.0688, 1.8036, 1.1806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.1381877362728119 
model_pd.l_d.mean(): -24.951536178588867 
model_pd.lagr.mean(): -24.8133487701416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0780], device='cuda:0')), ('power', tensor([-25.0296], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.1381877362728119
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.1406579166650772
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.15102924406528473
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.0942346528172493
epoch£º1088	 i:4 	 global-step:21764	 l-p:0.541355550289154
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.16010113060474396
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.08553206920623779
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.18010105192661285
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.023917503654956818
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.10282381623983383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0547, 2.7704, 2.9509],
        [3.0547, 2.7550, 2.9407],
        [3.0547, 1.7331, 1.2386],
        [3.0547, 3.0547, 3.0547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.0025734519585967064 
model_pd.l_d.mean(): -25.190410614013672 
model_pd.lagr.mean(): -25.187837600708008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0247], device='cuda:0')), ('power', tensor([-25.2151], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.0025734519585967064
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.13171258568763733
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.3854644000530243
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.14317041635513306
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.060854826122522354
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.1201271042227745
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.1275106966495514
epoch£º1089	 i:7 	 global-step:21787	 l-p:-0.025927523151040077
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.1328086405992508
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.17387577891349792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0821, 1.9179, 1.6570],
        [3.0821, 1.7738, 1.3026],
        [3.0821, 1.8952, 1.2506],
        [3.0821, 3.0779, 3.0820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.14812473952770233 
model_pd.l_d.mean(): -25.147634506225586 
model_pd.lagr.mean(): -24.999509811401367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0227], device='cuda:0')), ('power', tensor([-25.1249], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.14812473952770233
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.20504741370677948
epoch£º1090	 i:2 	 global-step:21802	 l-p:0.3990834057331085
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.1219724640250206
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.13911139965057373
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.07714347541332245
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.04700103774666786
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.056917380541563034
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.12156074494123459
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11789209395647049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0829, 1.7845, 1.1699],
        [3.0829, 1.7785, 1.3141],
        [3.0829, 1.7521, 1.1584],
        [3.0829, 3.0824, 3.0829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.1484374850988388 
model_pd.l_d.mean(): -25.018278121948242 
model_pd.lagr.mean(): -24.869840621948242 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0170], device='cuda:0')), ('power', tensor([-25.0013], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.1484374850988388
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.1389468014240265
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.13528881967067719
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.14566539227962494
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.255877822637558
epoch£º1091	 i:5 	 global-step:21825	 l-p:-1.5448527336120605
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.08753341436386108
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.1831486076116562
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.08517172932624817
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.11898244917392731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0706, 2.5941, 2.8082],
        [3.0706, 3.0150, 3.0643],
        [3.0706, 1.8051, 1.1817],
        [3.0706, 2.1603, 2.1963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.062425028532743454 
model_pd.l_d.mean(): -25.14035415649414 
model_pd.lagr.mean(): -25.07792854309082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0720], device='cuda:0')), ('power', tensor([-25.2124], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.062425028532743454
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.19616051018238068
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.13764381408691406
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.13473573327064514
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.16207708418369293
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.1364952176809311
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.11141648888587952
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.15044978260993958
epoch£º1092	 i:8 	 global-step:21848	 l-p:0.2775885760784149
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12776070833206177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0807, 1.8141, 1.1888],
        [3.0807, 3.0803, 3.0807],
        [3.0807, 3.0807, 3.0807],
        [3.0807, 3.0805, 3.0807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.15417136251926422 
model_pd.l_d.mean(): -25.0972843170166 
model_pd.lagr.mean(): -24.943113327026367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0367], device='cuda:0')), ('power', tensor([-25.1340], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.15417136251926422
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.17723922431468964
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.0988423228263855
epoch£º1093	 i:3 	 global-step:21863	 l-p:0.3755734860897064
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.08293050527572632
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.014024276286363602
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.13876758515834808
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.11625737696886063
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.16533996164798737
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.15385347604751587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0847, 2.8353, 3.0022],
        [3.0847, 2.7594, 2.9526],
        [3.0847, 3.0847, 3.0847],
        [3.0847, 2.1936, 2.2450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.14571501314640045 
model_pd.l_d.mean(): -25.18403434753418 
model_pd.lagr.mean(): -25.038318634033203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0699], device='cuda:0')), ('power', tensor([-25.1141], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.14571501314640045
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.0564747229218483
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.1752852350473404
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.13369692862033844
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.15692085027694702
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12921473383903503
epoch£º1094	 i:6 	 global-step:21886	 l-p:0.23928172886371613
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.15639910101890564
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.07796929031610489
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.22346360981464386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0749, 3.0683, 3.0747],
        [3.0749, 2.5624, 2.7753],
        [3.0749, 2.1641, 2.1995],
        [3.0749, 3.0749, 3.0749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.14496994018554688 
model_pd.l_d.mean(): -25.06627655029297 
model_pd.lagr.mean(): -24.921306610107422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0534], device='cuda:0')), ('power', tensor([-25.1197], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.14496994018554688
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.17826679348945618
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.07084870338439941
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.06633324176073074
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.14224852621555328
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.1925278604030609
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.04670872539281845
epoch£º1095	 i:7 	 global-step:21907	 l-p:0.2683112323284149
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.1549769937992096
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.13535529375076294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0937, 2.2030, 2.2544],
        [3.0937, 2.1831, 2.2177],
        [3.0937, 3.0933, 3.0937],
        [3.0937, 2.3539, 2.5034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.16446992754936218 
model_pd.l_d.mean(): -24.100818634033203 
model_pd.lagr.mean(): -23.93634796142578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2199], device='cuda:0')), ('power', tensor([-24.3207], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.16446992754936218
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.2157762497663498
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.14209143817424774
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.15470771491527557
epoch£º1096	 i:4 	 global-step:21924	 l-p:-0.11994680762290955
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.10802549123764038
epoch£º1096	 i:6 	 global-step:21926	 l-p:1.6834040880203247
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.1381295919418335
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.1450117528438568
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.1461075395345688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1131, 3.1131, 3.1131],
        [3.1131, 3.1084, 3.1130],
        [3.1131, 3.1131, 3.1131],
        [3.1131, 1.9668, 1.3084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.16282151639461517 
model_pd.l_d.mean(): -24.752702713012695 
model_pd.lagr.mean(): -24.589881896972656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0256], device='cuda:0')), ('power', tensor([-24.7783], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.16282151639461517
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.1310231238603592
epoch£º1097	 i:2 	 global-step:21942	 l-p:0.287619024515152
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12469708919525146
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.12687057256698608
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.12725891172885895
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.18903769552707672
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.12941253185272217
epoch£º1097	 i:8 	 global-step:21948	 l-p:0.4279578626155853
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.7204369902610779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1168, 3.1031, 3.1162],
        [3.1168, 3.0792, 3.1135],
        [3.1168, 1.9188, 1.6067],
        [3.1168, 2.3632, 2.5048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.14695629477500916 
model_pd.l_d.mean(): -25.167865753173828 
model_pd.lagr.mean(): -25.020910263061523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0853], device='cuda:0')), ('power', tensor([-25.0826], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.14695629477500916
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.143846794962883
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.1464669108390808
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.14670464396476746
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.13053850829601288
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11652061343193054
epoch£º1098	 i:6 	 global-step:21966	 l-p:-0.3483183681964874
epoch£º1098	 i:7 	 global-step:21967	 l-p:-0.49647256731987
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11736486852169037
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.27550506591796875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0916, 1.8522, 1.2172],
        [3.0916, 3.0911, 3.0916],
        [3.0916, 2.8542, 3.0159],
        [3.0916, 3.0915, 3.0916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.004813141655176878 
model_pd.l_d.mean(): -25.072118759155273 
model_pd.lagr.mean(): -25.067306518554688 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0018], device='cuda:0')), ('power', tensor([-25.0739], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.004813141655176878
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.17166668176651
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.02983163297176361
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.22743961215019226
epoch£º1099	 i:4 	 global-step:21984	 l-p:-0.07731232792139053
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.16479907929897308
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.15809366106987
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.16072098910808563
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.10428129136562347
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.1140628308057785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0959, 3.0959, 3.0960],
        [3.0959, 2.0968, 1.4151],
        [3.0959, 1.9215, 1.2716],
        [3.0959, 2.1707, 2.1925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): -0.5087345242500305 
model_pd.l_d.mean(): -25.019977569580078 
model_pd.lagr.mean(): -25.528711318969727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0295], device='cuda:0')), ('power', tensor([-25.0495], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:-0.5087345242500305
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.25228190422058105
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10379233211278915
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.1562926471233368
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.1268526017665863
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.005111312959343195
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.18525145947933197
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.15277449786663055
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.18482501804828644
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.1388697326183319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0893, 1.7486, 1.2084],
        [3.0893, 3.0097, 3.0777],
        [3.0893, 3.0745, 3.0886],
        [3.0893, 1.7452, 1.1810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.19198077917099 
model_pd.l_d.mean(): -25.04454803466797 
model_pd.lagr.mean(): -24.852567672729492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0699], device='cuda:0')), ('power', tensor([-25.1144], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.19198077917099
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.24507948756217957
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.13169226050376892
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.15751464664936066
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.037933848798274994
epoch£º1101	 i:5 	 global-step:22025	 l-p:-0.2308267205953598
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.10584405809640884
epoch£º1101	 i:7 	 global-step:22027	 l-p:-0.17149420082569122
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.11467836052179337
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.18218857049942017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1048, 2.1382, 2.1206],
        [3.1048, 1.7700, 1.1733],
        [3.1048, 3.0797, 3.1031],
        [3.1048, 1.9083, 1.5999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.12737706303596497 
model_pd.l_d.mean(): -24.775266647338867 
model_pd.lagr.mean(): -24.647890090942383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0028], device='cuda:0')), ('power', tensor([-24.7781], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.12737706303596497
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.18863166868686676
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.14286106824874878
epoch£º1102	 i:3 	 global-step:22043	 l-p:1.5119773149490356
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.14092351496219635
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.18866324424743652
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.13097402453422546
epoch£º1102	 i:7 	 global-step:22047	 l-p:-0.2177533060312271
epoch£º1102	 i:8 	 global-step:22048	 l-p:-0.06280535459518433
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.12492138892412186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1061, 3.0410, 3.0977],
        [3.1061, 2.5746, 2.7854],
        [3.1061, 1.8704, 1.2315],
        [3.1061, 3.1061, 3.1061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.11961038410663605 
model_pd.l_d.mean(): -25.054813385009766 
model_pd.lagr.mean(): -24.935203552246094 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0095], device='cuda:0')), ('power', tensor([-25.0643], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.11961038410663605
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.13234414160251617
epoch£º1103	 i:2 	 global-step:22062	 l-p:0.7251514196395874
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.1411619633436203
epoch£º1103	 i:4 	 global-step:22064	 l-p:0.2830960154533386
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.17682908475399017
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.16485090553760529
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.12316428869962692
epoch£º1103	 i:8 	 global-step:22068	 l-p:0.24561794102191925
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.14569567143917084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1352, 2.1054, 1.4231],
        [3.1352, 2.8567, 3.0347],
        [3.1352, 3.1352, 3.1352],
        [3.1352, 3.1227, 3.1347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.15578994154930115 
model_pd.l_d.mean(): -25.12063217163086 
model_pd.lagr.mean(): -24.964841842651367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0144], device='cuda:0')), ('power', tensor([-25.1350], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.15578994154930115
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.13204337656497955
epoch£º1104	 i:2 	 global-step:22082	 l-p:0.2941715121269226
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.1502010077238083
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.13673843443393707
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.133440762758255
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.14521020650863647
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11145936697721481
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.1565656214952469
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12789560854434967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1480, 1.8034, 1.2059],
        [3.1480, 3.1257, 3.1466],
        [3.1480, 1.8512, 1.2204],
        [3.1480, 2.5731, 2.7767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.16587302088737488 
model_pd.l_d.mean(): -25.026853561401367 
model_pd.lagr.mean(): -24.860980987548828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0004], device='cuda:0')), ('power', tensor([-25.0265], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.16587302088737488
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.1281491369009018
epoch£º1105	 i:2 	 global-step:22102	 l-p:0.2115585207939148
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.13259188830852509
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.13419942557811737
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.13596990704536438
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.1838398277759552
epoch£º1105	 i:7 	 global-step:22107	 l-p:0.3189918100833893
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.20869699120521545
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.13897395133972168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1154, 1.9409, 1.2872],
        [3.1154, 3.1065, 3.1150],
        [3.1154, 3.0442, 3.1057],
        [3.1154, 3.0155, 3.0983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.16728423535823822 
model_pd.l_d.mean(): -24.97688102722168 
model_pd.lagr.mean(): -24.80959701538086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0129], device='cuda:0')), ('power', tensor([-24.9640], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.16728423535823822
epoch£º1106	 i:1 	 global-step:22121	 l-p:0.8938232064247131
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.1250518411397934
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.1254962980747223
epoch£º1106	 i:4 	 global-step:22124	 l-p:0.47829845547676086
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.2116033136844635
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.1337120532989502
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.1351492553949356
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.3685239851474762
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.14206348359584808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1115, 1.9759, 1.3156],
        [3.1115, 3.1115, 3.1115],
        [3.1115, 2.8029, 2.9911],
        [3.1115, 3.1026, 3.1112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.39102402329444885 
model_pd.l_d.mean(): -25.076622009277344 
model_pd.lagr.mean(): -24.685598373413086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0314], device='cuda:0')), ('power', tensor([-25.1080], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.39102402329444885
epoch£º1107	 i:1 	 global-step:22141	 l-p:-3.130479335784912
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.13448332250118256
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.11125579476356506
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.1331331729888916
epoch£º1107	 i:5 	 global-step:22145	 l-p:2.5912857055664062
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12798002362251282
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.1756613850593567
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.09214188158512115
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.14572620391845703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1022, 1.9771, 1.7718],
        [3.1022, 3.1022, 3.1022],
        [3.1022, 2.2661, 2.3589],
        [3.1022, 1.8057, 1.3539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.1436968892812729 
model_pd.l_d.mean(): -25.044357299804688 
model_pd.lagr.mean(): -24.900659561157227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0783], device='cuda:0')), ('power', tensor([-25.1227], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.1436968892812729
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.13968335092067719
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.14478404819965363
epoch£º1108	 i:3 	 global-step:22163	 l-p:-0.05657112970948219
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.1554250419139862
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.18774381279945374
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.25050875544548035
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.04732158035039902
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.12523940205574036
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.21049803495407104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0838, 1.9135, 1.2649],
        [3.0838, 3.0835, 3.0838],
        [3.0838, 2.4299, 2.6152],
        [3.0838, 2.3156, 2.4514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.1310127079486847 
model_pd.l_d.mean(): -24.9837703704834 
model_pd.lagr.mean(): -24.852758407592773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0039], device='cuda:0')), ('power', tensor([-24.9877], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.1310127079486847
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.1752168983221054
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.1733647882938385
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.14056682586669922
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.1431211233139038
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.03552910313010216
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.1968798190355301
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.18310017883777618
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.12247302383184433
epoch£º1109	 i:9 	 global-step:22189	 l-p:-0.012979383580386639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[3.0900, 2.4858, 2.6848],
        [3.0900, 1.7625, 1.1625],
        [3.0900, 1.8803, 1.2386],
        [3.0900, 1.9133, 1.2647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.05863776430487633 
model_pd.l_d.mean(): -24.992897033691406 
model_pd.lagr.mean(): -24.93425941467285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0968], device='cuda:0')), ('power', tensor([-25.0897], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.05863776430487633
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.19230960309505463
epoch£º1110	 i:2 	 global-step:22202	 l-p:-0.06782273203134537
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.13961689174175262
epoch£º1110	 i:4 	 global-step:22204	 l-p:0.5443187952041626
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.1181630790233612
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.12950222194194794
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.20267023146152496
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.11739294975996017
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.13471436500549316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1265, 1.8735, 1.2343],
        [3.1265, 3.0108, 3.1046],
        [3.1265, 3.1265, 3.1265],
        [3.1265, 1.9277, 1.6148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.13661746680736542 
model_pd.l_d.mean(): -24.578041076660156 
model_pd.lagr.mean(): -24.441423416137695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1459], device='cuda:0')), ('power', tensor([-24.7239], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.13661746680736542
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.25998455286026
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.1440703570842743
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.1438927799463272
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.13114047050476074
epoch£º1111	 i:5 	 global-step:22225	 l-p:0.41074180603027344
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.1140386164188385
epoch£º1111	 i:7 	 global-step:22227	 l-p:0.35191065073013306
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.1564742475748062
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.14246125519275665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[3.1160, 1.7701, 1.2149],
        [3.1160, 1.7693, 1.1921],
        [3.1160, 2.1476, 2.1282],
        [3.1160, 1.7700, 1.2148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.16424736380577087 
model_pd.l_d.mean(): -24.879636764526367 
model_pd.lagr.mean(): -24.715389251708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0575], device='cuda:0')), ('power', tensor([-24.9372], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.16424736380577087
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.144723579287529
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.11263051629066467
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.14986157417297363
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.20658808946609497
epoch£º1112	 i:5 	 global-step:22245	 l-p:0.2914392650127411
epoch£º1112	 i:6 	 global-step:22246	 l-p:0.8048897981643677
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.1299109011888504
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.13531433045864105
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.12634965777397156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1200, 3.1164, 3.1199],
        [3.1200, 3.1183, 3.1199],
        [3.1200, 3.1200, 3.1200],
        [3.1200, 2.2239, 2.2703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.13839565217494965 
model_pd.l_d.mean(): -24.67083740234375 
model_pd.lagr.mean(): -24.532442092895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0786], device='cuda:0')), ('power', tensor([-24.7495], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.13839565217494965
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.18981117010116577
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12706048786640167
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.12527963519096375
epoch£º1113	 i:4 	 global-step:22264	 l-p:-2.2970516681671143
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.13569584488868713
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.10996333509683609
epoch£º1113	 i:7 	 global-step:22267	 l-p:0.6066859364509583
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.19178740680217743
epoch£º1113	 i:9 	 global-step:22269	 l-p:-6.172274589538574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1106, 2.8564, 3.0252],
        [3.1106, 1.9028, 1.5789],
        [3.1106, 1.7878, 1.2911],
        [3.1106, 3.1021, 3.1103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.12714092433452606 
model_pd.l_d.mean(): -24.388389587402344 
model_pd.lagr.mean(): -24.261249542236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0984], device='cuda:0')), ('power', tensor([-24.4868], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.12714092433452606
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.14207175374031067
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.899232029914856
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.1600988358259201
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.1251828968524933
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.13196271657943726
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.18391931056976318
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.14320549368858337
epoch£º1114	 i:8 	 global-step:22288	 l-p:1.399468183517456
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.14308570325374603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1171, 1.7802, 1.1808],
        [3.1171, 3.0972, 3.1160],
        [3.1171, 1.9384, 1.2849],
        [3.1171, 3.0748, 3.1131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.1626146286725998 
model_pd.l_d.mean(): -24.937944412231445 
model_pd.lagr.mean(): -24.77532958984375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0546], device='cuda:0')), ('power', tensor([-24.9926], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.1626146286725998
epoch£º1115	 i:1 	 global-step:22301	 l-p:0.2573230564594269
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.13005343079566956
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.16658684611320496
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.14476142823696136
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.14563241600990295
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.25017839670181274
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.09240635484457016
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12583428621292114
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.23182174563407898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1384, 2.3531, 2.4771],
        [3.1384, 2.2489, 2.2999],
        [3.1384, 1.7927, 1.2429],
        [3.1384, 3.1384, 3.1384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.12114621698856354 
model_pd.l_d.mean(): -25.056089401245117 
model_pd.lagr.mean(): -24.9349422454834 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0330], device='cuda:0')), ('power', tensor([-25.0231], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.12114621698856354
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.14420399069786072
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12167692929506302
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.12874601781368256
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.239343523979187
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.12688115239143372
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.15408432483673096
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.21768401563167572
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13284920156002045
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.28052908182144165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1297, 2.9390, 3.0781],
        [3.1297, 3.1058, 3.1281],
        [3.1297, 2.1945, 2.2064],
        [3.1297, 3.1295, 3.1297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.1280284821987152 
model_pd.l_d.mean(): -25.1353702545166 
model_pd.lagr.mean(): -25.007341384887695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0677], device='cuda:0')), ('power', tensor([-25.0677], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.1280284821987152
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.11976289749145508
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.2421702742576599
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.29368120431900024
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.12687984108924866
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.15596342086791992
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.15296491980552673
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.2343236654996872
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.13216008245944977
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.1318551003932953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1351, 3.1192, 3.1343],
        [3.1351, 2.5592, 2.7632],
        [3.1351, 2.8563, 3.0345],
        [3.1351, 3.1271, 3.1348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.20925527811050415 
model_pd.l_d.mean(): -24.577049255371094 
model_pd.lagr.mean(): -24.367794036865234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0138], device='cuda:0')), ('power', tensor([-24.5909], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.20925527811050415
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.11773305386304855
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.19517019391059875
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12481722235679626
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.12916278839111328
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.14758221805095673
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.1495673656463623
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.1416611224412918
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.1188347190618515
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.5135514736175537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1165, 2.2574, 2.3334],
        [3.1165, 1.9732, 1.3132],
        [3.1165, 3.1011, 3.1157],
        [3.1165, 2.1387, 1.4501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.46309715509414673 
model_pd.l_d.mean(): -24.667442321777344 
model_pd.lagr.mean(): -24.204345703125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1697], device='cuda:0')), ('power', tensor([-24.8372], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:0.46309715509414673
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.11849309504032135
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.13897590339183807
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.17546872794628143
epoch£º1119	 i:4 	 global-step:22384	 l-p:-0.15967705845832825
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.16826131939888
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.7416698336601257
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.13792070746421814
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.14864161610603333
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.13435803353786469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1045, 1.7958, 1.3250],
        [3.1045, 2.7079, 2.9162],
        [3.1045, 2.9544, 3.0704],
        [3.1045, 1.9354, 1.2824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 1.082140326499939 
model_pd.l_d.mean(): -24.659360885620117 
model_pd.lagr.mean(): -23.577220916748047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1031], device='cuda:0')), ('power', tensor([-24.7625], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:1.082140326499939
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.15508006513118744
epoch£º1120	 i:2 	 global-step:22402	 l-p:-0.1746828556060791
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.12503166496753693
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.13443736732006073
epoch£º1120	 i:5 	 global-step:22405	 l-p:0.22207777202129364
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.14183388650417328
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.1150410994887352
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.13505485653877258
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.046683721244335175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0909, 1.7852, 1.1711],
        [3.0909, 1.8757, 1.2348],
        [3.0909, 3.0591, 3.0884],
        [3.0909, 3.0899, 3.0909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): -0.053598422557115555 
model_pd.l_d.mean(): -25.248497009277344 
model_pd.lagr.mean(): -25.302095413208008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0444], device='cuda:0')), ('power', tensor([-25.2041], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:-0.053598422557115555
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.009583311155438423
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.19088684022426605
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.1262277066707611
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.12728159129619598
epoch£º1121	 i:5 	 global-step:22425	 l-p:-0.03390680253505707
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.15410906076431274
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.23753032088279724
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.12598761916160583
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.154262512922287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0988, 2.9696, 3.0724],
        [3.0988, 2.9992, 3.0819],
        [3.0988, 2.8241, 3.0010],
        [3.0988, 2.0468, 1.3734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.14675447344779968 
model_pd.l_d.mean(): -24.956823348999023 
model_pd.lagr.mean(): -24.810068130493164 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1448], device='cuda:0')), ('power', tensor([-25.1017], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.14675447344779968
epoch£º1122	 i:1 	 global-step:22441	 l-p:-0.23836128413677216
epoch£º1122	 i:2 	 global-step:22442	 l-p:0.05428477004170418
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.17903375625610352
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13348934054374695
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.1422584354877472
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.14218229055404663
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.1348632276058197
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.17742377519607544
epoch£º1122	 i:9 	 global-step:22449	 l-p:-0.024198217317461967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0974, 1.7905, 1.3235],
        [3.0974, 1.7556, 1.1702],
        [3.0974, 2.8227, 2.9996],
        [3.0974, 3.0815, 3.0966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): -0.00945372600108385 
model_pd.l_d.mean(): -25.055849075317383 
model_pd.lagr.mean(): -25.0653018951416 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0665], device='cuda:0')), ('power', tensor([-25.1224], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:-0.00945372600108385
epoch£º1123	 i:1 	 global-step:22461	 l-p:-0.06357315927743912
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.13660019636154175
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.7835865616798401
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.11921264231204987
epoch£º1123	 i:5 	 global-step:22465	 l-p:0.1955442726612091
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.15604886412620544
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14982959628105164
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.15833264589309692
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12227433919906616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1178, 1.7841, 1.1808],
        [3.1178, 2.7443, 2.9485],
        [3.1178, 2.1220, 2.0749],
        [3.1178, 2.7925, 2.9856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.14050136506557465 
model_pd.l_d.mean(): -25.100860595703125 
model_pd.lagr.mean(): -24.960359573364258 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0622], device='cuda:0')), ('power', tensor([-25.0387], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.14050136506557465
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.8997805118560791
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13038058578968048
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.32225513458251953
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13349096477031708
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.15305212140083313
epoch£º1124	 i:6 	 global-step:22486	 l-p:1.213919758796692
epoch£º1124	 i:7 	 global-step:22487	 l-p:0.1967514008283615
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.14738768339157104
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.12506423890590668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1132, 2.8250, 3.0067],
        [3.1132, 1.8686, 1.4934],
        [3.1132, 1.9154, 1.6062],
        [3.1132, 3.0871, 3.1114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15600015223026276 
model_pd.l_d.mean(): -25.281850814819336 
model_pd.lagr.mean(): -25.125850677490234 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0068], device='cuda:0')), ('power', tensor([-25.2750], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15600015223026276
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.12514345347881317
epoch£º1125	 i:2 	 global-step:22502	 l-p:-1.6872291564941406
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.17107433080673218
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.12958858907222748
epoch£º1125	 i:5 	 global-step:22505	 l-p:0.7444469928741455
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.14459937810897827
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.35116639733314514
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.14040698111057281
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.1347268968820572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1128, 3.1128, 3.1128],
        [3.1128, 2.8371, 3.0143],
        [3.1128, 2.5664, 2.7757],
        [3.1128, 1.9871, 1.7815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.7026711702346802 
model_pd.l_d.mean(): -25.185253143310547 
model_pd.lagr.mean(): -24.482582092285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0062], device='cuda:0')), ('power', tensor([-25.1791], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.7026711702346802
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.15032099187374115
epoch£º1126	 i:2 	 global-step:22522	 l-p:-1.3011436462402344
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.3615048825740814
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.11763549596071243
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.1304761916399002
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.14476561546325684
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.18489964306354523
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.132082000374794
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.16405685245990753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1054, 1.9324, 1.6597],
        [3.1054, 3.1039, 3.1053],
        [3.1054, 3.1048, 3.1054],
        [3.1054, 1.7692, 1.1722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.16286516189575195 
model_pd.l_d.mean(): -25.26129150390625 
model_pd.lagr.mean(): -25.098426818847656 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0754], device='cuda:0')), ('power', tensor([-25.3367], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.16286516189575195
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.1315808743238449
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.12216395884752274
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11038386821746826
epoch£º1127	 i:4 	 global-step:22544	 l-p:-0.04339991509914398
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.14637599885463715
epoch£º1127	 i:6 	 global-step:22546	 l-p:-0.19622331857681274
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.24485602974891663
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.0004761981836054474
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.15085403621196747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0974, 3.0974, 3.0974],
        [3.0974, 3.0974, 3.0974],
        [3.0974, 2.9306, 3.0565],
        [3.0974, 1.8788, 1.5421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.20774367451667786 
model_pd.l_d.mean(): -24.519969940185547 
model_pd.lagr.mean(): -24.312225341796875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1546], device='cuda:0')), ('power', tensor([-24.6746], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.20774367451667786
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.2387150377035141
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.10768036544322968
epoch£º1128	 i:3 	 global-step:22563	 l-p:11.249879837036133
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12387523055076599
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.14928624033927917
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.0959247350692749
epoch£º1128	 i:7 	 global-step:22567	 l-p:-0.1971101462841034
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.13908778131008148
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12447722256183624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0989, 3.0988, 3.0989],
        [3.0989, 2.8147, 2.9950],
        [3.0989, 3.0629, 3.0958],
        [3.0989, 3.0518, 3.0940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.060973651707172394 
model_pd.l_d.mean(): -24.789825439453125 
model_pd.lagr.mean(): -24.728851318359375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0029], device='cuda:0')), ('power', tensor([-24.7928], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.060973651707172394
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.1470344364643097
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.09970781952142715
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12460349500179291
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.133094921708107
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.1405733972787857
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.16259323060512543
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.12303413450717926
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.187921941280365
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.19212913513183594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0634, 2.0019, 1.3358],
        [3.0634, 3.0633, 3.0634],
        [3.0634, 2.0228, 1.3528],
        [3.0634, 2.1701, 2.2223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.15165764093399048 
model_pd.l_d.mean(): -24.642824172973633 
model_pd.lagr.mean(): -24.491167068481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1731], device='cuda:0')), ('power', tensor([-24.8159], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.15165764093399048
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.10211478173732758
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.14437034726142883
epoch£º1130	 i:3 	 global-step:22603	 l-p:-0.5143521428108215
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.12122298777103424
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.15072883665561676
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.10556699335575104
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.20087188482284546
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.12843705713748932
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.14280478656291962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0812, 3.0813, 3.0813],
        [3.0812, 2.9518, 3.0548],
        [3.0812, 1.8931, 1.2482],
        [3.0812, 2.7872, 2.9710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.14467495679855347 
model_pd.l_d.mean(): -24.807409286499023 
model_pd.lagr.mean(): -24.662734985351562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0115], device='cuda:0')), ('power', tensor([-24.8189], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.14467495679855347
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.125691756606102
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.1335146278142929
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.21678714454174042
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.13564464449882507
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.15655237436294556
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.05261256918311119
epoch£º1131	 i:7 	 global-step:22627	 l-p:0.918671190738678
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.15822172164916992
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12701073288917542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[3.0710, 2.0190, 1.3498],
        [3.0710, 2.1579, 2.1934],
        [3.0710, 1.7324, 1.2034],
        [3.0710, 2.0199, 1.9130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12892164289951324 
model_pd.l_d.mean(): -25.130022048950195 
model_pd.lagr.mean(): -25.001100540161133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0151], device='cuda:0')), ('power', tensor([-25.1451], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12892164289951324
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.15544116497039795
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.15157775580883026
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.14050105214118958
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.17415907979011536
epoch£º1132	 i:5 	 global-step:22645	 l-p:-4.764288902282715
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.13981689512729645
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.10752595961093903
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.06471281498670578
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.13653387129306793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0443, 3.0444, 3.0443],
        [3.0443, 1.7036, 1.1406],
        [3.0443, 2.3003, 2.4511],
        [3.0443, 2.8969, 3.0114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): -0.28632402420043945 
model_pd.l_d.mean(): -24.92563247680664 
model_pd.lagr.mean(): -25.211956024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0877], device='cuda:0')), ('power', tensor([-25.0134], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:-0.28632402420043945
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.31566086411476135
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.14234766364097595
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.13224594295024872
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.14613160490989685
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.0769868791103363
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.09093917906284332
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.15133127570152283
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.12043380737304688
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.15269355475902557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0673, 2.1730, 2.2247],
        [3.0673, 2.2797, 2.4061],
        [3.0673, 2.7871, 2.9662],
        [3.0673, 2.4808, 2.6848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.15741373598575592 
model_pd.l_d.mean(): -25.049272537231445 
model_pd.lagr.mean(): -24.89185905456543 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0990], device='cuda:0')), ('power', tensor([-25.1482], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.15741373598575592
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.13288567960262299
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13504354655742645
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.07929998636245728
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.16709567606449127
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.11431848257780075
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.07539065927267075
epoch£º1134	 i:7 	 global-step:22687	 l-p:-0.17736448347568512
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.27521905303001404
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.19516637921333313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0751, 2.0337, 1.3618],
        [3.0751, 1.7556, 1.2698],
        [3.0751, 2.9204, 3.0392],
        [3.0751, 1.8736, 1.5644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.6923615336418152 
model_pd.l_d.mean(): -25.045923233032227 
model_pd.lagr.mean(): -24.353561401367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1300], device='cuda:0')), ('power', tensor([-25.1760], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:0.6923615336418152
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.041127823293209076
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.1307544708251953
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.11423761397600174
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.13737143576145172
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.016190538182854652
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.024098988622426987
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.18148548901081085
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.16177579760551453
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.13251535594463348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1070, 3.0229, 3.0943],
        [3.1070, 3.1013, 3.1068],
        [3.1070, 2.5504, 2.7587],
        [3.1070, 1.9100, 1.6038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): -0.7010130286216736 
model_pd.l_d.mean(): -25.171710968017578 
model_pd.lagr.mean(): -25.872724533081055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1057], device='cuda:0')), ('power', tensor([-25.0660], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:-0.7010130286216736
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.15893079340457916
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.1269242912530899
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.12842243909835815
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.39015427231788635
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.16518710553646088
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.14314031600952148
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.1349530816078186
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.5540186762809753
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.15817943215370178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1194, 3.1182, 3.1194],
        [3.1194, 3.1193, 3.1194],
        [3.1194, 3.1045, 3.1186],
        [3.1194, 1.8610, 1.4672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.20724481344223022 
model_pd.l_d.mean(): -24.94206428527832 
model_pd.lagr.mean(): -24.734819412231445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-6.8879e-05], device='cuda:0')), ('power', tensor([-24.9420], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.20724481344223022
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.22745712101459503
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.15877030789852142
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.12702451646327972
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.15177035331726074
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.15145204961299896
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.13645032048225403
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11236061155796051
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.1543906033039093
epoch£º1137	 i:9 	 global-step:22749	 l-p:-2.708881139755249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1057, 3.0934, 3.1051],
        [3.1057, 3.1057, 3.1057],
        [3.1057, 2.9410, 3.0657],
        [3.1057, 1.8366, 1.2055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.20214375853538513 
model_pd.l_d.mean(): -24.72427749633789 
model_pd.lagr.mean(): -24.522132873535156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0590], device='cuda:0')), ('power', tensor([-24.7832], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.20214375853538513
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.1444728672504425
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.15374639630317688
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.1270328164100647
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.15786956250667572
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.024224895983934402
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.04800798371434212
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.14105385541915894
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.20287059247493744
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.05350058898329735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0782, 3.0782, 3.0782],
        [3.0782, 3.0382, 3.0745],
        [3.0782, 1.7376, 1.2050],
        [3.0782, 3.0311, 3.0734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.13239531219005585 
model_pd.l_d.mean(): -24.973997116088867 
model_pd.lagr.mean(): -24.841602325439453 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0640], device='cuda:0')), ('power', tensor([-25.0380], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.13239531219005585
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.12102612853050232
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.1495014727115631
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.09671705216169357
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.14944909512996674
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.13623100519180298
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.18812404572963715
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.22652670741081238
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.07755192369222641
epoch£º1139	 i:9 	 global-step:22789	 l-p:-0.7359092831611633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0723, 3.0723, 3.0723],
        [3.0723, 2.9204, 3.0376],
        [3.0723, 3.0692, 3.0723],
        [3.0723, 3.0723, 3.0723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.1581140160560608 
model_pd.l_d.mean(): -25.21958351135254 
model_pd.lagr.mean(): -25.06147003173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0910], device='cuda:0')), ('power', tensor([-25.1286], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.1581140160560608
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.14104054868221283
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.22267569601535797
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.18745674192905426
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.1304362714290619
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.11876466125249863
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.14184348285198212
epoch£º1140	 i:7 	 global-step:22807	 l-p:0.4092361330986023
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.1145085021853447
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.13218435645103455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0735, 2.6594, 2.8705],
        [3.0735, 3.0715, 3.0735],
        [3.0735, 1.7767, 1.1622],
        [3.0735, 2.2835, 2.4087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 1.2326195240020752 
model_pd.l_d.mean(): -24.584186553955078 
model_pd.lagr.mean(): -23.351566314697266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2232], device='cuda:0')), ('power', tensor([-24.8074], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:1.2326195240020752
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.17685961723327637
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.14397595822811127
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.07587528973817825
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.13026946783065796
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.14848513901233673
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.148387148976326
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.43122372031211853
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.09450377523899078
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.07919759303331375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0644, 3.0630, 3.0643],
        [3.0644, 2.0943, 1.4108],
        [3.0644, 3.0641, 3.0644],
        [3.0644, 1.7963, 1.1737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.1317385882139206 
model_pd.l_d.mean(): -24.817462921142578 
model_pd.lagr.mean(): -24.68572425842285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0107], device='cuda:0')), ('power', tensor([-24.8068], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.1317385882139206
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.10595233738422394
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.20825833082199097
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.1411447823047638
epoch£º1142	 i:4 	 global-step:22844	 l-p:-1.5581809282302856
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.12332998216152191
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.13849511742591858
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.24955038726329803
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.049293942749500275
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.11456309258937836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0770, 2.0325, 1.3605],
        [3.0770, 2.9766, 3.0599],
        [3.0770, 2.4209, 2.6068],
        [3.0770, 3.0734, 3.0769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.03943292796611786 
model_pd.l_d.mean(): -24.905046463012695 
model_pd.lagr.mean(): -24.86561393737793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0571], device='cuda:0')), ('power', tensor([-24.8480], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.03943292796611786
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.13926681876182556
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.15331050753593445
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.1293715089559555
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.20404700934886932
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.12114465236663818
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.26969459652900696
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.14181384444236755
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.1508864015340805
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.1388414204120636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0864, 3.0863, 3.0864],
        [3.0864, 3.0864, 3.0864],
        [3.0864, 3.0858, 3.0864],
        [3.0864, 1.8751, 1.5530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.16567443311214447 
model_pd.l_d.mean(): -25.061725616455078 
model_pd.lagr.mean(): -24.89605140686035 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0146], device='cuda:0')), ('power', tensor([-25.0763], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.16567443311214447
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.15006211400032043
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.10259369015693665
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.06351056694984436
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.049826059490442276
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.16716663539409637
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.150362029671669
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.19372908771038055
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.17205478250980377
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.1410391926765442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0949, 2.8183, 2.9961],
        [3.0949, 2.6432, 2.8573],
        [3.0949, 3.0765, 3.0939],
        [3.0949, 3.0630, 3.0924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): -0.046741560101509094 
model_pd.l_d.mean(): -24.882068634033203 
model_pd.lagr.mean(): -24.928810119628906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0869], device='cuda:0')), ('power', tensor([-24.9689], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:-0.046741560101509094
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.11599698662757874
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.12231757491827011
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.1489851027727127
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.1315850466489792
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.15023347735404968
epoch£º1145	 i:6 	 global-step:22906	 l-p:-0.0360305979847908
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.29187628626823425
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.13580422103405
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.14175613224506378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0837, 3.0837, 3.0837],
        [3.0837, 2.1303, 1.4410],
        [3.0837, 1.8886, 1.5898],
        [3.0837, 1.9148, 1.6531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.20899257063865662 
model_pd.l_d.mean(): -24.943544387817383 
model_pd.lagr.mean(): -24.73455238342285 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2047], device='cuda:0')), ('power', tensor([-25.1483], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.20899257063865662
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.13401708006858826
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.061241958290338516
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.45608457922935486
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.14380887150764465
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.04187940061092377
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.13434508442878723
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.14768245816230774
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.04919009283185005
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.2272370457649231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0852, 1.9413, 1.2862],
        [3.0852, 3.0837, 3.0852],
        [3.0852, 3.0820, 3.0851],
        [3.0852, 1.7487, 1.1555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.3712732493877411 
model_pd.l_d.mean(): -24.997766494750977 
model_pd.lagr.mean(): -24.626493453979492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0820], device='cuda:0')), ('power', tensor([-25.0798], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.3712732493877411
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.06453453749418259
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.18693725764751434
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.10945729911327362
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.12745313346385956
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.14563238620758057
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.13099808990955353
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.13004082441329956
epoch£º1147	 i:8 	 global-step:22948	 l-p:11.986298561096191
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.1361677199602127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1117, 2.0573, 1.3815],
        [3.1117, 2.8616, 3.0290],
        [3.1117, 2.7476, 2.9504],
        [3.1117, 3.0991, 3.1112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.11747977882623672 
model_pd.l_d.mean(): -24.766321182250977 
model_pd.lagr.mean(): -24.648841857910156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0579], device='cuda:0')), ('power', tensor([-24.8242], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.11747977882623672
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.11931363493204117
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.1481155902147293
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.6598334908485413
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.14446526765823364
epoch£º1148	 i:5 	 global-step:22965	 l-p:-0.4216381311416626
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.12536168098449707
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11637042462825775
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.18275509774684906
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.032862767577171326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1038, 1.7816, 1.1713],
        [3.1038, 1.7577, 1.2138],
        [3.1038, 2.6786, 2.8907],
        [3.1038, 2.7375, 2.9409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.1591530591249466 
model_pd.l_d.mean(): -24.907461166381836 
model_pd.lagr.mean(): -24.748308181762695 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1369], device='cuda:0')), ('power', tensor([-25.0443], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.1591530591249466
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.22086650133132935
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.15391862392425537
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.13391292095184326
epoch£º1149	 i:4 	 global-step:22984	 l-p:-0.541999101638794
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.1251891553401947
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.13854435086250305
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.15567615628242493
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.03351500257849693
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.08764956146478653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0899, 3.0899, 3.0899],
        [3.0899, 3.0889, 3.0899],
        [3.0899, 3.0892, 3.0899],
        [3.0899, 3.0885, 3.0899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11508138477802277 
model_pd.l_d.mean(): -24.991771697998047 
model_pd.lagr.mean(): -24.876689910888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0932], device='cuda:0')), ('power', tensor([-24.8986], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11508138477802277
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.04343805089592934
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.1501385122537613
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.14006315171718597
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.16272608935832977
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.026962855830788612
epoch£º1150	 i:6 	 global-step:23006	 l-p:0.2881881594657898
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.13997764885425568
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.0639820471405983
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.2098206728696823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0901, 3.0901, 3.0901],
        [3.0901, 3.0900, 3.0901],
        [3.0901, 2.8145, 2.9920],
        [3.0901, 2.1053, 2.0738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.133159801363945 
model_pd.l_d.mean(): -25.152095794677734 
model_pd.lagr.mean(): -25.018936157226562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0283], device='cuda:0')), ('power', tensor([-25.1238], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.133159801363945
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.17260417342185974
epoch£º1151	 i:2 	 global-step:23022	 l-p:-0.1671251505613327
epoch£º1151	 i:3 	 global-step:23023	 l-p:-0.11976293474435806
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12281795591115952
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.13555549085140228
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.16768193244934082
epoch£º1151	 i:7 	 global-step:23027	 l-p:1.4710708856582642
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.1259927898645401
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.2051529586315155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1268, 3.1248, 3.1268],
        [3.1268, 1.9047, 1.5632],
        [3.1268, 3.1268, 3.1268],
        [3.1268, 2.4909, 2.6818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.13214091956615448 
model_pd.l_d.mean(): -25.15837287902832 
model_pd.lagr.mean(): -25.02623176574707 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1403], device='cuda:0')), ('power', tensor([-25.0181], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.13214091956615448
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.15758748352527618
epoch£º1152	 i:2 	 global-step:23042	 l-p:0.3175458610057831
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.21650205552577972
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.1275216042995453
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.18109992146492004
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11313503235578537
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.13719628751277924
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.13954520225524902
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.15318213403224945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1461, 3.1430, 3.1461],
        [3.1461, 2.9877, 3.1087],
        [3.1461, 2.7593, 2.9660],
        [3.1461, 3.1047, 3.1422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.31654953956604004 
model_pd.l_d.mean(): -25.049972534179688 
model_pd.lagr.mean(): -24.733423233032227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0055], device='cuda:0')), ('power', tensor([-25.0555], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.31654953956604004
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.1398697793483734
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.08726068586111069
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.18483996391296387
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.11910045146942139
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.13665641844272614
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12007620185613632
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.15834316611289978
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.13574859499931335
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.14466124773025513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1270, 3.0088, 3.1043],
        [3.1270, 2.7226, 2.9321],
        [3.1270, 3.1270, 3.1270],
        [3.1270, 3.0267, 3.1098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13435928523540497 
model_pd.l_d.mean(): -25.137380599975586 
model_pd.lagr.mean(): -25.003021240234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0093], device='cuda:0')), ('power', tensor([-25.1281], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13435928523540497
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.19137336313724518
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.12277404218912125
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.18840450048446655
epoch£º1154	 i:4 	 global-step:23084	 l-p:-0.8817420601844788
epoch£º1154	 i:5 	 global-step:23085	 l-p:-0.13707436621189117
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.9931131601333618
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.1384892761707306
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.10929250717163086
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.13414746522903442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1083, 3.0945, 3.1077],
        [3.1083, 3.1083, 3.1083],
        [3.1083, 3.1083, 3.1083],
        [3.1083, 2.0917, 1.4098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.1763629913330078 
model_pd.l_d.mean(): -24.62732696533203 
model_pd.lagr.mean(): -24.450963973999023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0930], device='cuda:0')), ('power', tensor([-24.7203], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.1763629913330078
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12858742475509644
epoch£º1155	 i:2 	 global-step:23102	 l-p:-0.18483039736747742
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.16555939614772797
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.12461330741643906
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.14215247333049774
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.18916799128055573
epoch£º1155	 i:7 	 global-step:23107	 l-p:-2.3995773792266846
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.4116753935813904
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.12966130673885345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1166, 2.9816, 3.0882],
        [3.1166, 3.1166, 3.1166],
        [3.1166, 2.9983, 3.0939],
        [3.1166, 1.7829, 1.2710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.14497506618499756 
model_pd.l_d.mean(): -25.282039642333984 
model_pd.lagr.mean(): -25.13706398010254 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0862], device='cuda:0')), ('power', tensor([-25.1958], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.14497506618499756
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.12743675708770752
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.08936520665884018
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.15104594826698303
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.12367010116577148
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.1431429088115692
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.20469611883163452
epoch£º1156	 i:7 	 global-step:23127	 l-p:7.331071376800537
epoch£º1156	 i:8 	 global-step:23128	 l-p:-0.3946824073791504
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.24383237957954407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1066, 3.1066, 3.1066],
        [3.1066, 2.1779, 2.1997],
        [3.1066, 3.0980, 3.1063],
        [3.1066, 2.7944, 2.9841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.16840291023254395 
model_pd.l_d.mean(): -24.998687744140625 
model_pd.lagr.mean(): -24.830284118652344 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0738], device='cuda:0')), ('power', tensor([-25.0725], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.16840291023254395
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.15150795876979828
epoch£º1157	 i:2 	 global-step:23142	 l-p:-5.5135955810546875
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.15191638469696045
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.09622462093830109
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.12796789407730103
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.14610709249973297
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.14206938445568085
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12588666379451752
epoch£º1157	 i:9 	 global-step:23149	 l-p:0.3779672682285309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1169, 2.7527, 2.9556],
        [3.1169, 3.1168, 3.1169],
        [3.1169, 3.1169, 3.1170],
        [3.1169, 2.1183, 2.0711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.13402128219604492 
model_pd.l_d.mean(): -24.880279541015625 
model_pd.lagr.mean(): -24.746257781982422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0315], device='cuda:0')), ('power', tensor([-24.9118], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.13402128219604492
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12485705316066742
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.12704357504844666
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.19323091208934784
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.1176161989569664
epoch£º1158	 i:5 	 global-step:23165	 l-p:-0.548261284828186
epoch£º1158	 i:6 	 global-step:23166	 l-p:-0.12948855757713318
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.14797069132328033
epoch£º1158	 i:8 	 global-step:23168	 l-p:1.2441965341567993
epoch£º1158	 i:9 	 global-step:23169	 l-p:0.20971129834651947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1086, 1.8356, 1.4246],
        [3.1086, 3.1083, 3.1086],
        [3.1086, 3.1075, 3.1086],
        [3.1086, 2.9372, 3.0658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.1426529735326767 
model_pd.l_d.mean(): -25.116867065429688 
model_pd.lagr.mean(): -24.974214553833008 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0009], device='cuda:0')), ('power', tensor([-25.1178], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.1426529735326767
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.13703377544879913
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.1730320304632187
epoch£º1159	 i:3 	 global-step:23183	 l-p:-0.5097752213478088
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.576746940612793
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10793459415435791
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.1732720285654068
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14190958440303802
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.10445860028266907
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.9187173843383789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1211, 1.7844, 1.2663],
        [3.1211, 3.1052, 3.1203],
        [3.1211, 1.7698, 1.2011],
        [3.1211, 3.1208, 3.1211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.1305299997329712 
model_pd.l_d.mean(): -24.930524826049805 
model_pd.lagr.mean(): -24.79999542236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0076], device='cuda:0')), ('power', tensor([-24.9230], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.1305299997329712
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.1793251931667328
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.1442582607269287
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.20853441953659058
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.14004315435886383
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.2722010910511017
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.12047602981328964
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.2535746991634369
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.1509101390838623
epoch£º1160	 i:9 	 global-step:23209	 l-p:0.15790854394435883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1347, 1.7907, 1.1902],
        [3.1347, 3.1346, 3.1347],
        [3.1347, 2.8217, 3.0115],
        [3.1347, 2.3785, 2.5205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.15635943412780762 
model_pd.l_d.mean(): -24.445316314697266 
model_pd.lagr.mean(): -24.288957595825195 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2553], device='cuda:0')), ('power', tensor([-24.7006], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:0.15635943412780762
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.12857554852962494
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.1553518921136856
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.20607346296310425
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.20271168649196625
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.16998249292373657
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.14411017298698425
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.13057956099510193
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.19392913579940796
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.1197650283575058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[3.1375, 1.8640, 1.2266],
        [3.1375, 2.6929, 2.9061],
        [3.1375, 2.2250, 2.2595],
        [3.1375, 1.7929, 1.2556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.15186548233032227 
model_pd.l_d.mean(): -25.04326820373535 
model_pd.lagr.mean(): -24.891403198242188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1369], device='cuda:0')), ('power', tensor([-25.1802], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.15186548233032227
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.14439815282821655
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.1199348196387291
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.2186727076768875
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.12253846973180771
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.26878777146339417
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.16348281502723694
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.3237474858760834
epoch£º1162	 i:8 	 global-step:23248	 l-p:0.15050622820854187
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.1275968700647354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1270, 1.8050, 1.1885],
        [3.1270, 3.1270, 3.1270],
        [3.1270, 3.1270, 3.1270],
        [3.1270, 2.1221, 1.4355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.3078230321407318 
model_pd.l_d.mean(): -24.836898803710938 
model_pd.lagr.mean(): -24.529075622558594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0333], device='cuda:0')), ('power', tensor([-24.8702], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.3078230321407318
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.2879270017147064
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.11298435181379318
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11030988395214081
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.14654375612735748
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.14230670034885406
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.1537339687347412
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.22645284235477448
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.1464141309261322
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.1396586000919342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[3.1343, 2.7104, 2.9221],
        [3.1343, 2.6014, 2.8127],
        [3.1343, 2.1660, 2.1490],
        [3.1343, 2.1639, 1.4707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.12302868813276291 
model_pd.l_d.mean(): -24.848194122314453 
model_pd.lagr.mean(): -24.72516632080078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0180], device='cuda:0')), ('power', tensor([-24.8662], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.12302868813276291
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.22725827991962433
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.2970678210258484
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.1599683314561844
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.13011255860328674
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.14632704854011536
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.13476833701133728
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.1393684595823288
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.14071981608867645
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.2250804901123047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1278, 1.7772, 1.2194],
        [3.1278, 1.8145, 1.1933],
        [3.1278, 2.3711, 2.5133],
        [3.1278, 3.1278, 3.1278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.11662938445806503 
model_pd.l_d.mean(): -24.88133430480957 
model_pd.lagr.mean(): -24.764705657958984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0923], device='cuda:0')), ('power', tensor([-24.9736], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.11662938445806503
epoch£º1165	 i:1 	 global-step:23301	 l-p:0.20416171848773956
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.13908542692661285
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.16114673018455505
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.2394203096628189
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.49962279200553894
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.14660882949829102
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.1691485345363617
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.14878392219543457
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.29722291231155396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1223, 3.1223, 3.1223],
        [3.1223, 2.6709, 2.8847],
        [3.1223, 3.0804, 3.1183],
        [3.1223, 2.6980, 2.9099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.17041927576065063 
model_pd.l_d.mean(): -25.04351234436035 
model_pd.lagr.mean(): -24.873092651367188 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0184], device='cuda:0')), ('power', tensor([-25.0252], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.17041927576065063
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.13874971866607666
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.13136956095695496
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.2463032454252243
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.4327365756034851
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.2906443476676941
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.14895761013031006
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.1646554321050644
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.14019274711608887
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.12620852887630463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1383, 2.8124, 3.0058],
        [3.1383, 3.0730, 3.1299],
        [3.1383, 2.1701, 2.1531],
        [3.1383, 2.3753, 2.5137]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.11126083880662918 
model_pd.l_d.mean(): -25.14042091369629 
model_pd.lagr.mean(): -25.029159545898438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1128], device='cuda:0')), ('power', tensor([-25.2532], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.11126083880662918
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.17513467371463776
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.13097189366817474
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.14451289176940918
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.20759357511997223
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12418875098228455
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.12334270030260086
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.1253523826599121
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.1901049017906189
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.21958205103874207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1442, 3.1310, 3.1435],
        [3.1442, 1.9223, 1.2712],
        [3.1442, 3.1268, 3.1432],
        [3.1442, 2.2212, 1.5192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.1345413774251938 
model_pd.l_d.mean(): -24.00602149963379 
model_pd.lagr.mean(): -23.87148094177246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2645], device='cuda:0')), ('power', tensor([-24.2706], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.1345413774251938
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11290807276964188
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.1318676471710205
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.16548392176628113
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.36771056056022644
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.13118797540664673
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.17295622825622559
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.17411349713802338
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.12865298986434937
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.16600966453552246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1194, 1.7971, 1.1826],
        [3.1194, 3.1127, 3.1192],
        [3.1194, 1.8449, 1.2117],
        [3.1194, 2.7931, 2.9868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.1340261846780777 
model_pd.l_d.mean(): -25.091089248657227 
model_pd.lagr.mean(): -24.957063674926758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0637], device='cuda:0')), ('power', tensor([-25.0274], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.1340261846780777
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.1761690080165863
epoch£º1169	 i:2 	 global-step:23382	 l-p:-2.833178758621216
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.3747273087501526
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13502635061740875
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.6803606748580933
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.21799860894680023
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.14063414931297302
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.12409544736146927
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11954444646835327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1214, 3.1214, 3.1214],
        [3.1214, 2.1114, 2.0519],
        [3.1214, 2.2828, 2.3757],
        [3.1214, 1.7711, 1.2146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.27052736282348633 
model_pd.l_d.mean(): -24.94476890563965 
model_pd.lagr.mean(): -24.67424201965332 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0882], device='cuda:0')), ('power', tensor([-25.0330], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:0.27052736282348633
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.09139560163021088
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.12150093168020248
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.20284728705883026
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12183380126953125
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.1434192806482315
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.2594869136810303
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.14516820013523102
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.14167557656764984
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.33030155301094055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1279, 3.1259, 3.1278],
        [3.1279, 1.8205, 1.1966],
        [3.1279, 1.8144, 1.1932],
        [3.1279, 2.7305, 2.9391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.1327696144580841 
model_pd.l_d.mean(): -25.049585342407227 
model_pd.lagr.mean(): -24.91681480407715 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0054], device='cuda:0')), ('power', tensor([-25.0441], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.1327696144580841
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.24806445837020874
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.10636010020971298
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.1110333800315857
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.1493549644947052
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.324468195438385
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.1394408643245697
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.16032655537128448
epoch£º1171	 i:8 	 global-step:23428	 l-p:0.17332099378108978
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.38023024797439575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1272, 3.1186, 3.1269],
        [3.1272, 3.1183, 3.1269],
        [3.1272, 3.1157, 3.1267],
        [3.1272, 2.6759, 2.8897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.23308028280735016 
model_pd.l_d.mean(): -25.17550277709961 
model_pd.lagr.mean(): -24.94242286682129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0659], device='cuda:0')), ('power', tensor([-25.1096], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.23308028280735016
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.14048783481121063
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.10233239084482193
epoch£º1172	 i:3 	 global-step:23443	 l-p:0.17789407074451447
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.1392931342124939
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.12720631062984467
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.12123974412679672
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.14771564304828644
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.21926681697368622
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.22551082074642181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1413, 1.8984, 1.5265],
        [3.1413, 3.1413, 3.1413],
        [3.1413, 2.0907, 1.9835],
        [3.1413, 3.1413, 3.1413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.16451457142829895 
model_pd.l_d.mean(): -24.953981399536133 
model_pd.lagr.mean(): -24.789466857910156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0116], device='cuda:0')), ('power', tensor([-24.9655], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.16451457142829895
epoch£º1173	 i:1 	 global-step:23461	 l-p:0.23638184368610382
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.2246827334165573
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12316983938217163
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.21585232019424438
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.08177120238542557
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.11840108782052994
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.11822950094938278
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.13324618339538574
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.14463311433792114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1470, 2.5959, 2.8047],
        [3.1470, 2.7429, 2.9522],
        [3.1470, 1.8346, 1.3589],
        [3.1470, 1.9516, 1.2947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.15221315622329712 
model_pd.l_d.mean(): -24.8078670501709 
model_pd.lagr.mean(): -24.65565299987793 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0789], device='cuda:0')), ('power', tensor([-24.8868], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.15221315622329712
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.1416049599647522
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.20648401975631714
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.10584424436092377
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.17749953269958496
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11745374649763107
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.23515702784061432
epoch£º1174	 i:7 	 global-step:23487	 l-p:0.15669415891170502
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.12776881456375122
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.2027955949306488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1388, 2.1690, 1.4749],
        [3.1388, 2.6134, 2.8254],
        [3.1388, 1.9646, 1.6922],
        [3.1388, 3.1385, 3.1388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.1990433633327484 
model_pd.l_d.mean(): -24.53644561767578 
model_pd.lagr.mean(): -24.33740234375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1679], device='cuda:0')), ('power', tensor([-24.7044], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.1990433633327484
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.1462651789188385
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.16701728105545044
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.09139906615018845
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.2095997929573059
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.10561060905456543
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.16741329431533813
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.142470583319664
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.14537109434604645
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.16536010801792145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1528, 2.2407, 2.2750],
        [3.1528, 1.9304, 1.5864],
        [3.1528, 3.1528, 3.1528],
        [3.1528, 2.2518, 2.2956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.1228400394320488 
model_pd.l_d.mean(): -24.828750610351562 
model_pd.lagr.mean(): -24.705909729003906 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0769], device='cuda:0')), ('power', tensor([-24.7518], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.1228400394320488
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.1460142433643341
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.21251627802848816
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.15317626297473907
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.1480906754732132
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.15273486077785492
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.14606311917304993
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.12977585196495056
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.15284638106822968
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.14248980581760406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1514, 3.0654, 3.1382],
        [3.1514, 2.9803, 3.1087],
        [3.1514, 3.1439, 3.1511],
        [3.1514, 1.9696, 1.3093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.09985734522342682 
model_pd.l_d.mean(): -25.009794235229492 
model_pd.lagr.mean(): -24.909936904907227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0028], device='cuda:0')), ('power', tensor([-25.0125], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.09985734522342682
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.20242932438850403
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.12679943442344666
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13054679334163666
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.15330862998962402
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.11529510468244553
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.14798492193222046
epoch£º1177	 i:7 	 global-step:23547	 l-p:0.2193872332572937
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.21722698211669922
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13692158460617065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1338, 3.1338, 3.1338],
        [3.1338, 3.0540, 3.1221],
        [3.1338, 1.9796, 1.7375],
        [3.1338, 3.1338, 3.1338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.14929138123989105 
model_pd.l_d.mean(): -25.21985626220703 
model_pd.lagr.mean(): -25.07056427001953 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0925], device='cuda:0')), ('power', tensor([-25.1274], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.14929138123989105
epoch£º1178	 i:1 	 global-step:23561	 l-p:0.2090524286031723
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.15728335082530975
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.1367659568786621
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.1724139302968979
epoch£º1178	 i:5 	 global-step:23565	 l-p:0.2478361427783966
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.137527197599411
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.15922413766384125
epoch£º1178	 i:8 	 global-step:23568	 l-p:0.6089308261871338
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.1339951455593109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1218, 2.5430, 2.7481],
        [3.1218, 3.0650, 3.1152],
        [3.1218, 1.8467, 1.2129],
        [3.1218, 3.0215, 3.1047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.12977539002895355 
model_pd.l_d.mean(): -25.164121627807617 
model_pd.lagr.mean(): -25.034345626831055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0358], device='cuda:0')), ('power', tensor([-25.1283], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.12977539002895355
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.11242927610874176
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.24602484703063965
epoch£º1179	 i:3 	 global-step:23583	 l-p:0.5699682831764221
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.3002708852291107
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.12191640585660934
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.12293878942728043
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.15481363236904144
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.1288290023803711
epoch£º1179	 i:9 	 global-step:23589	 l-p:0.7400986552238464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1197, 3.0936, 3.1178],
        [3.1197, 2.9482, 3.0769],
        [3.1197, 2.8953, 3.0513],
        [3.1197, 3.0625, 3.1130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.13093239068984985 
model_pd.l_d.mean(): -24.77145004272461 
model_pd.lagr.mean(): -24.640518188476562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0125], device='cuda:0')), ('power', tensor([-24.7589], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.13093239068984985
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.12560179829597473
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.1328982561826706
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.1403217762708664
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13830651342868805
epoch£º1180	 i:5 	 global-step:23605	 l-p:-0.09788995236158371
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.042069677263498306
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.10384240746498108
epoch£º1180	 i:8 	 global-step:23608	 l-p:-0.07149725407361984
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.16206824779510498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0932, 2.0731, 1.3938],
        [3.0932, 3.0683, 3.0915],
        [3.0932, 3.0277, 3.0849],
        [3.0932, 2.0383, 1.3652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.11980555951595306 
model_pd.l_d.mean(): -24.880739212036133 
model_pd.lagr.mean(): -24.76093292236328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1112], device='cuda:0')), ('power', tensor([-24.9919], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.11980555951595306
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.15557748079299927
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.06692308187484741
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.21973209083080292
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.1506470888853073
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.173780158162117
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11787354946136475
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.5156823396682739
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.15150338411331177
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.15580156445503235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0727, 1.7676, 1.1557],
        [3.0727, 2.4920, 2.6980],
        [3.0727, 2.3895, 2.5671],
        [3.0727, 3.0726, 3.0727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.13098576664924622 
model_pd.l_d.mean(): -25.12116241455078 
model_pd.lagr.mean(): -24.990177154541016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0288], device='cuda:0')), ('power', tensor([-25.0924], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.13098576664924622
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.14251242578029633
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.14374922215938568
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12735271453857422
epoch£º1182	 i:4 	 global-step:23644	 l-p:-0.04603154584765434
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.1532413214445114
epoch£º1182	 i:6 	 global-step:23646	 l-p:11.588173866271973
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.12264761328697205
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13908131420612335
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.16217875480651855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0502, 2.0040, 1.3360],
        [3.0502, 1.9328, 1.2785],
        [3.0502, 3.0502, 3.0502],
        [3.0502, 1.7418, 1.2819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.07635707408189774 
model_pd.l_d.mean(): -25.02617073059082 
model_pd.lagr.mean(): -24.949813842773438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1146], device='cuda:0')), ('power', tensor([-25.1407], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.07635707408189774
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.10493702441453934
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.25415462255477905
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.15095354616641998
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.16735979914665222
epoch£º1183	 i:5 	 global-step:23665	 l-p:2.3879103660583496
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.12176844477653503
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.13501951098442078
epoch£º1183	 i:8 	 global-step:23668	 l-p:-0.06732819229364395
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.14452336728572845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0674, 1.7996, 1.1755],
        [3.0674, 3.0432, 3.0658],
        [3.0674, 1.8696, 1.2284],
        [3.0674, 2.2253, 2.3190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.15660418570041656 
model_pd.l_d.mean(): -25.09204864501953 
model_pd.lagr.mean(): -24.935443878173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0200], device='cuda:0')), ('power', tensor([-25.0721], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.15660418570041656
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.1496826559305191
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.17581336200237274
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.08351121842861176
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.09930772334337234
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.14506764709949493
epoch£º1184	 i:6 	 global-step:23686	 l-p:-0.018978934735059738
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.17590934038162231
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.1608445644378662
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.8475425243377686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0565, 1.9041, 1.6696],
        [3.0565, 2.2843, 2.4219],
        [3.0565, 2.2694, 2.3986],
        [3.0565, 2.6671, 2.8754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.1550212800502777 
model_pd.l_d.mean(): -25.130268096923828 
model_pd.lagr.mean(): -24.97524642944336 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0391], device='cuda:0')), ('power', tensor([-25.1694], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.1550212800502777
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.12869161367416382
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.1591215431690216
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.1265053153038025
epoch£º1185	 i:4 	 global-step:23704	 l-p:-0.1436658501625061
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.15140801668167114
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.12177535891532898
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.11790023744106293
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.3349755108356476
epoch£º1185	 i:9 	 global-step:23709	 l-p:0.05006790533661842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0505, 2.9204, 3.0240],
        [3.0505, 2.8935, 3.0139],
        [3.0505, 2.0631, 2.0321],
        [3.0505, 1.7969, 1.1725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.2588878870010376 
model_pd.l_d.mean(): -25.0728816986084 
model_pd.lagr.mean(): -24.813993453979492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0244], device='cuda:0')), ('power', tensor([-25.0973], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.2588878870010376
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.12671436369419098
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.6716749668121338
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.1518871784210205
epoch£º1186	 i:4 	 global-step:23724	 l-p:-0.05929486081004143
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.10265706479549408
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.06813469529151917
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13982945680618286
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.12352081388235092
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.09921037405729294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0829, 2.5930, 2.8082],
        [3.0829, 1.9380, 1.2831],
        [3.0829, 2.8320, 2.9999],
        [3.0829, 3.0787, 3.0828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.40465247631073 
model_pd.l_d.mean(): -25.08388328552246 
model_pd.lagr.mean(): -24.679231643676758 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0290], device='cuda:0')), ('power', tensor([-25.1129], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.40465247631073
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.1392732709646225
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.1175679937005043
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.1435740888118744
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12660323083400726
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.13963912427425385
epoch£º1187	 i:6 	 global-step:23746	 l-p:-0.04683893918991089
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.05644211545586586
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.25880199670791626
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.15745452046394348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0838, 2.7982, 2.9794],
        [3.0838, 3.0838, 3.0838],
        [3.0838, 2.1446, 1.4524],
        [3.0838, 2.9484, 3.0553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.13007640838623047 
model_pd.l_d.mean(): -25.074874877929688 
model_pd.lagr.mean(): -24.94479751586914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0631], device='cuda:0')), ('power', tensor([-25.0117], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.13007640838623047
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.1519644558429718
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.1157078966498375
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.2413593828678131
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.06259703636169434
epoch£º1188	 i:5 	 global-step:23765	 l-p:3.070707082748413
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.11882981657981873
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.16475169360637665
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.07329177856445312
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.12202774733304977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0803, 2.6335, 2.8479],
        [3.0803, 2.3202, 2.4634],
        [3.0803, 3.0803, 3.0803],
        [3.0803, 3.0749, 3.0801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.06346109509468079 
model_pd.l_d.mean(): -25.228347778320312 
model_pd.lagr.mean(): -25.164886474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1212], device='cuda:0')), ('power', tensor([-25.1071], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.06346109509468079
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.45627135038375854
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.15291480720043182
epoch£º1189	 i:3 	 global-step:23783	 l-p:-0.007984447292983532
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.14588569104671478
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12849587202072144
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.163309246301651
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.18950709700584412
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.1234152689576149
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.09237825870513916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0830, 3.0830, 3.0830],
        [3.0830, 3.0270, 3.0766],
        [3.0830, 3.0830, 3.0830],
        [3.0830, 3.0357, 3.0781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.17597778141498566 
model_pd.l_d.mean(): -24.797800064086914 
model_pd.lagr.mean(): -24.621822357177734 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0565], device='cuda:0')), ('power', tensor([-24.8543], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.17597778141498566
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.1382545381784439
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.18153288960456848
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.16053731739521027
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.10134537518024445
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.18490256369113922
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.13850562274456024
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.1351872682571411
epoch£º1190	 i:8 	 global-step:23808	 l-p:0.003507704706862569
epoch£º1190	 i:9 	 global-step:23809	 l-p:-0.20897309482097626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0672, 2.0854, 1.4027],
        [3.0672, 3.0671, 3.0673],
        [3.0672, 2.9372, 3.0407],
        [3.0672, 2.2995, 2.4393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.2664138674736023 
model_pd.l_d.mean(): -24.607112884521484 
model_pd.lagr.mean(): -24.3406982421875 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1604], device='cuda:0')), ('power', tensor([-24.7675], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.2664138674736023
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.09519309550523758
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.06489387899637222
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.16031606495380402
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.1835738569498062
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.0816640853881836
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.15271113812923431
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.40026792883872986
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.1445474922657013
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.16091148555278778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0886, 2.8495, 3.0123],
        [3.0886, 2.8076, 2.9872],
        [3.0886, 2.5683, 2.7821],
        [3.0886, 3.0739, 3.0879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.1331390142440796 
model_pd.l_d.mean(): -24.76275634765625 
model_pd.lagr.mean(): -24.62961769104004 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0650], device='cuda:0')), ('power', tensor([-24.8278], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.1331390142440796
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.1535848081111908
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.13734741508960724
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.21354399621486664
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.035819653421640396
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.16427846252918243
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.12136149406433105
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.18539990484714508
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.06645756214857101
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.19243696331977844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0878, 1.7811, 1.3232],
        [3.0878, 2.9201, 3.0467],
        [3.0878, 3.0739, 3.0871],
        [3.0878, 3.0214, 3.0793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.12517939507961273 
model_pd.l_d.mean(): -24.963090896606445 
model_pd.lagr.mean(): -24.83791160583496 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1129], device='cuda:0')), ('power', tensor([-25.0759], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.12517939507961273
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.09201060235500336
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.1663205474615097
epoch£º1193	 i:3 	 global-step:23863	 l-p:-0.2209138423204422
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.12771233916282654
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.15081532299518585
epoch£º1193	 i:6 	 global-step:23866	 l-p:-0.27609145641326904
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.15036606788635254
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.12673164904117584
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.19406497478485107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1182, 1.7665, 1.2102],
        [3.1182, 3.1150, 3.1181],
        [3.1182, 1.7945, 1.1800],
        [3.1182, 1.8445, 1.4342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.12030412256717682 
model_pd.l_d.mean(): -25.092344284057617 
model_pd.lagr.mean(): -24.9720401763916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0085], device='cuda:0')), ('power', tensor([-25.1008], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.12030412256717682
epoch£º1194	 i:1 	 global-step:23881	 l-p:1.6792638301849365
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.1317712962627411
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.14681467413902283
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.130592480301857
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.18572218716144562
epoch£º1194	 i:6 	 global-step:23886	 l-p:1.0566376447677612
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.1483779102563858
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.13321898877620697
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.37181898951530457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1188, 3.1039, 3.1181],
        [3.1188, 3.1177, 3.1188],
        [3.1188, 2.6937, 2.9060],
        [3.1188, 2.1183, 2.0709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.17254085838794708 
model_pd.l_d.mean(): -25.084203720092773 
model_pd.lagr.mean(): -24.911663055419922 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0057], device='cuda:0')), ('power', tensor([-25.0785], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.17254085838794708
epoch£º1195	 i:1 	 global-step:23901	 l-p:0.8850759863853455
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.09263100475072861
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.136623814702034
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12326238304376602
epoch£º1195	 i:5 	 global-step:23905	 l-p:0.23625029623508453
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.13520550727844238
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.17323754727840424
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.12558656930923462
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.148258239030838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1316, 2.8812, 3.0487],
        [3.1316, 3.0395, 3.1168],
        [3.1316, 2.0530, 1.3775],
        [3.1316, 1.9514, 1.2940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.3320501148700714 
model_pd.l_d.mean(): -24.953718185424805 
model_pd.lagr.mean(): -24.621667861938477 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0202], device='cuda:0')), ('power', tensor([-24.9739], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:0.3320501148700714
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.1317288875579834
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.14342929422855377
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.14837369322776794
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.16208516061306
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.16658075153827667
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.13798236846923828
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12453993409872055
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.1300068497657776
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.7403481602668762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1193, 2.0656, 1.9578],
        [3.1193, 2.8638, 3.0335],
        [3.1193, 2.1128, 1.4269],
        [3.1193, 1.7710, 1.1796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.14537286758422852 
model_pd.l_d.mean(): -24.98810577392578 
model_pd.lagr.mean(): -24.84273338317871 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1004], device='cuda:0')), ('power', tensor([-25.0885], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.14537286758422852
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.16765466332435608
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.8882637023925781
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.12973672151565552
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.13140136003494263
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.1287550926208496
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.14338789880275726
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.13258369266986847
epoch£º1197	 i:8 	 global-step:23948	 l-p:1.077335000038147
epoch£º1197	 i:9 	 global-step:23949	 l-p:-0.0890197604894638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1074, 1.8611, 1.2224],
        [3.1074, 3.1074, 3.1074],
        [3.1074, 3.0947, 3.1068],
        [3.1074, 3.0583, 3.1022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 1.664631962776184 
model_pd.l_d.mean(): -24.844423294067383 
model_pd.lagr.mean(): -23.179790496826172 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0271], device='cuda:0')), ('power', tensor([-24.8715], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:1.664631962776184
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.18406221270561218
epoch£º1198	 i:2 	 global-step:23962	 l-p:-0.21474787592887878
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.14186637103557587
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.12794974446296692
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.12250519543886185
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.125411719083786
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.10213293880224228
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.1611439436674118
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.20839248597621918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1113, 3.1070, 3.1112],
        [3.1113, 3.1112, 3.1113],
        [3.1113, 3.1087, 3.1112],
        [3.1113, 3.1113, 3.1113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.09865768998861313 
model_pd.l_d.mean(): -24.629146575927734 
model_pd.lagr.mean(): -24.530488967895508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1183], device='cuda:0')), ('power', tensor([-24.7475], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.09865768998861313
epoch£º1199	 i:1 	 global-step:23981	 l-p:-0.5319516062736511
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.15819936990737915
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14589324593544006
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.13216069340705872
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.14306196570396423
epoch£º1199	 i:6 	 global-step:23986	 l-p:-0.02006043866276741
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.1972701996564865
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13760705292224884
epoch£º1199	 i:9 	 global-step:23989	 l-p:-1.412097692489624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1053, 2.8618, 3.0265],
        [3.1053, 3.1051, 3.1053],
        [3.1053, 2.3139, 2.4391],
        [3.1053, 2.2908, 2.4020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.007631911896169186 
model_pd.l_d.mean(): -24.992551803588867 
model_pd.lagr.mean(): -24.984920501708984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0206], device='cuda:0')), ('power', tensor([-25.0131], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.007631911896169186
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.1160706952214241
epoch£º1200	 i:2 	 global-step:24002	 l-p:-5.532743453979492
epoch£º1200	 i:3 	 global-step:24003	 l-p:-0.2106342613697052
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.134487584233284
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.13056856393814087
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.15774157643318176
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.17169123888015747
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.09783166646957397
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.13836880028247833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1030, 3.1030, 3.1030],
        [3.1030, 3.0313, 3.0933],
        [3.1030, 1.7952, 1.3349],
        [3.1030, 1.8220, 1.4031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.14257930219173431 
model_pd.l_d.mean(): -24.594520568847656 
model_pd.lagr.mean(): -24.451940536499023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1393], device='cuda:0')), ('power', tensor([-24.7338], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.14257930219173431
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.19767682254314423
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.14246739447116852
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10618419200181961
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.013751811347901821
epoch£º1201	 i:5 	 global-step:24025	 l-p:-0.04601028189063072
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.1219201609492302
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.2722262442111969
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.1916293352842331
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.028176655992865562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0926, 2.8366, 3.0066],
        [3.0926, 2.0131, 1.8756],
        [3.0926, 3.0926, 3.0926],
        [3.0926, 2.0581, 1.3809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.09375348687171936 
model_pd.l_d.mean(): -24.903499603271484 
model_pd.lagr.mean(): -24.80974578857422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1243], device='cuda:0')), ('power', tensor([-25.0278], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.09375348687171936
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.1317220777273178
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14881718158721924
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.13802357017993927
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.23002803325653076
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.014223027043044567
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.12575645744800568
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.17492195963859558
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.13034898042678833
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.023740729317069054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0946, 3.0248, 3.0853],
        [3.0946, 3.0936, 3.0946],
        [3.0946, 1.7609, 1.1595],
        [3.0946, 1.7447, 1.1932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): -0.021281994879245758 
model_pd.l_d.mean(): -25.092159271240234 
model_pd.lagr.mean(): -25.113441467285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1019], device='cuda:0')), ('power', tensor([-25.1941], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:-0.021281994879245758
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.1776956468820572
epoch£º1203	 i:2 	 global-step:24062	 l-p:-0.007065858691930771
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.13148759305477142
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12264955043792725
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.14487960934638977
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.21015898883342743
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.13563403487205505
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.17721544206142426
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.0006472587701864541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1016, 3.0754, 3.0997],
        [3.1016, 3.0152, 3.0883],
        [3.1016, 2.0010, 1.8362],
        [3.1016, 2.5863, 2.8003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.00333652482368052 
model_pd.l_d.mean(): -25.195219039916992 
model_pd.lagr.mean(): -25.191883087158203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0212], device='cuda:0')), ('power', tensor([-25.1740], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.00333652482368052
epoch£º1204	 i:1 	 global-step:24081	 l-p:-0.08070411533117294
epoch£º1204	 i:2 	 global-step:24082	 l-p:-0.9923884868621826
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.21100535988807678
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12367910891771317
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.1679028421640396
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.19543327391147614
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.12094113975763321
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.09392374753952026
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.14873334765434265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1127, 1.7616, 1.2087],
        [3.1127, 2.9536, 3.0751],
        [3.1127, 1.9653, 1.3052],
        [3.1127, 1.7605, 1.1835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.19334548711776733 
model_pd.l_d.mean(): -25.12965965270996 
model_pd.lagr.mean(): -24.93631362915039 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0519], device='cuda:0')), ('power', tensor([-25.1816], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.19334548711776733
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.13419725000858307
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12713435292243958
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.1466377079486847
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.15746822953224182
epoch£º1205	 i:5 	 global-step:24105	 l-p:-0.1982221156358719
epoch£º1205	 i:6 	 global-step:24106	 l-p:-0.5370718836784363
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.13289785385131836
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.157221257686615
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.8492067456245422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1096, 3.1096, 3.1096],
        [3.1096, 1.8590, 1.4834],
        [3.1096, 2.9573, 3.0747],
        [3.1096, 2.0978, 2.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.14858482778072357 
model_pd.l_d.mean(): -25.169702529907227 
model_pd.lagr.mean(): -25.0211181640625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0548], device='cuda:0')), ('power', tensor([-25.1149], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.14858482778072357
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.12510643899440765
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.19161197543144226
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.10407008975744247
epoch£º1206	 i:4 	 global-step:24124	 l-p:1.7805263996124268
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.1482270210981369
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.1565989851951599
epoch£º1206	 i:7 	 global-step:24127	 l-p:-0.523773729801178
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.1950543224811554
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.14768807590007782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1081, 2.5204, 2.7247],
        [3.1081, 1.7699, 1.2539],
        [3.1081, 3.1045, 3.1080],
        [3.1081, 1.9066, 1.6000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.21434572339057922 
model_pd.l_d.mean(): -24.985759735107422 
model_pd.lagr.mean(): -24.771413803100586 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0855], device='cuda:0')), ('power', tensor([-25.0712], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.21434572339057922
epoch£º1207	 i:1 	 global-step:24141	 l-p:-0.12155458331108093
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.11724153161048889
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.1389915943145752
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.14606867730617523
epoch£º1207	 i:5 	 global-step:24145	 l-p:-0.38976794481277466
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.18941909074783325
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.9810298085212708
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.10896390676498413
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.1197916567325592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1100, 1.7748, 1.1702],
        [3.1100, 2.6951, 2.9066],
        [3.1100, 3.1100, 3.1100],
        [3.1100, 2.6311, 2.8462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.7553204298019409 
model_pd.l_d.mean(): -24.965185165405273 
model_pd.lagr.mean(): -24.20986557006836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1063], device='cuda:0')), ('power', tensor([-25.0715], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.7553204298019409
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.18983598053455353
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.11412942409515381
epoch£º1208	 i:3 	 global-step:24163	 l-p:-0.3013076186180115
epoch£º1208	 i:4 	 global-step:24164	 l-p:3.8191890716552734
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.14996157586574554
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.19878341257572174
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.13820065557956696
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.13218392431735992
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.10457103699445724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1219, 2.8324, 3.0149],
        [3.1219, 3.0858, 3.1188],
        [3.1219, 3.1219, 3.1219],
        [3.1219, 2.0865, 1.4051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.13189734518527985 
model_pd.l_d.mean(): -24.707637786865234 
model_pd.lagr.mean(): -24.575740814208984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0290], device='cuda:0')), ('power', tensor([-24.6787], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.13189734518527985
epoch£º1209	 i:1 	 global-step:24181	 l-p:0.8419018387794495
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.14346785843372345
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.15287186205387115
epoch£º1209	 i:4 	 global-step:24184	 l-p:0.2742499113082886
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.11042997986078262
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.09684475511312485
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.15231819450855255
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.1712150126695633
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.1701364815235138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1272, 3.1223, 3.1271],
        [3.1272, 2.0484, 1.3736],
        [3.1272, 3.1118, 3.1264],
        [3.1272, 2.9722, 3.0913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.2770400643348694 
model_pd.l_d.mean(): -24.910940170288086 
model_pd.lagr.mean(): -24.633899688720703 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0988], device='cuda:0')), ('power', tensor([-25.0098], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.2770400643348694
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.12510114908218384
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12189903855323792
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.196878582239151
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.25746989250183105
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.15143762528896332
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.16532279551029205
epoch£º1210	 i:7 	 global-step:24207	 l-p:0.2203003466129303
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.15998241305351257
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.11443153768777847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1363, 3.1363, 3.1363],
        [3.1363, 3.0948, 3.1324],
        [3.1363, 3.1363, 3.1363],
        [3.1363, 3.1363, 3.1363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.1491445004940033 
model_pd.l_d.mean(): -25.10118293762207 
model_pd.lagr.mean(): -24.952037811279297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0051], device='cuda:0')), ('power', tensor([-25.1063], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.1491445004940033
epoch£º1211	 i:1 	 global-step:24221	 l-p:0.30571427941322327
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.12863172590732574
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.11545222997665405
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.2687559127807617
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13652795553207397
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.12876132130622864
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.22129423916339874
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.10527290403842926
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.11900418996810913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1319, 3.1141, 3.1310],
        [3.1319, 3.1266, 3.1318],
        [3.1319, 2.5811, 2.7909],
        [3.1319, 3.1317, 3.1320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.11757762730121613 
model_pd.l_d.mean(): -24.601104736328125 
model_pd.lagr.mean(): -24.4835262298584 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1013], device='cuda:0')), ('power', tensor([-24.7024], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.11757762730121613
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.1442897766828537
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.14045213162899017
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.18146835267543793
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.30749741196632385
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.15702274441719055
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.1613287329673767
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.42861875891685486
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.08818371593952179
epoch£º1212	 i:9 	 global-step:24249	 l-p:0.2599804997444153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1240, 3.1236, 3.1240],
        [3.1240, 3.1238, 3.1240],
        [3.1240, 2.8664, 3.0370],
        [3.1240, 2.7184, 2.9286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.5869608521461487 
model_pd.l_d.mean(): -25.02873420715332 
model_pd.lagr.mean(): -24.4417724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0117], device='cuda:0')), ('power', tensor([-25.0404], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:0.5869608521461487
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.13335184752941132
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.16153407096862793
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.26838019490242004
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.1531897932291031
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.12094661593437195
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.18113891780376434
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.13290837407112122
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.12635891139507294
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.15948791801929474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1306, 2.6460, 2.8608],
        [3.1306, 1.7809, 1.1872],
        [3.1306, 2.9380, 3.0783],
        [3.1306, 2.8953, 3.0563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.1376996636390686 
model_pd.l_d.mean(): -25.0672664642334 
model_pd.lagr.mean(): -24.929567337036133 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0025], device='cuda:0')), ('power', tensor([-25.0698], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.1376996636390686
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.23694926500320435
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.1557246893644333
epoch£º1214	 i:3 	 global-step:24283	 l-p:0.2378063201904297
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.13438060879707336
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.12782494723796844
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.1942722648382187
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.13772813975811005
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11045601963996887
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.3307172358036041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1290, 1.7839, 1.1836],
        [3.1290, 1.7916, 1.1834],
        [3.1290, 2.2972, 2.3959],
        [3.1290, 3.1290, 3.1290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.13193665444850922 
model_pd.l_d.mean(): -25.144088745117188 
model_pd.lagr.mean(): -25.01215171813965 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0085], device='cuda:0')), ('power', tensor([-25.1356], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.13193665444850922
epoch£º1215	 i:1 	 global-step:24301	 l-p:0.43440839648246765
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.10654450207948685
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.13148757815361023
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.1612922102212906
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.15087230503559113
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.145819753408432
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.14807090163230896
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.27555033564567566
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.17245623469352722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1266, 1.7894, 1.1816],
        [3.1266, 1.8649, 1.2259],
        [3.1266, 2.8370, 3.0195],
        [3.1266, 3.1256, 3.1266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.1225292831659317 
model_pd.l_d.mean(): -24.675283432006836 
model_pd.lagr.mean(): -24.552753448486328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0102], device='cuda:0')), ('power', tensor([-24.6651], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.1225292831659317
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.1307421773672104
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.14851106703281403
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.12943796813488007
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.15878555178642273
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.3393044173717499
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12254457920789719
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.44045257568359375
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.13801424205303192
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.8959531188011169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1209, 3.1120, 3.1205],
        [3.1209, 1.9683, 1.7317],
        [3.1209, 1.7968, 1.1816],
        [3.1209, 2.5118, 2.7111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.13133008778095245 
model_pd.l_d.mean(): -24.68876075744629 
model_pd.lagr.mean(): -24.557430267333984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0898], device='cuda:0')), ('power', tensor([-24.7785], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.13133008778095245
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.1319749802350998
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.13543646037578583
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.15942515432834625
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.5662900805473328
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.1147349551320076
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.1463279277086258
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.1292383223772049
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.39678260684013367
epoch£º1217	 i:9 	 global-step:24349	 l-p:-1.9138054847717285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1163, 1.7723, 1.1747],
        [3.1163, 1.8922, 1.5528],
        [3.1163, 3.1162, 3.1163],
        [3.1163, 2.7550, 2.9576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.09732308983802795 
model_pd.l_d.mean(): -24.868465423583984 
model_pd.lagr.mean(): -24.771142959594727 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0526], device='cuda:0')), ('power', tensor([-24.9211], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.09732308983802795
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.15254844725131989
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.1501794457435608
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.13272778689861298
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.1372811198234558
epoch£º1218	 i:5 	 global-step:24365	 l-p:2.1152119636535645
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.14474162459373474
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.16202320158481598
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.1322183907032013
epoch£º1218	 i:9 	 global-step:24369	 l-p:-0.20287150144577026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1110, 1.8479, 1.4546],
        [3.1110, 2.8552, 3.0251],
        [3.1110, 2.9945, 3.0890],
        [3.1110, 3.1110, 3.1110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.12454497814178467 
model_pd.l_d.mean(): -24.64426040649414 
model_pd.lagr.mean(): -24.519716262817383 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0530], device='cuda:0')), ('power', tensor([-24.6973], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.12454497814178467
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13319192826747894
epoch£º1219	 i:2 	 global-step:24382	 l-p:1.1046550273895264
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.13391715288162231
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.1717456579208374
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.1442156583070755
epoch£º1219	 i:6 	 global-step:24386	 l-p:-0.11380979418754578
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.12023640424013138
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.15110968053340912
epoch£º1219	 i:9 	 global-step:24389	 l-p:-0.34731853008270264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1085, 3.1082, 3.1085],
        [3.1085, 2.0300, 1.3581],
        [3.1085, 1.8204, 1.1929],
        [3.1085, 1.7737, 1.1690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.12513341009616852 
model_pd.l_d.mean(): -25.176753997802734 
model_pd.lagr.mean(): -25.051620483398438 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0281], device='cuda:0')), ('power', tensor([-25.1487], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.12513341009616852
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13431619107723236
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.19179408252239227
epoch£º1220	 i:3 	 global-step:24403	 l-p:-0.1394365429878235
epoch£º1220	 i:4 	 global-step:24404	 l-p:-0.563055694103241
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12039410322904587
epoch£º1220	 i:6 	 global-step:24406	 l-p:0.21483169496059418
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.12444806843996048
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.1290227770805359
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.4387017786502838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1140, 3.1108, 3.1139],
        [3.1140, 3.1138, 3.1140],
        [3.1140, 1.8669, 1.2269],
        [3.1140, 1.8598, 1.4789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): -0.4783881902694702 
model_pd.l_d.mean(): -25.06731414794922 
model_pd.lagr.mean(): -25.54570198059082 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0658], device='cuda:0')), ('power', tensor([-25.1332], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:-0.4783881902694702
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.38825076818466187
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.13242192566394806
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.14810694754123688
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.11471986770629883
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.13828973472118378
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.1480196714401245
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.7185810804367065
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.16077342629432678
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.1171668991446495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1178, 3.1178, 3.1178],
        [3.1178, 2.9503, 3.0767],
        [3.1178, 3.0687, 3.1126],
        [3.1178, 3.1163, 3.1178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.1097867488861084 
model_pd.l_d.mean(): -25.164094924926758 
model_pd.lagr.mean(): -25.05430793762207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0701], device='cuda:0')), ('power', tensor([-25.0940], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.1097867488861084
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.1348784863948822
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.13641245663166046
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.16188466548919678
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.31322500109672546
epoch£º1222	 i:5 	 global-step:24445	 l-p:2.2128796577453613
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.12457721680402756
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.13905538618564606
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.5351307392120361
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.12996837496757507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1197, 2.9689, 3.0854],
        [3.1197, 3.0796, 3.1160],
        [3.1197, 2.8763, 3.0409],
        [3.1197, 2.3719, 2.5203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11749328672885895 
model_pd.l_d.mean(): -24.731672286987305 
model_pd.lagr.mean(): -24.614179611206055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0665], device='cuda:0')), ('power', tensor([-24.7981], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11749328672885895
epoch£º1223	 i:1 	 global-step:24461	 l-p:0.19090530276298523
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.1354249119758606
epoch£º1223	 i:3 	 global-step:24463	 l-p:2.0627455711364746
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.7656964659690857
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.18653541803359985
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.11969918757677078
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.08854690194129944
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.1710672378540039
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.12913353741168976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1227, 2.8765, 3.0423],
        [3.1227, 3.1227, 3.1227],
        [3.1227, 2.1503, 1.4582],
        [3.1227, 1.8468, 1.4335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.13474255800247192 
model_pd.l_d.mean(): -24.57568359375 
model_pd.lagr.mean(): -24.440940856933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0558], device='cuda:0')), ('power', tensor([-24.6315], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.13474255800247192
epoch£º1224	 i:1 	 global-step:24481	 l-p:0.4226306676864624
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.12968751788139343
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.15447691082954407
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.1311631053686142
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.2594338655471802
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.138203427195549
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.3506770133972168
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.14588062465190887
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.1312851905822754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1284, 3.0620, 3.1198],
        [3.1284, 3.1278, 3.1284],
        [3.1284, 3.0745, 3.1223],
        [3.1284, 3.1284, 3.1284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.16620507836341858 
model_pd.l_d.mean(): -25.00937271118164 
model_pd.lagr.mean(): -24.843168258666992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0571], device='cuda:0')), ('power', tensor([-24.9523], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:0.16620507836341858
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.2867407500743866
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.1368628442287445
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12522627413272858
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.19190306961536407
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.12097235769033432
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.13907207548618317
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.16453219950199127
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.3436199426651001
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.14159734547138214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1295, 2.7150, 2.9262],
        [3.1295, 1.7829, 1.2469],
        [3.1295, 3.1295, 3.1295],
        [3.1295, 3.1294, 3.1295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.15113568305969238 
model_pd.l_d.mean(): -24.566123962402344 
model_pd.lagr.mean(): -24.414987564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1344], device='cuda:0')), ('power', tensor([-24.7006], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.15113568305969238
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.15559424459934235
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.0939793661236763
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.14805802702903748
epoch£º1226	 i:4 	 global-step:24524	 l-p:0.1759748011827469
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.13530486822128296
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.4263038635253906
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.1468050479888916
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.14080247282981873
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.2137255221605301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1320, 3.1162, 3.1312],
        [3.1320, 3.1287, 3.1320],
        [3.1320, 2.8550, 3.0330],
        [3.1320, 3.1320, 3.1320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.1438920795917511 
model_pd.l_d.mean(): -24.933618545532227 
model_pd.lagr.mean(): -24.78972625732422 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0459], device='cuda:0')), ('power', tensor([-24.9795], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.1438920795917511
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.15199902653694153
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.09582357108592987
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.13588617742061615
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.39057180285453796
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.15998825430870056
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.22468295693397522
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.13223643600940704
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.1309540569782257
epoch£º1227	 i:9 	 global-step:24549	 l-p:0.1801968514919281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1343, 1.8051, 1.1893],
        [3.1343, 3.1024, 3.1318],
        [3.1343, 3.1254, 3.1340],
        [3.1343, 1.7962, 1.1869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.2968350350856781 
model_pd.l_d.mean(): -25.038835525512695 
model_pd.lagr.mean(): -24.742000579833984 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0296], device='cuda:0')), ('power', tensor([-25.0092], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.2968350350856781
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.15813903510570526
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.2173246145248413
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12217149138450623
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.1201956495642662
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.15410540997982025
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.23703250288963318
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.14986535906791687
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.08065391331911087
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.12194890528917313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1399, 2.9850, 3.1040],
        [3.1399, 3.1174, 3.1385],
        [3.1399, 1.9168, 1.5763],
        [3.1399, 1.9876, 1.7505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.1417294591665268 
model_pd.l_d.mean(): -24.658321380615234 
model_pd.lagr.mean(): -24.516592025756836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0102], device='cuda:0')), ('power', tensor([-24.6685], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.1417294591665268
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.24192357063293457
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12474067509174347
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.13541536033153534
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.13626466691493988
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11014938354492188
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.12739691138267517
epoch£º1229	 i:7 	 global-step:24587	 l-p:0.3582339882850647
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13673639297485352
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.19740740954875946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1284, 2.9631, 3.0883],
        [3.1284, 3.1032, 3.1267],
        [3.1284, 3.1268, 3.1284],
        [3.1284, 1.8161, 1.1930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.1667952835559845 
model_pd.l_d.mean(): -25.23408317565918 
model_pd.lagr.mean(): -25.06728744506836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0803], device='cuda:0')), ('power', tensor([-25.1538], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.1667952835559845
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.1423635482788086
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.3297309875488281
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12576177716255188
epoch£º1230	 i:4 	 global-step:24604	 l-p:0.17288026213645935
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.6112164258956909
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13257457315921783
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.17344476282596588
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.26498374342918396
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.13865706324577332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1208, 1.7673, 1.2046],
        [3.1208, 3.1206, 3.1208],
        [3.1208, 3.1208, 3.1208],
        [3.1208, 1.8069, 1.1864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.14925377070903778 
model_pd.l_d.mean(): -25.126176834106445 
model_pd.lagr.mean(): -24.9769229888916 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0261], device='cuda:0')), ('power', tensor([-25.1000], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.14925377070903778
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13046088814735413
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.1452503502368927
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.13458089530467987
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.14846497774124146
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.14709855616092682
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12242535501718521
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.17093108594417572
epoch£º1231	 i:8 	 global-step:24628	 l-p:-0.5784944891929626
epoch£º1231	 i:9 	 global-step:24629	 l-p:0.6198586821556091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1096, 1.7601, 1.2148],
        [3.1096, 3.0623, 3.1048],
        [3.1096, 1.7765, 1.2714],
        [3.1096, 2.1228, 2.0911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.15120548009872437 
model_pd.l_d.mean(): -25.20464324951172 
model_pd.lagr.mean(): -25.053438186645508 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0245], device='cuda:0')), ('power', tensor([-25.1802], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.15120548009872437
epoch£º1232	 i:1 	 global-step:24641	 l-p:1.025382161140442
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.13322578370571136
epoch£º1232	 i:3 	 global-step:24643	 l-p:-0.311048299074173
epoch£º1232	 i:4 	 global-step:24644	 l-p:0.21069161593914032
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13910776376724243
epoch£º1232	 i:6 	 global-step:24646	 l-p:-0.11905170977115631
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.13469839096069336
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.13612693548202515
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.10124003887176514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1090, 2.0547, 1.9475],
        [3.1090, 1.8047, 1.1829],
        [3.1090, 3.1090, 3.1090],
        [3.1090, 3.0829, 3.1072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 1.0212762355804443 
model_pd.l_d.mean(): -25.076129913330078 
model_pd.lagr.mean(): -24.054853439331055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0486], device='cuda:0')), ('power', tensor([-25.0275], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:1.0212762355804443
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.11401364952325821
epoch£º1233	 i:2 	 global-step:24662	 l-p:-0.09434721618890762
epoch£º1233	 i:3 	 global-step:24663	 l-p:-0.41346293687820435
epoch£º1233	 i:4 	 global-step:24664	 l-p:0.21603797376155853
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.15077175199985504
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.1618751883506775
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11565049737691879
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.14212296903133392
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.15919816493988037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1098, 3.1051, 3.1097],
        [3.1098, 3.1097, 3.1098],
        [3.1098, 3.1098, 3.1098],
        [3.1098, 2.1376, 2.1207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.12329255044460297 
model_pd.l_d.mean(): -25.102825164794922 
model_pd.lagr.mean(): -24.97953224182129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1180], device='cuda:0')), ('power', tensor([-24.9848], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.12329255044460297
epoch£º1234	 i:1 	 global-step:24681	 l-p:-0.33878207206726074
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.12009800970554352
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.15184256434440613
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.09346605837345123
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.15636783838272095
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.16029295325279236
epoch£º1234	 i:7 	 global-step:24687	 l-p:-0.03976442292332649
epoch£º1234	 i:8 	 global-step:24688	 l-p:-0.39729025959968567
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.18135638535022736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1036, 2.9743, 3.0773],
        [3.1036, 3.0947, 3.1033],
        [3.1036, 2.9997, 3.0855],
        [3.1036, 3.0795, 3.1020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.13008706271648407 
model_pd.l_d.mean(): -25.098407745361328 
model_pd.lagr.mean(): -24.968320846557617 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0916], device='cuda:0')), ('power', tensor([-25.0068], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.13008706271648407
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.13618896901607513
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.16292162239551544
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.1412542313337326
epoch£º1235	 i:4 	 global-step:24704	 l-p:0.20658165216445923
epoch£º1235	 i:5 	 global-step:24705	 l-p:-0.03366665169596672
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.032547492533922195
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.15643729269504547
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.12215011566877365
epoch£º1235	 i:9 	 global-step:24709	 l-p:-0.23822520673274994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1021, 2.1148, 2.0831],
        [3.1021, 2.5520, 2.7627],
        [3.1021, 2.5741, 2.7873],
        [3.1021, 3.1020, 3.1021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.07242326438426971 
model_pd.l_d.mean(): -25.039447784423828 
model_pd.lagr.mean(): -24.967023849487305 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0705], device='cuda:0')), ('power', tensor([-25.1099], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.07242326438426971
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.1354970932006836
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.1524527370929718
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.12307815998792648
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.13141527771949768
epoch£º1236	 i:5 	 global-step:24725	 l-p:-0.05307262763381004
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.1312730610370636
epoch£º1236	 i:7 	 global-step:24727	 l-p:-0.2624029815196991
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.16256149113178253
epoch£º1236	 i:9 	 global-step:24729	 l-p:0.19344869256019592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1000, 3.1000, 3.1000],
        [3.1000, 3.1000, 3.1000],
        [3.1000, 3.0986, 3.1000],
        [3.1000, 2.9646, 3.0715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): -0.19585758447647095 
model_pd.l_d.mean(): -24.886070251464844 
model_pd.lagr.mean(): -25.081928253173828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0189], device='cuda:0')), ('power', tensor([-24.8672], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:-0.19585758447647095
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.16065947711467743
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.1368272304534912
epoch£º1237	 i:3 	 global-step:24743	 l-p:-0.012208156287670135
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.13790547847747803
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.1357983648777008
epoch£º1237	 i:6 	 global-step:24746	 l-p:-0.01039948407560587
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.15825960040092468
epoch£º1237	 i:8 	 global-step:24748	 l-p:0.20632770657539368
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.22993938624858856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0993, 1.8735, 1.5344],
        [3.0993, 2.1266, 2.1101],
        [3.0993, 2.9062, 3.0468],
        [3.0993, 2.9633, 3.0706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.13154804706573486 
model_pd.l_d.mean(): -25.292888641357422 
model_pd.lagr.mean(): -25.161340713500977 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1412], device='cuda:0')), ('power', tensor([-25.1516], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.13154804706573486
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.15941554307937622
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.14893579483032227
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.10089058429002762
epoch£º1238	 i:4 	 global-step:24764	 l-p:0.04842626303434372
epoch£º1238	 i:5 	 global-step:24765	 l-p:-0.015504970215260983
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.16673748195171356
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.11928422749042511
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.18289600312709808
epoch£º1238	 i:9 	 global-step:24769	 l-p:-7.91364049911499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1076, 3.1063, 3.1076],
        [3.1076, 3.0851, 3.1062],
        [3.1076, 1.9557, 1.2973],
        [3.1076, 3.0693, 3.1042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.17972682416439056 
model_pd.l_d.mean(): -25.248611450195312 
model_pd.lagr.mean(): -25.068883895874023 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0898], device='cuda:0')), ('power', tensor([-25.1588], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.17972682416439056
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13955169916152954
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.19894424080848694
epoch£º1239	 i:3 	 global-step:24783	 l-p:-1.022055745124817
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.1019657701253891
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.16943299770355225
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.412476122379303
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.11402856558561325
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.16137471795082092
epoch£º1239	 i:9 	 global-step:24789	 l-p:2.485653877258301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1204, 2.1657, 1.4709],
        [3.1204, 2.0907, 1.4085],
        [3.1204, 2.7447, 2.9501],
        [3.1204, 1.7697, 1.1815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.12071716040372849 
model_pd.l_d.mean(): -24.97027015686035 
model_pd.lagr.mean(): -24.849552154541016 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0683], device='cuda:0')), ('power', tensor([-25.0386], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.12071716040372849
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.4043485224246979
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.19692973792552948
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.42385923862457275
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.12445804476737976
epoch£º1240	 i:5 	 global-step:24805	 l-p:0.19655779004096985
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.129991814494133
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.16436877846717834
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.15454070270061493
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.1334601193666458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1369, 2.7386, 2.9476],
        [3.1369, 2.0584, 1.9203],
        [3.1369, 2.4470, 2.6205],
        [3.1369, 2.9328, 3.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.18877848982810974 
model_pd.l_d.mean(): -24.370397567749023 
model_pd.lagr.mean(): -24.18161964416504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1492], device='cuda:0')), ('power', tensor([-24.5196], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:0.18877848982810974
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.15271863341331482
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.24092376232147217
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.1269698441028595
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.14587105810642242
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.14142857491970062
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12106355279684067
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.18697601556777954
epoch£º1241	 i:8 	 global-step:24828	 l-p:0.16737480461597443
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.14827829599380493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1404, 1.8332, 1.2051],
        [3.1404, 1.9061, 1.2577],
        [3.1404, 3.1405, 3.1404],
        [3.1404, 3.1281, 3.1399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.1409752368927002 
model_pd.l_d.mean(): -24.963964462280273 
model_pd.lagr.mean(): -24.822988510131836 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0793], device='cuda:0')), ('power', tensor([-25.0433], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.1409752368927002
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.10783924162387848
epoch£º1242	 i:2 	 global-step:24842	 l-p:0.22598443925380707
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.18413369357585907
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.12084260582923889
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.13482965528964996
epoch£º1242	 i:6 	 global-step:24846	 l-p:0.21084244549274445
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.13387912511825562
epoch£º1242	 i:8 	 global-step:24848	 l-p:0.20714107155799866
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.13617099821567535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1412, 3.1402, 3.1412],
        [3.1412, 2.8952, 3.0609],
        [3.1412, 2.7357, 2.9457],
        [3.1412, 2.0215, 1.8297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.1815544068813324 
model_pd.l_d.mean(): -24.96884536743164 
model_pd.lagr.mean(): -24.787290573120117 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0094], device='cuda:0')), ('power', tensor([-24.9595], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:0.1815544068813324
epoch£º1243	 i:1 	 global-step:24861	 l-p:0.2276560217142105
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.16096170246601105
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.09067797660827637
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.13139957189559937
epoch£º1243	 i:5 	 global-step:24865	 l-p:0.19221146404743195
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.13178372383117676
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.1668468415737152
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.15931759774684906
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.13536763191223145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1482, 2.9339, 3.0851],
        [3.1482, 2.6701, 2.8847],
        [3.1482, 2.3605, 2.4864],
        [3.1482, 3.1482, 3.1482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.14322760701179504 
model_pd.l_d.mean(): -24.97393226623535 
model_pd.lagr.mean(): -24.830703735351562 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0740], device='cuda:0')), ('power', tensor([-24.8999], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.14322760701179504
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.13114766776561737
epoch£º1244	 i:2 	 global-step:24882	 l-p:0.19922378659248352
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.13174131512641907
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.11944440752267838
epoch£º1244	 i:5 	 global-step:24885	 l-p:0.17298012971878052
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.13335515558719635
epoch£º1244	 i:7 	 global-step:24887	 l-p:0.20548933744430542
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.1661691963672638
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13344737887382507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[3.1482, 1.8366, 1.2083],
        [3.1482, 1.8216, 1.2005],
        [3.1482, 1.9507, 1.2933],
        [3.1482, 1.7931, 1.2309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.2846239507198334 
model_pd.l_d.mean(): -25.101764678955078 
model_pd.lagr.mean(): -24.817140579223633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0060], device='cuda:0')), ('power', tensor([-25.1078], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:0.2846239507198334
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.13910968601703644
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.0780690535902977
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.1546032577753067
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.14273449778556824
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.14161349833011627
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.11828920245170593
epoch£º1245	 i:7 	 global-step:24907	 l-p:0.20558319985866547
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.1407565027475357
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12238538265228271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1479, 1.7929, 1.2308],
        [3.1479, 3.1479, 3.1479],
        [3.1479, 3.0182, 3.1214],
        [3.1479, 3.1479, 3.1479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.14200523495674133 
model_pd.l_d.mean(): -24.99088478088379 
model_pd.lagr.mean(): -24.848878860473633 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1265], device='cuda:0')), ('power', tensor([-25.1174], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.14200523495674133
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.1079242080450058
epoch£º1246	 i:2 	 global-step:24922	 l-p:0.210067480802536
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.13381074368953705
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.122255340218544
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.1608719527721405
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11453615874052048
epoch£º1246	 i:7 	 global-step:24927	 l-p:0.2264707386493683
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.217932790517807
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.13772884011268616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1418, 1.9650, 1.6921],
        [3.1418, 3.1419, 3.1418],
        [3.1418, 2.5836, 2.7923],
        [3.1418, 1.9881, 1.3237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.2260112315416336 
model_pd.l_d.mean(): -24.866065979003906 
model_pd.lagr.mean(): -24.64005470275879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0009], device='cuda:0')), ('power', tensor([-24.8652], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:0.2260112315416336
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.09114737063646317
epoch£º1247	 i:2 	 global-step:24942	 l-p:0.1809777468442917
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.1557205617427826
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.18189406394958496
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.12062112241983414
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.13062752783298492
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.13231821358203888
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.1361488699913025
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.22502630949020386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1432, 2.8663, 3.0442],
        [3.1432, 3.1432, 3.1432],
        [3.1432, 2.0936, 1.9904],
        [3.1432, 1.9376, 1.6223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.13570444285869598 
model_pd.l_d.mean(): -25.283035278320312 
model_pd.lagr.mean(): -25.14733123779297 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1007], device='cuda:0')), ('power', tensor([-25.1823], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.13570444285869598
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.14026153087615967
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12517786026000977
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.1711570769548416
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.1024136170744896
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.2618281841278076
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.1295912265777588
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.14066553115844727
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.12403278052806854
epoch£º1248	 i:9 	 global-step:24969	 l-p:0.32941901683807373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1349, 1.8600, 1.4473],
        [3.1349, 3.1350, 3.1349],
        [3.1349, 1.7944, 1.2733],
        [3.1349, 3.1350, 3.1350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.10702662914991379 
model_pd.l_d.mean(): -24.978271484375 
model_pd.lagr.mean(): -24.871244430541992 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0251], device='cuda:0')), ('power', tensor([-25.0034], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.10702662914991379
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.14020664989948273
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.17313790321350098
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.12416142225265503
epoch£º1249	 i:4 	 global-step:24984	 l-p:0.2151421308517456
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.1420982927083969
epoch£º1249	 i:6 	 global-step:24986	 l-p:0.35170698165893555
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.15575292706489563
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.16541950404644012
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.13981695473194122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[3.1329, 2.4426, 2.6163],
        [3.1329, 2.3422, 2.4671],
        [3.1329, 2.6435, 2.8582],
        [3.1329, 2.6674, 2.8822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.13284648954868317 
model_pd.l_d.mean(): -25.044530868530273 
model_pd.lagr.mean(): -24.911684036254883 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0178], device='cuda:0')), ('power', tensor([-25.0268], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.13284648954868317
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.13303786516189575
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.13573040068149567
epoch£º1250	 i:3 	 global-step:25003	 l-p:0.2764156460762024
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.1626519411802292
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.1543819010257721
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.1751081943511963
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.10052473098039627
epoch£º1250	 i:8 	 global-step:25008	 l-p:0.23412658274173737
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.36245062947273254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1290, 2.0784, 1.9748],
        [3.1290, 3.1265, 3.1290],
        [3.1290, 3.1290, 3.1290],
        [3.1290, 2.4386, 2.6123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.14192035794258118 
model_pd.l_d.mean(): -24.90561866760254 
model_pd.lagr.mean(): -24.76369857788086 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0358], device='cuda:0')), ('power', tensor([-24.9415], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.14192035794258118
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.1325683891773224
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.1379116028547287
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.32661864161491394
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.14816632866859436
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.10734136402606964
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.15008804202079773
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.1502028852701187
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.1681853085756302
epoch£º1251	 i:9 	 global-step:25029	 l-p:0.41065728664398193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1270, 2.6614, 2.8762],
        [3.1270, 1.7895, 1.1814],
        [3.1270, 3.1269, 3.1270],
        [3.1270, 1.7830, 1.2548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.13301286101341248 
model_pd.l_d.mean(): -24.95523452758789 
model_pd.lagr.mean(): -24.822221755981445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0861], device='cuda:0')), ('power', tensor([-25.0413], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.13301286101341248
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.18571841716766357
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.137006938457489
epoch£º1252	 i:3 	 global-step:25043	 l-p:0.2958233654499054
epoch£º1252	 i:4 	 global-step:25044	 l-p:0.2336055338382721
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.13863582909107208
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.15264451503753662
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.15124139189720154
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.2867754399776459
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.12114845216274261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1335, 3.1335, 3.1335],
        [3.1335, 3.1318, 3.1335],
        [3.1335, 3.1335, 3.1335],
        [3.1335, 3.1335, 3.1335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.12457303702831268 
model_pd.l_d.mean(): -24.92070960998535 
model_pd.lagr.mean(): -24.7961368560791 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0785], device='cuda:0')), ('power', tensor([-24.8422], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.12457303702831268
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.17670659720897675
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.12837731838226318
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.14114125072956085
epoch£º1253	 i:4 	 global-step:25064	 l-p:0.25923895835876465
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.10109676420688629
epoch£º1253	 i:6 	 global-step:25066	 l-p:0.23756980895996094
epoch£º1253	 i:7 	 global-step:25067	 l-p:0.22897590696811676
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.11040889471769333
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.18949829041957855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1389, 2.0898, 1.4079],
        [3.1389, 2.5804, 2.7893],
        [3.1389, 3.1300, 3.1386],
        [3.1389, 1.9372, 1.6283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.10938119888305664 
model_pd.l_d.mean(): -24.610376358032227 
model_pd.lagr.mean(): -24.500995635986328 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0675], device='cuda:0')), ('power', tensor([-24.6779], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.10938119888305664
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.1836724877357483
epoch£º1254	 i:2 	 global-step:25082	 l-p:0.19138556718826294
epoch£º1254	 i:3 	 global-step:25083	 l-p:0.3014482855796814
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.08008946478366852
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.12092071026563644
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.15878185629844666
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.16172555088996887
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.15758241713047028
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.127508282661438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1468, 2.5601, 2.7638],
        [3.1468, 1.9943, 1.7569],
        [3.1468, 2.7325, 2.9436],
        [3.1468, 3.1469, 3.1469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.14111551642417908 
model_pd.l_d.mean(): -24.574569702148438 
model_pd.lagr.mean(): -24.433454513549805 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0967], device='cuda:0')), ('power', tensor([-24.6713], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.14111551642417908
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.12689624726772308
epoch£º1255	 i:2 	 global-step:25102	 l-p:0.1982806771993637
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.12605686485767365
epoch£º1255	 i:4 	 global-step:25104	 l-p:0.18181242048740387
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.1434287577867508
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.14796672761440277
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.15966090559959412
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.13081620633602142
epoch£º1255	 i:9 	 global-step:25109	 l-p:0.19706422090530396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1447, 2.8321, 3.0220],
        [3.1447, 2.1337, 2.0749],
        [3.1447, 2.1087, 1.4237],
        [3.1447, 3.1198, 3.1430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.13251927495002747 
model_pd.l_d.mean(): -25.079374313354492 
model_pd.lagr.mean(): -24.946855545043945 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0455], device='cuda:0')), ('power', tensor([-25.0339], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.13251927495002747
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.17824894189834595
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.141473188996315
epoch£º1256	 i:3 	 global-step:25123	 l-p:0.1987071931362152
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.13554172217845917
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.1413833349943161
epoch£º1256	 i:6 	 global-step:25126	 l-p:0.1532360464334488
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.13052991032600403
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.13619987666606903
epoch£º1256	 i:9 	 global-step:25129	 l-p:0.2680355906486511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1368, 2.0168, 1.8251],
        [3.1368, 1.8310, 1.2030],
        [3.1368, 3.1357, 3.1368],
        [3.1368, 3.1368, 3.1368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.2389611154794693 
model_pd.l_d.mean(): -25.043182373046875 
model_pd.lagr.mean(): -24.804222106933594 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0154], device='cuda:0')), ('power', tensor([-25.0586], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:0.2389611154794693
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.12474822998046875
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.15957330167293549
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.1459200233221054
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.14672669768333435
epoch£º1257	 i:5 	 global-step:25145	 l-p:0.21662698686122894
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.1184767335653305
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.1581745743751526
epoch£º1257	 i:8 	 global-step:25148	 l-p:0.21017390489578247
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.13251225650310516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1388, 3.1376, 3.1387],
        [3.1388, 3.1388, 3.1388],
        [3.1388, 2.2242, 2.2594],
        [3.1388, 1.8076, 1.3052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.1355239748954773 
model_pd.l_d.mean(): -25.224641799926758 
model_pd.lagr.mean(): -25.0891170501709 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0445], device='cuda:0')), ('power', tensor([-25.1801], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.1355239748954773
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.16860900819301605
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.1447944939136505
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.28427380323410034
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.11903562396764755
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.1603933721780777
epoch£º1258	 i:6 	 global-step:25166	 l-p:0.2887217700481415
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.09745611250400543
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.12231845408678055
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.12347192317247391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1366, 3.1239, 3.1360],
        [3.1366, 2.1244, 2.0647],
        [3.1366, 3.1245, 3.1360],
        [3.1366, 1.7985, 1.1881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.16569922864437103 
model_pd.l_d.mean(): -24.92656707763672 
model_pd.lagr.mean(): -24.760868072509766 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0935], device='cuda:0')), ('power', tensor([-25.0201], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.16569922864437103
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.14152264595031738
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.11362811923027039
epoch£º1259	 i:3 	 global-step:25183	 l-p:0.2593379616737366
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.1803790181875229
epoch£º1259	 i:5 	 global-step:25185	 l-p:0.20923103392124176
epoch£º1259	 i:6 	 global-step:25186	 l-p:0.22268874943256378
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.13125672936439514
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.13906866312026978
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.1290951818227768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1358, 3.1233, 3.1353],
        [3.1358, 2.1949, 2.2067],
        [3.1358, 3.1358, 3.1358],
        [3.1358, 2.8742, 3.0463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.2664755880832672 
model_pd.l_d.mean(): -25.0301513671875 
model_pd.lagr.mean(): -24.763675689697266 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0212], device='cuda:0')), ('power', tensor([-25.0090], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:0.2664755880832672
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.1344674527645111
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.14594827592372894
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.1175403743982315
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.1337835192680359
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.1253511905670166
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.13593719899654388
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.1484670341014862
epoch£º1260	 i:8 	 global-step:25208	 l-p:0.3042747676372528
epoch£º1260	 i:9 	 global-step:25209	 l-p:0.3108561933040619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1248, 3.1248, 3.1248],
        [3.1248, 3.1244, 3.1248],
        [3.1248, 3.0757, 3.1196],
        [3.1248, 3.1109, 3.1242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.26434507966041565 
model_pd.l_d.mean(): -25.217689514160156 
model_pd.lagr.mean(): -24.953344345092773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0424], device='cuda:0')), ('power', tensor([-25.1752], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:0.26434507966041565
epoch£º1261	 i:1 	 global-step:25221	 l-p:0.35123053193092346
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.15938854217529297
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.09548898786306381
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.16041432321071625
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.14076828956604004
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.17773574590682983
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.11482516676187515
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.13260991871356964
epoch£º1261	 i:9 	 global-step:25229	 l-p:0.2861082851886749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1325, 1.9090, 1.2598],
        [3.1325, 3.0608, 3.1227],
        [3.1325, 1.9078, 1.2588],
        [3.1325, 1.8255, 1.1990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.1500045210123062 
model_pd.l_d.mean(): -24.76426124572754 
model_pd.lagr.mean(): -24.614255905151367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0333], device='cuda:0')), ('power', tensor([-24.7975], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.1500045210123062
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.14525148272514343
epoch£º1262	 i:2 	 global-step:25242	 l-p:0.26246553659439087
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.12983869016170502
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.09282907098531723
epoch£º1262	 i:5 	 global-step:25245	 l-p:0.26306793093681335
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.17512480914592743
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.1699412763118744
epoch£º1262	 i:8 	 global-step:25248	 l-p:0.1891348958015442
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.14219653606414795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1396, 1.9986, 1.3323],
        [3.1396, 2.9956, 3.1079],
        [3.1396, 2.8447, 3.0290],
        [3.1396, 2.6201, 2.8333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.3338044285774231 
model_pd.l_d.mean(): -24.480792999267578 
model_pd.lagr.mean(): -24.146987915039062 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0488], device='cuda:0')), ('power', tensor([-24.5296], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:0.3338044285774231
epoch£º1263	 i:1 	 global-step:25261	 l-p:0.20001155138015747
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.12911301851272583
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.08626113086938858
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.1468007117509842
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.15888290107250214
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.1389569491147995
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.09851150959730148
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.16841652989387512
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.14401862025260925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1457, 1.8631, 1.4380],
        [3.1457, 2.4143, 2.5699],
        [3.1457, 3.1457, 3.1457],
        [3.1457, 3.1457, 3.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.15294037759304047 
model_pd.l_d.mean(): -24.598766326904297 
model_pd.lagr.mean(): -24.445825576782227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0675], device='cuda:0')), ('power', tensor([-24.6662], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.15294037759304047
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.18099017441272736
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.12733179330825806
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.13534259796142578
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12365420907735825
epoch£º1264	 i:5 	 global-step:25285	 l-p:0.16666148602962494
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.14636896550655365
epoch£º1264	 i:7 	 global-step:25287	 l-p:0.2543472945690155
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.1288238912820816
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.15310658514499664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1413, 3.1264, 3.1406],
        [3.1413, 2.4402, 2.6094],
        [3.1413, 2.9813, 3.1034],
        [3.1413, 3.1414, 3.1414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10178080946207047 
model_pd.l_d.mean(): -24.65351676940918 
model_pd.lagr.mean(): -24.55173683166504 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0670], device='cuda:0')), ('power', tensor([-24.7205], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10178080946207047
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.13160957396030426
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.13823112845420837
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.13592326641082764
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.1365545094013214
epoch£º1265	 i:5 	 global-step:25305	 l-p:0.2488405555486679
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.31737327575683594
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.16288572549819946
epoch£º1265	 i:8 	 global-step:25308	 l-p:0.25018274784088135
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.15752284228801727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[3.1293, 2.3633, 2.5023],
        [3.1293, 2.6397, 2.8546],
        [3.1293, 2.4306, 2.6013],
        [3.1293, 2.2266, 2.2724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.1442776918411255 
model_pd.l_d.mean(): -24.933570861816406 
model_pd.lagr.mean(): -24.78929328918457 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0455], device='cuda:0')), ('power', tensor([-24.9790], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.1442776918411255
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.13297753036022186
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.13324327766895294
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.10602322220802307
epoch£º1266	 i:4 	 global-step:25324	 l-p:0.28462690114974976
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.4080210030078888
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.14511869847774506
epoch£º1266	 i:7 	 global-step:25327	 l-p:0.2666813135147095
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.12754172086715698
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.18421195447444916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1310, 2.1453, 2.1142],
        [3.1310, 3.1294, 3.1310],
        [3.1310, 3.1126, 3.1300],
        [3.1310, 3.1310, 3.1310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.10526169836521149 
model_pd.l_d.mean(): -25.231502532958984 
model_pd.lagr.mean(): -25.12624168395996 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0246], device='cuda:0')), ('power', tensor([-25.2069], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.10526169836521149
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.1214330643415451
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12543104588985443
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.29940611124038696
epoch£º1267	 i:4 	 global-step:25344	 l-p:0.22398677468299866
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.13291074335575104
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.13609836995601654
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.11435575038194656
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.17449511587619781
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.2690322995185852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1363, 3.1314, 3.1362],
        [3.1363, 2.4856, 2.6733],
        [3.1363, 3.1363, 3.1363],
        [3.1363, 2.2413, 2.2933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.12454956769943237 
model_pd.l_d.mean(): -24.680511474609375 
model_pd.lagr.mean(): -24.55596160888672 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0333], device='cuda:0')), ('power', tensor([-24.6472], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.12454956769943237
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.12724940478801727
epoch£º1268	 i:2 	 global-step:25362	 l-p:0.21924136579036713
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.144643634557724
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.13786448538303375
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.3031364679336548
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.13924852013587952
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.13845446705818176
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.13155587017536163
epoch£º1268	 i:9 	 global-step:25369	 l-p:0.21969908475875854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1344, 2.5545, 2.7599],
        [3.1344, 3.1119, 3.1330],
        [3.1344, 3.1344, 3.1344],
        [3.1344, 3.1343, 3.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.2874700129032135 
model_pd.l_d.mean(): -24.971771240234375 
model_pd.lagr.mean(): -24.684301376342773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0039], device='cuda:0')), ('power', tensor([-24.9679], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.2874700129032135
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.1078876480460167
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.16033969819545746
epoch£º1269	 i:3 	 global-step:25383	 l-p:0.20403161644935608
epoch£º1269	 i:4 	 global-step:25384	 l-p:0.23959700763225555
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.14534378051757812
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.13378135859966278
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.13491490483283997
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.12683038413524628
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.12932714819908142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1399, 1.9860, 1.3220],
        [3.1399, 3.1365, 3.1398],
        [3.1399, 1.8760, 1.4788],
        [3.1399, 1.8297, 1.3633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.15951815247535706 
model_pd.l_d.mean(): -25.070690155029297 
model_pd.lagr.mean(): -24.91117286682129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0022], device='cuda:0')), ('power', tensor([-25.0685], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.15951815247535706
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13632868230342865
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.1484469324350357
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.13894566893577576
epoch£º1270	 i:4 	 global-step:25404	 l-p:0.3397584557533264
epoch£º1270	 i:5 	 global-step:25405	 l-p:0.23578618466854095
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.1464109718799591
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.12348458915948868
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.09229820966720581
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.12378264963626862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1367, 1.9023, 1.2545],
        [3.1367, 1.7930, 1.1877],
        [3.1367, 2.9800, 3.1001],
        [3.1367, 1.9837, 1.7464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.12688589096069336 
model_pd.l_d.mean(): -24.649904251098633 
model_pd.lagr.mean(): -24.52301788330078 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0405], device='cuda:0')), ('power', tensor([-24.6904], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.12688589096069336
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.1391812115907669
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.13532693684101105
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.11647357791662216
epoch£º1271	 i:4 	 global-step:25424	 l-p:0.2516366243362427
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.14219729602336884
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.15929707884788513
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.1956353783607483
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.1739402711391449
epoch£º1271	 i:9 	 global-step:25429	 l-p:0.6210761070251465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[3.1233, 1.7779, 1.1788],
        [3.1233, 1.8186, 1.3626],
        [3.1233, 1.7783, 1.2481],
        [3.1233, 2.0440, 1.3696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.2557962238788605 
model_pd.l_d.mean(): -24.941553115844727 
model_pd.lagr.mean(): -24.68575668334961 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1186], device='cuda:0')), ('power', tensor([-25.0601], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:0.2557962238788605
epoch£º1272	 i:1 	 global-step:25441	 l-p:0.738667368888855
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12270686775445938
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.18153664469718933
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.15603333711624146
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.10341818630695343
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.1317313313484192
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.1625843346118927
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.1352277398109436
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.1533409059047699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1310, 1.8705, 1.4796],
        [3.1310, 2.9656, 3.0908],
        [3.1310, 3.1299, 3.1310],
        [3.1310, 3.1176, 3.1303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.43605512380599976 
model_pd.l_d.mean(): -24.83649253845215 
model_pd.lagr.mean(): -24.40043830871582 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1016], device='cuda:0')), ('power', tensor([-24.9381], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:0.43605512380599976
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.1568363606929779
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.14585044980049133
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.14329177141189575
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.10869129747152328
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.13648132979869843
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.17463599145412445
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12551751732826233
epoch£º1273	 i:8 	 global-step:25468	 l-p:0.23199738562107086
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.14184005558490753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1270, 3.1271, 3.1271],
        [3.1270, 2.5468, 2.7524],
        [3.1270, 2.4252, 2.5946],
        [3.1270, 3.1207, 3.1269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.39089614152908325 
model_pd.l_d.mean(): -24.916446685791016 
model_pd.lagr.mean(): -24.525550842285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0047], device='cuda:0')), ('power', tensor([-24.9211], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.39089614152908325
epoch£º1274	 i:1 	 global-step:25481	 l-p:0.24551738798618317
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.30294927954673767
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.09456503391265869
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.13453149795532227
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12604494392871857
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.22542206943035126
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.11109043657779694
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.15497979521751404
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.12640619277954102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1277, 2.7290, 2.9383],
        [3.1277, 3.1220, 3.1275],
        [3.1277, 3.1257, 3.1277],
        [3.1277, 2.1454, 1.4540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.14322960376739502 
model_pd.l_d.mean(): -24.95121955871582 
model_pd.lagr.mean(): -24.8079891204834 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0328], device='cuda:0')), ('power', tensor([-24.9840], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.14322960376739502
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.30302050709724426
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.16020484268665314
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.1303449422121048
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.11534437537193298
epoch£º1275	 i:5 	 global-step:25505	 l-p:0.25389328598976135
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.338506281375885
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.13431186974048615
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.1652517020702362
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.1223268210887909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1312, 3.1298, 3.1312],
        [3.1312, 1.7867, 1.1839],
        [3.1312, 1.8059, 1.1882],
        [3.1312, 1.9744, 1.7322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.2899073362350464 
model_pd.l_d.mean(): -25.11115264892578 
model_pd.lagr.mean(): -24.821245193481445 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0425], device='cuda:0')), ('power', tensor([-25.1537], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:0.2899073362350464
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.1328367441892624
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.1353059709072113
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.18440936505794525
epoch£º1276	 i:4 	 global-step:25524	 l-p:0.24995559453964233
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.102274090051651
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.10835107415914536
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.14353236556053162
epoch£º1276	 i:8 	 global-step:25528	 l-p:0.21858257055282593
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.14916545152664185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1417, 1.7853, 1.2099],
        [3.1417, 3.1375, 3.1416],
        [3.1417, 2.0956, 1.4127],
        [3.1417, 3.1402, 3.1417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.14977490901947021 
model_pd.l_d.mean(): -25.103620529174805 
model_pd.lagr.mean(): -24.953845977783203 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1258], device='cuda:0')), ('power', tensor([-25.2294], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.14977490901947021
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11686822026968002
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.13825103640556335
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.1579120010137558
epoch£º1277	 i:4 	 global-step:25544	 l-p:0.17592772841453552
epoch£º1277	 i:5 	 global-step:25545	 l-p:0.1928805112838745
epoch£º1277	 i:6 	 global-step:25546	 l-p:0.2266404926776886
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.14063197374343872
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.17290586233139038
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.14281953871250153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1437, 3.1437, 3.1437],
        [3.1437, 1.9180, 1.2669],
        [3.1437, 1.8509, 1.2167],
        [3.1437, 3.1011, 3.1396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.19602900743484497 
model_pd.l_d.mean(): -25.0411434173584 
model_pd.lagr.mean(): -24.84511375427246 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0562], device='cuda:0')), ('power', tensor([-24.9850], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.19602900743484497
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.13546663522720337
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.10952351242303848
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.1932346075773239
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.13063737750053406
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.14330430328845978
epoch£º1278	 i:6 	 global-step:25566	 l-p:0.20306506752967834
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.10346449911594391
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.13545621931552887
epoch£º1278	 i:9 	 global-step:25569	 l-p:0.2085532397031784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1465, 2.8625, 3.0430],
        [3.1465, 3.1459, 3.1465],
        [3.1465, 1.8014, 1.1944],
        [3.1465, 3.1463, 3.1465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.11749329417943954 
model_pd.l_d.mean(): -24.20774269104004 
model_pd.lagr.mean(): -24.09025001525879 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2541], device='cuda:0')), ('power', tensor([-24.4619], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.11749329417943954
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.1294458955526352
epoch£º1279	 i:2 	 global-step:25582	 l-p:0.2602238357067108
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.11784864217042923
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.2119561731815338
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.07576273381710052
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.17875616252422333
epoch£º1279	 i:7 	 global-step:25587	 l-p:0.2064765840768814
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.1307700127363205
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.12699712812900543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1459, 1.8704, 1.4561],
        [3.1459, 2.6566, 2.8713],
        [3.1459, 3.1378, 3.1456],
        [3.1459, 3.1411, 3.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.26268482208251953 
model_pd.l_d.mean(): -24.849815368652344 
model_pd.lagr.mean(): -24.58713150024414 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0721], device='cuda:0')), ('power', tensor([-24.9219], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:0.26268482208251953
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.1483609974384308
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.12467511743307114
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.12089260667562485
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.09887553006410599
epoch£º1280	 i:5 	 global-step:25605	 l-p:0.21018704771995544
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.13208281993865967
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.16533008217811584
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.1929698884487152
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.1204560250043869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1420, 3.0799, 3.1344],
        [3.1420, 2.1566, 2.1254],
        [3.1420, 2.7149, 2.9274],
        [3.1420, 2.0914, 1.9877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.17924270033836365 
model_pd.l_d.mean(): -25.034786224365234 
model_pd.lagr.mean(): -24.85554313659668 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0720], device='cuda:0')), ('power', tensor([-24.9628], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.17924270033836365
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.11084207892417908
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.12720586359500885
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.1341625303030014
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.2027135193347931
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.1242678239941597
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.16664016246795654
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.1242227703332901
epoch£º1281	 i:8 	 global-step:25628	 l-p:0.2783161401748657
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.13418829441070557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1435, 3.1432, 3.1435],
        [3.1435, 3.1370, 3.1433],
        [3.1435, 2.3002, 2.3911],
        [3.1435, 1.8272, 1.2018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.07386598736047745 
model_pd.l_d.mean(): -25.019990921020508 
model_pd.lagr.mean(): -24.946125030517578 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0009], device='cuda:0')), ('power', tensor([-25.0191], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.07386598736047745
epoch£º1282	 i:1 	 global-step:25641	 l-p:0.22539584338665009
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.14308728277683258
epoch£º1282	 i:3 	 global-step:25643	 l-p:0.21072933077812195
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.16201435029506683
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.12573014199733734
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.15168841183185577
epoch£º1282	 i:7 	 global-step:25647	 l-p:0.19408437609672546
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.15502367913722992
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.12381459772586823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1466, 1.9898, 1.7468],
        [3.1466, 3.1446, 3.1466],
        [3.1466, 3.1400, 3.1464],
        [3.1466, 3.1466, 3.1467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.1629953533411026 
model_pd.l_d.mean(): -24.880306243896484 
model_pd.lagr.mean(): -24.717309951782227 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0380], device='cuda:0')), ('power', tensor([-24.8423], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.1629953533411026
epoch£º1283	 i:1 	 global-step:25661	 l-p:0.1885758489370346
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.14419084787368774
epoch£º1283	 i:3 	 global-step:25663	 l-p:0.22145670652389526
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.1347121000289917
epoch£º1283	 i:5 	 global-step:25665	 l-p:0.2150808423757553
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12239827960729599
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.08610990643501282
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.13643847405910492
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.13311734795570374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1472, 1.9612, 1.3016],
        [3.1472, 3.1386, 3.1469],
        [3.1472, 2.8917, 3.0613],
        [3.1472, 3.1471, 3.1472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.1625702977180481 
model_pd.l_d.mean(): -25.216480255126953 
model_pd.lagr.mean(): -25.053909301757812 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0365], device='cuda:0')), ('power', tensor([-25.1800], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.1625702977180481
epoch£º1284	 i:1 	 global-step:25681	 l-p:0.20307502150535583
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12332470715045929
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12303704023361206
epoch£º1284	 i:4 	 global-step:25684	 l-p:0.20050840079784393
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.140473872423172
epoch£º1284	 i:6 	 global-step:25686	 l-p:0.2341567873954773
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.13332082331180573
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.13696786761283875
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.09218229353427887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1454, 1.7932, 1.1966],
        [3.1454, 3.1296, 3.1446],
        [3.1454, 2.1742, 2.1570],
        [3.1454, 1.8369, 1.2078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.11214739829301834 
model_pd.l_d.mean(): -25.139427185058594 
model_pd.lagr.mean(): -25.027278900146484 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0152], device='cuda:0')), ('power', tensor([-25.1242], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.11214739829301834
epoch£º1285	 i:1 	 global-step:25701	 l-p:0.2315537929534912
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.15857191383838654
epoch£º1285	 i:3 	 global-step:25703	 l-p:0.16292080283164978
epoch£º1285	 i:4 	 global-step:25704	 l-p:0.1967478096485138
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.12166064977645874
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.13171972334384918
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.17544929683208466
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.13855431973934174
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.11448048055171967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1503, 2.0999, 1.9962],
        [3.1503, 3.0205, 3.1238],
        [3.1503, 2.1456, 1.4546],
        [3.1503, 2.0085, 1.3404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.23842008411884308 
model_pd.l_d.mean(): -24.74932861328125 
model_pd.lagr.mean(): -24.510908126831055 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0722], device='cuda:0')), ('power', tensor([-24.8215], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:0.23842008411884308
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.0925804153084755
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.14242954552173615
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.15434405207633972
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.12856295704841614
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.13790345191955566
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.13333606719970703
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11745092272758484
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.11944963037967682
epoch£º1286	 i:9 	 global-step:25729	 l-p:0.27585792541503906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1427, 1.8657, 1.2270],
        [3.1427, 1.9404, 1.6309],
        [3.1427, 3.1426, 3.1427],
        [3.1427, 3.1427, 3.1427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.1652006357908249 
model_pd.l_d.mean(): -25.08898162841797 
model_pd.lagr.mean(): -24.92378044128418 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0846], device='cuda:0')), ('power', tensor([-25.1735], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.1652006357908249
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.1135048046708107
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.13099724054336548
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.12394120544195175
epoch£º1287	 i:4 	 global-step:25744	 l-p:0.2172263264656067
epoch£º1287	 i:5 	 global-step:25745	 l-p:0.23185229301452637
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.10972970724105835
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12234119325876236
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.16451434791088104
epoch£º1287	 i:9 	 global-step:25749	 l-p:0.2754742205142975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1353, 3.1353, 3.1353],
        [3.1353, 3.1351, 3.1354],
        [3.1353, 2.0811, 1.9735],
        [3.1353, 1.8243, 1.1986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.1449170708656311 
model_pd.l_d.mean(): -24.879364013671875 
model_pd.lagr.mean(): -24.734447479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0350], device='cuda:0')), ('power', tensor([-24.9143], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.1449170708656311
epoch£º1288	 i:1 	 global-step:25761	 l-p:0.24234889447689056
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.12698200345039368
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.14162085950374603
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.08237293362617493
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.18135425448417664
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.12582607567310333
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.12674136459827423
epoch£º1288	 i:8 	 global-step:25768	 l-p:0.47582313418388367
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.1449192464351654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1282, 1.9860, 1.3218],
        [3.1282, 2.8439, 3.0247],
        [3.1282, 3.1283, 3.1282],
        [3.1282, 2.9990, 3.1020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.2258443385362625 
model_pd.l_d.mean(): -24.959218978881836 
model_pd.lagr.mean(): -24.733375549316406 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0708], device='cuda:0')), ('power', tensor([-24.8884], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:0.2258443385362625
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.38173404335975647
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.3315766453742981
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.15106387436389923
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.12241405993700027
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.15948310494422913
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12317630648612976
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.15611284971237183
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.13792358338832855
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.09652707725763321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1260, 3.1260, 3.1260],
        [3.1260, 2.3662, 2.5090],
        [3.1260, 2.4615, 2.6451],
        [3.1260, 2.8520, 3.0290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.1327328234910965 
model_pd.l_d.mean(): -24.785236358642578 
model_pd.lagr.mean(): -24.652503967285156 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0930], device='cuda:0')), ('power', tensor([-24.8783], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.1327328234910965
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.1594439148902893
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.1383906900882721
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.12382335215806961
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.16605588793754578
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.1302240639925003
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.34093546867370605
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.15403033792972565
epoch£º1290	 i:8 	 global-step:25808	 l-p:-1.8556345701217651
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.1405472606420517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1164, 2.3437, 2.4800],
        [3.1164, 2.3009, 2.4119],
        [3.1164, 1.9382, 1.6662],
        [3.1164, 3.1163, 3.1164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.13208821415901184 
model_pd.l_d.mean(): -25.27177619934082 
model_pd.lagr.mean(): -25.13968849182129 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0488], device='cuda:0')), ('power', tensor([-25.2229], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.13208821415901184
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.13249890506267548
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12886156141757965
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.23402994871139526
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.18506325781345367
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.1377554088830948
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.12007012218236923
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.11161891371011734
epoch£º1291	 i:8 	 global-step:25828	 l-p:-10.370786666870117
epoch£º1291	 i:9 	 global-step:25829	 l-p:-0.16515380144119263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1073, 3.1062, 3.1073],
        [3.1073, 2.1517, 1.4585],
        [3.1073, 3.1038, 3.1072],
        [3.1073, 2.8606, 3.0268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.16831031441688538 
model_pd.l_d.mean(): -24.48508071899414 
model_pd.lagr.mean(): -24.316770553588867 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0683], device='cuda:0')), ('power', tensor([-24.5533], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.16831031441688538
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.19919192790985107
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.1436254233121872
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.20929625630378723
epoch£º1292	 i:4 	 global-step:25844	 l-p:-0.5833508968353271
epoch£º1292	 i:5 	 global-step:25845	 l-p:0.5666788220405579
epoch£º1292	 i:6 	 global-step:25846	 l-p:-0.4103867709636688
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.12969836592674255
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.11822248995304108
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.12941840291023254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1184, 3.1063, 3.1179],
        [3.1184, 1.9459, 1.6821],
        [3.1184, 2.6015, 2.8155],
        [3.1184, 2.7527, 2.9564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.16293102502822876 
model_pd.l_d.mean(): -25.033842086791992 
model_pd.lagr.mean(): -24.87091064453125 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0312], device='cuda:0')), ('power', tensor([-25.0650], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.16293102502822876
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.5707054734230042
epoch£º1293	 i:2 	 global-step:25862	 l-p:0.3366318345069885
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.1489635854959488
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.1375756859779358
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.13737371563911438
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.11678462475538254
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.5452669858932495
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.15021923184394836
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.13171470165252686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1238, 3.1212, 3.1238],
        [3.1238, 2.9761, 3.0908],
        [3.1238, 1.9251, 1.6228],
        [3.1238, 3.1225, 3.1238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.5226937532424927 
model_pd.l_d.mean(): -24.672016143798828 
model_pd.lagr.mean(): -24.149322509765625 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1026], device='cuda:0')), ('power', tensor([-24.7746], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:0.5226937532424927
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.13654360175132751
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11200916022062302
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.15047068893909454
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.1236625462770462
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.19430269300937653
epoch£º1294	 i:6 	 global-step:25886	 l-p:2.780081033706665
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.12167716771364212
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.1459350883960724
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.14350266754627228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1159, 3.1093, 3.1157],
        [3.1159, 3.0757, 3.1122],
        [3.1159, 2.9649, 3.0816],
        [3.1159, 1.9377, 1.2824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.3999258577823639 
model_pd.l_d.mean(): -25.073862075805664 
model_pd.lagr.mean(): -24.67393684387207 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0131], device='cuda:0')), ('power', tensor([-25.0870], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:0.3999258577823639
epoch£º1295	 i:1 	 global-step:25901	 l-p:1.3356802463531494
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.19461797177791595
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.1276755928993225
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.11713778972625732
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.1063561737537384
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.15375983715057373
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.18849433958530426
epoch£º1295	 i:8 	 global-step:25908	 l-p:-0.5540386438369751
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.13694526255130768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1145, 2.8524, 3.0249],
        [3.1145, 2.9099, 3.0565],
        [3.1145, 3.1145, 3.1145],
        [3.1145, 3.0572, 3.1079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): -0.47192779183387756 
model_pd.l_d.mean(): -24.717065811157227 
model_pd.lagr.mean(): -25.188993453979492 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0956], device='cuda:0')), ('power', tensor([-24.8127], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:-0.47192779183387756
epoch£º1296	 i:1 	 global-step:25921	 l-p:3.9096286296844482
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.19802822172641754
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.41934776306152344
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.15391512215137482
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.15515640377998352
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.11404155194759369
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.11837845295667648
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.11836932599544525
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.11050163209438324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[3.1180, 1.7647, 1.2085],
        [3.1180, 2.2210, 2.2725],
        [3.1180, 2.2120, 2.2562],
        [3.1180, 1.8934, 1.5548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.12317239493131638 
model_pd.l_d.mean(): -24.51835060119629 
model_pd.lagr.mean(): -24.395177841186523 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1005], device='cuda:0')), ('power', tensor([-24.6189], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.12317239493131638
epoch£º1297	 i:1 	 global-step:25941	 l-p:0.3692736327648163
epoch£º1297	 i:2 	 global-step:25942	 l-p:1.1780319213867188
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14463618397712708
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.1596662998199463
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.13604138791561127
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.14970119297504425
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.1443777233362198
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.1378834843635559
epoch£º1297	 i:9 	 global-step:25949	 l-p:-0.16044163703918457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1115, 3.1109, 3.1115],
        [3.1115, 1.7717, 1.2549],
        [3.1115, 1.7686, 1.1698],
        [3.1115, 1.7953, 1.3226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.1862356960773468 
model_pd.l_d.mean(): -25.100616455078125 
model_pd.lagr.mean(): -24.91438102722168 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0574], device='cuda:0')), ('power', tensor([-25.1580], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.1862356960773468
epoch£º1298	 i:1 	 global-step:25961	 l-p:-0.20016176998615265
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.15876516699790955
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.5479556322097778
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.08608120679855347
epoch£º1298	 i:5 	 global-step:25965	 l-p:-22.328691482543945
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.17075194418430328
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.1337592899799347
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.14821361005306244
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.13822555541992188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1132, 3.1115, 3.1132],
        [3.1132, 1.8612, 1.4852],
        [3.1132, 3.1132, 3.1132],
        [3.1132, 3.0182, 3.0976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): -0.3048921823501587 
model_pd.l_d.mean(): -25.2606201171875 
model_pd.lagr.mean(): -25.56551170349121 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0466], device='cuda:0')), ('power', tensor([-25.2140], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:-0.3048921823501587
epoch£º1299	 i:1 	 global-step:25981	 l-p:-3.5054707527160645
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.15496084094047546
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.13857704401016235
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.1643717885017395
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.12165172398090363
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.12329953908920288
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.19996623694896698
epoch£º1299	 i:8 	 global-step:25988	 l-p:0.5237534046173096
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.17308364808559418
