
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.12033862620592117 
model_pd.l_d.mean(): -24.47382926940918 
model_pd.lagr.mean(): -24.353490829467773 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0805], device='cuda:0')), ('power', tensor([-24.5543], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.12033862620592117
epoch£º0	 i:1 	 global-step:1	 l-p:0.15623173117637634
epoch£º0	 i:2 	 global-step:2	 l-p:0.15599572658538818
epoch£º0	 i:3 	 global-step:3	 l-p:0.13998126983642578
epoch£º0	 i:4 	 global-step:4	 l-p:-0.8872278928756714
epoch£º0	 i:5 	 global-step:5	 l-p:0.11750897765159607
epoch£º0	 i:6 	 global-step:6	 l-p:0.13981249928474426
epoch£º0	 i:7 	 global-step:7	 l-p:0.11974195390939713
epoch£º0	 i:8 	 global-step:8	 l-p:0.03877580910921097
epoch£º0	 i:9 	 global-step:9	 l-p:0.11709308624267578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.2256, 3.2260, 3.2256],
        [3.2256, 3.4164, 3.3781],
        [3.2256, 3.2803, 3.2454],
        [3.2256, 4.1137, 4.7313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.10776599496603012 
model_pd.l_d.mean(): -23.923336029052734 
model_pd.lagr.mean(): -23.815570831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2747], device='cuda:0')), ('power', tensor([-23.6487], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.10776599496603012
epoch£º1	 i:1 	 global-step:21	 l-p:0.1131468340754509
epoch£º1	 i:2 	 global-step:22	 l-p:0.10735069215297699
epoch£º1	 i:3 	 global-step:23	 l-p:0.09871791303157806
epoch£º1	 i:4 	 global-step:24	 l-p:0.1287594437599182
epoch£º1	 i:5 	 global-step:25	 l-p:0.11391725391149521
epoch£º1	 i:6 	 global-step:26	 l-p:0.12586137652397156
epoch£º1	 i:7 	 global-step:27	 l-p:0.1297922134399414
epoch£º1	 i:8 	 global-step:28	 l-p:0.1225266084074974
epoch£º1	 i:9 	 global-step:29	 l-p:0.09531986713409424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9208, 3.4956, 3.7844],
        [2.9208, 2.9750, 2.9420],
        [2.9208, 3.3585, 3.5046],
        [2.9208, 2.9663, 2.9367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13646908104419708 
model_pd.l_d.mean(): -24.47289276123047 
model_pd.lagr.mean(): -24.336423873901367 
model_pd.lambdas: dict_items([('pout', tensor([1.], device='cuda:0')), ('power', tensor([1.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1897], device='cuda:0')), ('power', tensor([-24.2832], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13646908104419708
epoch£º2	 i:1 	 global-step:41	 l-p:0.18635110557079315
epoch£º2	 i:2 	 global-step:42	 l-p:0.13883858919143677
epoch£º2	 i:3 	 global-step:43	 l-p:0.12673039734363556
epoch£º2	 i:4 	 global-step:44	 l-p:0.135629341006279
epoch£º2	 i:5 	 global-step:45	 l-p:0.2187756448984146
epoch£º2	 i:6 	 global-step:46	 l-p:0.1181279793381691
epoch£º2	 i:7 	 global-step:47	 l-p:0.2314419001340866
epoch£º2	 i:8 	 global-step:48	 l-p:0.135161355137825
epoch£º2	 i:9 	 global-step:49	 l-p:0.127161905169487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8749, 3.3876, 3.6167],
        [2.8749, 2.8999, 2.8809],
        [2.8749, 2.9266, 2.8948],
        [2.8749, 3.6070, 4.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.1339344084262848 
model_pd.l_d.mean(): -23.677711486816406 
model_pd.lagr.mean(): -23.543777465820312 
model_pd.lambdas: 