
bounds:tensor([-2.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.6167, 2.6493],
        [2.3753, 2.3852, 2.3768],
        [2.3753, 2.9405, 3.3198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.13074924051761627 
model_pd.l_d.mean(): -15.083145141601562 
model_pd.lagr.mean(): -15.213894844055176 
model_pd.lambdas: dict_items([('pout', tensor([1.0026], device='cuda:0')), ('power', tensor([0.9991], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.5963], device='cuda:0')), ('power', tensor([-17.6794], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.13074924051761627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4887, 2.4992, 2.4903],
        [2.4887, 2.7447, 2.7786],
        [2.4887, 3.0904, 3.4927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): -0.16455626487731934 
model_pd.l_d.mean(): -15.373940467834473 
model_pd.lagr.mean(): -15.538496971130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0051], device='cuda:0')), ('power', tensor([0.9982], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.5342], device='cuda:0')), ('power', tensor([-17.9305], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:-0.16455626487731934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.6034, 2.8739, 2.9093],
        [2.6034, 3.2418, 3.6673],
        [2.6034, 2.6143, 2.6050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): -0.2170141637325287 
model_pd.l_d.mean(): -15.633195877075195 
model_pd.lagr.mean(): -15.850210189819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0076], device='cuda:0')), ('power', tensor([0.9973], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.4743], device='cuda:0')), ('power', tensor([-18.1525], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:-0.2170141637325287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7192, 2.7307, 2.7209],
        [2.7192, 3.0045, 3.0413],
        [2.7192, 3.3949, 3.8438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): -0.3101373314857483 
model_pd.l_d.mean(): -15.864236831665039 
model_pd.lagr.mean(): -16.174373626708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0100], device='cuda:0')), ('power', tensor([0.9964], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.4164], device='cuda:0')), ('power', tensor([-18.3484], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:-0.3101373314857483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8362, 3.1366, 3.1748],
        [2.8362, 2.8483, 2.8381],
        [2.8362, 3.5496, 4.0222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): -0.5243432521820068 
model_pd.l_d.mean(): -16.069902420043945 
model_pd.lagr.mean(): -16.59424591064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0124], device='cuda:0')), ('power', tensor([0.9955], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.3604], device='cuda:0')), ('power', tensor([-18.5208], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:-0.5243432521820068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9546, 3.7059, 4.2025],
        [2.9546, 2.9672, 2.9565],
        [2.9546, 3.2700, 3.3097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): -1.6545991897583008 
model_pd.l_d.mean(): -16.252647399902344 
model_pd.lagr.mean(): -17.907245635986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0147], device='cuda:0')), ('power', tensor([0.9945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.3062], device='cuda:0')), ('power', tensor([-18.6720], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:-1.6545991897583008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0742, 3.8640, 4.3848],
        [3.0742, 3.4049, 3.4461],
        [3.0742, 3.0874, 3.0762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 2.7405648231506348 
model_pd.l_d.mean(): -16.414609909057617 
model_pd.lagr.mean(): -13.67404556274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0169], device='cuda:0')), ('power', tensor([0.9936], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.2536], device='cuda:0')), ('power', tensor([-18.8041], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:2.7405648231506348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1951, 4.0238, 4.5692],
        [3.1951, 3.2089, 3.1972],
        [3.1951, 3.5413, 3.5839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6179846525192261 
model_pd.l_d.mean(): -16.557666778564453 
model_pd.lagr.mean(): -15.939682006835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0191], device='cuda:0')), ('power', tensor([0.9926], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.2025], device='cuda:0')), ('power', tensor([-18.9186], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.6179846525192261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.3174, 3.3318, 3.3196],
        [3.3174, 3.6794, 3.7235],
        [3.3174, 4.1856, 4.7557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.36033910512924194 
model_pd.l_d.mean(): -16.683534622192383 
model_pd.lagr.mean(): -16.323196411132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0213], device='cuda:0')), ('power', tensor([0.9917], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.1527], device='cuda:0')), ('power', tensor([-19.0173], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.36033910512924194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4413, 3.4563, 3.4436],
        [3.4413, 3.8191, 3.8647],
        [3.4413, 4.3493, 4.9445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.2551038861274719 
model_pd.l_d.mean(): -16.79366683959961 
model_pd.lagr.mean(): -16.538562774658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0234], device='cuda:0')), ('power', tensor([0.9907], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.1042], device='cuda:0')), ('power', tensor([-19.1013], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.2551038861274719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 4.5151, 5.1358],
        [3.5667, 3.9606, 4.0078],
        [3.5667, 3.5823, 3.5691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.19686684012413025 
model_pd.l_d.mean(): -16.88931655883789 
model_pd.lagr.mean(): -16.69244956970215 
model_pd.lambdas: dict_items([('pout', tensor([1.0255], device='cuda:0')), ('power', tensor([0.9898], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.0568], device='cuda:0')), ('power', tensor([-19.1718], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.19686684012413025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6938, 3.7100, 3.6963],
        [3.6938, 4.6831, 5.3296],
        [3.6938, 4.1040, 4.1527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.15916414558887482 
model_pd.l_d.mean(): -16.971572875976562 
model_pd.lagr.mean(): -16.812408447265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0275], device='cuda:0')), ('power', tensor([0.9888], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.0106], device='cuda:0')), ('power', tensor([-19.2298], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.15916414558887482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8227, 4.2494, 4.2997],
        [3.8227, 3.8395, 3.8252],
        [3.8227, 4.8535, 5.5260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.1321202516555786 
model_pd.l_d.mean(): -17.041391372680664 
model_pd.lagr.mean(): -16.909271240234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0294], device='cuda:0')), ('power', tensor([0.9879], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.9652], device='cuda:0')), ('power', tensor([-19.2761], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.1321202516555786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9533, 4.3968, 4.4487],
        [3.9533, 5.0262, 5.7253],
        [3.9533, 3.9707, 3.9559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.1111510694026947 
model_pd.l_d.mean(): -17.0996150970459 
model_pd.lagr.mean(): -16.98846435546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0314], device='cuda:0')), ('power', tensor([0.9869], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.9209], device='cuda:0')), ('power', tensor([-19.3115], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.1111510694026947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0858, 4.5463, 4.5998],
        [4.0858, 5.2014, 5.9274],
        [4.0858, 4.1039, 4.0885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.0937727615237236 
model_pd.l_d.mean(): -17.14698028564453 
model_pd.lagr.mean(): -17.053207397460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0332], device='cuda:0')), ('power', tensor([0.9859], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.8773], device='cuda:0')), ('power', tensor([-19.3366], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.0937727615237236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2202, 4.6980, 4.7532],
        [4.2202, 4.2389, 4.2230],
        [4.2202, 5.3792, 6.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.07844134420156479 
model_pd.l_d.mean(): -17.18415641784668 
model_pd.lagr.mean(): -17.105714797973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0351], device='cuda:0')), ('power', tensor([0.9850], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.8346], device='cuda:0')), ('power', tensor([-19.3521], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.07844134420156479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3567, 4.3760, 4.3596],
        [4.3567, 5.5597, 6.3407],
        [4.3567, 4.8520, 4.9089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.0640362948179245 
model_pd.l_d.mean(): -17.211713790893555 
model_pd.lagr.mean(): -17.14767837524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0369], device='cuda:0')), ('power', tensor([0.9840], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.7926], device='cuda:0')), ('power', tensor([-19.3584], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.0640362948179245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4952, 5.7430, 6.5522],
        [4.4952, 4.5152, 4.4982],
        [4.4952, 5.0083, 5.0670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.04957130178809166 
model_pd.l_d.mean(): -17.230167388916016 
model_pd.lagr.mean(): -17.18059539794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0386], device='cuda:0')), ('power', tensor([0.9830], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.7514], device='cuda:0')), ('power', tensor([-19.3559], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.04957130178809166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6358, 5.1671, 5.2275],
        [4.6358, 5.9291, 6.7670],
        [4.6358, 4.6565, 4.6389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.03394993767142296 
model_pd.l_d.mean(): -17.239980697631836 
model_pd.lagr.mean(): -17.206029891967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0403], device='cuda:0')), ('power', tensor([0.9821], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.7107], device='cuda:0')), ('power', tensor([-19.3452], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.03394993767142296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7786, 4.8001, 4.7819],
        [4.7786, 6.1182, 6.9853],
        [4.7786, 5.3285, 5.3906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.015642385929822922 
model_pd.l_d.mean(): -17.241573333740234 
model_pd.lagr.mean(): -17.22593116760254 
model_pd.lambdas: dict_items([('pout', tensor([1.0420], device='cuda:0')), ('power', tensor([0.9811], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6707], device='cuda:0')), ('power', tensor([-19.3265], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.015642385929822922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9238, 6.3105, 7.2073],
        [4.9238, 4.9459, 4.9271],
        [4.9238, 5.4924, 5.5564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): -0.007965072989463806 
model_pd.l_d.mean(): -17.23531723022461 
model_pd.lagr.mean(): -17.243282318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.0436], device='cuda:0')), ('power', tensor([0.9801], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6313], device='cuda:0')), ('power', tensor([-19.3001], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:-0.007965072989463806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0714, 6.5059, 7.4330],
        [5.0714, 5.6591, 5.7250],
        [5.0714, 5.0942, 5.0748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): -0.04228507727384567 
model_pd.l_d.mean(): -17.221529006958008 
model_pd.lagr.mean(): -17.26381492614746 
model_pd.lambdas: dict_items([('pout', tensor([1.0452], device='cuda:0')), ('power', tensor([0.9792], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5924], device='cuda:0')), ('power', tensor([-19.2663], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:-0.04228507727384567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2214, 5.8286, 5.8964],
        [5.2214, 6.7047, 7.6627],
        [5.2214, 5.2449, 5.2249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): -0.10143715143203735 
model_pd.l_d.mean(): -17.200489044189453 
model_pd.lagr.mean(): -17.301925659179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0468], device='cuda:0')), ('power', tensor([0.9782], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5540], device='cuda:0')), ('power', tensor([-19.2254], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:-0.10143715143203735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3739, 5.3982, 5.3775],
        [5.3739, 6.9070, 7.8964],
        [5.3739, 6.0011, 6.0708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): -0.23915407061576843 
model_pd.l_d.mean(): -17.172468185424805 
model_pd.lagr.mean(): -17.411623001098633 
model_pd.lambdas: dict_items([('pout', tensor([1.0483], device='cuda:0')), ('power', tensor([0.9772], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5160], device='cuda:0')), ('power', tensor([-19.1775], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:-0.23915407061576843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5287, 5.5538, 5.5324],
        [5.5287, 7.1125, 8.1341],
        [5.5287, 6.1762, 6.2480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): -1.0230835676193237 
model_pd.l_d.mean(): -17.137664794921875 
model_pd.lagr.mean(): -18.160747528076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0498], device='cuda:0')), ('power', tensor([0.9763], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4786], device='cuda:0')), ('power', tensor([-19.1229], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:-1.0230835676193237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6850, 6.3532, 6.4272],
        [5.6850, 7.3202, 8.3744],
        [5.6850, 5.7109, 5.6889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.8805899620056152 
model_pd.l_d.mean(): -17.096494674682617 
model_pd.lagr.mean(): -16.215904235839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0512], device='cuda:0')), ('power', tensor([0.9753], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4419], device='cuda:0')), ('power', tensor([-19.0622], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.8805899620056152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8428, 7.5299, 8.6171],
        [5.8428, 6.5320, 6.6080],
        [5.8428, 5.8695, 5.8468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.37425696849823 
model_pd.l_d.mean(): -17.049354553222656 
model_pd.lagr.mean(): -16.675098419189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0526], device='cuda:0')), ('power', tensor([0.9744], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4058], device='cuda:0')), ('power', tensor([-18.9957], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.37425696849823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0022, 6.0297, 6.0063],
        [6.0022, 7.7417, 8.8623],
        [6.0022, 6.7125, 6.7907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.25876206159591675 
model_pd.l_d.mean(): -16.99652099609375 
model_pd.lagr.mean(): -16.73775863647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0540], device='cuda:0')), ('power', tensor([0.9734], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3703], device='cuda:0')), ('power', tensor([-18.9237], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.25876206159591675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1633, 6.8950, 6.9754],
        [6.1633, 6.1916, 6.1676],
        [6.1633, 7.9558, 9.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.20686450600624084 
model_pd.l_d.mean(): -16.938247680664062 
model_pd.lagr.mean(): -16.731382369995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0553], device='cuda:0')), ('power', tensor([0.9725], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3354], device='cuda:0')), ('power', tensor([-18.8464], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.20686450600624084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3093, 6.3383, 6.3136],
        [6.3093, 8.1498, 9.3347],
        [6.3093, 7.0603, 7.1427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.17951302230358124 
model_pd.l_d.mean(): -16.879545211791992 
model_pd.lagr.mean(): -16.700031280517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0566], device='cuda:0')), ('power', tensor([0.9716], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3046], device='cuda:0')), ('power', tensor([-18.7727], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.17951302230358124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4268, 8.3060, 9.5154],
        [6.4268, 6.4565, 6.4313],
        [6.4268, 7.1934, 7.2774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.16419267654418945 
model_pd.l_d.mean(): -16.82594108581543 
model_pd.lagr.mean(): -16.6617488861084 
model_pd.lambdas: dict_items([('pout', tensor([1.0579], device='cuda:0')), ('power', tensor([0.9706], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2803], device='cuda:0')), ('power', tensor([-18.7110], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.16419267654418945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5154, 6.5455, 6.5200],
        [6.5154, 8.4237, 9.6516],
        [6.5154, 7.2938, 7.3790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.15509915351867676 
model_pd.l_d.mean(): -16.77943229675293 
model_pd.lagr.mean(): -16.624332427978516 
model_pd.lambdas: dict_items([('pout', tensor([1.0592], device='cuda:0')), ('power', tensor([0.9697], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2623], device='cuda:0')), ('power', tensor([-18.6632], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.15509915351867676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5779, 8.5065, 9.7474],
        [6.5779, 6.6083, 6.5824],
        [6.5779, 7.3644, 7.4505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.14962121844291687 
model_pd.l_d.mean(): -16.74041748046875 
model_pd.lagr.mean(): -16.590795516967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0604], device='cuda:0')), ('power', tensor([0.9688], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2498], device='cuda:0')), ('power', tensor([-18.6288], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.14962121844291687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6167, 8.5580, 9.8070],
        [6.6167, 6.6473, 6.6213],
        [6.6167, 7.4083, 7.4949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.1465306431055069 
model_pd.l_d.mean(): -16.708776473999023 
model_pd.lagr.mean(): -16.562246322631836 
model_pd.lambdas: dict_items([('pout', tensor([1.0617], device='cuda:0')), ('power', tensor([0.9678], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2420], device='cuda:0')), ('power', tensor([-18.6072], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.1465306431055069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6345, 8.5814, 9.8340],
        [6.6345, 6.6651, 6.6391],
        [6.6345, 7.4283, 7.5151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.14519628882408142 
model_pd.l_d.mean(): -16.684080123901367 
model_pd.lagr.mean(): -16.538883209228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0629], device='cuda:0')), ('power', tensor([0.9669], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2385], device='cuda:0')), ('power', tensor([-18.5973], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.14519628882408142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6333, 7.4269, 7.5137],
        [6.6333, 6.6640, 6.6379],
        [6.6333, 8.5798, 9.8319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.14528891444206238 
model_pd.l_d.mean(): -16.665687561035156 
model_pd.lagr.mean(): -16.52039909362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0641], device='cuda:0')), ('power', tensor([0.9660], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2388], device='cuda:0')), ('power', tensor([-18.5981], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.14528891444206238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6154, 6.6459, 6.6199],
        [6.6154, 8.5557, 9.8040],
        [6.6154, 7.4065, 7.4930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.14665725827217102 
model_pd.l_d.mean(): -16.652841567993164 
model_pd.lagr.mean(): -16.506183624267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0654], device='cuda:0')), ('power', tensor([0.9650], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2423], device='cuda:0')), ('power', tensor([-18.6082], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.14665725827217102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5825, 8.5119, 9.7532],
        [6.5825, 7.3692, 7.4552],
        [6.5825, 6.6129, 6.5871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.14927488565444946 
model_pd.l_d.mean(): -16.64473533630371 
model_pd.lagr.mean(): -16.495460510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0666], device='cuda:0')), ('power', tensor([0.9641], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2489], device='cuda:0')), ('power', tensor([-18.6266], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.14927488565444946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5365, 8.4506, 9.6821],
        [6.5365, 6.5667, 6.5411],
        [6.5365, 7.3170, 7.4023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.15322420001029968 
model_pd.l_d.mean(): -16.640531539916992 
model_pd.lagr.mean(): -16.487306594848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0679], device='cuda:0')), ('power', tensor([0.9632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2581], device='cuda:0')), ('power', tensor([-18.6520], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.15322420001029968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4790, 8.3739, 9.5932],
        [6.4790, 7.2517, 7.3362],
        [6.4790, 6.5088, 6.4835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.15870752930641174 
model_pd.l_d.mean(): -16.639415740966797 
model_pd.lagr.mean(): -16.480709075927734 
model_pd.lambdas: dict_items([('pout', tensor([1.0692], device='cuda:0')), ('power', tensor([0.9622], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2698], device='cuda:0')), ('power', tensor([-18.6835], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.15870752930641174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4112, 6.4407, 6.4157],
        [6.4112, 7.1748, 7.2584],
        [6.4112, 8.2837, 9.4886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.16608598828315735 
model_pd.l_d.mean(): -16.64059066772461 
model_pd.lagr.mean(): -16.474504470825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0704], device='cuda:0')), ('power', tensor([0.9613], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2836], device='cuda:0')), ('power', tensor([-18.7199], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.16608598828315735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3346, 8.1817, 9.3704],
        [6.3346, 6.3637, 6.3390],
        [6.3346, 7.0880, 7.1704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.17596380412578583 
model_pd.l_d.mean(): -16.64328956604004 
model_pd.lagr.mean(): -16.46732521057129 
model_pd.lambdas: dict_items([('pout', tensor([1.0717], device='cuda:0')), ('power', tensor([0.9604], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2995], device='cuda:0')), ('power', tensor([-18.7603], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.17596380412578583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2518, 6.2805, 6.2561],
        [6.2518, 6.9941, 7.0754],
        [6.2518, 8.0715, 9.2426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.1890956461429596 
model_pd.l_d.mean(): -16.646379470825195 
model_pd.lagr.mean(): -16.45728302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0731], device='cuda:0')), ('power', tensor([0.9594], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3168], device='cuda:0')), ('power', tensor([-18.8030], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.1890956461429596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1686, 7.9606, 9.1142],
        [6.1686, 6.1968, 6.1728],
        [6.1686, 6.8996, 6.9798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.20594748854637146 
model_pd.l_d.mean(): -16.648122787475586 
model_pd.lagr.mean(): -16.442174911499023 
model_pd.lambdas: dict_items([('pout', tensor([1.0744], device='cuda:0')), ('power', tensor([0.9585], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3345], device='cuda:0')), ('power', tensor([-18.8448], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.20594748854637146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0919, 6.1198, 6.0961],
        [6.0919, 6.8127, 6.8917],
        [6.0919, 7.8585, 8.9958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.22627946734428406 
model_pd.l_d.mean(): -16.646835327148438 
model_pd.lagr.mean(): -16.420555114746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0757], device='cuda:0')), ('power', tensor([0.9575], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3510], device='cuda:0')), ('power', tensor([-18.8823], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.22627946734428406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0306, 6.7431, 6.8213],
        [6.0306, 6.0582, 6.0348],
        [6.0306, 7.7768, 8.9011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.24754449725151062 
model_pd.l_d.mean(): -16.640899658203125 
model_pd.lagr.mean(): -16.393354415893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0771], device='cuda:0')), ('power', tensor([0.9566], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3643], device='cuda:0')), ('power', tensor([-18.9116], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.24754449725151062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.9924, 7.7258, 8.8419],
        [5.9924, 6.6997, 6.7773],
        [5.9924, 6.0197, 5.9965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.2640743851661682 
model_pd.l_d.mean(): -16.629322052001953 
model_pd.lagr.mean(): -16.36524772644043 
model_pd.lambdas: dict_items([('pout', tensor([1.0785], device='cuda:0')), ('power', tensor([0.9556], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3727], device='cuda:0')), ('power', tensor([-18.9296], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.2640743851661682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.9746, 7.7019, 8.8142],
        [5.9746, 6.6794, 6.7567],
        [5.9746, 6.0018, 5.9787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.2729310393333435 
model_pd.l_d.mean(): -16.613243103027344 
model_pd.lagr.mean(): -16.340312957763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0799], device='cuda:0')), ('power', tensor([0.9547], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3766], device='cuda:0')), ('power', tensor([-18.9379], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.2729310393333435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.9749, 6.6797, 6.7570],
        [5.9749, 6.0022, 5.9790],
        [5.9749, 7.7022, 8.8144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.2728080153465271 
model_pd.l_d.mean(): -16.59341049194336 
model_pd.lagr.mean(): -16.320602416992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0812], device='cuda:0')), ('power', tensor([0.9537], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3766], device='cuda:0')), ('power', tensor([-18.9379], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.2728080153465271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.9916, 6.0189, 5.9957],
        [5.9916, 7.7241, 8.8396],
        [5.9916, 6.6984, 6.7759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.26463180780410767 
model_pd.l_d.mean(): -16.570261001586914 
model_pd.lagr.mean(): -16.30562973022461 
model_pd.lambdas: dict_items([('pout', tensor([1.0826], device='cuda:0')), ('power', tensor([0.9528], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3729], device='cuda:0')), ('power', tensor([-18.9302], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.26463180780410767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0228, 7.7656, 8.8875],
        [6.0228, 6.7337, 6.8117],
        [6.0228, 6.0503, 6.0270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.2508929371833801 
model_pd.l_d.mean(): -16.54401206970215 
model_pd.lagr.mean(): -16.293119430541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0840], device='cuda:0')), ('power', tensor([0.9519], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3661], device='cuda:0')), ('power', tensor([-18.9157], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.2508929371833801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0674, 7.8246, 8.9558],
        [6.0674, 6.0951, 6.0716],
        [6.0674, 6.7841, 6.8626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.23438313603401184 
model_pd.l_d.mean(): -16.514728546142578 
model_pd.lagr.mean(): -16.280345916748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0853], device='cuda:0')), ('power', tensor([0.9509], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3564], device='cuda:0')), ('power', tensor([-18.8946], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.23438313603401184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1198, 7.8942, 9.0361],
        [6.1198, 6.8434, 6.9226],
        [6.1198, 6.1478, 6.1240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.218465194106102 
model_pd.l_d.mean(): -16.483360290527344 
model_pd.lagr.mean(): -16.264894485473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0867], device='cuda:0')), ('power', tensor([0.9500], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3451], device='cuda:0')), ('power', tensor([-18.8694], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.218465194106102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1723, 7.9638, 9.1165],
        [6.1723, 6.9027, 6.9826],
        [6.1723, 6.2005, 6.1766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.20534424483776093 
model_pd.l_d.mean(): -16.45154571533203 
model_pd.lagr.mean(): -16.246200561523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0880], device='cuda:0')), ('power', tensor([0.9490], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3338], device='cuda:0')), ('power', tensor([-18.8437], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.20534424483776093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2191, 6.9556, 7.0361],
        [6.2191, 6.2476, 6.2234],
        [6.2191, 8.0257, 9.1882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.19545666873455048 
model_pd.l_d.mean(): -16.42074203491211 
model_pd.lagr.mean(): -16.225284576416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0893], device='cuda:0')), ('power', tensor([0.9481], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3239], device='cuda:0')), ('power', tensor([-18.8205], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.19545666873455048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2563, 6.2849, 6.2606],
        [6.2563, 6.9976, 7.0786],
        [6.2563, 8.0750, 9.2450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.18856236338615417 
model_pd.l_d.mean(): -16.39206886291504 
model_pd.lagr.mean(): -16.203506469726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0906], device='cuda:0')), ('power', tensor([0.9471], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3161], device='cuda:0')), ('power', tensor([-18.8018], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.18856236338615417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2817, 6.3105, 6.2860],
        [6.2817, 7.0263, 7.1075],
        [6.2817, 8.1085, 9.2837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.18427042663097382 
model_pd.l_d.mean(): -16.366260528564453 
model_pd.lagr.mean(): -16.181989669799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0920], device='cuda:0')), ('power', tensor([0.9462], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3107], device='cuda:0')), ('power', tensor([-18.7889], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.18427042663097382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2945, 7.0407, 7.1221],
        [6.2945, 6.3233, 6.2989],
        [6.2945, 8.1254, 9.3030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.18222638964653015 
model_pd.l_d.mean(): -16.34366226196289 
model_pd.lagr.mean(): -16.161436080932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0933], device='cuda:0')), ('power', tensor([0.9453], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3081], device='cuda:0')), ('power', tensor([-18.7824], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.18222638964653015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2951, 7.0412, 7.1226],
        [6.2951, 8.1259, 9.3035],
        [6.2951, 6.3239, 6.2994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.1821567863225937 
model_pd.l_d.mean(): -16.32423973083496 
model_pd.lagr.mean(): -16.14208221435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0946], device='cuda:0')), ('power', tensor([0.9443], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3080], device='cuda:0')), ('power', tensor([-18.7822], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.1821567863225937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2846, 7.0291, 7.1103],
        [6.2846, 8.1117, 9.2869],
        [6.2846, 6.3134, 6.2889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.1838621348142624 
model_pd.l_d.mean(): -16.307659149169922 
model_pd.lagr.mean(): -16.123796463012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0959], device='cuda:0')), ('power', tensor([0.9434], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3102], device='cuda:0')), ('power', tensor([-18.7877], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.1838621348142624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2649, 6.2935, 6.2692],
        [6.2649, 7.0067, 7.0875],
        [6.2649, 8.0853, 9.2561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.18718326091766357 
model_pd.l_d.mean(): -16.293354034423828 
model_pd.lagr.mean(): -16.106170654296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0972], device='cuda:0')), ('power', tensor([0.9424], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3143], device='cuda:0')), ('power', tensor([-18.7979], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.18718326091766357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2384, 6.2669, 6.2427],
        [6.2384, 6.9765, 7.0569],
        [6.2384, 8.0498, 9.2148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.19195598363876343 
model_pd.l_d.mean(): -16.28061294555664 
model_pd.lagr.mean(): -16.08865737915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0985], device='cuda:0')), ('power', tensor([0.9415], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3199], device='cuda:0')), ('power', tensor([-18.8115], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.19195598363876343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2078, 8.0089, 9.1673],
        [6.2078, 6.2362, 6.2121],
        [6.2078, 6.9417, 7.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.19794222712516785 
model_pd.l_d.mean(): -16.268646240234375 
model_pd.lagr.mean(): -16.070703506469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0998], device='cuda:0')), ('power', tensor([0.9406], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3264], device='cuda:0')), ('power', tensor([-18.8270], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.19794222712516785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1764, 6.9059, 6.9854],
        [6.1764, 6.2045, 6.1806],
        [6.1764, 7.9668, 9.1185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.2047407031059265 
model_pd.l_d.mean(): -16.25664520263672 
model_pd.lagr.mean(): -16.051904678344727 
model_pd.lambdas: dict_items([('pout', tensor([1.1012], device='cuda:0')), ('power', tensor([0.9396], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3331], device='cuda:0')), ('power', tensor([-18.8428], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.2047407031059265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1473, 7.9279, 9.0732],
        [6.1473, 6.1753, 6.1515],
        [6.1473, 6.8728, 6.9519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.21169912815093994 
model_pd.l_d.mean(): -16.243846893310547 
model_pd.lagr.mean(): -16.032148361206055 
model_pd.lambdas: dict_items([('pout', tensor([1.1025], device='cuda:0')), ('power', tensor([0.9387], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3394], device='cuda:0')), ('power', tensor([-18.8572], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.21169912815093994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1238, 7.8964, 9.0365],
        [6.1238, 6.1517, 6.1280],
        [6.1238, 6.8460, 6.9247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.21789072453975677 
model_pd.l_d.mean(): -16.2296085357666 
model_pd.lagr.mean(): -16.01171875 
model_pd.lambdas: dict_items([('pout', tensor([1.1039], device='cuda:0')), ('power', tensor([0.9377], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3445], device='cuda:0')), ('power', tensor([-18.8689], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.21789072453975677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1084, 6.8284, 6.9068],
        [6.1084, 6.1362, 6.1126],
        [6.1084, 7.8756, 9.0123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.22226443886756897 
model_pd.l_d.mean(): -16.213457107543945 
model_pd.lagr.mean(): -15.991192817687988 
model_pd.lambdas: dict_items([('pout', tensor([1.1052], device='cuda:0')), ('power', tensor([0.9368], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3478], device='cuda:0')), ('power', tensor([-18.8765], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.22226443886756897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1026, 6.1304, 6.1068],
        [6.1026, 6.8217, 6.9000],
        [6.1026, 7.8677, 9.0029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.224000483751297 
model_pd.l_d.mean(): -16.19516372680664 
model_pd.lagr.mean(): -15.971162796020508 
model_pd.lambdas: dict_items([('pout', tensor([1.1066], device='cuda:0')), ('power', tensor([0.9359], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3491], device='cuda:0')), ('power', tensor([-18.8794], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.224000483751297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1066, 7.8727, 9.0085],
        [6.1066, 6.8260, 6.9043],
        [6.1066, 6.1343, 6.1107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.2228909581899643 
model_pd.l_d.mean(): -16.174772262573242 
model_pd.lagr.mean(): -15.951881408691406 
model_pd.lambdas: dict_items([('pout', tensor([1.1079], device='cuda:0')), ('power', tensor([0.9349], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3482], device='cuda:0')), ('power', tensor([-18.8776], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.2228909581899643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1189, 7.8888, 9.0269],
        [6.1189, 6.1467, 6.1231],
        [6.1189, 6.8398, 6.9182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.21943329274654388 
model_pd.l_d.mean(): -16.152578353881836 
model_pd.lagr.mean(): -15.933145523071289 
model_pd.lambdas: dict_items([('pout', tensor([1.1092], device='cuda:0')), ('power', tensor([0.9340], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3456], device='cuda:0')), ('power', tensor([-18.8718], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.21943329274654388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1372, 6.1651, 6.1413],
        [6.1372, 7.9128, 9.0544],
        [6.1372, 6.8603, 6.9389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.2145591378211975 
model_pd.l_d.mean(): -16.12908172607422 
model_pd.lagr.mean(): -15.914522171020508 
model_pd.lambdas: dict_items([('pout', tensor([1.1106], device='cuda:0')), ('power', tensor([0.9330], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3417], device='cuda:0')), ('power', tensor([-18.8630], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.2145591378211975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1585, 7.9409, 9.0868],
        [6.1585, 6.8843, 6.9632],
        [6.1585, 6.1865, 6.1627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.20924012362957 
model_pd.l_d.mean(): -16.104869842529297 
model_pd.lagr.mean(): -15.8956298828125 
model_pd.lambdas: dict_items([('pout', tensor([1.1119], device='cuda:0')), ('power', tensor([0.9321], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3371], device='cuda:0')), ('power', tensor([-18.8526], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.20924012362957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1802, 6.2083, 6.1844],
        [6.1802, 6.9086, 6.9878],
        [6.1802, 7.9693, 9.1195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.20423445105552673 
model_pd.l_d.mean(): -16.08055877685547 
model_pd.lagr.mean(): -15.876324653625488 
model_pd.lambdas: dict_items([('pout', tensor([1.1133], device='cuda:0')), ('power', tensor([0.9311], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3325], device='cuda:0')), ('power', tensor([-18.8420], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.20423445105552673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1997, 6.9306, 7.0099],
        [6.1997, 7.9950, 9.1490],
        [6.1997, 6.2279, 6.2039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.2000223845243454 
model_pd.l_d.mean(): -16.05669403076172 
model_pd.lagr.mean(): -15.856671333312988 
model_pd.lambdas: dict_items([('pout', tensor([1.1146], device='cuda:0')), ('power', tensor([0.9302], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3284], device='cuda:0')), ('power', tensor([-18.8324], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.2000223845243454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2153, 6.9480, 7.0275],
        [6.2153, 6.2435, 6.2195],
        [6.2153, 8.0154, 9.1723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.19685238599777222 
model_pd.l_d.mean(): -16.03371810913086 
model_pd.lagr.mean(): -15.836865425109863 
model_pd.lambdas: dict_items([('pout', tensor([1.1159], device='cuda:0')), ('power', tensor([0.9293], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3251], device='cuda:0')), ('power', tensor([-18.8248], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.19685238599777222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2258, 6.2541, 6.2300],
        [6.2258, 6.9597, 7.0393],
        [6.2258, 8.0290, 9.1879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.19481289386749268 
model_pd.l_d.mean(): -16.01191520690918 
model_pd.lagr.mean(): -15.817102432250977 
model_pd.lambdas: dict_items([('pout', tensor([1.1172], device='cuda:0')), ('power', tensor([0.9283], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3229], device='cuda:0')), ('power', tensor([-18.8196], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.19481289386749268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2307, 6.9651, 7.0447],
        [6.2307, 8.0353, 9.1949],
        [6.2307, 6.2590, 6.2350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.19389596581459045 
model_pd.l_d.mean(): -15.991413116455078 
model_pd.lagr.mean(): -15.797516822814941 
model_pd.lambdas: dict_items([('pout', tensor([1.1186], device='cuda:0')), ('power', tensor([0.9274], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3219], device='cuda:0')), ('power', tensor([-18.8173], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.19389596581459045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2302, 8.0343, 9.1935],
        [6.2302, 6.9643, 7.0439],
        [6.2302, 6.2585, 6.2344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.19402974843978882 
model_pd.l_d.mean(): -15.97218132019043 
model_pd.lagr.mean(): -15.778151512145996 
model_pd.lambdas: dict_items([('pout', tensor([1.1199], device='cuda:0')), ('power', tensor([0.9264], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3220], device='cuda:0')), ('power', tensor([-18.8177], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.19402974843978882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2249, 8.0269, 9.1848],
        [6.2249, 6.2531, 6.2291],
        [6.2249, 6.9581, 7.0375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.19509653747081757 
model_pd.l_d.mean(): -15.954056739807129 
model_pd.lagr.mean(): -15.758959770202637 
model_pd.lambdas: dict_items([('pout', tensor([1.1212], device='cuda:0')), ('power', tensor([0.9255], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3232], device='cuda:0')), ('power', tensor([-18.8205], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.19509653747081757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2158, 8.0145, 9.1702],
        [6.2158, 6.9477, 7.0269],
        [6.2158, 6.2440, 6.2201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.19693312048912048 
model_pd.l_d.mean(): -15.936771392822266 
model_pd.lagr.mean(): -15.739838600158691 
model_pd.lambdas: dict_items([('pout', tensor([1.1225], device='cuda:0')), ('power', tensor([0.9245], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3251], device='cuda:0')), ('power', tensor([-18.8252], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.19693312048912048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2044, 6.9345, 7.0135],
        [6.2044, 7.9990, 9.1520],
        [6.2044, 6.2325, 6.2086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.19932445883750916 
model_pd.l_d.mean(): -15.919989585876465 
model_pd.lagr.mean(): -15.720664978027344 
model_pd.lambdas: dict_items([('pout', tensor([1.1239], device='cuda:0')), ('power', tensor([0.9236], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3276], device='cuda:0')), ('power', tensor([-18.8311], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.19932445883750916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1920, 6.2200, 6.1962],
        [6.1920, 6.9203, 6.9991],
        [6.1920, 7.9822, 9.1324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.20199927687644958 
model_pd.l_d.mean(): -15.903355598449707 
model_pd.lagr.mean(): -15.701355934143066 
model_pd.lambdas: dict_items([('pout', tensor([1.1252], device='cuda:0')), ('power', tensor([0.9227], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3302], device='cuda:0')), ('power', tensor([-18.8374], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.20199927687644958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1803, 6.2083, 6.1845],
        [6.1803, 7.9664, 9.1138],
        [6.1803, 6.9068, 6.9854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.20463737845420837 
model_pd.l_d.mean(): -15.886523246765137 
model_pd.lagr.mean(): -15.681885719299316 
model_pd.lambdas: dict_items([('pout', tensor([1.1265], device='cuda:0')), ('power', tensor([0.9217], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3328], device='cuda:0')), ('power', tensor([-18.8434], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.20463737845420837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1707, 7.9533, 9.0984],
        [6.1707, 6.8957, 6.9741],
        [6.1707, 6.1986, 6.1749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.2068982720375061 
model_pd.l_d.mean(): -15.869192123413086 
model_pd.lagr.mean(): -15.662293434143066 
model_pd.lambdas: dict_items([('pout', tensor([1.1279], device='cuda:0')), ('power', tensor([0.9208], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3348], device='cuda:0')), ('power', tensor([-18.8483], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.2068982720375061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1642, 6.8882, 6.9664],
        [6.1642, 6.1921, 6.1684],
        [6.1642, 7.9443, 9.0878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.20847640931606293 
model_pd.l_d.mean(): -15.851149559020996 
model_pd.lagr.mean(): -15.64267349243164 
model_pd.lambdas: dict_items([('pout', tensor([1.1292], device='cuda:0')), ('power', tensor([0.9198], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3362], device='cuda:0')), ('power', tensor([-18.8517], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.20847640931606293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1616, 7.9405, 9.0831],
        [6.1616, 6.1894, 6.1657],
        [6.1616, 6.8850, 6.9631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.20916418731212616 
model_pd.l_d.mean(): -15.83227252960205 
model_pd.lagr.mean(): -15.62310791015625 
model_pd.lambdas: dict_items([('pout', tensor([1.1305], device='cuda:0')), ('power', tensor([0.9189], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3368], device='cuda:0')), ('power', tensor([-18.8531], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.20916418731212616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1628, 7.9418, 9.0844],
        [6.1628, 6.1907, 6.1670],
        [6.1628, 6.8862, 6.9643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.20890623331069946 
model_pd.l_d.mean(): -15.812553405761719 
model_pd.lagr.mean(): -15.603647232055664 
model_pd.lambdas: dict_items([('pout', tensor([1.1319], device='cuda:0')), ('power', tensor([0.9180], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3366], device='cuda:0')), ('power', tensor([-18.8527], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.20890623331069946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1677, 6.1955, 6.1718],
        [6.1677, 7.9479, 9.0912],
        [6.1677, 6.8915, 6.9696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.2078070491552353 
model_pd.l_d.mean(): -15.792086601257324 
model_pd.lagr.mean(): -15.584280014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.1332], device='cuda:0')), ('power', tensor([0.9170], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3356], device='cuda:0')), ('power', tensor([-18.8504], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.2078070491552353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1752, 7.9575, 9.1021],
        [6.1752, 6.2031, 6.1794],
        [6.1752, 6.8998, 6.9779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.20609483122825623 
model_pd.l_d.mean(): -15.77104377746582 
model_pd.lagr.mean(): -15.564949035644531 
model_pd.lambdas: dict_items([('pout', tensor([1.1345], device='cuda:0')), ('power', tensor([0.9161], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3340], device='cuda:0')), ('power', tensor([-18.8469], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.20609483122825623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1843, 7.9694, 9.1155],
        [6.1843, 6.2123, 6.1885],
        [6.1843, 6.9099, 6.9881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.2040565013885498 
model_pd.l_d.mean(): -15.74966049194336 
model_pd.lagr.mean(): -15.54560375213623 
model_pd.lambdas: dict_items([('pout', tensor([1.1359], device='cuda:0')), ('power', tensor([0.9151], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3321], device='cuda:0')), ('power', tensor([-18.8425], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.2040565013885498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1940, 6.2219, 6.1982],
        [6.1940, 6.9206, 6.9989],
        [6.1940, 7.9818, 9.1297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.20197546482086182 
model_pd.l_d.mean(): -15.728179931640625 
model_pd.lagr.mean(): -15.526204109191895 
model_pd.lambdas: dict_items([('pout', tensor([1.1372], device='cuda:0')), ('power', tensor([0.9142], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3301], device='cuda:0')), ('power', tensor([-18.8378], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.20197546482086182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2030, 6.9306, 7.0089],
        [6.2030, 7.9934, 9.1429],
        [6.2030, 6.2310, 6.2072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.20008671283721924 
model_pd.l_d.mean(): -15.706836700439453 
model_pd.lagr.mean(): -15.506750106811523 
model_pd.lambdas: dict_items([('pout', tensor([1.1385], device='cuda:0')), ('power', tensor([0.9132], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3282], device='cuda:0')), ('power', tensor([-18.8335], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.20008671283721924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2105, 8.0031, 9.1538],
        [6.2105, 6.2385, 6.2147],
        [6.2105, 6.9389, 7.0173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.19856053590774536 
model_pd.l_d.mean(): -15.685824394226074 
model_pd.lagr.mean(): -15.487263679504395 
model_pd.lambdas: dict_items([('pout', tensor([1.1398], device='cuda:0')), ('power', tensor([0.9123], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3266], device='cuda:0')), ('power', tensor([-18.8298], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.19856053590774536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2159, 8.0099, 9.1614],
        [6.2159, 6.9448, 7.0232],
        [6.2159, 6.2440, 6.2201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.19749665260314941 
model_pd.l_d.mean(): -15.665277481079102 
model_pd.lagr.mean(): -15.467781066894531 
model_pd.lambdas: dict_items([('pout', tensor([1.1412], device='cuda:0')), ('power', tensor([0.9114], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3255], device='cuda:0')), ('power', tensor([-18.8273], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.19749665260314941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2189, 6.2470, 6.2231],
        [6.2189, 8.0135, 9.1653],
        [6.2189, 6.9480, 7.0263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1969350427389145 
model_pd.l_d.mean(): -15.645261764526367 
model_pd.lagr.mean(): -15.44832706451416 
model_pd.lambdas: dict_items([('pout', tensor([1.1425], device='cuda:0')), ('power', tensor([0.9104], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3249], device='cuda:0')), ('power', tensor([-18.8259], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1969350427389145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2195, 6.9484, 7.0267],
        [6.2195, 8.0140, 9.1655],
        [6.2195, 6.2475, 6.2237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.19686228036880493 
model_pd.l_d.mean(): -15.625777244567871 
model_pd.lagr.mean(): -15.428915023803711 
model_pd.lambdas: dict_items([('pout', tensor([1.1438], device='cuda:0')), ('power', tensor([0.9095], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3248], device='cuda:0')), ('power', tensor([-18.8258], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.19686228036880493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2180, 6.2459, 6.2221],
        [6.2180, 6.9464, 7.0246],
        [6.2180, 8.0115, 9.1625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.19722190499305725 
model_pd.l_d.mean(): -15.606755256652832 
model_pd.lagr.mean(): -15.409533500671387 
model_pd.lambdas: dict_items([('pout', tensor([1.1451], device='cuda:0')), ('power', tensor([0.9085], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3251], device='cuda:0')), ('power', tensor([-18.8268], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.19722190499305725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2147, 6.2427, 6.2189],
        [6.2147, 8.0069, 9.1568],
        [6.2147, 6.9426, 7.0207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.19792096316814423 
model_pd.l_d.mean(): -15.588082313537598 
model_pd.lagr.mean(): -15.390161514282227 
model_pd.lambdas: dict_items([('pout', tensor([1.1465], device='cuda:0')), ('power', tensor([0.9076], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3258], device='cuda:0')), ('power', tensor([-18.8285], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.19792096316814423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2105, 8.0009, 9.1496],
        [6.2105, 6.9376, 7.0155],
        [6.2105, 6.2384, 6.2147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.19883732497692108 
model_pd.l_d.mean(): -15.569616317749023 
model_pd.lagr.mean(): -15.370779037475586 
model_pd.lambdas: dict_items([('pout', tensor([1.1478], device='cuda:0')), ('power', tensor([0.9067], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3268], device='cuda:0')), ('power', tensor([-18.8308], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.19883732497692108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2060, 7.9945, 9.1420],
        [6.2060, 6.9322, 7.0100],
        [6.2060, 6.2339, 6.2102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.1998310536146164 
model_pd.l_d.mean(): -15.551202774047852 
model_pd.lagr.mean(): -15.351371765136719 
model_pd.lambdas: dict_items([('pout', tensor([1.1491], device='cuda:0')), ('power', tensor([0.9057], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3278], device='cuda:0')), ('power', tensor([-18.8333], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.1998310536146164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2018, 6.9273, 7.0050],
        [6.2018, 6.2297, 6.2060],
        [6.2018, 7.9886, 9.1349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.20075684785842896 
model_pd.l_d.mean(): -15.532693862915039 
model_pd.lagr.mean(): -15.331936836242676 
model_pd.lambdas: dict_items([('pout', tensor([1.1505], device='cuda:0')), ('power', tensor([0.9048], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3287], device='cuda:0')), ('power', tensor([-18.8355], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.20075684785842896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1987, 7.9841, 9.1294],
        [6.1987, 6.9235, 7.0011],
        [6.1987, 6.2265, 6.2029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.20148198306560516 
model_pd.l_d.mean(): -15.51396369934082 
model_pd.lagr.mean(): -15.312481880187988 
model_pd.lambdas: dict_items([('pout', tensor([1.1518], device='cuda:0')), ('power', tensor([0.9038], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3294], device='cuda:0')), ('power', tensor([-18.8373], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.20148198306560516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1970, 7.9814, 9.1260],
        [6.1970, 6.9213, 6.9988],
        [6.1970, 6.2248, 6.2011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.20190629363059998 
model_pd.l_d.mean(): -15.494935035705566 
model_pd.lagr.mean(): -15.293028831481934 
model_pd.lambdas: dict_items([('pout', tensor([1.1531], device='cuda:0')), ('power', tensor([0.9029], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3298], device='cuda:0')), ('power', tensor([-18.8383], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.20190629363059998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1969, 6.2247, 6.2010],
        [6.1969, 7.9809, 9.1252],
        [6.1969, 6.9210, 6.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.20197531580924988 
model_pd.l_d.mean(): -15.475560188293457 
model_pd.lagr.mean(): -15.273585319519043 
model_pd.lambdas: dict_items([('pout', tensor([1.1544], device='cuda:0')), ('power', tensor([0.9019], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3298], device='cuda:0')), ('power', tensor([-18.8385], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.20197531580924988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1984, 7.9826, 9.1268],
        [6.1984, 6.2262, 6.2026],
        [6.1984, 6.9225, 6.9999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.20168940722942352 
model_pd.l_d.mean(): -15.455848693847656 
model_pd.lagr.mean(): -15.254158973693848 
model_pd.lambdas: dict_items([('pout', tensor([1.1558], device='cuda:0')), ('power', tensor([0.9010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3295], device='cuda:0')), ('power', tensor([-18.8379], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.20168940722942352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2014, 6.9256, 7.0030],
        [6.2014, 7.9862, 9.1307],
        [6.2014, 6.2292, 6.2056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.20109856128692627 
model_pd.l_d.mean(): -15.435848236083984 
model_pd.lagr.mean(): -15.234749794006348 
model_pd.lambdas: dict_items([('pout', tensor([1.1571], device='cuda:0')), ('power', tensor([0.9001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3289], device='cuda:0')), ('power', tensor([-18.8366], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.20109856128692627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2054, 7.9911, 9.1361],
        [6.2054, 6.9299, 7.0073],
        [6.2054, 6.2332, 6.2096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.20029056072235107 
model_pd.l_d.mean(): -15.415637969970703 
model_pd.lagr.mean(): -15.215347290039062 
model_pd.lambdas: dict_items([('pout', tensor([1.1584], device='cuda:0')), ('power', tensor([0.8991], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3281], device='cuda:0')), ('power', tensor([-18.8347], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.20029056072235107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2100, 7.9968, 9.1424],
        [6.2100, 6.2378, 6.2142],
        [6.2100, 6.9349, 7.0122]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.19937199354171753 
model_pd.l_d.mean(): -15.39531421661377 
model_pd.lagr.mean(): -15.195941925048828 
model_pd.lambdas: dict_items([('pout', tensor([1.1598], device='cuda:0')), ('power', tensor([0.8982], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3271], device='cuda:0')), ('power', tensor([-18.8326], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.19937199354171753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2147, 6.2425, 6.2189],
        [6.2147, 6.9399, 7.0173],
        [6.2147, 8.0027, 9.1489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.19844965636730194 
model_pd.l_d.mean(): -15.374979019165039 
model_pd.lagr.mean(): -15.176528930664062 
model_pd.lambdas: dict_items([('pout', tensor([1.1611], device='cuda:0')), ('power', tensor([0.8972], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3262], device='cuda:0')), ('power', tensor([-18.8304], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.19844965636730194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2190, 8.0080, 9.1548],
        [6.2190, 6.2468, 6.2232],
        [6.2190, 6.9446, 7.0219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.19761693477630615 
model_pd.l_d.mean(): -15.354729652404785 
model_pd.lagr.mean(): -15.157113075256348 
model_pd.lambdas: dict_items([('pout', tensor([1.1624], device='cuda:0')), ('power', tensor([0.8963], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3253], device='cuda:0')), ('power', tensor([-18.8284], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.19761693477630615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2226, 6.2504, 6.2267],
        [6.2226, 6.9483, 7.0256],
        [6.2226, 8.0123, 9.1595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.19694392383098602 
model_pd.l_d.mean(): -15.334640502929688 
model_pd.lagr.mean(): -15.137696266174316 
model_pd.lambdas: dict_items([('pout', tensor([1.1637], device='cuda:0')), ('power', tensor([0.8954], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3246], device='cuda:0')), ('power', tensor([-18.8268], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.19694392383098602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2252, 6.9510, 7.0283],
        [6.2252, 6.2530, 6.2293],
        [6.2252, 8.0154, 9.1627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.19647231698036194 
model_pd.l_d.mean(): -15.314754486083984 
model_pd.lagr.mean(): -15.118282318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.1651], device='cuda:0')), ('power', tensor([0.8944], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3241], device='cuda:0')), ('power', tensor([-18.8257], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.19647231698036194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2267, 6.9525, 7.0297],
        [6.2267, 8.0170, 9.1643],
        [6.2267, 6.2545, 6.2308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.1962166428565979 
model_pd.l_d.mean(): -15.29509449005127 
model_pd.lagr.mean(): -15.098877906799316 
model_pd.lambdas: dict_items([('pout', tensor([1.1664], device='cuda:0')), ('power', tensor([0.8935], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3238], device='cuda:0')), ('power', tensor([-18.8251], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.1962166428565979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2272, 6.9528, 7.0300],
        [6.2272, 6.2550, 6.2314],
        [6.2272, 8.0173, 9.1643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.19616501033306122 
model_pd.l_d.mean(): -15.275643348693848 
model_pd.lagr.mean(): -15.07947826385498 
model_pd.lambdas: dict_items([('pout', tensor([1.1677], device='cuda:0')), ('power', tensor([0.8925], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3237], device='cuda:0')), ('power', tensor([-18.8250], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.19616501033306122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2268, 6.9522, 7.0292],
        [6.2268, 6.2546, 6.2310],
        [6.2268, 8.0164, 9.1630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.196283221244812 
model_pd.l_d.mean(): -15.256367683410645 
model_pd.lagr.mean(): -15.060084342956543 
model_pd.lambdas: dict_items([('pout', tensor([1.1690], device='cuda:0')), ('power', tensor([0.8916], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3238], device='cuda:0')), ('power', tensor([-18.8254], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.196283221244812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2259, 6.2537, 6.2301],
        [6.2259, 6.9509, 7.0278],
        [6.2259, 8.0147, 9.1608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.19652090966701508 
model_pd.l_d.mean(): -15.237215042114258 
model_pd.lagr.mean(): -15.040694236755371 
model_pd.lambdas: dict_items([('pout', tensor([1.1704], device='cuda:0')), ('power', tensor([0.8906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3240], device='cuda:0')), ('power', tensor([-18.8261], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.19652090966701508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2247, 6.9492, 7.0261],
        [6.2247, 8.0127, 9.1581],
        [6.2247, 6.2524, 6.2288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.19681653380393982 
model_pd.l_d.mean(): -15.218114852905273 
model_pd.lagr.mean(): -15.0212984085083 
model_pd.lambdas: dict_items([('pout', tensor([1.1717], device='cuda:0')), ('power', tensor([0.8897], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3243], device='cuda:0')), ('power', tensor([-18.8269], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.19681653380393982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2235, 6.9476, 7.0244],
        [6.2235, 6.2512, 6.2276],
        [6.2235, 8.0107, 9.1555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.19710668921470642 
model_pd.l_d.mean(): -15.199004173278809 
model_pd.lagr.mean(): -15.001897811889648 
model_pd.lambdas: dict_items([('pout', tensor([1.1730], device='cuda:0')), ('power', tensor([0.8888], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3246], device='cuda:0')), ('power', tensor([-18.8276], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.19710668921470642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2226, 8.0092, 9.1534],
        [6.2226, 6.2503, 6.2268],
        [6.2226, 6.9464, 7.0231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.19733232259750366 
model_pd.l_d.mean(): -15.179825782775879 
model_pd.lagr.mean(): -14.98249340057373 
model_pd.lambdas: dict_items([('pout', tensor([1.1743], device='cuda:0')), ('power', tensor([0.8878], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3248], device='cuda:0')), ('power', tensor([-18.8283], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.19733232259750366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2223, 8.0083, 9.1522],
        [6.2223, 6.9458, 7.0224],
        [6.2223, 6.2500, 6.2264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.19744740426540375 
model_pd.l_d.mean(): -15.160537719726562 
model_pd.lagr.mean(): -14.963089942932129 
model_pd.lambdas: dict_items([('pout', tensor([1.1757], device='cuda:0')), ('power', tensor([0.8869], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3249], device='cuda:0')), ('power', tensor([-18.8286], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.19744740426540375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2227, 8.0084, 9.1519],
        [6.2227, 6.9459, 7.0225],
        [6.2227, 6.2503, 6.2268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.19742363691329956 
model_pd.l_d.mean(): -15.141111373901367 
model_pd.lagr.mean(): -14.943687438964844 
model_pd.lambdas: dict_items([('pout', tensor([1.1770], device='cuda:0')), ('power', tensor([0.8859], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3249], device='cuda:0')), ('power', tensor([-18.8286], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.19742363691329956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2238, 6.9469, 7.0234],
        [6.2238, 6.2514, 6.2279],
        [6.2238, 8.0094, 9.1528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.19725452363491058 
model_pd.l_d.mean(): -15.121542930603027 
model_pd.lagr.mean(): -14.924288749694824 
model_pd.lambdas: dict_items([('pout', tensor([1.1783], device='cuda:0')), ('power', tensor([0.8850], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3247], device='cuda:0')), ('power', tensor([-18.8283], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.19725452363491058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2255, 6.9486, 7.0250],
        [6.2255, 8.0114, 9.1547],
        [6.2255, 6.2532, 6.2297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.1969524621963501 
model_pd.l_d.mean(): -15.101847648620605 
model_pd.lagr.mean(): -14.904894828796387 
model_pd.lambdas: dict_items([('pout', tensor([1.1796], device='cuda:0')), ('power', tensor([0.8841], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3243], device='cuda:0')), ('power', tensor([-18.8276], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.1969524621963501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2278, 6.2554, 6.2319],
        [6.2278, 6.9509, 7.0273],
        [6.2278, 8.0139, 9.1574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.1965482085943222 
model_pd.l_d.mean(): -15.082051277160645 
model_pd.lagr.mean(): -14.885502815246582 
model_pd.lambdas: dict_items([('pout', tensor([1.1810], device='cuda:0')), ('power', tensor([0.8831], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3239], device='cuda:0')), ('power', tensor([-18.8266], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.1965482085943222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2304, 8.0170, 9.1606],
        [6.2304, 6.9536, 7.0299],
        [6.2304, 6.2580, 6.2345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.19608190655708313 
model_pd.l_d.mean(): -15.062196731567383 
model_pd.lagr.mean(): -14.866114616394043 
model_pd.lambdas: dict_items([('pout', tensor([1.1823], device='cuda:0')), ('power', tensor([0.8822], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3234], device='cuda:0')), ('power', tensor([-18.8255], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.19608190655708313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2331, 8.0201, 9.1639],
        [6.2331, 6.9564, 7.0327],
        [6.2331, 6.2607, 6.2372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.19559764862060547 
model_pd.l_d.mean(): -15.042325019836426 
model_pd.lagr.mean(): -14.84672737121582 
model_pd.lambdas: dict_items([('pout', tensor([1.1836], device='cuda:0')), ('power', tensor([0.8812], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3228], device='cuda:0')), ('power', tensor([-18.8243], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.19559764862060547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2357, 8.0232, 9.1671],
        [6.2357, 6.9591, 7.0353],
        [6.2357, 6.2633, 6.2398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.19513742625713348 
model_pd.l_d.mean(): -15.022477149963379 
model_pd.lagr.mean(): -14.827340126037598 
model_pd.lambdas: dict_items([('pout', tensor([1.1849], device='cuda:0')), ('power', tensor([0.8803], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3223], device='cuda:0')), ('power', tensor([-18.8232], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.19513742625713348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2380, 6.2657, 6.2422],
        [6.2380, 8.0258, 9.1698],
        [6.2380, 6.9614, 7.0376]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.19473545253276825 
model_pd.l_d.mean(): -15.002690315246582 
model_pd.lagr.mean(): -14.807954788208008 
model_pd.lambdas: dict_items([('pout', tensor([1.1862], device='cuda:0')), ('power', tensor([0.8793], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3219], device='cuda:0')), ('power', tensor([-18.8222], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.19473545253276825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2399, 6.2676, 6.2441],
        [6.2399, 6.9633, 7.0394],
        [6.2399, 8.0279, 9.1719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.1944139003753662 
model_pd.l_d.mean(): -14.982989311218262 
model_pd.lagr.mean(): -14.788575172424316 
model_pd.lambdas: dict_items([('pout', tensor([1.1876], device='cuda:0')), ('power', tensor([0.8784], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3215], device='cuda:0')), ('power', tensor([-18.8214], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.1944139003753662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2414, 6.2690, 6.2455],
        [6.2414, 8.0294, 9.1733],
        [6.2414, 6.9647, 7.0408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.19418323040008545 
model_pd.l_d.mean(): -14.963376998901367 
model_pd.lagr.mean(): -14.769193649291992 
model_pd.lambdas: dict_items([('pout', tensor([1.1889], device='cuda:0')), ('power', tensor([0.8775], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3212], device='cuda:0')), ('power', tensor([-18.8209], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.19418323040008545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2424, 6.9655, 7.0415],
        [6.2424, 6.2700, 6.2465],
        [6.2424, 8.0303, 9.1741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.1940414160490036 
model_pd.l_d.mean(): -14.94385814666748 
model_pd.lagr.mean(): -14.74981689453125 
model_pd.lambdas: dict_items([('pout', tensor([1.1902], device='cuda:0')), ('power', tensor([0.8765], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3210], device='cuda:0')), ('power', tensor([-18.8206], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.1940414160490036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2430, 6.9660, 7.0419],
        [6.2430, 8.0307, 9.1742],
        [6.2430, 6.2706, 6.2471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.19397544860839844 
model_pd.l_d.mean(): -14.924418449401855 
model_pd.lagr.mean(): -14.730443000793457 
model_pd.lambdas: dict_items([('pout', tensor([1.1915], device='cuda:0')), ('power', tensor([0.8756], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([-18.8205], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.19397544860839844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2433, 8.0307, 9.1738],
        [6.2433, 6.2709, 6.2475],
        [6.2433, 6.9661, 7.0419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.19396328926086426 
model_pd.l_d.mean(): -14.905030250549316 
model_pd.lagr.mean(): -14.711067199707031 
model_pd.lambdas: dict_items([('pout', tensor([1.1928], device='cuda:0')), ('power', tensor([0.8746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([-18.8205], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.19396328926086426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2435, 6.2711, 6.2477],
        [6.2435, 8.0305, 9.1733],
        [6.2435, 6.9660, 7.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.19397900998592377 
model_pd.l_d.mean(): -14.885675430297852 
model_pd.lagr.mean(): -14.691696166992188 
model_pd.lambdas: dict_items([('pout', tensor([1.1942], device='cuda:0')), ('power', tensor([0.8737], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([-18.8206], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.19397900998592377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2437, 6.9659, 7.0416],
        [6.2437, 6.2713, 6.2478],
        [6.2437, 8.0303, 9.1727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.1939942091703415 
model_pd.l_d.mean(): -14.866314888000488 
model_pd.lagr.mean(): -14.672320365905762 
model_pd.lambdas: dict_items([('pout', tensor([1.1955], device='cuda:0')), ('power', tensor([0.8728], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([-18.8207], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.1939942091703415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2440, 8.0303, 9.1724],
        [6.2440, 6.2716, 6.2482],
        [6.2440, 6.9660, 7.0416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.1939837634563446 
model_pd.l_d.mean(): -14.846931457519531 
model_pd.lagr.mean(): -14.652947425842285 
model_pd.lambdas: dict_items([('pout', tensor([1.1968], device='cuda:0')), ('power', tensor([0.8718], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3209], device='cuda:0')), ('power', tensor([-18.8207], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.1939837634563446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2446, 6.9664, 7.0419],
        [6.2446, 8.0306, 9.1724],
        [6.2446, 6.2721, 6.2487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.1939271092414856 
model_pd.l_d.mean(): -14.827503204345703 
model_pd.lagr.mean(): -14.633576393127441 
model_pd.lambdas: dict_items([('pout', tensor([1.1981], device='cuda:0')), ('power', tensor([0.8709], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3208], device='cuda:0')), ('power', tensor([-18.8207], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.1939271092414856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2455, 6.2730, 6.2496],
        [6.2455, 6.9671, 7.0425],
        [6.2455, 8.0313, 9.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.19381241500377655 
model_pd.l_d.mean(): -14.808019638061523 
model_pd.lagr.mean(): -14.61420726776123 
model_pd.lambdas: dict_items([('pout', tensor([1.1995], device='cuda:0')), ('power', tensor([0.8699], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3206], device='cuda:0')), ('power', tensor([-18.8204], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.19381241500377655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2467, 6.2742, 6.2508],
        [6.2467, 6.9681, 7.0435],
        [6.2467, 8.0324, 9.1738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.19363725185394287 
model_pd.l_d.mean(): -14.788473129272461 
model_pd.lagr.mean(): -14.594836235046387 
model_pd.lambdas: dict_items([('pout', tensor([1.2008], device='cuda:0')), ('power', tensor([0.8690], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3204], device='cuda:0')), ('power', tensor([-18.8200], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.19363725185394287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2482, 8.0340, 9.1752],
        [6.2482, 6.2757, 6.2523],
        [6.2482, 6.9695, 7.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.1934061497449875 
model_pd.l_d.mean(): -14.768877029418945 
model_pd.lagr.mean(): -14.575470924377441 
model_pd.lambdas: dict_items([('pout', tensor([1.2021], device='cuda:0')), ('power', tensor([0.8681], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3201], device='cuda:0')), ('power', tensor([-18.8195], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.1934061497449875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2499, 6.9712, 7.0464],
        [6.2499, 8.0358, 9.1770],
        [6.2499, 6.2774, 6.2540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.19313201308250427 
model_pd.l_d.mean(): -14.749238967895508 
model_pd.lagr.mean(): -14.556106567382812 
model_pd.lambdas: dict_items([('pout', tensor([1.2034], device='cuda:0')), ('power', tensor([0.8671], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3198], device='cuda:0')), ('power', tensor([-18.8188], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.19313201308250427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2518, 6.2792, 6.2559],
        [6.2518, 6.9730, 7.0482],
        [6.2518, 8.0378, 9.1790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.19283199310302734 
model_pd.l_d.mean(): -14.729578018188477 
model_pd.lagr.mean(): -14.53674602508545 
model_pd.lambdas: dict_items([('pout', tensor([1.2047], device='cuda:0')), ('power', tensor([0.8662], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3195], device='cuda:0')), ('power', tensor([-18.8181], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.19283199310302734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2537, 8.0399, 9.1810],
        [6.2537, 6.9749, 7.0500],
        [6.2537, 6.2811, 6.2578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.19252490997314453 
model_pd.l_d.mean(): -14.709908485412598 
model_pd.lagr.mean(): -14.517383575439453 
model_pd.lambdas: dict_items([('pout', tensor([1.2061], device='cuda:0')), ('power', tensor([0.8652], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3191], device='cuda:0')), ('power', tensor([-18.8173], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.19252490997314453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2555, 8.0419, 9.1830],
        [6.2555, 6.2830, 6.2596],
        [6.2555, 6.9767, 7.0517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.19222846627235413 
model_pd.l_d.mean(): -14.690255165100098 
model_pd.lagr.mean(): -14.498026847839355 
model_pd.lambdas: dict_items([('pout', tensor([1.2074], device='cuda:0')), ('power', tensor([0.8643], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3187], device='cuda:0')), ('power', tensor([-18.8166], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.19222846627235413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2573, 8.0437, 9.1847],
        [6.2573, 6.2847, 6.2614],
        [6.2573, 6.9784, 7.0533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.19195781648159027 
model_pd.l_d.mean(): -14.67063045501709 
model_pd.lagr.mean(): -14.478672981262207 
model_pd.lambdas: dict_items([('pout', tensor([1.2087], device='cuda:0')), ('power', tensor([0.8634], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3184], device='cuda:0')), ('power', tensor([-18.8159], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.19195781648159027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2588, 8.0453, 9.1862],
        [6.2588, 6.9798, 7.0547],
        [6.2588, 6.2863, 6.2629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.19172191619873047 
model_pd.l_d.mean(): -14.651037216186523 
model_pd.lagr.mean(): -14.459315299987793 
model_pd.lambdas: dict_items([('pout', tensor([1.2100], device='cuda:0')), ('power', tensor([0.8624], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3181], device='cuda:0')), ('power', tensor([-18.8153], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.19172191619873047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2602, 6.9810, 7.0559],
        [6.2602, 8.0466, 9.1874],
        [6.2602, 6.2876, 6.2643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.19152586162090302 
model_pd.l_d.mean(): -14.631488800048828 
model_pd.lagr.mean(): -14.439963340759277 
model_pd.lambdas: dict_items([('pout', tensor([1.2113], device='cuda:0')), ('power', tensor([0.8615], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3179], device='cuda:0')), ('power', tensor([-18.8148], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.19152586162090302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2613, 8.0477, 9.1883],
        [6.2613, 6.2887, 6.2654],
        [6.2613, 6.9820, 7.0568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.19136732816696167 
model_pd.l_d.mean(): -14.611980438232422 
model_pd.lagr.mean(): -14.420613288879395 
model_pd.lambdas: dict_items([('pout', tensor([1.2126], device='cuda:0')), ('power', tensor([0.8605], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3177], device='cuda:0')), ('power', tensor([-18.8145], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.19136732816696167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2623, 6.2897, 6.2664],
        [6.2623, 8.0485, 9.1889],
        [6.2623, 6.9828, 7.0575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.19123977422714233 
model_pd.l_d.mean(): -14.59250259399414 
model_pd.lagr.mean(): -14.401263236999512 
model_pd.lambdas: dict_items([('pout', tensor([1.2140], device='cuda:0')), ('power', tensor([0.8596], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3175], device='cuda:0')), ('power', tensor([-18.8142], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.19123977422714233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2632, 8.0492, 9.1893],
        [6.2632, 6.2906, 6.2672],
        [6.2632, 6.9835, 7.0581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.1911332607269287 
model_pd.l_d.mean(): -14.57304859161377 
model_pd.lagr.mean(): -14.381915092468262 
model_pd.lambdas: dict_items([('pout', tensor([1.2153], device='cuda:0')), ('power', tensor([0.8586], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3174], device='cuda:0')), ('power', tensor([-18.8139], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.1911332607269287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2640, 8.0498, 9.1896],
        [6.2640, 6.9841, 7.0587],
        [6.2640, 6.2914, 6.2681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.19103537499904633 
model_pd.l_d.mean(): -14.553605079650879 
model_pd.lagr.mean(): -14.362569808959961 
model_pd.lambdas: dict_items([('pout', tensor([1.2166], device='cuda:0')), ('power', tensor([0.8577], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3172], device='cuda:0')), ('power', tensor([-18.8137], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.19103537499904633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2648, 8.0505, 9.1900],
        [6.2648, 6.2922, 6.2689],
        [6.2648, 6.9848, 7.0592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.1909341812133789 
model_pd.l_d.mean(): -14.534156799316406 
model_pd.lagr.mean(): -14.343222618103027 
model_pd.lambdas: dict_items([('pout', tensor([1.2179], device='cuda:0')), ('power', tensor([0.8568], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3171], device='cuda:0')), ('power', tensor([-18.8135], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.1909341812133789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2657, 8.0512, 9.1905],
        [6.2657, 6.2931, 6.2698],
        [6.2657, 6.9855, 7.0599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.1908183991909027 
model_pd.l_d.mean(): -14.51469898223877 
model_pd.lagr.mean(): -14.323880195617676 
model_pd.lambdas: dict_items([('pout', tensor([1.2192], device='cuda:0')), ('power', tensor([0.8558], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3169], device='cuda:0')), ('power', tensor([-18.8133], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.1908183991909027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2668, 6.9864, 7.0607],
        [6.2668, 8.0521, 9.1912],
        [6.2668, 6.2941, 6.2709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.19068042933940887 
model_pd.l_d.mean(): -14.49521541595459 
model_pd.lagr.mean(): -14.304534912109375 
model_pd.lambdas: dict_items([('pout', tensor([1.2205], device='cuda:0')), ('power', tensor([0.8549], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3167], device='cuda:0')), ('power', tensor([-18.8130], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.19068042933940887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2680, 6.9875, 7.0616],
        [6.2680, 8.0532, 9.1921],
        [6.2680, 6.2953, 6.2721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.19051635265350342 
model_pd.l_d.mean(): -14.475712776184082 
model_pd.lagr.mean(): -14.285196304321289 
model_pd.lambdas: dict_items([('pout', tensor([1.2219], device='cuda:0')), ('power', tensor([0.8539], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3165], device='cuda:0')), ('power', tensor([-18.8126], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.19051635265350342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2693, 8.0545, 9.1932],
        [6.2693, 6.9887, 7.0628],
        [6.2693, 6.2966, 6.2734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.19032588601112366 
model_pd.l_d.mean(): -14.456180572509766 
model_pd.lagr.mean(): -14.265854835510254 
model_pd.lambdas: dict_items([('pout', tensor([1.2232], device='cuda:0')), ('power', tensor([0.8530], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3163], device='cuda:0')), ('power', tensor([-18.8121], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.19032588601112366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2708, 6.9900, 7.0640],
        [6.2708, 8.0560, 9.1946],
        [6.2708, 6.2981, 6.2749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.19011300802230835 
model_pd.l_d.mean(): -14.436630249023438 
model_pd.lagr.mean(): -14.246517181396484 
model_pd.lambdas: dict_items([('pout', tensor([1.2245], device='cuda:0')), ('power', tensor([0.8521], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3160], device='cuda:0')), ('power', tensor([-18.8116], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.19011300802230835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2724, 6.9915, 7.0654],
        [6.2724, 8.0576, 9.1960],
        [6.2724, 6.2997, 6.2764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.18988430500030518 
model_pd.l_d.mean(): -14.417064666748047 
model_pd.lagr.mean(): -14.227180480957031 
model_pd.lambdas: dict_items([('pout', tensor([1.2258], device='cuda:0')), ('power', tensor([0.8511], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3157], device='cuda:0')), ('power', tensor([-18.8110], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.18988430500030518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2740, 6.9930, 7.0668],
        [6.2740, 6.3013, 6.2780],
        [6.2740, 8.0592, 9.1976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.18964797258377075 
model_pd.l_d.mean(): -14.397493362426758 
model_pd.lagr.mean(): -14.207845687866211 
model_pd.lambdas: dict_items([('pout', tensor([1.2271], device='cuda:0')), ('power', tensor([0.8502], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3154], device='cuda:0')), ('power', tensor([-18.8104], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.18964797258377075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2756, 6.9945, 7.0683],
        [6.2756, 6.3028, 6.2796],
        [6.2756, 8.0609, 9.1991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.18941275775432587 
model_pd.l_d.mean(): -14.377924919128418 
model_pd.lagr.mean(): -14.188511848449707 
model_pd.lambdas: dict_items([('pout', tensor([1.2284], device='cuda:0')), ('power', tensor([0.8492], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3151], device='cuda:0')), ('power', tensor([-18.8098], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.18941275775432587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2771, 6.3044, 6.2812],
        [6.2771, 8.0625, 9.2006],
        [6.2771, 6.9959, 7.0697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.18918606638908386 
model_pd.l_d.mean(): -14.358366966247559 
model_pd.lagr.mean(): -14.169180870056152 
model_pd.lambdas: dict_items([('pout', tensor([1.2298], device='cuda:0')), ('power', tensor([0.8483], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3148], device='cuda:0')), ('power', tensor([-18.8092], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.18918606638908386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2786, 6.3059, 6.2827],
        [6.2786, 6.9973, 7.0709],
        [6.2786, 8.0640, 9.2019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.18897360563278198 
model_pd.l_d.mean(): -14.338825225830078 
model_pd.lagr.mean(): -14.14985179901123 
model_pd.lambdas: dict_items([('pout', tensor([1.2311], device='cuda:0')), ('power', tensor([0.8474], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3146], device='cuda:0')), ('power', tensor([-18.8086], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.18897360563278198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2800, 6.3073, 6.2841],
        [6.2800, 8.0653, 9.2031],
        [6.2800, 6.9986, 7.0721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.1887788027524948 
model_pd.l_d.mean(): -14.31930160522461 
model_pd.lagr.mean(): -14.130522727966309 
model_pd.lambdas: dict_items([('pout', tensor([1.2324], device='cuda:0')), ('power', tensor([0.8464], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3143], device='cuda:0')), ('power', tensor([-18.8081], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.1887788027524948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2813, 6.3085, 6.2854],
        [6.2813, 6.9997, 7.0732],
        [6.2813, 8.0666, 9.2042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.18860164284706116 
model_pd.l_d.mean(): -14.299797058105469 
model_pd.lagr.mean(): -14.11119556427002 
model_pd.lambdas: dict_items([('pout', tensor([1.2337], device='cuda:0')), ('power', tensor([0.8455], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3141], device='cuda:0')), ('power', tensor([-18.8077], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.18860164284706116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2825, 8.0677, 9.2051],
        [6.2825, 7.0008, 7.0742],
        [6.2825, 6.3097, 6.2866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1884399950504303 
model_pd.l_d.mean(): -14.280309677124023 
model_pd.lagr.mean(): -14.091869354248047 
model_pd.lambdas: dict_items([('pout', tensor([1.2350], device='cuda:0')), ('power', tensor([0.8445], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3139], device='cuda:0')), ('power', tensor([-18.8073], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.1884399950504303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2837, 6.3109, 6.2877],
        [6.2837, 7.0018, 7.0751],
        [6.2837, 8.0687, 9.2059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.18828973174095154 
model_pd.l_d.mean(): -14.260835647583008 
model_pd.lagr.mean(): -14.072546005249023 
model_pd.lambdas: dict_items([('pout', tensor([1.2363], device='cuda:0')), ('power', tensor([0.8436], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3137], device='cuda:0')), ('power', tensor([-18.8069], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.18828973174095154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2848, 7.0027, 7.0759],
        [6.2848, 8.0698, 9.2067],
        [6.2848, 6.3120, 6.2889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.18814489245414734 
model_pd.l_d.mean(): -14.24136734008789 
model_pd.lagr.mean(): -14.05322265625 
model_pd.lambdas: dict_items([('pout', tensor([1.2376], device='cuda:0')), ('power', tensor([0.8427], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3135], device='cuda:0')), ('power', tensor([-18.8066], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.18814489245414734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2859, 8.0708, 9.2075],
        [6.2859, 7.0037, 7.0768],
        [6.2859, 6.3131, 6.2900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.18800030648708344 
model_pd.l_d.mean(): -14.221901893615723 
model_pd.lagr.mean(): -14.03390121459961 
model_pd.lambdas: dict_items([('pout', tensor([1.2390], device='cuda:0')), ('power', tensor([0.8417], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3133], device='cuda:0')), ('power', tensor([-18.8062], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.18800030648708344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2870, 7.0047, 7.0777],
        [6.2870, 6.3142, 6.2911],
        [6.2870, 8.0718, 9.2083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.1878543496131897 
model_pd.l_d.mean(): -14.202432632446289 
model_pd.lagr.mean(): -14.014577865600586 
model_pd.lambdas: dict_items([('pout', tensor([1.2403], device='cuda:0')), ('power', tensor([0.8408], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3131], device='cuda:0')), ('power', tensor([-18.8059], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.1878543496131897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2882, 6.3153, 6.2922],
        [6.2882, 7.0057, 7.0786],
        [6.2882, 8.0728, 9.2092]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.18770556151866913 
model_pd.l_d.mean(): -14.182963371276855 
model_pd.lagr.mean(): -13.995257377624512 
model_pd.lambdas: dict_items([('pout', tensor([1.2416], device='cuda:0')), ('power', tensor([0.8398], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3129], device='cuda:0')), ('power', tensor([-18.8055], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.18770556151866913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2894, 8.0739, 9.2100],
        [6.2894, 7.0067, 7.0796],
        [6.2894, 6.3165, 6.2934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.18755239248275757 
model_pd.l_d.mean(): -14.163488388061523 
model_pd.lagr.mean(): -13.975935935974121 
model_pd.lambdas: dict_items([('pout', tensor([1.2429], device='cuda:0')), ('power', tensor([0.8389], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3126], device='cuda:0')), ('power', tensor([-18.8051], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.18755239248275757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2906, 8.0750, 9.2110],
        [6.2906, 7.0078, 7.0806],
        [6.2906, 6.3177, 6.2946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.18739324808120728 
model_pd.l_d.mean(): -14.144007682800293 
model_pd.lagr.mean(): -13.95661449432373 
model_pd.lambdas: dict_items([('pout', tensor([1.2442], device='cuda:0')), ('power', tensor([0.8380], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3124], device='cuda:0')), ('power', tensor([-18.8047], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.18739324808120728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2918, 6.3189, 6.2958],
        [6.2918, 7.0089, 7.0816],
        [6.2918, 8.0762, 9.2120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.18722689151763916 
model_pd.l_d.mean(): -14.124519348144531 
model_pd.lagr.mean(): -13.937292098999023 
model_pd.lambdas: dict_items([('pout', tensor([1.2455], device='cuda:0')), ('power', tensor([0.8370], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3122], device='cuda:0')), ('power', tensor([-18.8042], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.18722689151763916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2931, 7.0100, 7.0827],
        [6.2931, 8.0775, 9.2131],
        [6.2931, 6.3202, 6.2971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.18705418705940247 
model_pd.l_d.mean(): -14.105026245117188 
model_pd.lagr.mean(): -13.91797161102295 
model_pd.lambdas: dict_items([('pout', tensor([1.2468], device='cuda:0')), ('power', tensor([0.8361], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3120], device='cuda:0')), ('power', tensor([-18.8038], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.18705418705940247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2944, 6.3215, 6.2984],
        [6.2944, 8.0788, 9.2142],
        [6.2944, 7.0112, 7.0838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.18687574565410614 
model_pd.l_d.mean(): -14.085528373718262 
model_pd.lagr.mean(): -13.898653030395508 
model_pd.lambdas: dict_items([('pout', tensor([1.2481], device='cuda:0')), ('power', tensor([0.8351], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3117], device='cuda:0')), ('power', tensor([-18.8033], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.18687574565410614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2957, 8.0801, 9.2154],
        [6.2957, 6.3228, 6.2998],
        [6.2957, 7.0124, 7.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.18669359385967255 
model_pd.l_d.mean(): -14.066027641296387 
model_pd.lagr.mean(): -13.879334449768066 
model_pd.lambdas: dict_items([('pout', tensor([1.2495], device='cuda:0')), ('power', tensor([0.8342], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3115], device='cuda:0')), ('power', tensor([-18.8028], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.18669359385967255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2971, 8.0814, 9.2166],
        [6.2971, 6.3242, 6.3011],
        [6.2971, 7.0137, 7.0861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.18650972843170166 
model_pd.l_d.mean(): -14.046525955200195 
model_pd.lagr.mean(): -13.860015869140625 
model_pd.lambdas: dict_items([('pout', tensor([1.2508], device='cuda:0')), ('power', tensor([0.8333], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3112], device='cuda:0')), ('power', tensor([-18.8023], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.18650972843170166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2984, 6.3255, 6.3025],
        [6.2984, 7.0149, 7.0873],
        [6.2984, 8.0828, 9.2178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.18632625043392181 
model_pd.l_d.mean(): -14.02702522277832 
model_pd.lagr.mean(): -13.840699195861816 
model_pd.lambdas: dict_items([('pout', tensor([1.2521], device='cuda:0')), ('power', tensor([0.8323], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3110], device='cuda:0')), ('power', tensor([-18.8018], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.18632625043392181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2997, 7.0161, 7.0884],
        [6.2997, 6.3268, 6.3038],
        [6.2997, 8.0841, 9.2190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.1861470639705658 
model_pd.l_d.mean(): -14.007532119750977 
model_pd.lagr.mean(): -13.821385383605957 
model_pd.lambdas: dict_items([('pout', tensor([1.2534], device='cuda:0')), ('power', tensor([0.8314], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3108], device='cuda:0')), ('power', tensor([-18.8014], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.1861470639705658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3010, 7.0173, 7.0895],
        [6.3010, 6.3281, 6.3051],
        [6.3010, 8.0854, 9.2201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.18597470223903656 
model_pd.l_d.mean(): -13.988043785095215 
model_pd.lagr.mean(): -13.802068710327148 
model_pd.lambdas: dict_items([('pout', tensor([1.2547], device='cuda:0')), ('power', tensor([0.8304], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3105], device='cuda:0')), ('power', tensor([-18.8009], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.18597470223903656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3023, 8.0866, 9.2212],
        [6.3023, 6.3293, 6.3063],
        [6.3023, 7.0184, 7.0906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.18581020832061768 
model_pd.l_d.mean(): -13.968565940856934 
model_pd.lagr.mean(): -13.782755851745605 
model_pd.lambdas: dict_items([('pout', tensor([1.2560], device='cuda:0')), ('power', tensor([0.8295], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3103], device='cuda:0')), ('power', tensor([-18.8004], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.18581020832061768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3035, 7.0195, 7.0916],
        [6.3035, 6.3305, 6.3075],
        [6.3035, 8.0877, 9.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.18565325438976288 
model_pd.l_d.mean(): -13.949094772338867 
model_pd.lagr.mean(): -13.76344108581543 
model_pd.lambdas: dict_items([('pout', tensor([1.2573], device='cuda:0')), ('power', tensor([0.8286], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3101], device='cuda:0')), ('power', tensor([-18.8000], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.18565325438976288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3046, 7.0206, 7.0926],
        [6.3046, 8.0888, 9.2232],
        [6.3046, 6.3317, 6.3087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.18550154566764832 
model_pd.l_d.mean(): -13.929631233215332 
model_pd.lagr.mean(): -13.74413013458252 
model_pd.lambdas: dict_items([('pout', tensor([1.2586], device='cuda:0')), ('power', tensor([0.8276], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3099], device='cuda:0')), ('power', tensor([-18.7996], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.18550154566764832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3058, 8.0899, 9.2241],
        [6.3058, 7.0216, 7.0936],
        [6.3058, 6.3328, 6.3098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.18535226583480835 
model_pd.l_d.mean(): -13.910170555114746 
model_pd.lagr.mean(): -13.724818229675293 
model_pd.lambdas: dict_items([('pout', tensor([1.2599], device='cuda:0')), ('power', tensor([0.8267], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3097], device='cuda:0')), ('power', tensor([-18.7992], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.18535226583480835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3070, 6.3340, 6.3110],
        [6.3070, 8.0911, 9.2250],
        [6.3070, 7.0227, 7.0945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.18520107865333557 
model_pd.l_d.mean(): -13.890710830688477 
model_pd.lagr.mean(): -13.705510139465332 
model_pd.lambdas: dict_items([('pout', tensor([1.2612], device='cuda:0')), ('power', tensor([0.8257], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3095], device='cuda:0')), ('power', tensor([-18.7988], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.18520107865333557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3082, 7.0237, 7.0956],
        [6.3082, 6.3351, 6.3122],
        [6.3082, 8.0922, 9.2261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.18504486978054047 
model_pd.l_d.mean(): -13.871243476867676 
model_pd.lagr.mean(): -13.686198234558105 
model_pd.lambdas: dict_items([('pout', tensor([1.2626], device='cuda:0')), ('power', tensor([0.8248], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3092], device='cuda:0')), ('power', tensor([-18.7984], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.18504486978054047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3094, 8.0935, 9.2271],
        [6.3094, 6.3364, 6.3134],
        [6.3094, 7.0249, 7.0966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.1848801076412201 
model_pd.l_d.mean(): -13.851770401000977 
model_pd.lagr.mean(): -13.666890144348145 
model_pd.lambdas: dict_items([('pout', tensor([1.2639], device='cuda:0')), ('power', tensor([0.8239], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3090], device='cuda:0')), ('power', tensor([-18.7979], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.1848801076412201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3107, 6.3377, 6.3147],
        [6.3107, 8.0948, 9.2283],
        [6.3107, 7.0261, 7.0978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.18470582365989685 
model_pd.l_d.mean(): -13.832291603088379 
model_pd.lagr.mean(): -13.64758586883545 
model_pd.lambdas: dict_items([('pout', tensor([1.2652], device='cuda:0')), ('power', tensor([0.8229], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3088], device='cuda:0')), ('power', tensor([-18.7974], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.18470582365989685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3121, 7.0274, 7.0990],
        [6.3121, 8.0962, 9.2296],
        [6.3121, 6.3391, 6.3161]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.1845216602087021 
model_pd.l_d.mean(): -13.812804222106934 
model_pd.lagr.mean(): -13.62828254699707 
model_pd.lambdas: dict_items([('pout', tensor([1.2665], device='cuda:0')), ('power', tensor([0.8220], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3085], device='cuda:0')), ('power', tensor([-18.7969], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.1845216602087021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3135, 8.0976, 9.2310],
        [6.3135, 6.3405, 6.3175],
        [6.3135, 7.0287, 7.1003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.184329554438591 
model_pd.l_d.mean(): -13.793309211730957 
model_pd.lagr.mean(): -13.608979225158691 
model_pd.lambdas: dict_items([('pout', tensor([1.2678], device='cuda:0')), ('power', tensor([0.8210], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3083], device='cuda:0')), ('power', tensor([-18.7963], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.184329554438591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3150, 7.0301, 7.1016],
        [6.3150, 6.3419, 6.3190],
        [6.3150, 8.0991, 9.2324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.18413236737251282 
model_pd.l_d.mean(): -13.773811340332031 
model_pd.lagr.mean(): -13.589678764343262 
model_pd.lambdas: dict_items([('pout', tensor([1.2691], device='cuda:0')), ('power', tensor([0.8201], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3080], device='cuda:0')), ('power', tensor([-18.7958], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.18413236737251282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3165, 8.1007, 9.2338],
        [6.3165, 7.0315, 7.1029],
        [6.3165, 6.3434, 6.3205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.1839328110218048 
model_pd.l_d.mean(): -13.754312515258789 
model_pd.lagr.mean(): -13.570379257202148 
model_pd.lambdas: dict_items([('pout', tensor([1.2704], device='cuda:0')), ('power', tensor([0.8192], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3077], device='cuda:0')), ('power', tensor([-18.7952], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.1839328110218048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3180, 6.3449, 6.3220],
        [6.3180, 8.1022, 9.2352],
        [6.3180, 7.0329, 7.1043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.18373438715934753 
model_pd.l_d.mean(): -13.734818458557129 
model_pd.lagr.mean(): -13.551084518432617 
model_pd.lambdas: dict_items([('pout', tensor([1.2717], device='cuda:0')), ('power', tensor([0.8182], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3074], device='cuda:0')), ('power', tensor([-18.7946], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.18373438715934753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3194, 7.0343, 7.1056],
        [6.3194, 6.3463, 6.3234],
        [6.3194, 8.1037, 9.2366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.18353909254074097 
model_pd.l_d.mean(): -13.715327262878418 
model_pd.lagr.mean(): -13.531787872314453 
model_pd.lambdas: dict_items([('pout', tensor([1.2730], device='cuda:0')), ('power', tensor([0.8173], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3072], device='cuda:0')), ('power', tensor([-18.7941], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.18353909254074097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3209, 6.3478, 6.3249],
        [6.3209, 8.1052, 9.2380],
        [6.3209, 7.0356, 7.1068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.18334899842739105 
model_pd.l_d.mean(): -13.695843696594238 
model_pd.lagr.mean(): -13.512495040893555 
model_pd.lambdas: dict_items([('pout', tensor([1.2743], device='cuda:0')), ('power', tensor([0.8163], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3069], device='cuda:0')), ('power', tensor([-18.7935], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.18334899842739105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3223, 8.1066, 9.2393],
        [6.3223, 6.3492, 6.3263],
        [6.3223, 7.0370, 7.1081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.18316414952278137 
model_pd.l_d.mean(): -13.676368713378906 
model_pd.lagr.mean(): -13.493204116821289 
model_pd.lambdas: dict_items([('pout', tensor([1.2756], device='cuda:0')), ('power', tensor([0.8154], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3066], device='cuda:0')), ('power', tensor([-18.7930], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.18316414952278137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3237, 6.3506, 6.3277],
        [6.3237, 8.1080, 9.2406],
        [6.3237, 7.0382, 7.1093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.18298405408859253 
model_pd.l_d.mean(): -13.656899452209473 
model_pd.lagr.mean(): -13.473915100097656 
model_pd.lambdas: dict_items([('pout', tensor([1.2769], device='cuda:0')), ('power', tensor([0.8145], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3064], device='cuda:0')), ('power', tensor([-18.7924], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.18298405408859253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3250, 6.3519, 6.3290],
        [6.3250, 8.1094, 9.2418],
        [6.3250, 7.0395, 7.1105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.1828075647354126 
model_pd.l_d.mean(): -13.637435913085938 
model_pd.lagr.mean(): -13.454627990722656 
model_pd.lambdas: dict_items([('pout', tensor([1.2782], device='cuda:0')), ('power', tensor([0.8135], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3061], device='cuda:0')), ('power', tensor([-18.7919], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.1828075647354126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3264, 8.1107, 9.2431],
        [6.3264, 6.3533, 6.3304],
        [6.3264, 7.0408, 7.1117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.1826322078704834 
model_pd.l_d.mean(): -13.617974281311035 
model_pd.lagr.mean(): -13.435341835021973 
model_pd.lambdas: dict_items([('pout', tensor([1.2796], device='cuda:0')), ('power', tensor([0.8126], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3059], device='cuda:0')), ('power', tensor([-18.7914], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.1826322078704834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3278, 7.0420, 7.1129],
        [6.3278, 6.3546, 6.3318],
        [6.3278, 8.1121, 9.2443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.18245577812194824 
model_pd.l_d.mean(): -13.598514556884766 
model_pd.lagr.mean(): -13.416058540344238 
model_pd.lambdas: dict_items([('pout', tensor([1.2809], device='cuda:0')), ('power', tensor([0.8116], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3056], device='cuda:0')), ('power', tensor([-18.7909], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.18245577812194824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3292, 6.3560, 6.3332],
        [6.3292, 8.1135, 9.2456],
        [6.3292, 7.0433, 7.1141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.1822768747806549 
model_pd.l_d.mean(): -13.579051971435547 
model_pd.lagr.mean(): -13.396775245666504 
model_pd.lambdas: dict_items([('pout', tensor([1.2822], device='cuda:0')), ('power', tensor([0.8107], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3054], device='cuda:0')), ('power', tensor([-18.7904], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.1822768747806549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3306, 7.0447, 7.1154],
        [6.3306, 8.1149, 9.2469],
        [6.3306, 6.3575, 6.3346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.1820940226316452 
model_pd.l_d.mean(): -13.559588432312012 
model_pd.lagr.mean(): -13.377494812011719 
model_pd.lambdas: dict_items([('pout', tensor([1.2835], device='cuda:0')), ('power', tensor([0.8098], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3051], device='cuda:0')), ('power', tensor([-18.7898], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.1820940226316452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3321, 8.1164, 9.2483],
        [6.3321, 7.0460, 7.1167],
        [6.3321, 6.3589, 6.3361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.1819068193435669 
model_pd.l_d.mean(): -13.540122985839844 
model_pd.lagr.mean(): -13.358216285705566 
model_pd.lambdas: dict_items([('pout', tensor([1.2848], device='cuda:0')), ('power', tensor([0.8088], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3048], device='cuda:0')), ('power', tensor([-18.7893], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.1819068193435669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3336, 7.0474, 7.1180],
        [6.3336, 6.3604, 6.3376],
        [6.3336, 8.1180, 9.2497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.18171504139900208 
model_pd.l_d.mean(): -13.520654678344727 
model_pd.lagr.mean(): -13.338939666748047 
model_pd.lambdas: dict_items([('pout', tensor([1.2861], device='cuda:0')), ('power', tensor([0.8079], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3046], device='cuda:0')), ('power', tensor([-18.7887], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.18171504139900208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3351, 7.0488, 7.1193],
        [6.3351, 8.1195, 9.2511],
        [6.3351, 6.3619, 6.3391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.18151991069316864 
model_pd.l_d.mean(): -13.501184463500977 
model_pd.lagr.mean(): -13.31966495513916 
model_pd.lambdas: dict_items([('pout', tensor([1.2874], device='cuda:0')), ('power', tensor([0.8069], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3043], device='cuda:0')), ('power', tensor([-18.7881], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.18151991069316864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3366, 6.3634, 6.3406],
        [6.3366, 7.0503, 7.1207],
        [6.3366, 8.1211, 9.2526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.18132250010967255 
model_pd.l_d.mean(): -13.481712341308594 
model_pd.lagr.mean(): -13.300390243530273 
model_pd.lambdas: dict_items([('pout', tensor([1.2887], device='cuda:0')), ('power', tensor([0.8060], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3040], device='cuda:0')), ('power', tensor([-18.7875], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.18132250010967255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3382, 6.3650, 6.3421],
        [6.3382, 7.0517, 7.1221],
        [6.3382, 8.1227, 9.2541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.18112415075302124 
model_pd.l_d.mean(): -13.462244033813477 
model_pd.lagr.mean(): -13.281120300292969 
model_pd.lambdas: dict_items([('pout', tensor([1.2900], device='cuda:0')), ('power', tensor([0.8051], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3037], device='cuda:0')), ('power', tensor([-18.7869], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.18112415075302124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3397, 7.0532, 7.1235],
        [6.3397, 6.3665, 6.3437],
        [6.3397, 8.1243, 9.2556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.18092676997184753 
model_pd.l_d.mean(): -13.442776679992676 
model_pd.lagr.mean(): -13.261850357055664 
model_pd.lambdas: dict_items([('pout', tensor([1.2913], device='cuda:0')), ('power', tensor([0.8041], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3034], device='cuda:0')), ('power', tensor([-18.7863], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.18092676997184753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3412, 6.3680, 6.3452],
        [6.3412, 8.1259, 9.2571],
        [6.3412, 7.0546, 7.1249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.1807308793067932 
model_pd.l_d.mean(): -13.42331314086914 
model_pd.lagr.mean(): -13.242582321166992 
model_pd.lambdas: dict_items([('pout', tensor([1.2926], device='cuda:0')), ('power', tensor([0.8032], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3031], device='cuda:0')), ('power', tensor([-18.7857], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.1807308793067932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3428, 8.1274, 9.2586],
        [6.3428, 6.3695, 6.3468],
        [6.3428, 7.0561, 7.1262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.18053731322288513 
model_pd.l_d.mean(): -13.403851509094238 
model_pd.lagr.mean(): -13.22331428527832 
model_pd.lambdas: dict_items([('pout', tensor([1.2939], device='cuda:0')), ('power', tensor([0.8022], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3029], device='cuda:0')), ('power', tensor([-18.7851], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.18053731322288513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3443, 8.1290, 9.2600],
        [6.3443, 6.3710, 6.3483],
        [6.3443, 7.0575, 7.1276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.1803462952375412 
model_pd.l_d.mean(): -13.38439655303955 
model_pd.lagr.mean(): -13.204050064086914 
model_pd.lambdas: dict_items([('pout', tensor([1.2952], device='cuda:0')), ('power', tensor([0.8013], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3026], device='cuda:0')), ('power', tensor([-18.7845], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.1803462952375412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3458, 6.3725, 6.3498],
        [6.3458, 8.1305, 9.2614],
        [6.3458, 7.0589, 7.1289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.18015727400779724 
model_pd.l_d.mean(): -13.364944458007812 
model_pd.lagr.mean(): -13.184786796569824 
model_pd.lambdas: dict_items([('pout', tensor([1.2965], device='cuda:0')), ('power', tensor([0.8004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3023], device='cuda:0')), ('power', tensor([-18.7839], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.18015727400779724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3473, 8.1321, 9.2629],
        [6.3473, 6.3740, 6.3513],
        [6.3473, 7.0603, 7.1303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1799696385860443 
model_pd.l_d.mean(): -13.345497131347656 
model_pd.lagr.mean(): -13.16552734375 
model_pd.lambdas: dict_items([('pout', tensor([1.2978], device='cuda:0')), ('power', tensor([0.7994], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3020], device='cuda:0')), ('power', tensor([-18.7833], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.1799696385860443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3488, 6.3755, 6.3528],
        [6.3488, 7.0617, 7.1316],
        [6.3488, 8.1336, 9.2643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.1797824203968048 
model_pd.l_d.mean(): -13.326047897338867 
model_pd.lagr.mean(): -13.146265029907227 
model_pd.lambdas: dict_items([('pout', tensor([1.2991], device='cuda:0')), ('power', tensor([0.7985], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3017], device='cuda:0')), ('power', tensor([-18.7827], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.1797824203968048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3503, 7.0632, 7.1330],
        [6.3503, 6.3770, 6.3543],
        [6.3503, 8.1352, 9.2657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.17959466576576233 
model_pd.l_d.mean(): -13.306602478027344 
model_pd.lagr.mean(): -13.127007484436035 
model_pd.lambdas: dict_items([('pout', tensor([1.3004], device='cuda:0')), ('power', tensor([0.7976], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3015], device='cuda:0')), ('power', tensor([-18.7821], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.17959466576576233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3518, 8.1367, 9.2672],
        [6.3518, 6.3785, 6.3558],
        [6.3518, 7.0646, 7.1343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.17940571904182434 
model_pd.l_d.mean(): -13.287155151367188 
model_pd.lagr.mean(): -13.107748985290527 
model_pd.lambdas: dict_items([('pout', tensor([1.3017], device='cuda:0')), ('power', tensor([0.7966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3012], device='cuda:0')), ('power', tensor([-18.7815], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.17940571904182434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3534, 6.3801, 6.3573],
        [6.3534, 8.1383, 9.2687],
        [6.3534, 7.0660, 7.1357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.17921464145183563 
model_pd.l_d.mean(): -13.26771068572998 
model_pd.lagr.mean(): -13.088496208190918 
model_pd.lambdas: dict_items([('pout', tensor([1.3030], device='cuda:0')), ('power', tensor([0.7957], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3009], device='cuda:0')), ('power', tensor([-18.7809], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.17921464145183563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3549, 8.1399, 9.2702],
        [6.3549, 7.0675, 7.1371],
        [6.3549, 6.3816, 6.3589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.17902140319347382 
model_pd.l_d.mean(): -13.248262405395508 
model_pd.lagr.mean(): -13.06924057006836 
model_pd.lambdas: dict_items([('pout', tensor([1.3043], device='cuda:0')), ('power', tensor([0.7947], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3006], device='cuda:0')), ('power', tensor([-18.7803], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.17902140319347382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3565, 6.3832, 6.3605],
        [6.3565, 7.0690, 7.1385],
        [6.3565, 8.1416, 9.2718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.17882585525512695 
model_pd.l_d.mean(): -13.228816032409668 
model_pd.lagr.mean(): -13.049989700317383 
model_pd.lambdas: dict_items([('pout', tensor([1.3056], device='cuda:0')), ('power', tensor([0.7938], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3003], device='cuda:0')), ('power', tensor([-18.7797], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.17882585525512695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3581, 8.1432, 9.2733],
        [6.3581, 7.0705, 7.1400],
        [6.3581, 6.3848, 6.3621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.17862848937511444 
model_pd.l_d.mean(): -13.209366798400879 
model_pd.lagr.mean(): -13.03073787689209 
model_pd.lambdas: dict_items([('pout', tensor([1.3069], device='cuda:0')), ('power', tensor([0.7929], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3000], device='cuda:0')), ('power', tensor([-18.7790], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.17862848937511444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3597, 7.0720, 7.1414],
        [6.3597, 8.1449, 9.2749],
        [6.3597, 6.3864, 6.3637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.17843008041381836 
model_pd.l_d.mean(): -13.189920425415039 
model_pd.lagr.mean(): -13.011489868164062 
model_pd.lambdas: dict_items([('pout', tensor([1.3082], device='cuda:0')), ('power', tensor([0.7919], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2997], device='cuda:0')), ('power', tensor([-18.7784], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.17843008041381836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3613, 8.1466, 9.2766],
        [6.3613, 6.3880, 6.3653],
        [6.3613, 7.0736, 7.1429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.178231343626976 
model_pd.l_d.mean(): -13.1704740524292 
model_pd.lagr.mean(): -12.992242813110352 
model_pd.lambdas: dict_items([('pout', tensor([1.3095], device='cuda:0')), ('power', tensor([0.7910], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2994], device='cuda:0')), ('power', tensor([-18.7777], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.178231343626976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3629, 7.0751, 7.1443],
        [6.3629, 6.3895, 6.3669],
        [6.3629, 8.1483, 9.2782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.17803335189819336 
model_pd.l_d.mean(): -13.151031494140625 
model_pd.lagr.mean(): -12.972997665405273 
model_pd.lambdas: dict_items([('pout', tensor([1.3108], device='cuda:0')), ('power', tensor([0.7900], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2991], device='cuda:0')), ('power', tensor([-18.7771], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.17803335189819336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3645, 6.3911, 6.3685],
        [6.3645, 8.1500, 9.2798],
        [6.3645, 7.0766, 7.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.17783638834953308 
model_pd.l_d.mean(): -13.13158893585205 
model_pd.lagr.mean(): -12.953752517700195 
model_pd.lambdas: dict_items([('pout', tensor([1.3121], device='cuda:0')), ('power', tensor([0.7891], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2988], device='cuda:0')), ('power', tensor([-18.7764], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.17783638834953308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3661, 6.3927, 6.3700],
        [6.3661, 7.0781, 7.1473],
        [6.3661, 8.1516, 9.2814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.177641361951828 
model_pd.l_d.mean(): -13.112151145935059 
model_pd.lagr.mean(): -12.934510231018066 
model_pd.lambdas: dict_items([('pout', tensor([1.3134], device='cuda:0')), ('power', tensor([0.7882], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2985], device='cuda:0')), ('power', tensor([-18.7758], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.177641361951828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3677, 7.0797, 7.1487],
        [6.3677, 6.3943, 6.3716],
        [6.3677, 8.1533, 9.2830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.17744788527488708 
model_pd.l_d.mean(): -13.092716217041016 
model_pd.lagr.mean(): -12.915267944335938 
model_pd.lambdas: dict_items([('pout', tensor([1.3147], device='cuda:0')), ('power', tensor([0.7872], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2982], device='cuda:0')), ('power', tensor([-18.7751], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.17744788527488708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3693, 8.1550, 9.2846],
        [6.3693, 7.0812, 7.1501],
        [6.3693, 6.3959, 6.3732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.17725598812103271 
model_pd.l_d.mean(): -13.073282241821289 
model_pd.lagr.mean(): -12.896026611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.3160], device='cuda:0')), ('power', tensor([0.7863], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2980], device='cuda:0')), ('power', tensor([-18.7745], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.17725598812103271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3708, 8.1566, 9.2861],
        [6.3708, 6.3974, 6.3748],
        [6.3708, 7.0827, 7.1516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.17706525325775146 
model_pd.l_d.mean(): -13.053853988647461 
model_pd.lagr.mean(): -12.876789093017578 
model_pd.lambdas: dict_items([('pout', tensor([1.3173], device='cuda:0')), ('power', tensor([0.7853], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2977], device='cuda:0')), ('power', tensor([-18.7738], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.17706525325775146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3724, 6.3990, 6.3763],
        [6.3724, 7.0842, 7.1530],
        [6.3724, 8.1583, 9.2877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.1768748164176941 
model_pd.l_d.mean(): -13.034425735473633 
model_pd.lagr.mean(): -12.857550621032715 
model_pd.lambdas: dict_items([('pout', tensor([1.3186], device='cuda:0')), ('power', tensor([0.7844], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2974], device='cuda:0')), ('power', tensor([-18.7732], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.1768748164176941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3740, 6.4005, 6.3779],
        [6.3740, 7.0857, 7.1544],
        [6.3740, 8.1600, 9.2893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.17668423056602478 
model_pd.l_d.mean(): -13.014998435974121 
model_pd.lagr.mean(): -12.838314056396484 
model_pd.lambdas: dict_items([('pout', tensor([1.3199], device='cuda:0')), ('power', tensor([0.7835], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2971], device='cuda:0')), ('power', tensor([-18.7726], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.17668423056602478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3755, 8.1616, 9.2910],
        [6.3755, 6.4021, 6.3795],
        [6.3755, 7.0872, 7.1559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.17649304866790771 
model_pd.l_d.mean(): -12.995573997497559 
model_pd.lagr.mean(): -12.81908130645752 
model_pd.lambdas: dict_items([('pout', tensor([1.3212], device='cuda:0')), ('power', tensor([0.7825], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2968], device='cuda:0')), ('power', tensor([-18.7719], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.17649304866790771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3771, 6.4037, 6.3811],
        [6.3771, 8.1633, 9.2926],
        [6.3771, 7.0887, 7.1574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.17630121111869812 
model_pd.l_d.mean(): -12.97614860534668 
model_pd.lagr.mean(): -12.799847602844238 
model_pd.lambdas: dict_items([('pout', tensor([1.3225], device='cuda:0')), ('power', tensor([0.7816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2965], device='cuda:0')), ('power', tensor([-18.7712], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.17630121111869812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3787, 7.0902, 7.1588],
        [6.3787, 6.4052, 6.3826],
        [6.3787, 8.1650, 9.2942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.17611420154571533 
model_pd.l_d.mean(): -12.956727981567383 
model_pd.lagr.mean(): -12.780613899230957 
model_pd.lambdas: dict_items([('pout', tensor([1.3238], device='cuda:0')), ('power', tensor([0.7807], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2962], device='cuda:0')), ('power', tensor([-18.7706], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.17611420154571533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3802, 6.4067, 6.3841],
        [6.3802, 7.0917, 7.1602],
        [6.3802, 8.1666, 9.2957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.1759355068206787 
model_pd.l_d.mean(): -12.9373140335083 
model_pd.lagr.mean(): -12.761378288269043 
model_pd.lambdas: dict_items([('pout', tensor([1.3251], device='cuda:0')), ('power', tensor([0.7797], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2959], device='cuda:0')), ('power', tensor([-18.7700], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.1759355068206787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3816, 7.0931, 7.1615],
        [6.3816, 6.4081, 6.3855],
        [6.3816, 8.1681, 9.2972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.17576587200164795 
model_pd.l_d.mean(): -12.91790771484375 
model_pd.lagr.mean(): -12.742141723632812 
model_pd.lambdas: dict_items([('pout', tensor([1.3264], device='cuda:0')), ('power', tensor([0.7788], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2957], device='cuda:0')), ('power', tensor([-18.7694], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.17576587200164795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3829, 7.0944, 7.1628],
        [6.3829, 8.1695, 9.2986],
        [6.3829, 6.4095, 6.3869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.17560385167598724 
model_pd.l_d.mean(): -12.898507118225098 
model_pd.lagr.mean(): -12.72290325164795 
model_pd.lambdas: dict_items([('pout', tensor([1.3277], device='cuda:0')), ('power', tensor([0.7778], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2954], device='cuda:0')), ('power', tensor([-18.7688], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.17560385167598724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3843, 8.1710, 9.2999],
        [6.3843, 6.4108, 6.3882],
        [6.3843, 7.0957, 7.1641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.17544510960578918 
model_pd.l_d.mean(): -12.879114151000977 
model_pd.lagr.mean(): -12.703668594360352 
model_pd.lambdas: dict_items([('pout', tensor([1.3290], device='cuda:0')), ('power', tensor([0.7769], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2952], device='cuda:0')), ('power', tensor([-18.7682], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.17544510960578918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3856, 8.1724, 9.3013],
        [6.3856, 6.4121, 6.3895],
        [6.3856, 7.0970, 7.1654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.17528551816940308 
model_pd.l_d.mean(): -12.859720230102539 
model_pd.lagr.mean(): -12.68443489074707 
model_pd.lambdas: dict_items([('pout', tensor([1.3303], device='cuda:0')), ('power', tensor([0.7760], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2949], device='cuda:0')), ('power', tensor([-18.7677], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.17528551816940308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3870, 7.0983, 7.1666],
        [6.3870, 8.1739, 9.3028],
        [6.3870, 6.4135, 6.3909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.1751239150762558 
model_pd.l_d.mean(): -12.840323448181152 
model_pd.lagr.mean(): -12.665199279785156 
model_pd.lambdas: dict_items([('pout', tensor([1.3316], device='cuda:0')), ('power', tensor([0.7750], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2946], device='cuda:0')), ('power', tensor([-18.7671], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.1751239150762558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3883, 7.0997, 7.1679],
        [6.3883, 6.4149, 6.3923],
        [6.3883, 8.1754, 9.3042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.17495904862880707 
model_pd.l_d.mean(): -12.82092571258545 
model_pd.lagr.mean(): -12.645966529846191 
model_pd.lambdas: dict_items([('pout', tensor([1.3328], device='cuda:0')), ('power', tensor([0.7741], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2944], device='cuda:0')), ('power', tensor([-18.7665], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.17495904862880707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3897, 8.1769, 9.3058],
        [6.3897, 6.4163, 6.3937],
        [6.3897, 7.1011, 7.1693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.17479053139686584 
model_pd.l_d.mean(): -12.801526069641113 
model_pd.lagr.mean(): -12.62673568725586 
model_pd.lambdas: dict_items([('pout', tensor([1.3341], device='cuda:0')), ('power', tensor([0.7731], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2941], device='cuda:0')), ('power', tensor([-18.7659], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.17479053139686584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3912, 8.1785, 9.3073],
        [6.3912, 7.1025, 7.1707],
        [6.3912, 6.4177, 6.3951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.17461857199668884 
model_pd.l_d.mean(): -12.782122611999512 
model_pd.lagr.mean(): -12.607503890991211 
model_pd.lambdas: dict_items([('pout', tensor([1.3354], device='cuda:0')), ('power', tensor([0.7722], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2939], device='cuda:0')), ('power', tensor([-18.7653], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.17461857199668884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3926, 8.1801, 9.3090],
        [6.3926, 6.4191, 6.3965],
        [6.3926, 7.1039, 7.1721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.17444394528865814 
model_pd.l_d.mean(): -12.762720108032227 
model_pd.lagr.mean(): -12.588275909423828 
model_pd.lambdas: dict_items([('pout', tensor([1.3367], device='cuda:0')), ('power', tensor([0.7713], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2936], device='cuda:0')), ('power', tensor([-18.7647], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.17444394528865814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3941, 6.4206, 6.3980],
        [6.3941, 8.1817, 9.3106],
        [6.3941, 7.1054, 7.1735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.17426767945289612 
model_pd.l_d.mean(): -12.743316650390625 
model_pd.lagr.mean(): -12.569048881530762 
model_pd.lambdas: dict_items([('pout', tensor([1.3380], device='cuda:0')), ('power', tensor([0.7703], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2933], device='cuda:0')), ('power', tensor([-18.7640], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.17426767945289612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3955, 8.1834, 9.3123],
        [6.3955, 6.4221, 6.3995],
        [6.3955, 7.1069, 7.1749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.17408990859985352 
model_pd.l_d.mean(): -12.723912239074707 
model_pd.lagr.mean(): -12.549821853637695 
model_pd.lambdas: dict_items([('pout', tensor([1.3393], device='cuda:0')), ('power', tensor([0.7694], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2930], device='cuda:0')), ('power', tensor([-18.7634], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.17408990859985352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3970, 8.1850, 9.3139],
        [6.3970, 7.1084, 7.1764],
        [6.3970, 6.4235, 6.4010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.17391064763069153 
model_pd.l_d.mean(): -12.704509735107422 
model_pd.lagr.mean(): -12.530598640441895 
model_pd.lambdas: dict_items([('pout', tensor([1.3406], device='cuda:0')), ('power', tensor([0.7685], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2927], device='cuda:0')), ('power', tensor([-18.7627], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.17391064763069153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3986, 6.4251, 6.4025],
        [6.3986, 7.1099, 7.1778],
        [6.3986, 8.1867, 9.3157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.17372891306877136 
model_pd.l_d.mean(): -12.68510627746582 
model_pd.lagr.mean(): -12.511377334594727 
model_pd.lambdas: dict_items([('pout', tensor([1.3419], device='cuda:0')), ('power', tensor([0.7675], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2924], device='cuda:0')), ('power', tensor([-18.7620], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.17372891306877136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4001, 7.1114, 7.1793],
        [6.4001, 6.4266, 6.4040],
        [6.4001, 8.1884, 9.3174]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.17354458570480347 
model_pd.l_d.mean(): -12.66569995880127 
model_pd.lagr.mean(): -12.492155075073242 
model_pd.lambdas: dict_items([('pout', tensor([1.3432], device='cuda:0')), ('power', tensor([0.7666], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2922], device='cuda:0')), ('power', tensor([-18.7613], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.17354458570480347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4017, 6.4281, 6.4056],
        [6.4017, 7.1130, 7.1809],
        [6.4017, 8.1902, 9.3192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.17335712909698486 
model_pd.l_d.mean(): -12.646295547485352 
model_pd.lagr.mean(): -12.472938537597656 
model_pd.lambdas: dict_items([('pout', tensor([1.3445], device='cuda:0')), ('power', tensor([0.7656], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2919], device='cuda:0')), ('power', tensor([-18.7606], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.17335712909698486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4032, 8.1920, 9.3211],
        [6.4032, 7.1146, 7.1824],
        [6.4032, 6.4297, 6.4072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.1731669157743454 
model_pd.l_d.mean(): -12.626888275146484 
model_pd.lagr.mean(): -12.453721046447754 
model_pd.lambdas: dict_items([('pout', tensor([1.3458], device='cuda:0')), ('power', tensor([0.7647], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2916], device='cuda:0')), ('power', tensor([-18.7599], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.1731669157743454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4049, 8.1938, 9.3229],
        [6.4049, 6.4313, 6.4088],
        [6.4049, 7.1162, 7.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.17297425866127014 
model_pd.l_d.mean(): -12.607481002807617 
model_pd.lagr.mean(): -12.4345064163208 
model_pd.lambdas: dict_items([('pout', tensor([1.3471], device='cuda:0')), ('power', tensor([0.7638], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2912], device='cuda:0')), ('power', tensor([-18.7592], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.17297425866127014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4065, 7.1179, 7.1856],
        [6.4065, 8.1957, 9.3248],
        [6.4065, 6.4330, 6.4104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.17277978360652924 
model_pd.l_d.mean(): -12.588074684143066 
model_pd.lagr.mean(): -12.415294647216797 
model_pd.lambdas: dict_items([('pout', tensor([1.3484], device='cuda:0')), ('power', tensor([0.7628], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2909], device='cuda:0')), ('power', tensor([-18.7585], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.17277978360652924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4081, 6.4346, 6.4121],
        [6.4081, 8.1975, 9.3268],
        [6.4081, 7.1195, 7.1873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.17258398234844208 
model_pd.l_d.mean(): -12.568668365478516 
model_pd.lagr.mean(): -12.396084785461426 
model_pd.lambdas: dict_items([('pout', tensor([1.3497], device='cuda:0')), ('power', tensor([0.7619], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2906], device='cuda:0')), ('power', tensor([-18.7577], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.17258398234844208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4098, 7.1212, 7.1889],
        [6.4098, 6.4363, 6.4137],
        [6.4098, 8.1994, 9.3287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.17238806188106537 
model_pd.l_d.mean(): -12.549264907836914 
model_pd.lagr.mean(): -12.376876831054688 
model_pd.lambdas: dict_items([('pout', tensor([1.3509], device='cuda:0')), ('power', tensor([0.7610], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2903], device='cuda:0')), ('power', tensor([-18.7570], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.17238806188106537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4115, 7.1229, 7.1905],
        [6.4115, 6.4379, 6.4154],
        [6.4115, 8.2013, 9.3307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.17219194769859314 
model_pd.l_d.mean(): -12.529864311218262 
model_pd.lagr.mean(): -12.357672691345215 
model_pd.lambdas: dict_items([('pout', tensor([1.3522], device='cuda:0')), ('power', tensor([0.7600], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2900], device='cuda:0')), ('power', tensor([-18.7562], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.17219194769859314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4131, 6.4396, 6.4170],
        [6.4131, 8.2032, 9.3326],
        [6.4131, 7.1246, 7.1922]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.1719958782196045 
model_pd.l_d.mean(): -12.51046371459961 
model_pd.lagr.mean(): -12.338467597961426 
model_pd.lambdas: dict_items([('pout', tensor([1.3535], device='cuda:0')), ('power', tensor([0.7591], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2897], device='cuda:0')), ('power', tensor([-18.7554], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.1719958782196045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4148, 7.1262, 7.1938],
        [6.4148, 6.4413, 6.4187],
        [6.4148, 8.2051, 9.3346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.17180025577545166 
model_pd.l_d.mean(): -12.491069793701172 
model_pd.lagr.mean(): -12.319269180297852 
model_pd.lambdas: dict_items([('pout', tensor([1.3548], device='cuda:0')), ('power', tensor([0.7581], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2893], device='cuda:0')), ('power', tensor([-18.7547], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.17180025577545166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4165, 7.1279, 7.1955],
        [6.4165, 6.4429, 6.4204],
        [6.4165, 8.2070, 9.3365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.17160460352897644 
model_pd.l_d.mean(): -12.471673965454102 
model_pd.lagr.mean(): -12.300069808959961 
model_pd.lambdas: dict_items([('pout', tensor([1.3561], device='cuda:0')), ('power', tensor([0.7572], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2890], device='cuda:0')), ('power', tensor([-18.7539], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.17160460352897644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4182, 7.1296, 7.1971],
        [6.4182, 6.4446, 6.4221],
        [6.4182, 8.2089, 9.3385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.1714089810848236 
model_pd.l_d.mean(): -12.452282905578613 
model_pd.lagr.mean(): -12.280874252319336 
model_pd.lambdas: dict_items([('pout', tensor([1.3574], device='cuda:0')), ('power', tensor([0.7563], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2887], device='cuda:0')), ('power', tensor([-18.7532], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.1714089810848236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4198, 7.1313, 7.1988],
        [6.4198, 6.4463, 6.4238],
        [6.4198, 8.2108, 9.3405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.171212837100029 
model_pd.l_d.mean(): -12.432891845703125 
model_pd.lagr.mean(): -12.261678695678711 
model_pd.lambdas: dict_items([('pout', tensor([1.3587], device='cuda:0')), ('power', tensor([0.7553], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2884], device='cuda:0')), ('power', tensor([-18.7524], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.171212837100029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4215, 8.2128, 9.3425],
        [6.4215, 7.1331, 7.2005],
        [6.4215, 6.4480, 6.4255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.17101602256298065 
model_pd.l_d.mean(): -12.41350269317627 
model_pd.lagr.mean(): -12.242486953735352 
model_pd.lambdas: dict_items([('pout', tensor([1.3600], device='cuda:0')), ('power', tensor([0.7544], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2881], device='cuda:0')), ('power', tensor([-18.7516], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.17101602256298065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4233, 7.1348, 7.2022],
        [6.4233, 6.4497, 6.4272],
        [6.4233, 8.2147, 9.3445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.1708182543516159 
model_pd.l_d.mean(): -12.394115447998047 
model_pd.lagr.mean(): -12.223297119140625 
model_pd.lambdas: dict_items([('pout', tensor([1.3613], device='cuda:0')), ('power', tensor([0.7534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2877], device='cuda:0')), ('power', tensor([-18.7508], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.1708182543516159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4250, 6.4514, 6.4289],
        [6.4250, 8.2167, 9.3465],
        [6.4250, 7.1365, 7.2039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.17061935365200043 
model_pd.l_d.mean(): -12.37472915649414 
model_pd.lagr.mean(): -12.204110145568848 
model_pd.lambdas: dict_items([('pout', tensor([1.3625], device='cuda:0')), ('power', tensor([0.7525], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2874], device='cuda:0')), ('power', tensor([-18.7500], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.17061935365200043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4267, 6.4532, 6.4307],
        [6.4267, 7.1383, 7.2056],
        [6.4267, 8.2187, 9.3486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.17041908204555511 
model_pd.l_d.mean(): -12.3553466796875 
model_pd.lagr.mean(): -12.184927940368652 
model_pd.lambdas: dict_items([('pout', tensor([1.3638], device='cuda:0')), ('power', tensor([0.7516], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2871], device='cuda:0')), ('power', tensor([-18.7492], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.17041908204555511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4285, 6.4549, 6.4324],
        [6.4285, 8.2207, 9.3507],
        [6.4285, 7.1401, 7.2073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.1702176332473755 
model_pd.l_d.mean(): -12.335960388183594 
model_pd.lagr.mean(): -12.165742874145508 
model_pd.lambdas: dict_items([('pout', tensor([1.3651], device='cuda:0')), ('power', tensor([0.7506], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2867], device='cuda:0')), ('power', tensor([-18.7484], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.1702176332473755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4303, 7.1419, 7.2091],
        [6.4303, 6.4567, 6.4342],
        [6.4303, 8.2227, 9.3528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.17001506686210632 
model_pd.l_d.mean(): -12.316579818725586 
model_pd.lagr.mean(): -12.146564483642578 
model_pd.lambdas: dict_items([('pout', tensor([1.3664], device='cuda:0')), ('power', tensor([0.7497], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2864], device='cuda:0')), ('power', tensor([-18.7476], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.17001506686210632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4321, 6.4585, 6.4360],
        [6.4321, 7.1437, 7.2109],
        [6.4321, 8.2248, 9.3549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.1698116958141327 
model_pd.l_d.mean(): -12.297197341918945 
model_pd.lagr.mean(): -12.127386093139648 
model_pd.lambdas: dict_items([('pout', tensor([1.3677], device='cuda:0')), ('power', tensor([0.7488], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2860], device='cuda:0')), ('power', tensor([-18.7468], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.1698116958141327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4339, 7.1455, 7.2127],
        [6.4339, 6.4603, 6.4378],
        [6.4339, 8.2268, 9.3571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.16960780322551727 
model_pd.l_d.mean(): -12.27781867980957 
model_pd.lagr.mean(): -12.108210563659668 
model_pd.lambdas: dict_items([('pout', tensor([1.3690], device='cuda:0')), ('power', tensor([0.7478], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2857], device='cuda:0')), ('power', tensor([-18.7459], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.16960780322551727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4357, 8.2289, 9.3592],
        [6.4357, 6.4621, 6.4396],
        [6.4357, 7.1474, 7.2145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.16940367221832275 
model_pd.l_d.mean(): -12.258442878723145 
model_pd.lagr.mean(): -12.089038848876953 
model_pd.lambdas: dict_items([('pout', tensor([1.3703], device='cuda:0')), ('power', tensor([0.7469], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2853], device='cuda:0')), ('power', tensor([-18.7451], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.16940367221832275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4375, 6.4639, 6.4414],
        [6.4375, 7.1492, 7.2163],
        [6.4375, 8.2310, 9.3614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.16919931769371033 
model_pd.l_d.mean(): -12.239068031311035 
model_pd.lagr.mean(): -12.069869041442871 
model_pd.lambdas: dict_items([('pout', tensor([1.3715], device='cuda:0')), ('power', tensor([0.7460], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2850], device='cuda:0')), ('power', tensor([-18.7442], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.16919931769371033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4393, 7.1511, 7.2181],
        [6.4393, 6.4657, 6.4432],
        [6.4393, 8.2331, 9.3636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.16899488866329193 
model_pd.l_d.mean(): -12.219694137573242 
model_pd.lagr.mean(): -12.050699234008789 
model_pd.lambdas: dict_items([('pout', tensor([1.3728], device='cuda:0')), ('power', tensor([0.7450], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2847], device='cuda:0')), ('power', tensor([-18.7434], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.16899488866329193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4411, 7.1529, 7.2199],
        [6.4411, 8.2352, 9.3658],
        [6.4411, 6.4675, 6.4451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.16879060864448547 
model_pd.l_d.mean(): -12.200325965881348 
model_pd.lagr.mean(): -12.031535148620605 
model_pd.lambdas: dict_items([('pout', tensor([1.3741], device='cuda:0')), ('power', tensor([0.7441], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2843], device='cuda:0')), ('power', tensor([-18.7425], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.16879060864448547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4430, 6.4694, 6.4469],
        [6.4430, 8.2373, 9.3680],
        [6.4430, 7.1548, 7.2217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.16858647763729095 
model_pd.l_d.mean(): -12.180959701538086 
model_pd.lagr.mean(): -12.012372970581055 
model_pd.lambdas: dict_items([('pout', tensor([1.3754], device='cuda:0')), ('power', tensor([0.7431], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2840], device='cuda:0')), ('power', tensor([-18.7417], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.16858647763729095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4448, 7.1567, 7.2236],
        [6.4448, 8.2394, 9.3702],
        [6.4448, 6.4712, 6.4487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.1683819741010666 
model_pd.l_d.mean(): -12.16159439086914 
model_pd.lagr.mean(): -11.993212699890137 
model_pd.lambdas: dict_items([('pout', tensor([1.3767], device='cuda:0')), ('power', tensor([0.7422], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2836], device='cuda:0')), ('power', tensor([-18.7408], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.1683819741010666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4467, 6.4731, 6.4506],
        [6.4467, 8.2415, 9.3724],
        [6.4467, 7.1585, 7.2254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.16817760467529297 
model_pd.l_d.mean(): -12.142231941223145 
model_pd.lagr.mean(): -11.974054336547852 
model_pd.lambdas: dict_items([('pout', tensor([1.3780], device='cuda:0')), ('power', tensor([0.7413], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2832], device='cuda:0')), ('power', tensor([-18.7400], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.16817760467529297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4485, 6.4749, 6.4524],
        [6.4485, 8.2436, 9.3746],
        [6.4485, 7.1604, 7.2272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.16797272861003876 
model_pd.l_d.mean(): -12.122870445251465 
model_pd.lagr.mean(): -11.9548978805542 
model_pd.lambdas: dict_items([('pout', tensor([1.3792], device='cuda:0')), ('power', tensor([0.7403], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2829], device='cuda:0')), ('power', tensor([-18.7391], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.16797272861003876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4504, 8.2458, 9.3769],
        [6.4504, 7.1623, 7.2291],
        [6.4504, 6.4768, 6.4543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.16776764392852783 
model_pd.l_d.mean(): -12.103513717651367 
model_pd.lagr.mean(): -11.935746192932129 
model_pd.lambdas: dict_items([('pout', tensor([1.3805], device='cuda:0')), ('power', tensor([0.7394], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2825], device='cuda:0')), ('power', tensor([-18.7382], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.16776764392852783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4523, 6.4787, 6.4562],
        [6.4523, 7.1643, 7.2310],
        [6.4523, 8.2480, 9.3791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.16756170988082886 
model_pd.l_d.mean(): -12.084157943725586 
model_pd.lagr.mean(): -11.916596412658691 
model_pd.lambdas: dict_items([('pout', tensor([1.3818], device='cuda:0')), ('power', tensor([0.7385], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2822], device='cuda:0')), ('power', tensor([-18.7373], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.16756170988082886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4542, 7.1662, 7.2329],
        [6.4542, 6.4806, 6.4581],
        [6.4542, 8.2501, 9.3814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.16735553741455078 
model_pd.l_d.mean(): -12.064803123474121 
model_pd.lagr.mean(): -11.89744758605957 
model_pd.lambdas: dict_items([('pout', tensor([1.3831], device='cuda:0')), ('power', tensor([0.7375], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2818], device='cuda:0')), ('power', tensor([-18.7365], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.16735553741455078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4561, 7.1681, 7.2348],
        [6.4561, 8.2523, 9.3837],
        [6.4561, 6.4825, 6.4600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.16714897751808167 
model_pd.l_d.mean(): -12.045450210571289 
model_pd.lagr.mean(): -11.878301620483398 
model_pd.lambdas: dict_items([('pout', tensor([1.3844], device='cuda:0')), ('power', tensor([0.7366], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2814], device='cuda:0')), ('power', tensor([-18.7356], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.16714897751808167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4580, 8.2545, 9.3860],
        [6.4580, 7.1701, 7.2367],
        [6.4580, 6.4844, 6.4619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.16694208979606628 
model_pd.l_d.mean(): -12.026100158691406 
model_pd.lagr.mean(): -11.859158515930176 
model_pd.lambdas: dict_items([('pout', tensor([1.3857], device='cuda:0')), ('power', tensor([0.7356], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2811], device='cuda:0')), ('power', tensor([-18.7347], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.16694208979606628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4599, 6.4863, 6.4638],
        [6.4599, 8.2567, 9.3883],
        [6.4599, 7.1720, 7.2386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.16673509776592255 
model_pd.l_d.mean(): -12.006752967834473 
model_pd.lagr.mean(): -11.840018272399902 
model_pd.lambdas: dict_items([('pout', tensor([1.3869], device='cuda:0')), ('power', tensor([0.7347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2807], device='cuda:0')), ('power', tensor([-18.7338], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.16673509776592255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4618, 7.1740, 7.2405],
        [6.4618, 6.4882, 6.4657],
        [6.4618, 8.2590, 9.3907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.1665278673171997 
model_pd.l_d.mean(): -11.987408638000488 
model_pd.lagr.mean(): -11.820880889892578 
model_pd.lambdas: dict_items([('pout', tensor([1.3882], device='cuda:0')), ('power', tensor([0.7338], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2803], device='cuda:0')), ('power', tensor([-18.7328], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.1665278673171997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4638, 7.1760, 7.2425],
        [6.4638, 8.2612, 9.3930],
        [6.4638, 6.4901, 6.4677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.16632063686847687 
model_pd.l_d.mean(): -11.968066215515137 
model_pd.lagr.mean(): -11.801745414733887 
model_pd.lambdas: dict_items([('pout', tensor([1.3895], device='cuda:0')), ('power', tensor([0.7328], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2800], device='cuda:0')), ('power', tensor([-18.7319], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.16632063686847687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4657, 8.2634, 9.3954],
        [6.4657, 7.1779, 7.2444],
        [6.4657, 6.4921, 6.4696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.16611352562904358 
model_pd.l_d.mean(): -11.94872760772705 
model_pd.lagr.mean(): -11.782613754272461 
model_pd.lambdas: dict_items([('pout', tensor([1.3908], device='cuda:0')), ('power', tensor([0.7319], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2796], device='cuda:0')), ('power', tensor([-18.7310], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.16611352562904358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4677, 8.2657, 9.3977],
        [6.4677, 7.1799, 7.2464],
        [6.4677, 6.4940, 6.4716]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.1659063845872879 
model_pd.l_d.mean(): -11.929389953613281 
model_pd.lagr.mean(): -11.763484001159668 
model_pd.lambdas: dict_items([('pout', tensor([1.3921], device='cuda:0')), ('power', tensor([0.7310], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2792], device='cuda:0')), ('power', tensor([-18.7301], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.1659063845872879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4696, 7.1819, 7.2483],
        [6.4696, 6.4960, 6.4735],
        [6.4696, 8.2680, 9.4001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.1656995415687561 
model_pd.l_d.mean(): -11.910057067871094 
model_pd.lagr.mean(): -11.744357109069824 
model_pd.lambdas: dict_items([('pout', tensor([1.3933], device='cuda:0')), ('power', tensor([0.7300], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2789], device='cuda:0')), ('power', tensor([-18.7292], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.1656995415687561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4716, 7.1839, 7.2503],
        [6.4716, 8.2702, 9.4025],
        [6.4716, 6.4979, 6.4755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.16549251973628998 
model_pd.l_d.mean(): -11.890725135803223 
model_pd.lagr.mean(): -11.72523307800293 
model_pd.lambdas: dict_items([('pout', tensor([1.3946], device='cuda:0')), ('power', tensor([0.7291], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2785], device='cuda:0')), ('power', tensor([-18.7282], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.16549251973628998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4735, 6.4999, 6.4774],
        [6.4735, 8.2725, 9.4049],
        [6.4735, 7.1860, 7.2523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.16528528928756714 
model_pd.l_d.mean(): -11.871395111083984 
model_pd.lagr.mean(): -11.706110000610352 
model_pd.lambdas: dict_items([('pout', tensor([1.3959], device='cuda:0')), ('power', tensor([0.7282], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2781], device='cuda:0')), ('power', tensor([-18.7273], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.16528528928756714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4755, 8.2748, 9.4073],
        [6.4755, 7.1880, 7.2542],
        [6.4755, 6.5019, 6.4794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.16507792472839355 
model_pd.l_d.mean(): -11.852066040039062 
model_pd.lagr.mean(): -11.68698787689209 
model_pd.lambdas: dict_items([('pout', tensor([1.3972], device='cuda:0')), ('power', tensor([0.7272], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2777], device='cuda:0')), ('power', tensor([-18.7263], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.16507792472839355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4775, 8.2771, 9.4097],
        [6.4775, 7.1900, 7.2562],
        [6.4775, 6.5039, 6.4814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.16487042605876923 
model_pd.l_d.mean(): -11.832742691040039 
model_pd.lagr.mean(): -11.667872428894043 
model_pd.lambdas: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([0.7263], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2773], device='cuda:0')), ('power', tensor([-18.7254], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.16487042605876923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4795, 6.5058, 6.4834],
        [6.4795, 8.2794, 9.4121],
        [6.4795, 7.1921, 7.2582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.16466256976127625 
model_pd.l_d.mean(): -11.813421249389648 
model_pd.lagr.mean(): -11.648758888244629 
model_pd.lambdas: dict_items([('pout', tensor([1.3997], device='cuda:0')), ('power', tensor([0.7253], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2770], device='cuda:0')), ('power', tensor([-18.7244], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.16466256976127625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4815, 7.1941, 7.2603],
        [6.4815, 8.2817, 9.4146],
        [6.4815, 6.5079, 6.4854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.16445448994636536 
model_pd.l_d.mean(): -11.794102668762207 
model_pd.lagr.mean(): -11.629648208618164 
model_pd.lambdas: dict_items([('pout', tensor([1.4010], device='cuda:0')), ('power', tensor([0.7244], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2766], device='cuda:0')), ('power', tensor([-18.7235], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.16445448994636536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4835, 7.1962, 7.2623],
        [6.4835, 8.2841, 9.4171],
        [6.4835, 6.5099, 6.4874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.16424623131752014 
model_pd.l_d.mean(): -11.774785041809082 
model_pd.lagr.mean(): -11.610538482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.4023], device='cuda:0')), ('power', tensor([0.7235], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2762], device='cuda:0')), ('power', tensor([-18.7225], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.16424623131752014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4856, 7.1983, 7.2643],
        [6.4856, 6.5119, 6.4895],
        [6.4856, 8.2864, 9.4195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.1640377640724182 
model_pd.l_d.mean(): -11.75546932220459 
model_pd.lagr.mean(): -11.591431617736816 
model_pd.lambdas: dict_items([('pout', tensor([1.4035], device='cuda:0')), ('power', tensor([0.7225], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2758], device='cuda:0')), ('power', tensor([-18.7215], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.1640377640724182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4876, 6.5139, 6.4915],
        [6.4876, 8.2888, 9.4220],
        [6.4876, 7.2003, 7.2664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.16382934153079987 
model_pd.l_d.mean(): -11.736156463623047 
model_pd.lagr.mean(): -11.57232666015625 
model_pd.lambdas: dict_items([('pout', tensor([1.4048], device='cuda:0')), ('power', tensor([0.7216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2754], device='cuda:0')), ('power', tensor([-18.7206], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.16382934153079987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4896, 8.2912, 9.4245],
        [6.4896, 7.2024, 7.2684],
        [6.4896, 6.5160, 6.4935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.16362062096595764 
model_pd.l_d.mean(): -11.716848373413086 
model_pd.lagr.mean(): -11.553227424621582 
model_pd.lambdas: dict_items([('pout', tensor([1.4061], device='cuda:0')), ('power', tensor([0.7207], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2750], device='cuda:0')), ('power', tensor([-18.7196], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.16362062096595764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4917, 6.5180, 6.4956],
        [6.4917, 8.2936, 9.4271],
        [6.4917, 7.2045, 7.2705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.1634119749069214 
model_pd.l_d.mean(): -11.697542190551758 
model_pd.lagr.mean(): -11.534130096435547 
model_pd.lambdas: dict_items([('pout', tensor([1.4074], device='cuda:0')), ('power', tensor([0.7197], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2746], device='cuda:0')), ('power', tensor([-18.7186], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.1634119749069214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4937, 8.2960, 9.4296],
        [6.4937, 7.2067, 7.2726],
        [6.4937, 6.5201, 6.4976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.16320331394672394 
model_pd.l_d.mean(): -11.678238868713379 
model_pd.lagr.mean(): -11.515035629272461 
model_pd.lambdas: dict_items([('pout', tensor([1.4086], device='cuda:0')), ('power', tensor([0.7188], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2742], device='cuda:0')), ('power', tensor([-18.7176], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.16320331394672394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4958, 7.2088, 7.2746],
        [6.4958, 8.2984, 9.4321],
        [6.4958, 6.5221, 6.4997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.16299466788768768 
model_pd.l_d.mean(): -11.658939361572266 
model_pd.lagr.mean(): -11.49594497680664 
model_pd.lambdas: dict_items([('pout', tensor([1.4099], device='cuda:0')), ('power', tensor([0.7179], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2738], device='cuda:0')), ('power', tensor([-18.7166], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.16299466788768768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4979, 7.2109, 7.2767],
        [6.4979, 8.3008, 9.4347],
        [6.4979, 6.5242, 6.5018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.16278599202632904 
model_pd.l_d.mean(): -11.639638900756836 
model_pd.lagr.mean(): -11.476853370666504 
model_pd.lambdas: dict_items([('pout', tensor([1.4112], device='cuda:0')), ('power', tensor([0.7169], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2734], device='cuda:0')), ('power', tensor([-18.7156], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.16278599202632904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5000, 8.3032, 9.4372],
        [6.5000, 6.5263, 6.5039],
        [6.5000, 7.2130, 7.2788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.1625773012638092 
model_pd.l_d.mean(): -11.620344161987305 
model_pd.lagr.mean(): -11.45776653289795 
model_pd.lambdas: dict_items([('pout', tensor([1.4125], device='cuda:0')), ('power', tensor([0.7160], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2731], device='cuda:0')), ('power', tensor([-18.7146], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.1625773012638092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5020, 6.5284, 6.5059],
        [6.5020, 7.2152, 7.2810],
        [6.5020, 8.3057, 9.4398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.1623685657978058 
model_pd.l_d.mean(): -11.601051330566406 
model_pd.lagr.mean(): -11.438682556152344 
model_pd.lambdas: dict_items([('pout', tensor([1.4137], device='cuda:0')), ('power', tensor([0.7150], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2727], device='cuda:0')), ('power', tensor([-18.7136], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.1623685657978058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5041, 8.3081, 9.4424],
        [6.5041, 7.2174, 7.2831],
        [6.5041, 6.5305, 6.5080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.1621597707271576 
model_pd.l_d.mean(): -11.581762313842773 
model_pd.lagr.mean(): -11.419602394104004 
model_pd.lambdas: dict_items([('pout', tensor([1.4150], device='cuda:0')), ('power', tensor([0.7141], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2723], device='cuda:0')), ('power', tensor([-18.7126], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.1621597707271576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5063, 8.3106, 9.4450],
        [6.5063, 6.5326, 6.5101],
        [6.5063, 7.2195, 7.2852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.16195061802864075 
model_pd.l_d.mean(): -11.56247329711914 
model_pd.lagr.mean(): -11.400522232055664 
model_pd.lambdas: dict_items([('pout', tensor([1.4163], device='cuda:0')), ('power', tensor([0.7132], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2718], device='cuda:0')), ('power', tensor([-18.7116], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.16195061802864075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5084, 6.5347, 6.5123],
        [6.5084, 8.3130, 9.4476],
        [6.5084, 7.2217, 7.2874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.16174128651618958 
model_pd.l_d.mean(): -11.543190002441406 
model_pd.lagr.mean(): -11.381448745727539 
model_pd.lambdas: dict_items([('pout', tensor([1.4176], device='cuda:0')), ('power', tensor([0.7122], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2714], device='cuda:0')), ('power', tensor([-18.7105], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.16174128651618958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5105, 6.5368, 6.5144],
        [6.5105, 8.3155, 9.4503],
        [6.5105, 7.2239, 7.2895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.16153180599212646 
model_pd.l_d.mean(): -11.523906707763672 
model_pd.lagr.mean(): -11.362375259399414 
model_pd.lambdas: dict_items([('pout', tensor([1.4188], device='cuda:0')), ('power', tensor([0.7113], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2710], device='cuda:0')), ('power', tensor([-18.7095], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.16153180599212646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5126, 8.3180, 9.4529],
        [6.5126, 6.5389, 6.5165],
        [6.5126, 7.2261, 7.2917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.16132225096225739 
model_pd.l_d.mean(): -11.504629135131836 
model_pd.lagr.mean(): -11.343306541442871 
model_pd.lambdas: dict_items([('pout', tensor([1.4201], device='cuda:0')), ('power', tensor([0.7104], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2706], device='cuda:0')), ('power', tensor([-18.7084], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.16132225096225739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5148, 7.2283, 7.2939],
        [6.5148, 6.5411, 6.5187],
        [6.5148, 8.3206, 9.4556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.16111214458942413 
model_pd.l_d.mean(): -11.485350608825684 
model_pd.lagr.mean(): -11.324238777160645 
model_pd.lambdas: dict_items([('pout', tensor([1.4214], device='cuda:0')), ('power', tensor([0.7094], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2702], device='cuda:0')), ('power', tensor([-18.7074], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.16111214458942413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5170, 7.2305, 7.2960],
        [6.5170, 8.3231, 9.4583],
        [6.5170, 6.5432, 6.5208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.160902202129364 
model_pd.l_d.mean(): -11.466078758239746 
model_pd.lagr.mean(): -11.305176734924316 
model_pd.lambdas: dict_items([('pout', tensor([1.4226], device='cuda:0')), ('power', tensor([0.7085], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2698], device='cuda:0')), ('power', tensor([-18.7063], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.160902202129364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5191, 7.2328, 7.2982],
        [6.5191, 8.3256, 9.4610],
        [6.5191, 6.5454, 6.5230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.16069194674491882 
model_pd.l_d.mean(): -11.446806907653809 
model_pd.lagr.mean(): -11.286114692687988 
model_pd.lambdas: dict_items([('pout', tensor([1.4239], device='cuda:0')), ('power', tensor([0.7076], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2694], device='cuda:0')), ('power', tensor([-18.7053], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.16069194674491882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5213, 8.3282, 9.4637],
        [6.5213, 6.5476, 6.5252],
        [6.5213, 7.2350, 7.3005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.16048181056976318 
model_pd.l_d.mean(): -11.427539825439453 
model_pd.lagr.mean(): -11.267058372497559 
model_pd.lambdas: dict_items([('pout', tensor([1.4252], device='cuda:0')), ('power', tensor([0.7066], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2690], device='cuda:0')), ('power', tensor([-18.7042], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.16048181056976318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5235, 6.5498, 6.5274],
        [6.5235, 7.2373, 7.3027],
        [6.5235, 8.3308, 9.4664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.16027143597602844 
model_pd.l_d.mean(): -11.40827465057373 
model_pd.lagr.mean(): -11.248003005981445 
model_pd.lambdas: dict_items([('pout', tensor([1.4264], device='cuda:0')), ('power', tensor([0.7057], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2686], device='cuda:0')), ('power', tensor([-18.7031], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.16027143597602844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5257, 7.2396, 7.3049],
        [6.5257, 6.5520, 6.5296],
        [6.5257, 8.3333, 9.4692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.16006095707416534 
model_pd.l_d.mean(): -11.389012336730957 
model_pd.lagr.mean(): -11.228951454162598 
model_pd.lambdas: dict_items([('pout', tensor([1.4277], device='cuda:0')), ('power', tensor([0.7048], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2681], device='cuda:0')), ('power', tensor([-18.7021], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.16006095707416534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5279, 7.2418, 7.3072],
        [6.5279, 8.3359, 9.4719],
        [6.5279, 6.5542, 6.5318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.15985041856765747 
model_pd.l_d.mean(): -11.369752883911133 
model_pd.lagr.mean(): -11.2099027633667 
model_pd.lambdas: dict_items([('pout', tensor([1.4290], device='cuda:0')), ('power', tensor([0.7038], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2677], device='cuda:0')), ('power', tensor([-18.7010], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.15985041856765747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5301, 8.3385, 9.4747],
        [6.5301, 6.5564, 6.5340],
        [6.5301, 7.2441, 7.3094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.15963977575302124 
model_pd.l_d.mean(): -11.350496292114258 
model_pd.lagr.mean(): -11.19085693359375 
model_pd.lambdas: dict_items([('pout', tensor([1.4302], device='cuda:0')), ('power', tensor([0.7029], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2673], device='cuda:0')), ('power', tensor([-18.6999], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.15963977575302124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5323, 8.3412, 9.4775],
        [6.5323, 7.2464, 7.3117],
        [6.5323, 6.5586, 6.5362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.15942907333374023 
model_pd.l_d.mean(): -11.331242561340332 
model_pd.lagr.mean(): -11.17181396484375 
model_pd.lambdas: dict_items([('pout', tensor([1.4315], device='cuda:0')), ('power', tensor([0.7020], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2669], device='cuda:0')), ('power', tensor([-18.6988], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.15942907333374023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5346, 6.5608, 6.5384],
        [6.5346, 7.2487, 7.3139],
        [6.5346, 8.3438, 9.4803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.15921840071678162 
model_pd.l_d.mean(): -11.311990737915039 
model_pd.lagr.mean(): -11.152771949768066 
model_pd.lambdas: dict_items([('pout', tensor([1.4328], device='cuda:0')), ('power', tensor([0.7010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2664], device='cuda:0')), ('power', tensor([-18.6977], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.15921840071678162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5368, 7.2511, 7.3162],
        [6.5368, 6.5631, 6.5407],
        [6.5368, 8.3464, 9.4831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.15900743007659912 
model_pd.l_d.mean(): -11.292743682861328 
model_pd.lagr.mean(): -11.133736610412598 
model_pd.lambdas: dict_items([('pout', tensor([1.4340], device='cuda:0')), ('power', tensor([0.7001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2660], device='cuda:0')), ('power', tensor([-18.6966], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.15900743007659912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5391, 8.3491, 9.4859],
        [6.5391, 6.5653, 6.5430],
        [6.5391, 7.2534, 7.3185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.15879637002944946 
model_pd.l_d.mean(): -11.27349853515625 
model_pd.lagr.mean(): -11.114702224731445 
model_pd.lambdas: dict_items([('pout', tensor([1.4353], device='cuda:0')), ('power', tensor([0.6991], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2656], device='cuda:0')), ('power', tensor([-18.6955], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.15879637002944946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5413, 6.5676, 6.5452],
        [6.5413, 8.3518, 9.4888],
        [6.5413, 7.2558, 7.3208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.15858504176139832 
model_pd.l_d.mean(): -11.254256248474121 
model_pd.lagr.mean(): -11.095671653747559 
model_pd.lambdas: dict_items([('pout', tensor([1.4366], device='cuda:0')), ('power', tensor([0.6982], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2651], device='cuda:0')), ('power', tensor([-18.6943], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.15858504176139832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5436, 7.2581, 7.3232],
        [6.5436, 8.3545, 9.4916],
        [6.5436, 6.5699, 6.5475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.15837369859218597 
model_pd.l_d.mean(): -11.235017776489258 
model_pd.lagr.mean(): -11.076643943786621 
model_pd.lambdas: dict_items([('pout', tensor([1.4378], device='cuda:0')), ('power', tensor([0.6973], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2647], device='cuda:0')), ('power', tensor([-18.6932], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.15837369859218597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5459, 6.5722, 6.5498],
        [6.5459, 8.3572, 9.4945],
        [6.5459, 7.2605, 7.3255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.15816208720207214 
model_pd.l_d.mean(): -11.215782165527344 
model_pd.lagr.mean(): -11.05762004852295 
model_pd.lambdas: dict_items([('pout', tensor([1.4391], device='cuda:0')), ('power', tensor([0.6963], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2643], device='cuda:0')), ('power', tensor([-18.6921], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.15816208720207214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5482, 6.5745, 6.5521],
        [6.5482, 8.3599, 9.4974],
        [6.5482, 7.2629, 7.3279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.15795034170150757 
model_pd.l_d.mean(): -11.196549415588379 
model_pd.lagr.mean(): -11.038599014282227 
model_pd.lambdas: dict_items([('pout', tensor([1.4404], device='cuda:0')), ('power', tensor([0.6954], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2638], device='cuda:0')), ('power', tensor([-18.6909], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.15795034170150757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5505, 7.2653, 7.3302],
        [6.5505, 8.3626, 9.5003],
        [6.5505, 6.5768, 6.5544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.1577383577823639 
model_pd.l_d.mean(): -11.177318572998047 
model_pd.lagr.mean(): -11.019579887390137 
model_pd.lambdas: dict_items([('pout', tensor([1.4416], device='cuda:0')), ('power', tensor([0.6945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2634], device='cuda:0')), ('power', tensor([-18.6898], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.1577383577823639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5528, 7.2677, 7.3326],
        [6.5528, 8.3653, 9.5033],
        [6.5528, 6.5791, 6.5567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.15752629935741425 
model_pd.l_d.mean(): -11.158092498779297 
model_pd.lagr.mean(): -11.000566482543945 
model_pd.lambdas: dict_items([('pout', tensor([1.4429], device='cuda:0')), ('power', tensor([0.6935], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2629], device='cuda:0')), ('power', tensor([-18.6886], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.15752629935741425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5552, 6.5814, 6.5590],
        [6.5552, 8.3681, 9.5062],
        [6.5552, 7.2701, 7.3350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.15731416642665863 
model_pd.l_d.mean(): -11.138870239257812 
model_pd.lagr.mean(): -10.981555938720703 
model_pd.lambdas: dict_items([('pout', tensor([1.4442], device='cuda:0')), ('power', tensor([0.6926], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2625], device='cuda:0')), ('power', tensor([-18.6875], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.15731416642665863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5575, 6.5838, 6.5614],
        [6.5575, 8.3709, 9.5092],
        [6.5575, 7.2725, 7.3374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.15710195899009705 
model_pd.l_d.mean(): -11.119649887084961 
model_pd.lagr.mean(): -10.96254825592041 
model_pd.lambdas: dict_items([('pout', tensor([1.4454], device='cuda:0')), ('power', tensor([0.6917], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2621], device='cuda:0')), ('power', tensor([-18.6863], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.15710195899009705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5599, 6.5861, 6.5637],
        [6.5599, 8.3737, 9.5121],
        [6.5599, 7.2750, 7.3398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.15688960254192352 
model_pd.l_d.mean(): -11.100432395935059 
model_pd.lagr.mean(): -10.94354248046875 
model_pd.lambdas: dict_items([('pout', tensor([1.4467], device='cuda:0')), ('power', tensor([0.6907], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2616], device='cuda:0')), ('power', tensor([-18.6851], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.15688960254192352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5622, 8.3765, 9.5151],
        [6.5622, 7.2774, 7.3422],
        [6.5622, 6.5885, 6.5661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.15667733550071716 
model_pd.l_d.mean(): -11.081216812133789 
model_pd.lagr.mean(): -10.924539566040039 
model_pd.lambdas: dict_items([('pout', tensor([1.4479], device='cuda:0')), ('power', tensor([0.6898], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2612], device='cuda:0')), ('power', tensor([-18.6839], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.15667733550071716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5646, 8.3793, 9.5182],
        [6.5646, 7.2799, 7.3446],
        [6.5646, 6.5909, 6.5685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.15646480023860931 
model_pd.l_d.mean(): -11.062006950378418 
model_pd.lagr.mean(): -10.905542373657227 
model_pd.lambdas: dict_items([('pout', tensor([1.4492], device='cuda:0')), ('power', tensor([0.6889], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2607], device='cuda:0')), ('power', tensor([-18.6827], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.15646480023860931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5670, 6.5932, 6.5709],
        [6.5670, 7.2824, 7.3471],
        [6.5670, 8.3821, 9.5212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.1562523990869522 
model_pd.l_d.mean(): -11.04279899597168 
model_pd.lagr.mean(): -10.88654613494873 
model_pd.lambdas: dict_items([('pout', tensor([1.4505], device='cuda:0')), ('power', tensor([0.6879], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2602], device='cuda:0')), ('power', tensor([-18.6815], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.1562523990869522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5694, 8.3850, 9.5242],
        [6.5694, 7.2849, 7.3495],
        [6.5694, 6.5956, 6.5732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.15603983402252197 
model_pd.l_d.mean(): -11.02359390258789 
model_pd.lagr.mean(): -10.8675537109375 
model_pd.lambdas: dict_items([('pout', tensor([1.4517], device='cuda:0')), ('power', tensor([0.6870], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2598], device='cuda:0')), ('power', tensor([-18.6803], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.15603983402252197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5718, 7.2874, 7.3520],
        [6.5718, 8.3878, 9.5273],
        [6.5718, 6.5980, 6.5757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.155827134847641 
model_pd.l_d.mean(): -11.004392623901367 
model_pd.lagr.mean(): -10.848565101623535 
model_pd.lambdas: dict_items([('pout', tensor([1.4530], device='cuda:0')), ('power', tensor([0.6861], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2593], device='cuda:0')), ('power', tensor([-18.6791], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.155827134847641
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5742, 6.6004, 6.5781],
        [6.5742, 8.3907, 9.5304],
        [6.5742, 7.2899, 7.3545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.15561436116695404 
model_pd.l_d.mean(): -10.98519515991211 
model_pd.lagr.mean(): -10.829581260681152 
model_pd.lambdas: dict_items([('pout', tensor([1.4542], device='cuda:0')), ('power', tensor([0.6851], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2589], device='cuda:0')), ('power', tensor([-18.6779], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.15561436116695404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5766, 6.6029, 6.5805],
        [6.5766, 8.3936, 9.5335],
        [6.5766, 7.2924, 7.3570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.1554013192653656 
model_pd.l_d.mean(): -10.965999603271484 
model_pd.lagr.mean(): -10.810598373413086 
model_pd.lambdas: dict_items([('pout', tensor([1.4555], device='cuda:0')), ('power', tensor([0.6842], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2584], device='cuda:0')), ('power', tensor([-18.6767], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.1554013192653656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5791, 8.3965, 9.5366],
        [6.5791, 7.2950, 7.3595],
        [6.5791, 6.6053, 6.5829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.15518826246261597 
model_pd.l_d.mean(): -10.946809768676758 
model_pd.lagr.mean(): -10.791621208190918 
model_pd.lambdas: dict_items([('pout', tensor([1.4568], device='cuda:0')), ('power', tensor([0.6833], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2579], device='cuda:0')), ('power', tensor([-18.6754], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.15518826246261597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5815, 6.6078, 6.5854],
        [6.5815, 7.2975, 7.3620],
        [6.5815, 8.3994, 9.5397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.1549750119447708 
model_pd.l_d.mean(): -10.927620887756348 
model_pd.lagr.mean(): -10.772645950317383 
model_pd.lambdas: dict_items([('pout', tensor([1.4580], device='cuda:0')), ('power', tensor([0.6823], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2575], device='cuda:0')), ('power', tensor([-18.6742], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.1549750119447708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5840, 8.4023, 9.5429],
        [6.5840, 7.3001, 7.3645],
        [6.5840, 6.6102, 6.5879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.15476146340370178 
model_pd.l_d.mean(): -10.90843677520752 
model_pd.lagr.mean(): -10.75367546081543 
model_pd.lambdas: dict_items([('pout', tensor([1.4593], device='cuda:0')), ('power', tensor([0.6814], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2570], device='cuda:0')), ('power', tensor([-18.6729], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.15476146340370178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5865, 6.6127, 6.5903],
        [6.5865, 7.3027, 7.3671],
        [6.5865, 8.4053, 9.5460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.15454789996147156 
model_pd.l_d.mean(): -10.889253616333008 
model_pd.lagr.mean(): -10.734705924987793 
model_pd.lambdas: dict_items([('pout', tensor([1.4605], device='cuda:0')), ('power', tensor([0.6805], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2565], device='cuda:0')), ('power', tensor([-18.6717], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.15454789996147156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5889, 7.3053, 7.3697],
        [6.5889, 8.4083, 9.5492],
        [6.5889, 6.6152, 6.5928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.15433406829833984 
model_pd.l_d.mean(): -10.870074272155762 
model_pd.lagr.mean(): -10.715740203857422 
model_pd.lambdas: dict_items([('pout', tensor([1.4618], device='cuda:0')), ('power', tensor([0.6795], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2561], device='cuda:0')), ('power', tensor([-18.6704], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.15433406829833984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5914, 8.4112, 9.5524],
        [6.5914, 7.3079, 7.3722],
        [6.5914, 6.6177, 6.5953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.15412017703056335 
model_pd.l_d.mean(): -10.850897789001465 
model_pd.lagr.mean(): -10.69677734375 
model_pd.lambdas: dict_items([('pout', tensor([1.4630], device='cuda:0')), ('power', tensor([0.6786], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2556], device='cuda:0')), ('power', tensor([-18.6691], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.15412017703056335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5940, 8.4142, 9.5556],
        [6.5940, 7.3105, 7.3748],
        [6.5940, 6.6202, 6.5978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.15390634536743164 
model_pd.l_d.mean(): -10.831725120544434 
model_pd.lagr.mean(): -10.677818298339844 
model_pd.lambdas: dict_items([('pout', tensor([1.4643], device='cuda:0')), ('power', tensor([0.6777], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2551], device='cuda:0')), ('power', tensor([-18.6679], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.15390634536743164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5965, 8.4172, 9.5589],
        [6.5965, 6.6227, 6.6004],
        [6.5965, 7.3132, 7.3774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.15369224548339844 
model_pd.l_d.mean(): -10.8125581741333 
model_pd.lagr.mean(): -10.658865928649902 
model_pd.lambdas: dict_items([('pout', tensor([1.4656], device='cuda:0')), ('power', tensor([0.6767], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2546], device='cuda:0')), ('power', tensor([-18.6666], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.15369224548339844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5990, 7.3158, 7.3801],
        [6.5990, 8.4203, 9.5621],
        [6.5990, 6.6253, 6.6029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.15347808599472046 
model_pd.l_d.mean(): -10.793392181396484 
model_pd.lagr.mean(): -10.639914512634277 
model_pd.lambdas: dict_items([('pout', tensor([1.4668], device='cuda:0')), ('power', tensor([0.6758], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2541], device='cuda:0')), ('power', tensor([-18.6653], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.15347808599472046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6016, 6.6278, 6.6054],
        [6.6016, 8.4233, 9.5654],
        [6.6016, 7.3185, 7.3827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.15326392650604248 
model_pd.l_d.mean(): -10.774230003356934 
model_pd.lagr.mean(): -10.620965957641602 
model_pd.lambdas: dict_items([('pout', tensor([1.4681], device='cuda:0')), ('power', tensor([0.6749], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2537], device='cuda:0')), ('power', tensor([-18.6640], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.15326392650604248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6041, 7.3212, 7.3853],
        [6.6041, 6.6304, 6.6080],
        [6.6041, 8.4264, 9.5687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.15304963290691376 
model_pd.l_d.mean(): -10.755071640014648 
model_pd.lagr.mean(): -10.602022171020508 
model_pd.lambdas: dict_items([('pout', tensor([1.4693], device='cuda:0')), ('power', tensor([0.6739], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2532], device='cuda:0')), ('power', tensor([-18.6627], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.15304963290691376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6067, 7.3238, 7.3880],
        [6.6067, 6.6329, 6.6106],
        [6.6067, 8.4295, 9.5720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.15283529460430145 
model_pd.l_d.mean(): -10.735918045043945 
model_pd.lagr.mean(): -10.583083152770996 
model_pd.lambdas: dict_items([('pout', tensor([1.4706], device='cuda:0')), ('power', tensor([0.6730], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2527], device='cuda:0')), ('power', tensor([-18.6613], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.15283529460430145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6093, 8.4326, 9.5754],
        [6.6093, 6.6355, 6.6131],
        [6.6093, 7.3266, 7.3907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.15262077748775482 
model_pd.l_d.mean(): -10.716768264770508 
model_pd.lagr.mean(): -10.56414794921875 
model_pd.lambdas: dict_items([('pout', tensor([1.4718], device='cuda:0')), ('power', tensor([0.6721], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2522], device='cuda:0')), ('power', tensor([-18.6600], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.15262077748775482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6119, 7.3293, 7.3934],
        [6.6119, 6.6381, 6.6157],
        [6.6119, 8.4357, 9.5787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.1524062156677246 
model_pd.l_d.mean(): -10.697620391845703 
model_pd.lagr.mean(): -10.54521369934082 
model_pd.lambdas: dict_items([('pout', tensor([1.4731], device='cuda:0')), ('power', tensor([0.6711], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2517], device='cuda:0')), ('power', tensor([-18.6587], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.1524062156677246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6145, 7.3320, 7.3961],
        [6.6145, 6.6407, 6.6184],
        [6.6145, 8.4388, 9.5821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.15219157934188843 
model_pd.l_d.mean(): -10.678475379943848 
model_pd.lagr.mean(): -10.526284217834473 
model_pd.lambdas: dict_items([('pout', tensor([1.4743], device='cuda:0')), ('power', tensor([0.6702], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2512], device='cuda:0')), ('power', tensor([-18.6573], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.15219157934188843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6171, 7.3348, 7.3988],
        [6.6171, 8.4419, 9.5855],
        [6.6171, 6.6433, 6.6210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.15197670459747314 
model_pd.l_d.mean(): -10.65933609008789 
model_pd.lagr.mean(): -10.507359504699707 
model_pd.lambdas: dict_items([('pout', tensor([1.4756], device='cuda:0')), ('power', tensor([0.6693], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2507], device='cuda:0')), ('power', tensor([-18.6560], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.15197670459747314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6197, 8.4451, 9.5889],
        [6.6197, 6.6460, 6.6236],
        [6.6197, 7.3375, 7.4015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.15176165103912354 
model_pd.l_d.mean(): -10.640198707580566 
model_pd.lagr.mean(): -10.488436698913574 
model_pd.lambdas: dict_items([('pout', tensor([1.4768], device='cuda:0')), ('power', tensor([0.6683], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2502], device='cuda:0')), ('power', tensor([-18.6546], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.15176165103912354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6224, 6.6486, 6.6263],
        [6.6224, 7.3403, 7.4043],
        [6.6224, 8.4483, 9.5923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.15154653787612915 
model_pd.l_d.mean(): -10.621064186096191 
model_pd.lagr.mean(): -10.469517707824707 
model_pd.lambdas: dict_items([('pout', tensor([1.4781], device='cuda:0')), ('power', tensor([0.6674], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2497], device='cuda:0')), ('power', tensor([-18.6532], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.15154653787612915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6250, 6.6513, 6.6289],
        [6.6250, 8.4515, 9.5958],
        [6.6250, 7.3431, 7.4070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.15133127570152283 
model_pd.l_d.mean(): -10.601933479309082 
model_pd.lagr.mean(): -10.450602531433105 
model_pd.lambdas: dict_items([('pout', tensor([1.4793], device='cuda:0')), ('power', tensor([0.6665], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2492], device='cuda:0')), ('power', tensor([-18.6519], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.15133127570152283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6277, 8.4547, 9.5993],
        [6.6277, 7.3459, 7.4098],
        [6.6277, 6.6540, 6.6316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.1511160284280777 
model_pd.l_d.mean(): -10.582808494567871 
model_pd.lagr.mean(): -10.431692123413086 
model_pd.lambdas: dict_items([('pout', tensor([1.4806], device='cuda:0')), ('power', tensor([0.6655], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2487], device='cuda:0')), ('power', tensor([-18.6505], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.1511160284280777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6304, 7.3487, 7.4126],
        [6.6304, 8.4579, 9.6028],
        [6.6304, 6.6567, 6.6343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.1509004533290863 
model_pd.l_d.mean(): -10.56368637084961 
model_pd.lagr.mean(): -10.412785530090332 
model_pd.lambdas: dict_items([('pout', tensor([1.4818], device='cuda:0')), ('power', tensor([0.6646], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2482], device='cuda:0')), ('power', tensor([-18.6491], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.1509004533290863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6331, 6.6594, 6.6370],
        [6.6331, 8.4612, 9.6063],
        [6.6331, 7.3516, 7.4154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.1506848931312561 
model_pd.l_d.mean(): -10.544567108154297 
model_pd.lagr.mean(): -10.393881797790527 
model_pd.lambdas: dict_items([('pout', tensor([1.4831], device='cuda:0')), ('power', tensor([0.6637], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2476], device='cuda:0')), ('power', tensor([-18.6477], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.1506848931312561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6358, 6.6621, 6.6397],
        [6.6358, 8.4645, 9.6098],
        [6.6358, 7.3544, 7.4182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.15046925842761993 
model_pd.l_d.mean(): -10.525452613830566 
model_pd.lagr.mean(): -10.374983787536621 
model_pd.lambdas: dict_items([('pout', tensor([1.4843], device='cuda:0')), ('power', tensor([0.6627], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2471], device='cuda:0')), ('power', tensor([-18.6463], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.15046925842761993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6385, 6.6648, 6.6424],
        [6.6385, 7.3573, 7.4210],
        [6.6385, 8.4677, 9.6134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1502535492181778 
model_pd.l_d.mean(): -10.506339073181152 
model_pd.lagr.mean(): -10.356085777282715 
model_pd.lambdas: dict_items([('pout', tensor([1.4856], device='cuda:0')), ('power', tensor([0.6618], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2466], device='cuda:0')), ('power', tensor([-18.6448], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1502535492181778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6413, 6.6675, 6.6452],
        [6.6413, 7.3602, 7.4239],
        [6.6413, 8.4711, 9.6169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.1500377058982849 
model_pd.l_d.mean(): -10.48723316192627 
model_pd.lagr.mean(): -10.33719539642334 
model_pd.lambdas: dict_items([('pout', tensor([1.4868], device='cuda:0')), ('power', tensor([0.6609], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2461], device='cuda:0')), ('power', tensor([-18.6434], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.1500377058982849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6440, 6.6703, 6.6479],
        [6.6440, 8.4744, 9.6205],
        [6.6440, 7.3631, 7.4268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.14982184767723083 
model_pd.l_d.mean(): -10.468130111694336 
model_pd.lagr.mean(): -10.318307876586914 
model_pd.lambdas: dict_items([('pout', tensor([1.4881], device='cuda:0')), ('power', tensor([0.6599], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2456], device='cuda:0')), ('power', tensor([-18.6420], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.14982184767723083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6468, 6.6731, 6.6507],
        [6.6468, 7.3660, 7.4297],
        [6.6468, 8.4777, 9.6242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.14960581064224243 
model_pd.l_d.mean(): -10.449029922485352 
model_pd.lagr.mean(): -10.299424171447754 
model_pd.lambdas: dict_items([('pout', tensor([1.4893], device='cuda:0')), ('power', tensor([0.6590], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2450], device='cuda:0')), ('power', tensor([-18.6405], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.14960581064224243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6496, 7.3689, 7.4326],
        [6.6496, 8.4811, 9.6278],
        [6.6496, 6.6758, 6.6535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.14938978850841522 
model_pd.l_d.mean(): -10.429932594299316 
model_pd.lagr.mean(): -10.280542373657227 
model_pd.lambdas: dict_items([('pout', tensor([1.4905], device='cuda:0')), ('power', tensor([0.6581], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2445], device='cuda:0')), ('power', tensor([-18.6391], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.14938978850841522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6524, 7.3719, 7.4355],
        [6.6524, 8.4844, 9.6315],
        [6.6524, 6.6786, 6.6563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14917364716529846 
model_pd.l_d.mean(): -10.410840034484863 
model_pd.lagr.mean(): -10.261666297912598 
model_pd.lambdas: dict_items([('pout', tensor([1.4918], device='cuda:0')), ('power', tensor([0.6571], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2440], device='cuda:0')), ('power', tensor([-18.6376], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14917364716529846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6552, 6.6815, 6.6591],
        [6.6552, 7.3748, 7.4384],
        [6.6552, 8.4878, 9.6351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.1489572376012802 
model_pd.l_d.mean(): -10.391751289367676 
model_pd.lagr.mean(): -10.242794036865234 
model_pd.lambdas: dict_items([('pout', tensor([1.4930], device='cuda:0')), ('power', tensor([0.6562], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2434], device='cuda:0')), ('power', tensor([-18.6361], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.1489572376012802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6580, 7.3778, 7.4413],
        [6.6580, 6.6843, 6.6619],
        [6.6580, 8.4913, 9.6388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.14874087274074554 
model_pd.l_d.mean(): -10.37266731262207 
model_pd.lagr.mean(): -10.223926544189453 
model_pd.lambdas: dict_items([('pout', tensor([1.4943], device='cuda:0')), ('power', tensor([0.6553], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2429], device='cuda:0')), ('power', tensor([-18.6347], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.14874087274074554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6609, 8.4947, 9.6426],
        [6.6609, 6.6871, 6.6647],
        [6.6609, 7.3808, 7.4443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.14852438867092133 
model_pd.l_d.mean(): -10.353586196899414 
model_pd.lagr.mean(): -10.205061912536621 
model_pd.lambdas: dict_items([('pout', tensor([1.4955], device='cuda:0')), ('power', tensor([0.6544], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2424], device='cuda:0')), ('power', tensor([-18.6332], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.14852438867092133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6637, 6.6900, 6.6676],
        [6.6637, 7.3838, 7.4473],
        [6.6637, 8.4981, 9.6463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.1483076512813568 
model_pd.l_d.mean(): -10.334508895874023 
model_pd.lagr.mean(): -10.186201095581055 
model_pd.lambdas: dict_items([('pout', tensor([1.4968], device='cuda:0')), ('power', tensor([0.6534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2418], device='cuda:0')), ('power', tensor([-18.6317], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.1483076512813568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6666, 7.3868, 7.4503],
        [6.6666, 8.5016, 9.6501],
        [6.6666, 6.6928, 6.6704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.14809100329875946 
model_pd.l_d.mean(): -10.315437316894531 
model_pd.lagr.mean(): -10.167346000671387 
model_pd.lambdas: dict_items([('pout', tensor([1.4980], device='cuda:0')), ('power', tensor([0.6525], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2413], device='cuda:0')), ('power', tensor([-18.6301], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.14809100329875946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6695, 8.5051, 9.6539],
        [6.6695, 7.3899, 7.4533],
        [6.6695, 6.6957, 6.6733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.1478741616010666 
model_pd.l_d.mean(): -10.296365737915039 
model_pd.lagr.mean(): -10.148491859436035 
model_pd.lambdas: dict_items([('pout', tensor([1.4992], device='cuda:0')), ('power', tensor([0.6516], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2407], device='cuda:0')), ('power', tensor([-18.6286], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.1478741616010666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6724, 7.3929, 7.4563],
        [6.6724, 8.5086, 9.6577],
        [6.6724, 6.6986, 6.6762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.14765726029872894 
model_pd.l_d.mean(): -10.277302742004395 
model_pd.lagr.mean(): -10.129645347595215 
model_pd.lambdas: dict_items([('pout', tensor([1.5005], device='cuda:0')), ('power', tensor([0.6506], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2402], device='cuda:0')), ('power', tensor([-18.6271], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.14765726029872894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6753, 8.5122, 9.6615],
        [6.6753, 6.7015, 6.6791],
        [6.6753, 7.3960, 7.4594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.1474403440952301 
model_pd.l_d.mean(): -10.258241653442383 
model_pd.lagr.mean(): -10.110801696777344 
model_pd.lambdas: dict_items([('pout', tensor([1.5017], device='cuda:0')), ('power', tensor([0.6497], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2396], device='cuda:0')), ('power', tensor([-18.6255], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.1474403440952301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6782, 6.7045, 6.6821],
        [6.6782, 7.3991, 7.4624],
        [6.6782, 8.5157, 9.6654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.14722323417663574 
model_pd.l_d.mean(): -10.23918342590332 
model_pd.lagr.mean(): -10.091959953308105 
model_pd.lambdas: dict_items([('pout', tensor([1.5030], device='cuda:0')), ('power', tensor([0.6488], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2391], device='cuda:0')), ('power', tensor([-18.6240], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.14722323417663574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6811, 6.7074, 6.6850],
        [6.6811, 8.5193, 9.6693],
        [6.6811, 7.4022, 7.4655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.14700615406036377 
model_pd.l_d.mean(): -10.220132827758789 
model_pd.lagr.mean(): -10.073126792907715 
model_pd.lambdas: dict_items([('pout', tensor([1.5042], device='cuda:0')), ('power', tensor([0.6478], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2385], device='cuda:0')), ('power', tensor([-18.6224], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.14700615406036377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6841, 7.4053, 7.4686],
        [6.6841, 6.7103, 6.6879],
        [6.6841, 8.5229, 9.6732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.14678892493247986 
model_pd.l_d.mean(): -10.201081275939941 
model_pd.lagr.mean(): -10.054292678833008 
model_pd.lambdas: dict_items([('pout', tensor([1.5054], device='cuda:0')), ('power', tensor([0.6469], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2380], device='cuda:0')), ('power', tensor([-18.6209], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.14678892493247986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6870, 7.4085, 7.4717],
        [6.6870, 6.7133, 6.6909],
        [6.6870, 8.5265, 9.6771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.14657162129878998 
model_pd.l_d.mean(): -10.182037353515625 
model_pd.lagr.mean(): -10.035466194152832 
model_pd.lambdas: dict_items([('pout', tensor([1.5067], device='cuda:0')), ('power', tensor([0.6460], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2374], device='cuda:0')), ('power', tensor([-18.6193], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.14657162129878998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6900, 7.4116, 7.4749],
        [6.6900, 8.5301, 9.6811],
        [6.6900, 6.7163, 6.6939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.14635434746742249 
model_pd.l_d.mean(): -10.16299819946289 
model_pd.lagr.mean(): -10.016643524169922 
model_pd.lambdas: dict_items([('pout', tensor([1.5079], device='cuda:0')), ('power', tensor([0.6450], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2368], device='cuda:0')), ('power', tensor([-18.6177], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.14635434746742249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6930, 6.7193, 6.6969],
        [6.6930, 8.5337, 9.6851],
        [6.6930, 7.4148, 7.4780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.14613690972328186 
model_pd.l_d.mean(): -10.143961906433105 
model_pd.lagr.mean(): -9.997824668884277 
model_pd.lambdas: dict_items([('pout', tensor([1.5091], device='cuda:0')), ('power', tensor([0.6441], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2363], device='cuda:0')), ('power', tensor([-18.6161], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.14613690972328186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6960, 6.7223, 6.6999],
        [6.6960, 7.4180, 7.4812],
        [6.6960, 8.5374, 9.6891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.14591936767101288 
model_pd.l_d.mean(): -10.124930381774902 
model_pd.lagr.mean(): -9.979010581970215 
model_pd.lambdas: dict_items([('pout', tensor([1.5104], device='cuda:0')), ('power', tensor([0.6432], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2357], device='cuda:0')), ('power', tensor([-18.6145], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.14591936767101288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6991, 7.4212, 7.4844],
        [6.6991, 6.7254, 6.7029],
        [6.6991, 8.5411, 9.6931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.14570167660713196 
model_pd.l_d.mean(): -10.105901718139648 
model_pd.lagr.mean(): -9.960200309753418 
model_pd.lambdas: dict_items([('pout', tensor([1.5116], device='cuda:0')), ('power', tensor([0.6422], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2351], device='cuda:0')), ('power', tensor([-18.6129], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.14570167660713196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7021, 8.5448, 9.6971],
        [6.7021, 6.7284, 6.7060],
        [6.7021, 7.4245, 7.4876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.14548389613628387 
model_pd.l_d.mean(): -10.086877822875977 
model_pd.lagr.mean(): -9.941393852233887 
model_pd.lambdas: dict_items([('pout', tensor([1.5128], device='cuda:0')), ('power', tensor([0.6413], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2345], device='cuda:0')), ('power', tensor([-18.6112], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.14548389613628387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7052, 8.5486, 9.7012],
        [6.7052, 6.7315, 6.7091],
        [6.7052, 7.4277, 7.4908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.14526580274105072 
model_pd.l_d.mean(): -10.067858695983887 
model_pd.lagr.mean(): -9.922593116760254 
model_pd.lambdas: dict_items([('pout', tensor([1.5141], device='cuda:0')), ('power', tensor([0.6404], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2340], device='cuda:0')), ('power', tensor([-18.6096], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.14526580274105072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7083, 8.5523, 9.7053],
        [6.7083, 7.4310, 7.4940],
        [6.7083, 6.7346, 6.7121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.14504757523536682 
model_pd.l_d.mean(): -10.048843383789062 
model_pd.lagr.mean(): -9.903796195983887 
model_pd.lambdas: dict_items([('pout', tensor([1.5153], device='cuda:0')), ('power', tensor([0.6395], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2334], device='cuda:0')), ('power', tensor([-18.6079], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.14504757523536682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7114, 8.5561, 9.7095],
        [6.7114, 6.7377, 6.7152],
        [6.7114, 7.4343, 7.4973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.14482928812503815 
model_pd.l_d.mean(): -10.02983283996582 
model_pd.lagr.mean(): -9.885003089904785 
model_pd.lambdas: dict_items([('pout', tensor([1.5165], device='cuda:0')), ('power', tensor([0.6385], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2328], device='cuda:0')), ('power', tensor([-18.6063], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.14482928812503815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7145, 6.7408, 6.7184],
        [6.7145, 8.5599, 9.7136],
        [6.7145, 7.4376, 7.5006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.14461083710193634 
model_pd.l_d.mean(): -10.01082706451416 
model_pd.lagr.mean(): -9.866216659545898 
model_pd.lambdas: dict_items([('pout', tensor([1.5178], device='cuda:0')), ('power', tensor([0.6376], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2322], device='cuda:0')), ('power', tensor([-18.6046], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.14461083710193634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7176, 7.4409, 7.5039],
        [6.7176, 6.7439, 6.7215],
        [6.7176, 8.5637, 9.7178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.1443922370672226 
model_pd.l_d.mean(): -9.991825103759766 
model_pd.lagr.mean(): -9.847433090209961 
model_pd.lambdas: dict_items([('pout', tensor([1.5190], device='cuda:0')), ('power', tensor([0.6367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2316], device='cuda:0')), ('power', tensor([-18.6029], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.1443922370672226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7208, 6.7471, 6.7246],
        [6.7208, 7.4442, 7.5072],
        [6.7208, 8.5676, 9.7220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.14417371153831482 
model_pd.l_d.mean(): -9.972826957702637 
model_pd.lagr.mean(): -9.828653335571289 
model_pd.lambdas: dict_items([('pout', tensor([1.5202], device='cuda:0')), ('power', tensor([0.6357], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2310], device='cuda:0')), ('power', tensor([-18.6012], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.14417371153831482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7239, 8.5715, 9.7262],
        [6.7239, 6.7502, 6.7278],
        [6.7239, 7.4476, 7.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.1439553052186966 
model_pd.l_d.mean(): -9.953834533691406 
model_pd.lagr.mean(): -9.809879302978516 
model_pd.lambdas: dict_items([('pout', tensor([1.5215], device='cuda:0')), ('power', tensor([0.6348], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2304], device='cuda:0')), ('power', tensor([-18.5995], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.1439553052186966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7271, 6.7534, 6.7310],
        [6.7271, 8.5753, 9.7305],
        [6.7271, 7.4510, 7.5139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.14373674988746643 
model_pd.l_d.mean(): -9.934844970703125 
model_pd.lagr.mean(): -9.791108131408691 
model_pd.lambdas: dict_items([('pout', tensor([1.5227], device='cuda:0')), ('power', tensor([0.6339], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2298], device='cuda:0')), ('power', tensor([-18.5978], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.14373674988746643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7303, 6.7566, 6.7342],
        [6.7303, 7.4544, 7.5172],
        [6.7303, 8.5793, 9.7348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.14351843297481537 
model_pd.l_d.mean(): -9.915861129760742 
model_pd.lagr.mean(): -9.772342681884766 
model_pd.lambdas: dict_items([('pout', tensor([1.5239], device='cuda:0')), ('power', tensor([0.6329], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2292], device='cuda:0')), ('power', tensor([-18.5960], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.14351843297481537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7335, 6.7598, 6.7374],
        [6.7335, 8.5832, 9.7391],
        [6.7335, 7.4578, 7.5206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.14329999685287476 
model_pd.l_d.mean(): -9.896882057189941 
model_pd.lagr.mean(): -9.753582000732422 
model_pd.lambdas: dict_items([('pout', tensor([1.5252], device='cuda:0')), ('power', tensor([0.6320], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2286], device='cuda:0')), ('power', tensor([-18.5943], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.14329999685287476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7367, 8.5871, 9.7434],
        [6.7367, 6.7630, 6.7406],
        [6.7367, 7.4612, 7.5240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.14308156073093414 
model_pd.l_d.mean(): -9.877908706665039 
model_pd.lagr.mean(): -9.734827041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.5264], device='cuda:0')), ('power', tensor([0.6311], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2280], device='cuda:0')), ('power', tensor([-18.5925], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.14308156073093414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7400, 8.5911, 9.7477],
        [6.7400, 7.4647, 7.5275],
        [6.7400, 6.7663, 6.7438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.14286310970783234 
model_pd.l_d.mean(): -9.858936309814453 
model_pd.lagr.mean(): -9.716073036193848 
model_pd.lambdas: dict_items([('pout', tensor([1.5276], device='cuda:0')), ('power', tensor([0.6302], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2274], device='cuda:0')), ('power', tensor([-18.5908], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.14286310970783234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7432, 6.7695, 6.7471],
        [6.7432, 7.4681, 7.5309],
        [6.7432, 8.5951, 9.7521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.14264465868473053 
model_pd.l_d.mean(): -9.839971542358398 
model_pd.lagr.mean(): -9.69732666015625 
model_pd.lambdas: dict_items([('pout', tensor([1.5288], device='cuda:0')), ('power', tensor([0.6292], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2268], device='cuda:0')), ('power', tensor([-18.5890], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.14264465868473053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7465, 7.4716, 7.5344],
        [6.7465, 8.5991, 9.7565],
        [6.7465, 6.7728, 6.7504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.1424262374639511 
model_pd.l_d.mean(): -9.82101058959961 
model_pd.lagr.mean(): -9.678584098815918 
model_pd.lambdas: dict_items([('pout', tensor([1.5301], device='cuda:0')), ('power', tensor([0.6283], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2262], device='cuda:0')), ('power', tensor([-18.5872], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.1424262374639511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7498, 8.6032, 9.7609],
        [6.7498, 6.7761, 6.7537],
        [6.7498, 7.4751, 7.5378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.14220768213272095 
model_pd.l_d.mean(): -9.802053451538086 
model_pd.lagr.mean(): -9.659845352172852 
model_pd.lambdas: dict_items([('pout', tensor([1.5313], device='cuda:0')), ('power', tensor([0.6274], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2255], device='cuda:0')), ('power', tensor([-18.5854], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.14220768213272095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7531, 8.6072, 9.7654],
        [6.7531, 6.7794, 6.7570],
        [6.7531, 7.4787, 7.5414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.14198905229568481 
model_pd.l_d.mean(): -9.783102989196777 
model_pd.lagr.mean(): -9.641114234924316 
model_pd.lambdas: dict_items([('pout', tensor([1.5325], device='cuda:0')), ('power', tensor([0.6264], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2249], device='cuda:0')), ('power', tensor([-18.5836], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.14198905229568481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7564, 6.7827, 6.7603],
        [6.7564, 8.6113, 9.7699],
        [6.7564, 7.4822, 7.5449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.1417703479528427 
model_pd.l_d.mean(): -9.764156341552734 
model_pd.lagr.mean(): -9.62238597869873 
model_pd.lambdas: dict_items([('pout', tensor([1.5337], device='cuda:0')), ('power', tensor([0.6255], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2243], device='cuda:0')), ('power', tensor([-18.5818], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.1417703479528427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7598, 7.4858, 7.5484],
        [6.7598, 6.7861, 6.7636],
        [6.7598, 8.6154, 9.7744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.14155158400535583 
model_pd.l_d.mean(): -9.745213508605957 
model_pd.lagr.mean(): -9.60366153717041 
model_pd.lambdas: dict_items([('pout', tensor([1.5350], device='cuda:0')), ('power', tensor([0.6246], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2236], device='cuda:0')), ('power', tensor([-18.5800], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.14155158400535583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7631, 7.4894, 7.5520],
        [6.7631, 6.7894, 6.7670],
        [6.7631, 8.6196, 9.7789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.14133280515670776 
model_pd.l_d.mean(): -9.726274490356445 
model_pd.lagr.mean(): -9.584941864013672 
model_pd.lambdas: dict_items([('pout', tensor([1.5362], device='cuda:0')), ('power', tensor([0.6237], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2230], device='cuda:0')), ('power', tensor([-18.5781], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.14133280515670776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7665, 7.4930, 7.5556],
        [6.7665, 6.7928, 6.7704],
        [6.7665, 8.6237, 9.7835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.14111390709877014 
model_pd.l_d.mean(): -9.707344055175781 
model_pd.lagr.mean(): -9.566229820251465 
model_pd.lambdas: dict_items([('pout', tensor([1.5374], device='cuda:0')), ('power', tensor([0.6227], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2224], device='cuda:0')), ('power', tensor([-18.5763], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.14111390709877014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7699, 7.4966, 7.5592],
        [6.7699, 8.6279, 9.7881],
        [6.7699, 6.7962, 6.7738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.1408950537443161 
model_pd.l_d.mean(): -9.68841552734375 
model_pd.lagr.mean(): -9.547520637512207 
model_pd.lambdas: dict_items([('pout', tensor([1.5386], device='cuda:0')), ('power', tensor([0.6218], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2217], device='cuda:0')), ('power', tensor([-18.5744], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.1408950537443161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7733, 8.6321, 9.7927],
        [6.7733, 7.5002, 7.5628],
        [6.7733, 6.7996, 6.7772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.1406760960817337 
model_pd.l_d.mean(): -9.669493675231934 
model_pd.lagr.mean(): -9.528817176818848 
model_pd.lambdas: dict_items([('pout', tensor([1.5399], device='cuda:0')), ('power', tensor([0.6209], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2211], device='cuda:0')), ('power', tensor([-18.5725], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.1406760960817337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7767, 8.6363, 9.7974],
        [6.7767, 7.5039, 7.5664],
        [6.7767, 6.8031, 6.7806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.14045721292495728 
model_pd.l_d.mean(): -9.6505765914917 
model_pd.lagr.mean(): -9.510119438171387 
model_pd.lambdas: dict_items([('pout', tensor([1.5411], device='cuda:0')), ('power', tensor([0.6199], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2204], device='cuda:0')), ('power', tensor([-18.5706], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.14045721292495728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7802, 8.6406, 9.8021],
        [6.7802, 6.8065, 6.7841],
        [6.7802, 7.5076, 7.5701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.14023831486701965 
model_pd.l_d.mean(): -9.631665229797363 
model_pd.lagr.mean(): -9.491426467895508 
model_pd.lambdas: dict_items([('pout', tensor([1.5423], device='cuda:0')), ('power', tensor([0.6190], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2198], device='cuda:0')), ('power', tensor([-18.5687], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.14023831486701965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7837, 7.5113, 7.5738],
        [6.7837, 6.8100, 6.7875],
        [6.7837, 8.6449, 9.8068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.14001943171024323 
model_pd.l_d.mean(): -9.61275577545166 
model_pd.lagr.mean(): -9.472736358642578 
model_pd.lambdas: dict_items([('pout', tensor([1.5435], device='cuda:0')), ('power', tensor([0.6181], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2191], device='cuda:0')), ('power', tensor([-18.5668], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.14001943171024323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7871, 8.6492, 9.8115],
        [6.7871, 6.8135, 6.7910],
        [6.7871, 7.5150, 7.5775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.13980066776275635 
model_pd.l_d.mean(): -9.593852043151855 
model_pd.lagr.mean(): -9.45405101776123 
model_pd.lambdas: dict_items([('pout', tensor([1.5447], device='cuda:0')), ('power', tensor([0.6172], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2185], device='cuda:0')), ('power', tensor([-18.5649], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.13980066776275635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7906, 6.8170, 6.7945],
        [6.7906, 7.5188, 7.5812],
        [6.7906, 8.6535, 9.8163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.1395817995071411 
model_pd.l_d.mean(): -9.574954986572266 
model_pd.lagr.mean(): -9.435373306274414 
model_pd.lambdas: dict_items([('pout', tensor([1.5460], device='cuda:0')), ('power', tensor([0.6162], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2178], device='cuda:0')), ('power', tensor([-18.5629], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.1395817995071411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7942, 7.5225, 7.5850],
        [6.7942, 6.8205, 6.7980],
        [6.7942, 8.6578, 9.8210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.13936293125152588 
model_pd.l_d.mean(): -9.556062698364258 
model_pd.lagr.mean(): -9.416699409484863 
model_pd.lambdas: dict_items([('pout', tensor([1.5472], device='cuda:0')), ('power', tensor([0.6153], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2172], device='cuda:0')), ('power', tensor([-18.5610], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.13936293125152588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7977, 6.8241, 6.8016],
        [6.7977, 8.6622, 9.8259],
        [6.7977, 7.5263, 7.5887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.13914421200752258 
model_pd.l_d.mean(): -9.537175178527832 
model_pd.lagr.mean(): -9.398031234741211 
model_pd.lambdas: dict_items([('pout', tensor([1.5484], device='cuda:0')), ('power', tensor([0.6144], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2165], device='cuda:0')), ('power', tensor([-18.5590], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.13914421200752258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8012, 6.8276, 6.8051],
        [6.8012, 8.6666, 9.8307],
        [6.8012, 7.5301, 7.5925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.13892537355422974 
model_pd.l_d.mean(): -9.518293380737305 
model_pd.lagr.mean(): -9.37936782836914 
model_pd.lambdas: dict_items([('pout', tensor([1.5496], device='cuda:0')), ('power', tensor([0.6134], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2158], device='cuda:0')), ('power', tensor([-18.5570], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.13892537355422974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8048, 8.6710, 9.8356],
        [6.8048, 7.5340, 7.5963],
        [6.8048, 6.8312, 6.8087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.13870660960674286 
model_pd.l_d.mean(): -9.499415397644043 
model_pd.lagr.mean(): -9.360709190368652 
model_pd.lambdas: dict_items([('pout', tensor([1.5508], device='cuda:0')), ('power', tensor([0.6125], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2152], device='cuda:0')), ('power', tensor([-18.5550], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.13870660960674286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8084, 6.8348, 6.8123],
        [6.8084, 8.6755, 9.8405],
        [6.8084, 7.5378, 7.6002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.13848786056041718 
model_pd.l_d.mean(): -9.48054313659668 
model_pd.lagr.mean(): -9.342055320739746 
model_pd.lambdas: dict_items([('pout', tensor([1.5520], device='cuda:0')), ('power', tensor([0.6116], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2145], device='cuda:0')), ('power', tensor([-18.5530], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.13848786056041718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8120, 8.6799, 9.8454],
        [6.8120, 7.5417, 7.6040],
        [6.8120, 6.8384, 6.8159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.13826912641525269 
model_pd.l_d.mean(): -9.461678504943848 
model_pd.lagr.mean(): -9.323409080505371 
model_pd.lambdas: dict_items([('pout', tensor([1.5532], device='cuda:0')), ('power', tensor([0.6107], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2138], device='cuda:0')), ('power', tensor([-18.5510], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.13826912641525269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8156, 7.5456, 7.6079],
        [6.8156, 6.8420, 6.8195],
        [6.8156, 8.6844, 9.8504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.1380503624677658 
model_pd.l_d.mean(): -9.442815780639648 
model_pd.lagr.mean(): -9.304765701293945 
model_pd.lambdas: dict_items([('pout', tensor([1.5545], device='cuda:0')), ('power', tensor([0.6097], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2131], device='cuda:0')), ('power', tensor([-18.5490], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.1380503624677658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8193, 8.6890, 9.8554],
        [6.8193, 7.5495, 7.6118],
        [6.8193, 6.8457, 6.8232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.13783149421215057 
model_pd.l_d.mean(): -9.423961639404297 
model_pd.lagr.mean(): -9.28612995147705 
model_pd.lambdas: dict_items([('pout', tensor([1.5557], device='cuda:0')), ('power', tensor([0.6088], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2124], device='cuda:0')), ('power', tensor([-18.5469], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.13783149421215057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8230, 6.8494, 6.8268],
        [6.8230, 8.6935, 9.8604],
        [6.8230, 7.5535, 7.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.13761267066001892 
model_pd.l_d.mean(): -9.405111312866211 
model_pd.lagr.mean(): -9.267498970031738 
model_pd.lambdas: dict_items([('pout', tensor([1.5569], device='cuda:0')), ('power', tensor([0.6079], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2118], device='cuda:0')), ('power', tensor([-18.5449], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.13761267066001892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8267, 7.5574, 7.6196],
        [6.8267, 8.6981, 9.8654],
        [6.8267, 6.8531, 6.8305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.13739387691020966 
model_pd.l_d.mean(): -9.386263847351074 
model_pd.lagr.mean(): -9.248869895935059 
model_pd.lambdas: dict_items([('pout', tensor([1.5581], device='cuda:0')), ('power', tensor([0.6069], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2111], device='cuda:0')), ('power', tensor([-18.5428], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.13739387691020966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8304, 7.5614, 7.6236],
        [6.8304, 6.8568, 6.8342],
        [6.8304, 8.7027, 9.8705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.13717502355575562 
model_pd.l_d.mean(): -9.367423057556152 
model_pd.lagr.mean(): -9.23024845123291 
model_pd.lambdas: dict_items([('pout', tensor([1.5593], device='cuda:0')), ('power', tensor([0.6060], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2104], device='cuda:0')), ('power', tensor([-18.5407], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.13717502355575562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8341, 6.8605, 6.8380],
        [6.8341, 8.7073, 9.8756],
        [6.8341, 7.5654, 7.6276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.13695620000362396 
model_pd.l_d.mean(): -9.348589897155762 
model_pd.lagr.mean(): -9.211633682250977 
model_pd.lambdas: dict_items([('pout', tensor([1.5605], device='cuda:0')), ('power', tensor([0.6051], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2097], device='cuda:0')), ('power', tensor([-18.5386], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.13695620000362396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8378, 6.8643, 6.8417],
        [6.8378, 8.7120, 9.8808],
        [6.8378, 7.5694, 7.6316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.13673752546310425 
model_pd.l_d.mean(): -9.329760551452637 
model_pd.lagr.mean(): -9.193022727966309 
model_pd.lambdas: dict_items([('pout', tensor([1.5617], device='cuda:0')), ('power', tensor([0.6042], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2090], device='cuda:0')), ('power', tensor([-18.5365], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.13673752546310425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8416, 7.5735, 7.6356],
        [6.8416, 8.7167, 9.8860],
        [6.8416, 6.8680, 6.8455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.13651880621910095 
model_pd.l_d.mean(): -9.31093692779541 
model_pd.lagr.mean(): -9.174418449401855 
model_pd.lambdas: dict_items([('pout', tensor([1.5629], device='cuda:0')), ('power', tensor([0.6032], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2083], device='cuda:0')), ('power', tensor([-18.5344], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.13651880621910095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8454, 6.8718, 6.8493],
        [6.8454, 7.5775, 7.6397],
        [6.8454, 8.7214, 9.8912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.136300191283226 
model_pd.l_d.mean(): -9.292119026184082 
model_pd.lagr.mean(): -9.155818939208984 
model_pd.lambdas: dict_items([('pout', tensor([1.5641], device='cuda:0')), ('power', tensor([0.6023], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2075], device='cuda:0')), ('power', tensor([-18.5322], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.136300191283226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8492, 6.8756, 6.8531],
        [6.8492, 7.5816, 7.6438],
        [6.8492, 8.7261, 9.8964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.1360815465450287 
model_pd.l_d.mean(): -9.273305892944336 
model_pd.lagr.mean(): -9.137224197387695 
model_pd.lambdas: dict_items([('pout', tensor([1.5653], device='cuda:0')), ('power', tensor([0.6014], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2068], device='cuda:0')), ('power', tensor([-18.5301], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.1360815465450287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8530, 7.5858, 7.6479],
        [6.8530, 8.7309, 9.9017],
        [6.8530, 6.8795, 6.8569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.13586293160915375 
model_pd.l_d.mean(): -9.254499435424805 
model_pd.lagr.mean(): -9.118636131286621 
model_pd.lambdas: dict_items([('pout', tensor([1.5666], device='cuda:0')), ('power', tensor([0.6005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2061], device='cuda:0')), ('power', tensor([-18.5279], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.13586293160915375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8569, 6.8833, 6.8608],
        [6.8569, 8.7357, 9.9070],
        [6.8569, 7.5899, 7.6520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.1356443613767624 
model_pd.l_d.mean(): -9.235697746276855 
model_pd.lagr.mean(): -9.100053787231445 
model_pd.lambdas: dict_items([('pout', tensor([1.5678], device='cuda:0')), ('power', tensor([0.5995], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2054], device='cuda:0')), ('power', tensor([-18.5257], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.1356443613767624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8607, 8.7405, 9.9123],
        [6.8607, 6.8872, 6.8646],
        [6.8607, 7.5941, 7.6561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.1354256272315979 
model_pd.l_d.mean(): -9.216902732849121 
model_pd.lagr.mean(): -9.081477165222168 
model_pd.lambdas: dict_items([('pout', tensor([1.5690], device='cuda:0')), ('power', tensor([0.5986], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2047], device='cuda:0')), ('power', tensor([-18.5235], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.1354256272315979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8646, 6.8911, 6.8685],
        [6.8646, 7.5983, 7.6603],
        [6.8646, 8.7453, 9.9177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.13520702719688416 
model_pd.l_d.mean(): -9.198112487792969 
model_pd.lagr.mean(): -9.062905311584473 
model_pd.lambdas: dict_items([('pout', tensor([1.5702], device='cuda:0')), ('power', tensor([0.5977], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2039], device='cuda:0')), ('power', tensor([-18.5213], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.13520702719688416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8685, 7.6025, 7.6645],
        [6.8685, 8.7502, 9.9231],
        [6.8685, 6.8950, 6.8724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.13498839735984802 
model_pd.l_d.mean(): -9.179328918457031 
model_pd.lagr.mean(): -9.044340133666992 
model_pd.lambdas: dict_items([('pout', tensor([1.5714], device='cuda:0')), ('power', tensor([0.5968], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2032], device='cuda:0')), ('power', tensor([-18.5191], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.13498839735984802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8725, 8.7551, 9.9285],
        [6.8725, 7.6067, 7.6687],
        [6.8725, 6.8990, 6.8764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.13476988673210144 
model_pd.l_d.mean(): -9.160550117492676 
model_pd.lagr.mean(): -9.02578067779541 
model_pd.lambdas: dict_items([('pout', tensor([1.5726], device='cuda:0')), ('power', tensor([0.5958], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2025], device='cuda:0')), ('power', tensor([-18.5169], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.13476988673210144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8764, 8.7600, 9.9340],
        [6.8764, 6.9029, 6.8803],
        [6.8764, 7.6110, 7.6730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.13455131649971008 
model_pd.l_d.mean(): -9.141777038574219 
model_pd.lagr.mean(): -9.00722599029541 
model_pd.lambdas: dict_items([('pout', tensor([1.5738], device='cuda:0')), ('power', tensor([0.5949], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2017], device='cuda:0')), ('power', tensor([-18.5146], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.13455131649971008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8804, 6.9069, 6.8843],
        [6.8804, 8.7650, 9.9395],
        [6.8804, 7.6153, 7.6772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1343328207731247 
model_pd.l_d.mean(): -9.123010635375977 
model_pd.lagr.mean(): -8.988677978515625 
model_pd.lambdas: dict_items([('pout', tensor([1.5750], device='cuda:0')), ('power', tensor([0.5940], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2010], device='cuda:0')), ('power', tensor([-18.5123], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1343328207731247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8844, 6.9109, 6.8883],
        [6.8844, 7.6196, 7.6815],
        [6.8844, 8.7700, 9.9450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13411429524421692 
model_pd.l_d.mean(): -9.104249954223633 
model_pd.lagr.mean(): -8.970135688781738 
model_pd.lambdas: dict_items([('pout', tensor([1.5762], device='cuda:0')), ('power', tensor([0.5931], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2003], device='cuda:0')), ('power', tensor([-18.5101], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13411429524421692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8884, 8.7750, 9.9506],
        [6.8884, 6.9149, 6.8923],
        [6.8884, 7.6239, 7.6859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.13389599323272705 
model_pd.l_d.mean(): -9.08549690246582 
model_pd.lagr.mean(): -8.951601028442383 
model_pd.lambdas: dict_items([('pout', tensor([1.5774], device='cuda:0')), ('power', tensor([0.5921], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1995], device='cuda:0')), ('power', tensor([-18.5078], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.13389599323272705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8924, 7.6283, 7.6902],
        [6.8924, 8.7801, 9.9562],
        [6.8924, 6.9190, 6.8963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.13367772102355957 
model_pd.l_d.mean(): -9.06674861907959 
model_pd.lagr.mean(): -8.93307113647461 
model_pd.lambdas: dict_items([('pout', tensor([1.5786], device='cuda:0')), ('power', tensor([0.5912], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1988], device='cuda:0')), ('power', tensor([-18.5054], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.13367772102355957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8965, 7.6326, 7.6946],
        [6.8965, 8.7851, 9.9618],
        [6.8965, 6.9230, 6.9004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.13345961272716522 
model_pd.l_d.mean(): -9.048005104064941 
model_pd.lagr.mean(): -8.914545059204102 
model_pd.lambdas: dict_items([('pout', tensor([1.5798], device='cuda:0')), ('power', tensor([0.5903], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1980], device='cuda:0')), ('power', tensor([-18.5031], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.13345961272716522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9006, 8.7902, 9.9675],
        [6.9006, 7.6370, 7.6990],
        [6.9006, 6.9271, 6.9045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.13324150443077087 
model_pd.l_d.mean(): -9.029268264770508 
model_pd.lagr.mean(): -8.896026611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.5810], device='cuda:0')), ('power', tensor([0.5894], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1972], device='cuda:0')), ('power', tensor([-18.5008], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.13324150443077087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9047, 6.9312, 6.9086],
        [6.9047, 7.6415, 7.7034],
        [6.9047, 8.7954, 9.9732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.13302357494831085 
model_pd.l_d.mean(): -9.010538101196289 
model_pd.lagr.mean(): -8.877514839172363 
model_pd.lambdas: dict_items([('pout', tensor([1.5822], device='cuda:0')), ('power', tensor([0.5884], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1965], device='cuda:0')), ('power', tensor([-18.4984], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.13302357494831085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9088, 6.9353, 6.9127],
        [6.9088, 7.6459, 7.7078],
        [6.9088, 8.8005, 9.9789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.13280564546585083 
model_pd.l_d.mean(): -8.991812705993652 
model_pd.lagr.mean(): -8.859006881713867 
model_pd.lambdas: dict_items([('pout', tensor([1.5834], device='cuda:0')), ('power', tensor([0.5875], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1957], device='cuda:0')), ('power', tensor([-18.4960], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.13280564546585083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9129, 7.6504, 7.7123],
        [6.9129, 8.8057, 9.9847],
        [6.9129, 6.9395, 6.9168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.13258790969848633 
model_pd.l_d.mean(): -8.97309398651123 
model_pd.lagr.mean(): -8.840505599975586 
model_pd.lambdas: dict_items([('pout', tensor([1.5846], device='cuda:0')), ('power', tensor([0.5866], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1949], device='cuda:0')), ('power', tensor([-18.4936], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.13258790969848633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9171, 7.6549, 7.7168],
        [6.9171, 6.9436, 6.9210],
        [6.9171, 8.8109, 9.9905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.1323702037334442 
model_pd.l_d.mean(): -8.954381942749023 
model_pd.lagr.mean(): -8.822011947631836 
model_pd.lambdas: dict_items([('pout', tensor([1.5857], device='cuda:0')), ('power', tensor([0.5857], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1942], device='cuda:0')), ('power', tensor([-18.4912], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.1323702037334442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9213, 6.9478, 6.9252],
        [6.9213, 8.8162, 9.9964],
        [6.9213, 7.6594, 7.7213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.1321524679660797 
model_pd.l_d.mean(): -8.935674667358398 
model_pd.lagr.mean(): -8.803522109985352 
model_pd.lambdas: dict_items([('pout', tensor([1.5869], device='cuda:0')), ('power', tensor([0.5847], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1934], device='cuda:0')), ('power', tensor([-18.4888], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.1321524679660797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9255,  6.9521,  6.9294],
        [ 6.9255,  7.6640,  7.7258],
        [ 6.9255,  8.8214, 10.0022]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.13193491101264954 
model_pd.l_d.mean(): -8.916975021362305 
model_pd.lagr.mean(): -8.785039901733398 
model_pd.lambdas: dict_items([('pout', tensor([1.5881], device='cuda:0')), ('power', tensor([0.5838], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1926], device='cuda:0')), ('power', tensor([-18.4864], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.13193491101264954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9297,  7.6686,  7.7304],
        [ 6.9297,  6.9563,  6.9336],
        [ 6.9297,  8.8268, 10.0081]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.13171735405921936 
model_pd.l_d.mean(): -8.898282051086426 
model_pd.lagr.mean(): -8.76656436920166 
model_pd.lambdas: dict_items([('pout', tensor([1.5893], device='cuda:0')), ('power', tensor([0.5829], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1918], device='cuda:0')), ('power', tensor([-18.4839], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.13171735405921936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9340,  7.6732,  7.7350],
        [ 6.9340,  6.9605,  6.9379],
        [ 6.9340,  8.8321, 10.0141]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.13149988651275635 
model_pd.l_d.mean(): -8.879595756530762 
model_pd.lagr.mean(): -8.748095512390137 
model_pd.lambdas: dict_items([('pout', tensor([1.5905], device='cuda:0')), ('power', tensor([0.5820], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1910], device='cuda:0')), ('power', tensor([-18.4815], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.13149988651275635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9382,  8.8375, 10.0201],
        [ 6.9382,  6.9648,  6.9421],
        [ 6.9382,  7.6778,  7.7396]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.1312825232744217 
model_pd.l_d.mean(): -8.860916137695312 
model_pd.lagr.mean(): -8.729633331298828 
model_pd.lambdas: dict_items([('pout', tensor([1.5917], device='cuda:0')), ('power', tensor([0.5810], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1902], device='cuda:0')), ('power', tensor([-18.4790], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.1312825232744217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9425,  6.9691,  6.9464],
        [ 6.9425,  7.6825,  7.7443],
        [ 6.9425,  8.8429, 10.0261]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.1310652494430542 
model_pd.l_d.mean(): -8.842241287231445 
model_pd.lagr.mean(): -8.711175918579102 
model_pd.lambdas: dict_items([('pout', tensor([1.5929], device='cuda:0')), ('power', tensor([0.5801], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1894], device='cuda:0')), ('power', tensor([-18.4765], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.1310652494430542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9468,  6.9735,  6.9507],
        [ 6.9468,  8.8483, 10.0321],
        [ 6.9468,  7.6871,  7.7489]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.13084803521633148 
model_pd.l_d.mean(): -8.823575019836426 
model_pd.lagr.mean(): -8.692727088928223 
model_pd.lambdas: dict_items([('pout', tensor([1.5941], device='cuda:0')), ('power', tensor([0.5792], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1886], device='cuda:0')), ('power', tensor([-18.4739], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.13084803521633148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9512,  7.6919,  7.7536],
        [ 6.9512,  8.8538, 10.0382],
        [ 6.9512,  6.9778,  6.9551]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.13063091039657593 
model_pd.l_d.mean(): -8.804912567138672 
model_pd.lagr.mean(): -8.67428207397461 
model_pd.lambdas: dict_items([('pout', tensor([1.5953], device='cuda:0')), ('power', tensor([0.5783], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1878], device='cuda:0')), ('power', tensor([-18.4714], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.13063091039657593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9556,  7.6966,  7.7584],
        [ 6.9556,  6.9822,  6.9595],
        [ 6.9556,  8.8593, 10.0444]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.1304139345884323 
model_pd.l_d.mean(): -8.786258697509766 
model_pd.lagr.mean(): -8.655844688415527 
model_pd.lambdas: dict_items([('pout', tensor([1.5965], device='cuda:0')), ('power', tensor([0.5773], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1870], device='cuda:0')), ('power', tensor([-18.4689], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.1304139345884323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9599,  6.9866,  6.9638],
        [ 6.9599,  8.8648, 10.0505],
        [ 6.9599,  7.7014,  7.7631]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13019710779190063 
model_pd.l_d.mean(): -8.767609596252441 
model_pd.lagr.mean(): -8.637412071228027 
model_pd.lambdas: dict_items([('pout', tensor([1.5976], device='cuda:0')), ('power', tensor([0.5764], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1862], device='cuda:0')), ('power', tensor([-18.4663], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13019710779190063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9644,  6.9910,  6.9683],
        [ 6.9644,  8.8703, 10.0567],
        [ 6.9644,  7.7061,  7.7679]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.12998032569885254 
model_pd.l_d.mean(): -8.748968124389648 
model_pd.lagr.mean(): -8.618988037109375 
model_pd.lambdas: dict_items([('pout', tensor([1.5988], device='cuda:0')), ('power', tensor([0.5755], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1854], device='cuda:0')), ('power', tensor([-18.4637], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.12998032569885254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9688,  8.8759, 10.0630],
        [ 6.9688,  6.9955,  6.9727],
        [ 6.9688,  7.7110,  7.7727]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.1297636777162552 
model_pd.l_d.mean(): -8.73033332824707 
model_pd.lagr.mean(): -8.600569725036621 
model_pd.lambdas: dict_items([('pout', tensor([1.6000], device='cuda:0')), ('power', tensor([0.5746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1846], device='cuda:0')), ('power', tensor([-18.4611], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.1297636777162552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9732,  6.9999,  6.9772],
        [ 6.9732,  7.7158,  7.7775],
        [ 6.9732,  8.8815, 10.0692]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.12954719364643097 
model_pd.l_d.mean(): -8.71170711517334 
model_pd.lagr.mean(): -8.582159996032715 
model_pd.lambdas: dict_items([('pout', tensor([1.6012], device='cuda:0')), ('power', tensor([0.5736], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1837], device='cuda:0')), ('power', tensor([-18.4585], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.12954719364643097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9777,  7.0044,  6.9816],
        [ 6.9777,  8.8872, 10.0755],
        [ 6.9777,  7.7207,  7.7824]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.12933078408241272 
model_pd.l_d.mean(): -8.693085670471191 
model_pd.lagr.mean(): -8.56375503540039 
model_pd.lambdas: dict_items([('pout', tensor([1.6024], device='cuda:0')), ('power', tensor([0.5727], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1829], device='cuda:0')), ('power', tensor([-18.4558], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.12933078408241272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9822,  7.0089,  6.9861],
        [ 6.9822,  8.8929, 10.0819],
        [ 6.9822,  7.7256,  7.7873]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.12911459803581238 
model_pd.l_d.mean(): -8.674470901489258 
model_pd.lagr.mean(): -8.545356750488281 
model_pd.lambdas: dict_items([('pout', tensor([1.6036], device='cuda:0')), ('power', tensor([0.5718], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1821], device='cuda:0')), ('power', tensor([-18.4532], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.12911459803581238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9868,  7.7305,  7.7922],
        [ 6.9868,  8.8986, 10.0883],
        [ 6.9868,  7.0135,  6.9907]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.1288984715938568 
model_pd.l_d.mean(): -8.655863761901855 
model_pd.lagr.mean(): -8.526965141296387 
model_pd.lambdas: dict_items([('pout', tensor([1.6047], device='cuda:0')), ('power', tensor([0.5709], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1812], device='cuda:0')), ('power', tensor([-18.4505], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.1288984715938568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9913,  7.7354,  7.7972],
        [ 6.9913,  7.0180,  6.9952],
        [ 6.9913,  8.9043, 10.0947]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.12868252396583557 
model_pd.l_d.mean(): -8.637262344360352 
model_pd.lagr.mean(): -8.508580207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.6059], device='cuda:0')), ('power', tensor([0.5700], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1804], device='cuda:0')), ('power', tensor([-18.4478], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.12868252396583557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.9959,  7.0226,  6.9998],
        [ 6.9959,  8.9101, 10.1012],
        [ 6.9959,  7.7404,  7.8021]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.1284666657447815 
model_pd.l_d.mean(): -8.618668556213379 
model_pd.lagr.mean(): -8.490201950073242 
model_pd.lambdas: dict_items([('pout', tensor([1.6071], device='cuda:0')), ('power', tensor([0.5690], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1795], device='cuda:0')), ('power', tensor([-18.4451], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.1284666657447815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0005,  8.9159, 10.1077],
        [ 7.0005,  7.0272,  7.0044],
        [ 7.0005,  7.7454,  7.8071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.12825089693069458 
model_pd.l_d.mean(): -8.600082397460938 
model_pd.lagr.mean(): -8.471831321716309 
model_pd.lambdas: dict_items([('pout', tensor([1.6083], device='cuda:0')), ('power', tensor([0.5681], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1787], device='cuda:0')), ('power', tensor([-18.4424], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.12825089693069458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0051,  7.0319,  7.0090],
        [ 7.0051,  7.7505,  7.8122],
        [ 7.0051,  8.9218, 10.1142]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.1280352771282196 
model_pd.l_d.mean(): -8.581501960754395 
model_pd.lagr.mean(): -8.453466415405273 
model_pd.lambdas: dict_items([('pout', tensor([1.6095], device='cuda:0')), ('power', tensor([0.5672], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1778], device='cuda:0')), ('power', tensor([-18.4397], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.1280352771282196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0098,  8.9277, 10.1208],
        [ 7.0098,  7.0365,  7.0137],
        [ 7.0098,  7.7555,  7.8172]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.12781976163387299 
model_pd.l_d.mean(): -8.562929153442383 
model_pd.lagr.mean(): -8.43510913848877 
model_pd.lambdas: dict_items([('pout', tensor([1.6106], device='cuda:0')), ('power', tensor([0.5663], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1770], device='cuda:0')), ('power', tensor([-18.4369], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.12781976163387299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0144,  7.7606,  7.8223],
        [ 7.0144,  8.9336, 10.1274],
        [ 7.0144,  7.0412,  7.0184]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.1276043951511383 
model_pd.l_d.mean(): -8.544363021850586 
model_pd.lagr.mean(): -8.41675853729248 
model_pd.lambdas: dict_items([('pout', tensor([1.6118], device='cuda:0')), ('power', tensor([0.5653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1761], device='cuda:0')), ('power', tensor([-18.4341], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.1276043951511383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0191,  8.9395, 10.1341],
        [ 7.0191,  7.0459,  7.0231],
        [ 7.0191,  7.7657,  7.8274]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.12738917768001556 
model_pd.l_d.mean(): -8.525805473327637 
model_pd.lagr.mean(): -8.398416519165039 
model_pd.lambdas: dict_items([('pout', tensor([1.6130], device='cuda:0')), ('power', tensor([0.5644], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1753], device='cuda:0')), ('power', tensor([-18.4313], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.12738917768001556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0239,  7.0507,  7.0278],
        [ 7.0239,  7.7709,  7.8326],
        [ 7.0239,  8.9455, 10.1408]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.12717410922050476 
model_pd.l_d.mean(): -8.507253646850586 
model_pd.lagr.mean(): -8.38007926940918 
model_pd.lambdas: dict_items([('pout', tensor([1.6142], device='cuda:0')), ('power', tensor([0.5635], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1744], device='cuda:0')), ('power', tensor([-18.4285], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.12717410922050476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0286,  7.7761,  7.8378],
        [ 7.0286,  7.0554,  7.0325],
        [ 7.0286,  8.9515, 10.1475]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.1269592046737671 
model_pd.l_d.mean(): -8.488710403442383 
model_pd.lagr.mean(): -8.361751556396484 
model_pd.lambdas: dict_items([('pout', tensor([1.6153], device='cuda:0')), ('power', tensor([0.5626], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1735], device='cuda:0')), ('power', tensor([-18.4257], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.1269592046737671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0334,  7.0602,  7.0373],
        [ 7.0334,  7.7813,  7.8430],
        [ 7.0334,  8.9576, 10.1543]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12674447894096375 
model_pd.l_d.mean(): -8.470172882080078 
model_pd.lagr.mean(): -8.343428611755371 
model_pd.lambdas: dict_items([('pout', tensor([1.6165], device='cuda:0')), ('power', tensor([0.5617], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1726], device='cuda:0')), ('power', tensor([-18.4228], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12674447894096375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0382,  7.0650,  7.0421],
        [ 7.0382,  8.9637, 10.1611],
        [ 7.0382,  7.7865,  7.8482]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.12652990221977234 
model_pd.l_d.mean(): -8.451642990112305 
model_pd.lagr.mean(): -8.325113296508789 
model_pd.lambdas: dict_items([('pout', tensor([1.6177], device='cuda:0')), ('power', tensor([0.5607], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1718], device='cuda:0')), ('power', tensor([-18.4200], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.12652990221977234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0430,  7.7918,  7.8535],
        [ 7.0430,  8.9698, 10.1680],
        [ 7.0430,  7.0698,  7.0469]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.12631545960903168 
model_pd.l_d.mean(): -8.433121681213379 
model_pd.lagr.mean(): -8.306806564331055 
model_pd.lambdas: dict_items([('pout', tensor([1.6189], device='cuda:0')), ('power', tensor([0.5598], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1709], device='cuda:0')), ('power', tensor([-18.4171], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.12631545960903168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0479,  8.9759, 10.1749],
        [ 7.0479,  7.0747,  7.0518],
        [ 7.0479,  7.7971,  7.8588]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12610121071338654 
model_pd.l_d.mean(): -8.414606094360352 
model_pd.lagr.mean(): -8.288504600524902 
model_pd.lambdas: dict_items([('pout', tensor([1.6200], device='cuda:0')), ('power', tensor([0.5589], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1700], device='cuda:0')), ('power', tensor([-18.4142], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12610121071338654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0527,  7.8024,  7.8641],
        [ 7.0527,  8.9821, 10.1818],
        [ 7.0527,  7.0796,  7.0567]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.12588714063167572 
model_pd.l_d.mean(): -8.396099090576172 
model_pd.lagr.mean(): -8.270212173461914 
model_pd.lambdas: dict_items([('pout', tensor([1.6212], device='cuda:0')), ('power', tensor([0.5580], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1691], device='cuda:0')), ('power', tensor([-18.4112], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.12588714063167572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0576,  7.8077,  7.8694],
        [ 7.0576,  8.9884, 10.1888],
        [ 7.0576,  7.0845,  7.0616]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12567320466041565 
model_pd.l_d.mean(): -8.37760066986084 
model_pd.lagr.mean(): -8.251927375793457 
model_pd.lambdas: dict_items([('pout', tensor([1.6224], device='cuda:0')), ('power', tensor([0.5571], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1682], device='cuda:0')), ('power', tensor([-18.4083], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12567320466041565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0625,  8.9946, 10.1958],
        [ 7.0625,  7.8131,  7.8748],
        [ 7.0625,  7.0894,  7.0665]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.1254594773054123 
model_pd.l_d.mean(): -8.359107971191406 
model_pd.lagr.mean(): -8.233648300170898 
model_pd.lambdas: dict_items([('pout', tensor([1.6235], device='cuda:0')), ('power', tensor([0.5561], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1673], device='cuda:0')), ('power', tensor([-18.4053], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.1254594773054123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0675,  9.0009, 10.2029],
        [ 7.0675,  7.8185,  7.8802],
        [ 7.0675,  7.0944,  7.0714]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.1252458691596985 
model_pd.l_d.mean(): -8.340621948242188 
model_pd.lagr.mean(): -8.215375900268555 
model_pd.lambdas: dict_items([('pout', tensor([1.6247], device='cuda:0')), ('power', tensor([0.5552], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1664], device='cuda:0')), ('power', tensor([-18.4023], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.1252458691596985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0725,  9.0072, 10.2100],
        [ 7.0725,  7.8240,  7.8857],
        [ 7.0725,  7.0994,  7.0764]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.1250324845314026 
model_pd.l_d.mean(): -8.3221435546875 
model_pd.lagr.mean(): -8.197111129760742 
model_pd.lambdas: dict_items([('pout', tensor([1.6259], device='cuda:0')), ('power', tensor([0.5543], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1655], device='cuda:0')), ('power', tensor([-18.3993], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.1250324845314026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0775,  7.1044,  7.0814],
        [ 7.0775,  9.0136, 10.2171],
        [ 7.0775,  7.8295,  7.8912]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12481926381587982 
model_pd.l_d.mean(): -8.30367660522461 
model_pd.lagr.mean(): -8.178857803344727 
model_pd.lambdas: dict_items([('pout', tensor([1.6270], device='cuda:0')), ('power', tensor([0.5534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1645], device='cuda:0')), ('power', tensor([-18.3963], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12481926381587982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0825,  7.8350,  7.8967],
        [ 7.0825,  9.0200, 10.2243],
        [ 7.0825,  7.1094,  7.0865]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12460623681545258 
model_pd.l_d.mean(): -8.285213470458984 
model_pd.lagr.mean(): -8.16060733795166 
model_pd.lambdas: dict_items([('pout', tensor([1.6282], device='cuda:0')), ('power', tensor([0.5525], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1636], device='cuda:0')), ('power', tensor([-18.3933], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12460623681545258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0876,  7.1145,  7.0915],
        [ 7.0876,  9.0264, 10.2316],
        [ 7.0876,  7.8405,  7.9022]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.12439336627721786 
model_pd.l_d.mean(): -8.266761779785156 
model_pd.lagr.mean(): -8.14236831665039 
model_pd.lambdas: dict_items([('pout', tensor([1.6294], device='cuda:0')), ('power', tensor([0.5515], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1627], device='cuda:0')), ('power', tensor([-18.3902], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.12439336627721786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0926,  7.8461,  7.9078],
        [ 7.0926,  9.0329, 10.2389],
        [ 7.0926,  7.1196,  7.0966]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.12418075650930405 
model_pd.l_d.mean(): -8.24831485748291 
model_pd.lagr.mean(): -8.124134063720703 
model_pd.lambdas: dict_items([('pout', tensor([1.6305], device='cuda:0')), ('power', tensor([0.5506], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1618], device='cuda:0')), ('power', tensor([-18.3871], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.12418075650930405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.0978,  7.8517,  7.9134],
        [ 7.0978,  9.0394, 10.2462],
        [ 7.0978,  7.1247,  7.1017]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.12396825850009918 
model_pd.l_d.mean(): -8.229875564575195 
model_pd.lagr.mean(): -8.105907440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.6317], device='cuda:0')), ('power', tensor([0.5497], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1608], device='cuda:0')), ('power', tensor([-18.3840], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.12396825850009918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1029,  7.1299,  7.1068],
        [ 7.1029,  7.8573,  7.9190],
        [ 7.1029,  9.0460, 10.2535]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.12375599145889282 
model_pd.l_d.mean(): -8.211444854736328 
model_pd.lagr.mean(): -8.087688446044922 
model_pd.lambdas: dict_items([('pout', tensor([1.6328], device='cuda:0')), ('power', tensor([0.5488], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1599], device='cuda:0')), ('power', tensor([-18.3809], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.12375599145889282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1081,  7.8629,  7.9247],
        [ 7.1081,  7.1351,  7.1120],
        [ 7.1081,  9.0526, 10.2610]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.123543880879879 
model_pd.l_d.mean(): -8.193022727966309 
model_pd.lagr.mean(): -8.069478988647461 
model_pd.lambdas: dict_items([('pout', tensor([1.6340], device='cuda:0')), ('power', tensor([0.5479], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1589], device='cuda:0')), ('power', tensor([-18.3778], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.123543880879879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1132,  7.8686,  7.9304],
        [ 7.1132,  7.1403,  7.1172],
        [ 7.1132,  9.0592, 10.2684]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.12333202362060547 
model_pd.l_d.mean(): -8.17460823059082 
model_pd.lagr.mean(): -8.051276206970215 
model_pd.lambdas: dict_items([('pout', tensor([1.6352], device='cuda:0')), ('power', tensor([0.5469], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1580], device='cuda:0')), ('power', tensor([-18.3746], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.12333202362060547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1185,  9.0658, 10.2759],
        [ 7.1185,  7.1455,  7.1224],
        [ 7.1185,  7.8743,  7.9361]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.12312033027410507 
model_pd.l_d.mean(): -8.156203269958496 
model_pd.lagr.mean(): -8.033082962036133 
model_pd.lambdas: dict_items([('pout', tensor([1.6363], device='cuda:0')), ('power', tensor([0.5460], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1570], device='cuda:0')), ('power', tensor([-18.3714], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.12312033027410507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1237,  7.8801,  7.9418],
        [ 7.1237,  7.1508,  7.1277],
        [ 7.1237,  9.0725, 10.2834]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.12290890514850616 
model_pd.l_d.mean(): -8.13780403137207 
model_pd.lagr.mean(): -8.01489543914795 
model_pd.lambdas: dict_items([('pout', tensor([1.6375], device='cuda:0')), ('power', tensor([0.5451], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1561], device='cuda:0')), ('power', tensor([-18.3682], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.12290890514850616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1290,  9.0793, 10.2910],
        [ 7.1290,  7.8859,  7.9476],
        [ 7.1290,  7.1560,  7.1329]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.122697614133358 
model_pd.l_d.mean(): -8.119414329528809 
model_pd.lagr.mean(): -7.996716499328613 
model_pd.lambdas: dict_items([('pout', tensor([1.6386], device='cuda:0')), ('power', tensor([0.5442], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1551], device='cuda:0')), ('power', tensor([-18.3650], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.122697614133358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1343,  7.8917,  7.9535],
        [ 7.1343,  9.0860, 10.2987],
        [ 7.1343,  7.1614,  7.1382]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.12248655408620834 
model_pd.l_d.mean(): -8.101032257080078 
model_pd.lagr.mean(): -7.978545665740967 
model_pd.lambdas: dict_items([('pout', tensor([1.6398], device='cuda:0')), ('power', tensor([0.5433], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1542], device='cuda:0')), ('power', tensor([-18.3617], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.12248655408620834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1396,  7.8975,  7.9593],
        [ 7.1396,  7.1667,  7.1436],
        [ 7.1396,  9.0929, 10.3063]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.122275710105896 
model_pd.l_d.mean(): -8.082657814025879 
model_pd.lagr.mean(): -7.960381984710693 
model_pd.lambdas: dict_items([('pout', tensor([1.6409], device='cuda:0')), ('power', tensor([0.5424], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1532], device='cuda:0')), ('power', tensor([-18.3585], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.122275710105896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1450,  9.0997, 10.3141],
        [ 7.1450,  7.1721,  7.1489],
        [ 7.1450,  7.9034,  7.9652]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.12206508964300156 
model_pd.l_d.mean(): -8.06429386138916 
model_pd.lagr.mean(): -7.9422287940979 
model_pd.lambdas: dict_items([('pout', tensor([1.6421], device='cuda:0')), ('power', tensor([0.5414], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1522], device='cuda:0')), ('power', tensor([-18.3552], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.12206508964300156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1503,  9.1066, 10.3218],
        [ 7.1503,  7.1775,  7.1543],
        [ 7.1503,  7.9093,  7.9711]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.1218547374010086 
model_pd.l_d.mean(): -8.045936584472656 
model_pd.lagr.mean(): -7.924081802368164 
model_pd.lambdas: dict_items([('pout', tensor([1.6432], device='cuda:0')), ('power', tensor([0.5405], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1512], device='cuda:0')), ('power', tensor([-18.3519], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.1218547374010086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1558,  9.1135, 10.3296],
        [ 7.1558,  7.1829,  7.1597],
        [ 7.1558,  7.9153,  7.9771]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.12164454162120819 
model_pd.l_d.mean(): -8.027586936950684 
model_pd.lagr.mean(): -7.905942440032959 
model_pd.lambdas: dict_items([('pout', tensor([1.6444], device='cuda:0')), ('power', tensor([0.5396], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1502], device='cuda:0')), ('power', tensor([-18.3485], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.12164454162120819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1612,  7.1884,  7.1652],
        [ 7.1612,  7.9213,  7.9831],
        [ 7.1612,  9.1205, 10.3375]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.12143471837043762 
model_pd.l_d.mean(): -8.009246826171875 
model_pd.lagr.mean(): -7.88781213760376 
model_pd.lambdas: dict_items([('pout', tensor([1.6455], device='cuda:0')), ('power', tensor([0.5387], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1493], device='cuda:0')), ('power', tensor([-18.3452], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.12143471837043762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1666,  7.9272,  7.9891],
        [ 7.1666,  7.1938,  7.1706],
        [ 7.1666,  9.1274, 10.3454]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.12122592329978943 
model_pd.l_d.mean(): -7.9909162521362305 
model_pd.lagr.mean(): -7.869690418243408 
model_pd.lambdas: dict_items([('pout', tensor([1.6467], device='cuda:0')), ('power', tensor([0.5378], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1483], device='cuda:0')), ('power', tensor([-18.3418], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.12122592329978943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1721,  7.9332,  7.9951],
        [ 7.1721,  7.1993,  7.1761],
        [ 7.1721,  9.1344, 10.3532]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.12101837992668152 
model_pd.l_d.mean(): -7.97259521484375 
model_pd.lagr.mean(): -7.851576805114746 
model_pd.lambdas: dict_items([('pout', tensor([1.6478], device='cuda:0')), ('power', tensor([0.5368], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1473], device='cuda:0')), ('power', tensor([-18.3385], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.12101837992668152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1775,  7.9392,  8.0011],
        [ 7.1775,  9.1414, 10.3611],
        [ 7.1775,  7.2047,  7.1815]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.12081222981214523 
model_pd.l_d.mean(): -7.954283714294434 
model_pd.lagr.mean(): -7.833471298217773 
model_pd.lambdas: dict_items([('pout', tensor([1.6490], device='cuda:0')), ('power', tensor([0.5359], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1463], device='cuda:0')), ('power', tensor([-18.3351], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.12081222981214523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1830,  9.1484, 10.3690],
        [ 7.1830,  7.2102,  7.1869],
        [ 7.1830,  7.9452,  8.0071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.1206073984503746 
model_pd.l_d.mean(): -7.93597936630249 
model_pd.lagr.mean(): -7.815371990203857 
model_pd.lambdas: dict_items([('pout', tensor([1.6501], device='cuda:0')), ('power', tensor([0.5350], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1453], device='cuda:0')), ('power', tensor([-18.3317], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.1206073984503746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1884,  9.1554, 10.3769],
        [ 7.1884,  7.9513,  8.0132],
        [ 7.1884,  7.2156,  7.1924]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.12040366977453232 
model_pd.l_d.mean(): -7.9176859855651855 
model_pd.lagr.mean(): -7.7972822189331055 
model_pd.lambdas: dict_items([('pout', tensor([1.6513], device='cuda:0')), ('power', tensor([0.5341], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1443], device='cuda:0')), ('power', tensor([-18.3284], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.12040366977453232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1939,  9.1624, 10.3848],
        [ 7.1939,  7.9573,  8.0192],
        [ 7.1939,  7.2211,  7.1978]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.12020072340965271 
model_pd.l_d.mean(): -7.89940071105957 
model_pd.lagr.mean(): -7.779200077056885 
model_pd.lambdas: dict_items([('pout', tensor([1.6524], device='cuda:0')), ('power', tensor([0.5332], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1433], device='cuda:0')), ('power', tensor([-18.3250], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.12020072340965271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5931e-01, 8.1696e-01,
         1.0000e+00, 7.7669e-01, 1.0000e+00, 9.5071e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2541e-02, 1.9679e-02,
         1.0000e+00, 7.3704e-03, 1.0000e+00, 3.7454e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9221e-01, 2.8710e-01,
         1.0000e+00, 2.1015e-01, 1.0000e+00, 7.3199e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.1993,  9.1694, 10.3928],
        [ 7.1993,  7.2266,  7.2033],
        [ 7.1993,  7.9633,  8.0253]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.11999808251857758 
model_pd.l_d.mean(): -7.881124496459961 
model_pd.lagr.mean(): -7.761126518249512 
model_pd.lambdas: dict_items([('pout', tensor([1.6535], device='cuda:0')), ('power', tensor([0.5323], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1423], device='cuda:0')), ('power', tensor([-18.3215], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.11999808251857758
