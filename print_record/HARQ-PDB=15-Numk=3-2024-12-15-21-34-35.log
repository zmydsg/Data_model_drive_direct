
bounds:tensor([-2.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.7571, 2.9162],
        [2.3753, 2.6743, 2.7534],
        [2.3753, 2.3910, 2.3785],
        [2.3753, 2.5079, 2.4824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.16718389093875885 
model_pd.l_d.mean(): -18.15465545654297 
model_pd.lagr.mean(): -18.321840286254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0024], device='cuda:0')), ('power', tensor([0.9990], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.3992], device='cuda:0')), ('power', tensor([-20.5539], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.16718389093875885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.4887, 2.4890, 2.4887],
        [2.4887, 2.4974, 2.4899],
        [2.4887, 3.1458, 3.6241],
        [2.4887, 3.1025, 3.5211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): -0.21218009293079376 
model_pd.l_d.mean(): -18.322587966918945 
model_pd.lagr.mean(): -18.534767150878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0047], device='cuda:0')), ('power', tensor([0.9979], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.3378], device='cuda:0')), ('power', tensor([-20.6873], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:-0.21218009293079376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]], device='cuda:0')
 pt:tensor([[2.6034, 3.1887, 3.5454],
        [2.6034, 3.3010, 3.8072],
        [2.6034, 2.6582, 2.6268],
        [2.6034, 2.6828, 2.6466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): -0.28285831212997437 
model_pd.l_d.mean(): -18.46811294555664 
model_pd.lagr.mean(): -18.7509708404541 
model_pd.lambdas: dict_items([('pout', tensor([1.0070], device='cuda:0')), ('power', tensor([0.9969], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.2786], device='cuda:0')), ('power', tensor([-20.8004], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:-0.28285831212997437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.7192, 2.7192, 2.7192],
        [2.7192, 2.8459, 2.8087],
        [2.7192, 3.1695, 3.3521],
        [2.7192, 3.1356, 3.2842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): -0.4108615517616272 
model_pd.l_d.mean(): -18.593576431274414 
model_pd.lagr.mean(): -19.004438400268555 
model_pd.lambdas: dict_items([('pout', tensor([1.0092], device='cuda:0')), ('power', tensor([0.9959], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.2213], device='cuda:0')), ('power', tensor([-20.8953], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:-0.4108615517616272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.8362, 3.0859, 3.0912],
        [2.8362, 2.8366, 2.8362],
        [2.8362, 2.8533, 2.8395],
        [2.8362, 2.8362, 2.8362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): -0.7185362577438354 
model_pd.l_d.mean(): -18.70096778869629 
model_pd.lagr.mean(): -19.419504165649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0114], device='cuda:0')), ('power', tensor([0.9948], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.1659], device='cuda:0')), ('power', tensor([-20.9738], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:-0.7185362577438354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.9546, 2.9887, 2.9643],
        [2.9546, 2.9584, 2.9548],
        [2.9546, 2.9883, 2.9641],
        [2.9546, 2.9546, 2.9546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): -2.670731544494629 
model_pd.l_d.mean(): -18.792015075683594 
model_pd.lagr.mean(): -21.462745666503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0135], device='cuda:0')), ('power', tensor([0.9938], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.1122], device='cuda:0')), ('power', tensor([-21.0376], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:-2.670731544494629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.0742, 3.1099, 3.0843],
        [3.0742, 3.0742, 3.0742],
        [3.0742, 3.0742, 3.0742],
        [3.0742, 3.2543, 3.2181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 1.3372666835784912 
model_pd.l_d.mean(): -18.86822509765625 
model_pd.lagr.mean(): -17.53095817565918 
model_pd.lambdas: dict_items([('pout', tensor([1.0156], device='cuda:0')), ('power', tensor([0.9927], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.0600], device='cuda:0')), ('power', tensor([-21.0878], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:1.3372666835784912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.1951, 3.5977, 3.6829],
        [3.1951, 3.5008, 3.5166],
        [3.1951, 3.1982, 3.1953],
        [3.1951, 3.4642, 3.4591]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6733238697052002 
model_pd.l_d.mean(): -18.930919647216797 
model_pd.lagr.mean(): -18.25759506225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0176], device='cuda:0')), ('power', tensor([0.9916], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([2.0093], device='cuda:0')), ('power', tensor([-21.1258], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.6733238697052002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[3.3175, 4.2036, 4.7973],
        [3.3175, 3.5987, 3.5930],
        [3.3175, 3.4565, 3.4070],
        [3.3175, 4.2680, 4.9481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.19233523309230804 
model_pd.l_d.mean(): -18.981233596801758 
model_pd.lagr.mean(): -18.788898468017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0195], device='cuda:0')), ('power', tensor([0.9906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.9600], device='cuda:0')), ('power', tensor([-21.1525], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.19233523309230804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.4413, 3.5862, 3.5345],
        [3.4413, 3.4817, 3.4527],
        [3.4413, 3.4813, 3.4526],
        [3.4413, 3.4417, 3.4413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.44251716136932373 
model_pd.l_d.mean(): -19.02016258239746 
model_pd.lagr.mean(): -18.577646255493164 
model_pd.lambdas: dict_items([('pout', tensor([1.0215], device='cuda:0')), ('power', tensor([0.9895], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.9119], device='cuda:0')), ('power', tensor([-21.1687], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.44251716136932373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 4.1451, 4.3462],
        [3.5667, 3.9364, 3.9672],
        [3.5667, 3.6598, 3.6107],
        [3.5667, 3.5671, 3.5667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.3055334687232971 
model_pd.l_d.mean(): -19.048593521118164 
model_pd.lagr.mean(): -18.743059158325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0233], device='cuda:0')), ('power', tensor([0.9885], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.8649], device='cuda:0')), ('power', tensor([-21.1753], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.3055334687232971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.6937, 3.8122, 3.7574],
        [3.6937, 4.0338, 4.0383],
        [3.6937, 3.7166, 3.6980],
        [3.6937, 3.7002, 3.6943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.23976925015449524 
model_pd.l_d.mean(): -19.067293167114258 
model_pd.lagr.mean(): -18.827524185180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0251], device='cuda:0')), ('power', tensor([0.9874], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.8190], device='cuda:0')), ('power', tensor([-21.1729], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.23976925015449524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.8225, 4.7249, 5.2381],
        [3.8225, 3.8292, 3.8231],
        [3.8225, 4.1762, 4.1806],
        [3.8225, 3.8228, 3.8225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.19279541075229645 
model_pd.l_d.mean(): -19.076913833618164 
model_pd.lagr.mean(): -18.884119033813477 
model_pd.lambdas: dict_items([('pout', tensor([1.0269], device='cuda:0')), ('power', tensor([0.9864], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.7740], device='cuda:0')), ('power', tensor([-21.1620], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.19279541075229645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.9530, 3.9530, 3.9530],
        [3.9530, 4.2644, 4.2402],
        [3.9530, 4.1476, 4.0888],
        [3.9530, 4.3440, 4.3620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.14153926074504852 
model_pd.l_d.mean(): -19.07802963256836 
model_pd.lagr.mean(): -18.936491012573242 
model_pd.lambdas: dict_items([('pout', tensor([1.0286], device='cuda:0')), ('power', tensor([0.9853], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.7300], device='cuda:0')), ('power', tensor([-21.1432], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.14153926074504852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.0854, 4.1337, 4.0990],
        [4.0854, 4.8200, 5.1084],
        [4.0854, 4.2180, 4.1565],
        [4.0854, 4.1344, 4.0993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.3138855993747711 
model_pd.l_d.mean(): -19.071136474609375 
model_pd.lagr.mean(): -18.757251739501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0303], device='cuda:0')), ('power', tensor([0.9842], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6868], device='cuda:0')), ('power', tensor([-21.1168], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.3138855993747711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.2197, 4.2197, 4.2197],
        [4.2197, 4.9875, 5.2916],
        [4.2197, 4.5548, 4.5284],
        [4.2197, 4.2702, 4.2339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.2469235509634018 
model_pd.l_d.mean(): -19.05667495727539 
model_pd.lagr.mean(): -18.809751510620117 
model_pd.lambdas: dict_items([('pout', tensor([1.0320], device='cuda:0')), ('power', tensor([0.9832], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6444], device='cuda:0')), ('power', tensor([-21.0833], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.2469235509634018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.3558, 4.5726, 4.5068],
        [4.3558, 4.3618, 4.3563],
        [4.3558, 4.3561, 4.3558],
        [4.3558, 4.3558, 4.3558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.14882946014404297 
model_pd.l_d.mean(): -19.035057067871094 
model_pd.lagr.mean(): -18.886226654052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0336], device='cuda:0')), ('power', tensor([0.9821], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.6028], device='cuda:0')), ('power', tensor([-21.0429], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.14882946014404297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.4940, 4.4985, 4.4943],
        [4.4940, 5.7680, 6.6110],
        [4.4940, 4.4940, 4.4940],
        [4.4940, 4.6143, 4.5505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.33151495456695557 
model_pd.l_d.mean(): -19.006635665893555 
model_pd.lagr.mean(): -18.675121307373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0351], device='cuda:0')), ('power', tensor([0.9811], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5618], device='cuda:0')), ('power', tensor([-20.9961], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.33151495456695557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]], device='cuda:0')
 pt:tensor([[4.6342, 5.1024, 5.1223],
        [4.6342, 5.2534, 5.3785],
        [4.6342, 5.2977, 5.4587],
        [4.6342, 4.7867, 4.7157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.1678915023803711 
model_pd.l_d.mean(): -18.971723556518555 
model_pd.lagr.mean(): -18.8038330078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0367], device='cuda:0')), ('power', tensor([0.9800], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5216], device='cuda:0')), ('power', tensor([-20.9430], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.1678915023803711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[4.7737, 4.9598, 4.8842],
        [4.7737, 5.6585, 6.0068],
        [4.7737, 4.9826, 4.9070],
        [4.7737, 5.2286, 5.2321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.14667575061321259 
model_pd.l_d.mean(): -18.931013107299805 
model_pd.lagr.mean(): -18.784337997436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0381], device='cuda:0')), ('power', tensor([0.9790], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4827], device='cuda:0')), ('power', tensor([-20.8850], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.14667575061321259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8994, 5.0519, 4.9778],
        [4.8994, 5.8118, 6.1711],
        [4.8994, 5.8045, 6.1566],
        [4.8994, 4.8994, 4.8994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.1353941559791565 
model_pd.l_d.mean(): -18.88738441467285 
model_pd.lagr.mean(): -18.751989364624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0396], device='cuda:0')), ('power', tensor([0.9779], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4486], device='cuda:0')), ('power', tensor([-20.8289], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.1353941559791565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0047, 5.0047, 5.0047],
        [5.0047, 5.1706, 5.0932],
        [5.0047, 5.0650, 5.0215],
        [5.0047, 5.4992, 5.5111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.12814809381961823 
model_pd.l_d.mean(): -18.843935012817383 
model_pd.lagr.mean(): -18.71578598022461 
model_pd.lambdas: dict_items([('pout', tensor([1.0410], device='cuda:0')), ('power', tensor([0.9769], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4208], device='cuda:0')), ('power', tensor([-20.7792], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.12814809381961823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0860, 5.2019, 5.1346],
        [5.0860, 6.0379, 6.4121],
        [5.0860, 5.0860, 5.0860],
        [5.0860, 5.0911, 5.0863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.12335195392370224 
model_pd.l_d.mean(): -18.803268432617188 
model_pd.lagr.mean(): -18.679916381835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0424], device='cuda:0')), ('power', tensor([0.9759], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3997], device='cuda:0')), ('power', tensor([-20.7392], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.12335195392370224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1423, 5.1427, 5.1423],
        [5.1423, 6.4163, 7.1329],
        [5.1423, 5.6682, 5.6897],
        [5.1423, 5.1423, 5.1423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.12031982094049454 
model_pd.l_d.mean(): -18.767101287841797 
model_pd.lagr.mean(): -18.64678192138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0438], device='cuda:0')), ('power', tensor([0.9748], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3853], device='cuda:0')), ('power', tensor([-20.7108], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.12031982094049454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1753, 5.1753, 5.1753],
        [5.1753, 5.1753, 5.1753],
        [5.1753, 5.7393, 5.7819],
        [5.1753, 5.4376, 5.3574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.11862903088331223 
model_pd.l_d.mean(): -18.735979080200195 
model_pd.lagr.mean(): -18.61734962463379 
model_pd.lambdas: dict_items([('pout', tensor([1.0452], device='cuda:0')), ('power', tensor([0.9738], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3769], device='cuda:0')), ('power', tensor([-20.6939], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.11862903088331223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[5.1877, 6.6914, 7.6824],
        [5.1877, 6.1610, 6.5433],
        [5.1877, 5.3602, 5.2797],
        [5.1877, 5.6543, 5.6404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.11800714582204819 
model_pd.l_d.mean(): -18.709707260131836 
model_pd.lagr.mean(): -18.591699600219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0465], device='cuda:0')), ('power', tensor([0.9728], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3737], device='cuda:0')), ('power', tensor([-20.6875], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.11800714582204819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1821, 5.4106, 5.3277],
        [5.1821, 5.1839, 5.1821],
        [5.1821, 5.1821, 5.1821],
        [5.1821, 6.0712, 6.3730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.11828971654176712 
model_pd.l_d.mean(): -18.68779182434082 
model_pd.lagr.mean(): -18.569501876831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0479], device='cuda:0')), ('power', tensor([0.9717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3752], device='cuda:0')), ('power', tensor([-20.6904], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.11828971654176712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1607, 5.1607, 5.1607],
        [5.1607, 5.4699, 5.3983],
        [5.1607, 5.6245, 5.6107],
        [5.1607, 5.1610, 5.1607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.11937477439641953 
model_pd.l_d.mean(): -18.669565200805664 
model_pd.lagr.mean(): -18.550189971923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0493], device='cuda:0')), ('power', tensor([0.9707], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3806], device='cuda:0')), ('power', tensor([-20.7015], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.11937477439641953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1256, 5.1448, 5.1282],
        [5.1256, 5.1256, 5.1256],
        [5.1256, 5.1274, 5.1257],
        [5.1256, 5.1259, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.12120767682790756 
model_pd.l_d.mean(): -18.654279708862305 
model_pd.lagr.mean(): -18.533071517944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0507], device='cuda:0')), ('power', tensor([0.9697], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3895], device='cuda:0')), ('power', tensor([-20.7194], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.12120767682790756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0788, 5.8553, 6.0672],
        [5.0788, 5.0858, 5.0793],
        [5.0788, 5.3825, 5.3122],
        [5.0788, 5.0788, 5.0788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12376957386732101 
model_pd.l_d.mean(): -18.641103744506836 
model_pd.lagr.mean(): -18.517333984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0521], device='cuda:0')), ('power', tensor([0.9686], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4016], device='cuda:0')), ('power', tensor([-20.7430], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.12376957386732101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0231, 5.2439, 5.1638],
        [5.0231, 5.1589, 5.0868],
        [5.0231, 5.4302, 5.3968],
        [5.0231, 5.3517, 5.2898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.1270274519920349 
model_pd.l_d.mean(): -18.629026412963867 
model_pd.lagr.mean(): -18.501998901367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0535], device='cuda:0')), ('power', tensor([0.9676], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4160], device='cuda:0')), ('power', tensor([-20.7705], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.1270274519920349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9614, 4.9617, 4.9614],
        [4.9614, 5.7175, 5.9241],
        [4.9614, 5.8861, 6.2500],
        [4.9614, 5.1255, 5.0490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13098232448101044 
model_pd.l_d.mean(): -18.617124557495117 
model_pd.lagr.mean(): -18.486143112182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0549], device='cuda:0')), ('power', tensor([0.9665], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4322], device='cuda:0')), ('power', tensor([-20.8002], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.13098232448101044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8963, 4.9011, 4.8965],
        [4.8963, 5.8070, 6.1656],
        [4.8963, 4.9550, 4.9127],
        [4.8963, 5.2155, 5.1554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13566803932189941 
model_pd.l_d.mean(): -18.604625701904297 
model_pd.lagr.mean(): -18.468957901000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0564], device='cuda:0')), ('power', tensor([0.9655], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4495], device='cuda:0')), ('power', tensor([-20.8307], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.13566803932189941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8303, 4.8306, 4.8303],
        [4.8303, 4.8320, 4.8304],
        [4.8303, 4.8303, 4.8303],
        [4.8303, 4.8351, 4.8306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.14115211367607117 
model_pd.l_d.mean(): -18.590923309326172 
model_pd.lagr.mean(): -18.449771881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0579], device='cuda:0')), ('power', tensor([0.9645], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4673], device='cuda:0')), ('power', tensor([-20.8606], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.14115211367607117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7665, 4.7665, 4.7665],
        [4.7665, 4.8240, 4.7826],
        [4.7665, 5.0762, 5.0180],
        [4.7665, 5.2489, 5.2691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.1475319266319275 
model_pd.l_d.mean(): -18.575576782226562 
model_pd.lagr.mean(): -18.42804527282715 
model_pd.lambdas: dict_items([('pout', tensor([1.0593], device='cuda:0')), ('power', tensor([0.9634], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4848], device='cuda:0')), ('power', tensor([-20.8886], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.1475319266319275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7077, 4.7093, 4.7077],
        [4.7077, 4.7412, 4.7144],
        [4.7077, 5.1257, 5.1139],
        [4.7077, 4.7648, 4.7238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.1548966020345688 
model_pd.l_d.mean(): -18.5583438873291 
model_pd.lagr.mean(): -18.4034481048584 
model_pd.lambdas: dict_items([('pout', tensor([1.0608], device='cuda:0')), ('power', tensor([0.9624], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5010], device='cuda:0')), ('power', tensor([-20.9135], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.1548966020345688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]], device='cuda:0')
 pt:tensor([[4.6578, 4.9596, 4.9030],
        [4.6578, 5.7717, 6.3869],
        [4.6578, 5.7934, 6.4341],
        [4.6578, 5.3611, 5.5541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.16313238441944122 
model_pd.l_d.mean(): -18.539138793945312 
model_pd.lagr.mean(): -18.376007080078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0624], device='cuda:0')), ('power', tensor([0.9613], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5150], device='cuda:0')), ('power', tensor([-20.9340], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.16313238441944122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6215, 5.4011, 5.6672],
        [4.6215, 4.6261, 4.6218],
        [4.6215, 4.6219, 4.6215],
        [4.6215, 5.3185, 5.5098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.17121975123882294 
model_pd.l_d.mean(): -18.518001556396484 
model_pd.lagr.mean(): -18.346782684326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0639], device='cuda:0')), ('power', tensor([0.9603], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5253], device='cuda:0')), ('power', tensor([-20.9486], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.17121975123882294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6044, 5.0538, 5.0650],
        [4.6044, 5.2177, 5.3416],
        [4.6044, 4.7080, 4.6479],
        [4.6044, 4.6044, 4.6044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.17611350119113922 
model_pd.l_d.mean(): -18.495054244995117 
model_pd.lagr.mean(): -18.318941116333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0654], device='cuda:0')), ('power', tensor([0.9592], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5301], device='cuda:0')), ('power', tensor([-20.9553], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.17611350119113922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6049, 4.6131, 4.6056],
        [4.6049, 5.4463, 5.7743],
        [4.6049, 4.6051, 4.6049],
        [4.6049, 5.2182, 5.3420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.17599821090698242 
model_pd.l_d.mean(): -18.470773696899414 
model_pd.lagr.mean(): -18.294775009155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0669], device='cuda:0')), ('power', tensor([0.9582], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5300], device='cuda:0')), ('power', tensor([-20.9552], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.17599821090698242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6210, 4.6210, 4.6210],
        [4.6210, 5.4001, 5.6659],
        [4.6210, 4.6256, 4.6213],
        [4.6210, 4.6761, 4.6364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.17143844068050385 
model_pd.l_d.mean(): -18.445354461669922 
model_pd.lagr.mean(): -18.273916244506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0685], device='cuda:0')), ('power', tensor([0.9571], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5254], device='cuda:0')), ('power', tensor([-20.9490], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.17143844068050385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6508, 5.0911, 5.0946],
        [4.6508, 4.6802, 4.6563],
        [4.6508, 4.8310, 4.7578],
        [4.6508, 4.6508, 4.6508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.1645999699831009 
model_pd.l_d.mean(): -18.418781280517578 
model_pd.lagr.mean(): -18.254180908203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0700], device='cuda:0')), ('power', tensor([0.9561], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5170], device='cuda:0')), ('power', tensor([-20.9372], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.1645999699831009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6878, 4.6878, 4.6878],
        [4.6878, 5.1320, 5.1354],
        [4.6878, 4.6879, 4.6878],
        [4.6878, 5.1605, 5.1803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.15798373520374298 
model_pd.l_d.mean(): -18.391340255737305 
model_pd.lagr.mean(): -18.233356475830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0715], device='cuda:0')), ('power', tensor([0.9550], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.5067], device='cuda:0')), ('power', tensor([-20.9222], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.15798373520374298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7271, 5.0333, 4.9757],
        [4.7271, 4.7287, 4.7271],
        [4.7271, 5.2349, 5.2739],
        [4.7271, 4.7271, 4.7271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.15235255658626556 
model_pd.l_d.mean(): -18.363344192504883 
model_pd.lagr.mean(): -18.21099090576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0730], device='cuda:0')), ('power', tensor([0.9540], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4957], device='cuda:0')), ('power', tensor([-20.9059], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.15235255658626556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7651, 4.9123, 4.8407],
        [4.7651, 5.4019, 5.5299],
        [4.7651, 4.9500, 4.8748],
        [4.7651, 4.8208, 4.7804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.1477779895067215 
model_pd.l_d.mean(): -18.33513641357422 
model_pd.lagr.mean(): -18.187358856201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0745], device='cuda:0')), ('power', tensor([0.9530], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4853], device='cuda:0')), ('power', tensor([-20.8898], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.1477779895067215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7995, 4.8012, 4.7996],
        [4.7995, 5.3159, 5.3553],
        [4.7995, 5.9508, 6.5857],
        [4.7995, 4.8080, 4.8002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.14416414499282837 
model_pd.l_d.mean(): -18.30705451965332 
model_pd.lagr.mean(): -18.16288948059082 
model_pd.lambdas: dict_items([('pout', tensor([1.0760], device='cuda:0')), ('power', tensor([0.9519], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4758], device='cuda:0')), ('power', tensor([-20.8749], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.14416414499282837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8287, 5.4747, 5.6044],
        [4.8287, 4.8334, 4.8290],
        [4.8287, 4.8293, 4.8287],
        [4.8287, 5.7219, 6.0728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.1413985639810562 
model_pd.l_d.mean(): -18.27942657470703 
model_pd.lagr.mean(): -18.13802719116211 
model_pd.lambdas: dict_items([('pout', tensor([1.0774], device='cuda:0')), ('power', tensor([0.9509], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4679], device='cuda:0')), ('power', tensor([-20.8621], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.1413985639810562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8516, 4.8516, 4.8516],
        [4.8516, 5.0946, 5.0204],
        [4.8516, 6.0928, 6.8251],
        [4.8516, 5.1664, 5.1070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.13938358426094055 
model_pd.l_d.mean(): -18.25250244140625 
model_pd.lagr.mean(): -18.11311912536621 
model_pd.lambdas: dict_items([('pout', tensor([1.0789], device='cuda:0')), ('power', tensor([0.9498], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4617], device='cuda:0')), ('power', tensor([-20.8520], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.13938358426094055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8677, 4.8985, 4.8734],
        [4.8677, 4.9982, 4.9289],
        [4.8677, 5.1689, 5.1053],
        [4.8677, 5.1115, 5.0370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.13803862035274506 
model_pd.l_d.mean(): -18.22645378112793 
model_pd.lagr.mean(): -18.088415145874023 
model_pd.lambdas: dict_items([('pout', tensor([1.0803], device='cuda:0')), ('power', tensor([0.9488], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4574], device='cuda:0')), ('power', tensor([-20.8448], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.13803862035274506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8769, 4.9339, 4.8926],
        [4.8769, 5.3548, 5.3661],
        [4.8769, 4.8836, 4.8774],
        [4.8769, 4.9077, 4.8827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.13729660212993622 
model_pd.l_d.mean(): -18.201370239257812 
model_pd.lagr.mean(): -18.06407356262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0818], device='cuda:0')), ('power', tensor([0.9477], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4549], device='cuda:0')), ('power', tensor([-20.8407], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.13729660212993622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8795, 4.9365, 4.8952],
        [4.8795, 4.8811, 4.8795],
        [4.8795, 4.8798, 4.8795],
        [4.8795, 6.1281, 6.8645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.13709932565689087 
model_pd.l_d.mean(): -18.17723846435547 
model_pd.lagr.mean(): -18.040138244628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0832], device='cuda:0')), ('power', tensor([0.9467], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4543], device='cuda:0')), ('power', tensor([-20.8396], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.13709932565689087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8759, 5.1646, 5.0977],
        [4.8759, 5.3088, 5.2960],
        [4.8759, 5.1200, 5.0453],
        [4.8759, 4.8762, 4.8759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.13739395141601562 
model_pd.l_d.mean(): -18.153980255126953 
model_pd.lagr.mean(): -18.016586303710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0847], device='cuda:0')), ('power', tensor([0.9457], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4552], device='cuda:0')), ('power', tensor([-20.8413], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.13739395141601562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8670, 4.9766, 4.9130],
        [4.8670, 4.9259, 4.8836],
        [4.8670, 5.2989, 5.2861],
        [4.8670, 4.8670, 4.8670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.13812975585460663 
model_pd.l_d.mean(): -18.13144302368164 
model_pd.lagr.mean(): -17.99331283569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0862], device='cuda:0')), ('power', tensor([0.9446], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4577], device='cuda:0')), ('power', tensor([-20.8454], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.13812975585460663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8537, 4.8843, 4.8595],
        [4.8537, 5.1535, 5.0901],
        [4.8537, 5.3140, 5.3170],
        [4.8537, 6.0180, 6.6592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.13925468921661377 
model_pd.l_d.mean(): -18.10944938659668 
model_pd.lagr.mean(): -17.97019386291504 
model_pd.lambdas: dict_items([('pout', tensor([1.0876], device='cuda:0')), ('power', tensor([0.9436], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4612], device='cuda:0')), ('power', tensor([-20.8515], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.13925468921661377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]], device='cuda:0')
 pt:tensor([[4.8372, 6.3215, 7.3675],
        [4.8372, 6.0722, 6.8006],
        [4.8372, 5.1230, 5.0568],
        [4.8372, 5.2249, 5.1931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.14071188867092133 
model_pd.l_d.mean(): -18.087791442871094 
model_pd.lagr.mean(): -17.947078704833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0891], device='cuda:0')), ('power', tensor([0.9425], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4657], device='cuda:0')), ('power', tensor([-20.8591], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.14071188867092133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8185, 5.0049, 4.9290],
        [4.8185, 5.4613, 5.5900],
        [4.8185, 4.8191, 4.8185],
        [4.8185, 4.8185, 4.8185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.14243479073047638 
model_pd.l_d.mean(): -18.066274642944336 
model_pd.lagr.mean(): -17.923839569091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0906], device='cuda:0')), ('power', tensor([0.9415], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4708], device='cuda:0')), ('power', tensor([-20.8675], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.14243479073047638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7990, 4.8330, 4.8058],
        [4.7990, 5.6090, 5.8842],
        [4.7990, 5.2822, 5.3019],
        [4.7990, 4.9559, 4.8827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.14434191584587097 
model_pd.l_d.mean(): -18.04471206665039 
model_pd.lagr.mean(): -17.90036964416504 
model_pd.lambdas: dict_items([('pout', tensor([1.0920], device='cuda:0')), ('power', tensor([0.9404], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4762], device='cuda:0')), ('power', tensor([-20.8762], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.14434191584587097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7799, 5.6541, 5.9936],
        [4.7799, 4.7799, 4.7799],
        [4.7799, 5.9449, 6.6006],
        [4.7799, 5.4164, 5.5439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.14633041620254517 
model_pd.l_d.mean(): -18.02294921875 
model_pd.lagr.mean(): -17.876619338989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0935], device='cuda:0')), ('power', tensor([0.9394], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4814], device='cuda:0')), ('power', tensor([-20.8846], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.14633041620254517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[4.7624, 5.6388, 5.9827],
        [4.7624, 4.8691, 4.8072],
        [4.7624, 5.2721, 5.3107],
        [4.7624, 5.6397, 5.9846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.14827242493629456 
model_pd.l_d.mean(): -18.00086784362793 
model_pd.lagr.mean(): -17.852596282958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0950], device='cuda:0')), ('power', tensor([0.9384], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4862], device='cuda:0')), ('power', tensor([-20.8923], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.14827242493629456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7477, 4.8741, 4.8069],
        [4.7477, 4.7477, 4.7477],
        [4.7477, 4.7812, 4.7544],
        [4.7477, 4.8934, 4.8225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.15001651644706726 
model_pd.l_d.mean(): -17.9783878326416 
model_pd.lagr.mean(): -17.828371047973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0965], device='cuda:0')), ('power', tensor([0.9373], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4903], device='cuda:0')), ('power', tensor([-20.8987], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.15001651644706726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7367, 4.7367, 4.7367],
        [4.7367, 5.4489, 5.6432],
        [4.7367, 4.9721, 4.9001],
        [4.7367, 5.0275, 4.9660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.1514016091823578 
model_pd.l_d.mean(): -17.955467224121094 
model_pd.lagr.mean(): -17.804065704345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0980], device='cuda:0')), ('power', tensor([0.9363], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4934], device='cuda:0')), ('power', tensor([-20.9035], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.1514016091823578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.7300, 5.4407, 5.6347],
        [4.7300, 5.2043, 5.2236],
        [4.7300, 5.4031, 5.5651],
        [4.7300, 4.9649, 4.8930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.15228469669818878 
model_pd.l_d.mean(): -17.932090759277344 
model_pd.lagr.mean(): -17.77980613708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0995], device='cuda:0')), ('power', tensor([0.9352], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4953], device='cuda:0')), ('power', tensor([-20.9064], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.15228469669818878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7279, 5.1872, 5.1979],
        [4.7279, 6.0705, 6.9559],
        [4.7279, 4.7575, 4.7334],
        [4.7279, 5.1443, 5.1319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.15257886052131653 
model_pd.l_d.mean(): -17.90827178955078 
model_pd.lagr.mean(): -17.755693435668945 
model_pd.lambdas: dict_items([('pout', tensor([1.1010], device='cuda:0')), ('power', tensor([0.9342], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4959], device='cuda:0')), ('power', tensor([-20.9074], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.15257886052131653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7303, 4.8842, 4.8124],
        [4.7303, 5.4406, 5.6343],
        [4.7303, 4.7304, 4.7303],
        [4.7303, 4.9122, 4.8381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.1522810161113739 
model_pd.l_d.mean(): -17.884052276611328 
model_pd.lagr.mean(): -17.73177146911621 
model_pd.lambdas: dict_items([('pout', tensor([1.1025], device='cuda:0')), ('power', tensor([0.9331], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4952], device='cuda:0')), ('power', tensor([-20.9065], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.1522810161113739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7368, 4.7368, 4.7368],
        [4.7368, 5.5326, 5.8027],
        [4.7368, 4.7935, 4.7527],
        [4.7368, 4.9188, 4.8446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.1514725685119629 
model_pd.l_d.mean(): -17.85947608947754 
model_pd.lagr.mean(): -17.708003997802734 
model_pd.lambdas: dict_items([('pout', tensor([1.1040], device='cuda:0')), ('power', tensor([0.9321], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4934], device='cuda:0')), ('power', tensor([-20.9039], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.1514725685119629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7464, 4.7798, 4.7531],
        [4.7464, 4.7464, 4.7464],
        [4.7464, 4.7480, 4.7465],
        [4.7464, 4.7464, 4.7464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.15029174089431763 
model_pd.l_d.mean(): -17.834611892700195 
model_pd.lagr.mean(): -17.6843204498291 
model_pd.lambdas: dict_items([('pout', tensor([1.1055], device='cuda:0')), ('power', tensor([0.9310], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4908], device='cuda:0')), ('power', tensor([-20.9000], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.15029174089431763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7583, 6.1095, 6.9999],
        [4.7583, 5.0497, 4.9880],
        [4.7583, 4.7587, 4.7583],
        [4.7583, 4.7589, 4.7583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14889535307884216 
model_pd.l_d.mean(): -17.809532165527344 
model_pd.lagr.mean(): -17.66063690185547 
model_pd.lambdas: dict_items([('pout', tensor([1.1069], device='cuda:0')), ('power', tensor([0.9300], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4876], device='cuda:0')), ('power', tensor([-20.8950], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14889535307884216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7714, 4.8012, 4.7769],
        [4.7714, 5.6478, 5.9918],
        [4.7714, 5.4496, 5.6125],
        [4.7714, 4.7714, 4.7714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.14742740988731384 
model_pd.l_d.mean(): -17.784326553344727 
model_pd.lagr.mean(): -17.636899948120117 
model_pd.lambdas: dict_items([('pout', tensor([1.1084], device='cuda:0')), ('power', tensor([0.9289], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4840], device='cuda:0')), ('power', tensor([-20.8895], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.14742740988731384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7847, 4.7847, 4.7847],
        [4.7847, 5.2949, 5.3331],
        [4.7847, 5.4192, 5.5457],
        [4.7847, 6.2453, 7.2735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.1460033506155014 
model_pd.l_d.mean(): -17.75908088684082 
model_pd.lagr.mean(): -17.61307716369629 
model_pd.lambdas: dict_items([('pout', tensor([1.1099], device='cuda:0')), ('power', tensor([0.9279], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4803], device='cuda:0')), ('power', tensor([-20.8839], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.1460033506155014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7974, 5.4792, 5.6428],
        [4.7974, 4.8020, 4.7976],
        [4.7974, 5.2777, 5.2968],
        [4.7974, 4.8539, 4.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.14470668137073517 
model_pd.l_d.mean(): -17.73388671875 
model_pd.lagr.mean(): -17.58917999267578 
model_pd.lambdas: dict_items([('pout', tensor([1.1114], device='cuda:0')), ('power', tensor([0.9269], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4769], device='cuda:0')), ('power', tensor([-20.8784], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.14470668137073517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8087, 4.8134, 4.8090],
        [4.8087, 4.8087, 4.8087],
        [4.8087, 4.8087, 4.8087],
        [4.8087, 4.8093, 4.8087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.14359277486801147 
model_pd.l_d.mean(): -17.70882225036621 
model_pd.lagr.mean(): -17.565229415893555 
model_pd.lambdas: dict_items([('pout', tensor([1.1129], device='cuda:0')), ('power', tensor([0.9258], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4738], device='cuda:0')), ('power', tensor([-20.8736], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.14359277486801147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8182, 6.2894, 7.3247],
        [4.8182, 5.6275, 5.9016],
        [4.8182, 4.8482, 4.8238],
        [4.8182, 4.8188, 4.8182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.14269402623176575 
model_pd.l_d.mean(): -17.683950424194336 
model_pd.lagr.mean(): -17.541255950927734 
model_pd.lambdas: dict_items([('pout', tensor([1.1143], device='cuda:0')), ('power', tensor([0.9248], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4713], device='cuda:0')), ('power', tensor([-20.8695], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.14269402623176575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8254, 6.2989, 7.3355],
        [4.8254, 4.8830, 4.8416],
        [4.8254, 4.8258, 4.8254],
        [4.8254, 4.8431, 4.8278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.14202508330345154 
model_pd.l_d.mean(): -17.659320831298828 
model_pd.lagr.mean(): -17.517295837402344 
model_pd.lambdas: dict_items([('pout', tensor([1.1158], device='cuda:0')), ('power', tensor([0.9237], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4693], device='cuda:0')), ('power', tensor([-20.8664], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.14202508330345154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8303, 5.2841, 5.2865],
        [4.8303, 5.2547, 5.2415],
        [4.8303, 4.9377, 4.8753],
        [4.8303, 4.8303, 4.8303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.14158684015274048 
model_pd.l_d.mean(): -17.634950637817383 
model_pd.lagr.mean(): -17.493364334106445 
model_pd.lambdas: dict_items([('pout', tensor([1.1173], device='cuda:0')), ('power', tensor([0.9227], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4680], device='cuda:0')), ('power', tensor([-20.8644], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.14158684015274048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[4.8328, 5.0180, 4.9424],
        [4.8328, 4.9895, 4.9162],
        [4.8328, 5.7128, 6.0533],
        [4.8328, 5.7199, 6.0674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.14136993885040283 
model_pd.l_d.mean(): -17.610849380493164 
model_pd.lagr.mean(): -17.469478607177734 
model_pd.lambdas: dict_items([('pout', tensor([1.1187], device='cuda:0')), ('power', tensor([0.9216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4674], device='cuda:0')), ('power', tensor([-20.8634], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.14136993885040283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8332, 4.8908, 4.8494],
        [4.8332, 6.0067, 6.6655],
        [4.8332, 5.1157, 5.0499],
        [4.8332, 4.8905, 4.8492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.14135611057281494 
model_pd.l_d.mean(): -17.586992263793945 
model_pd.lagr.mean(): -17.445636749267578 
model_pd.lambdas: dict_items([('pout', tensor([1.1202], device='cuda:0')), ('power', tensor([0.9206], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4673], device='cuda:0')), ('power', tensor([-20.8634], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.14135611057281494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8315, 4.8888, 4.8476],
        [4.8315, 4.8362, 4.8318],
        [4.8315, 5.7106, 6.0505],
        [4.8315, 6.0571, 6.7780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.14151974022388458 
model_pd.l_d.mean(): -17.563343048095703 
model_pd.lagr.mean(): -17.421823501586914 
model_pd.lambdas: dict_items([('pout', tensor([1.1217], device='cuda:0')), ('power', tensor([0.9196], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4678], device='cuda:0')), ('power', tensor([-20.8642], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.14151974022388458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8283, 4.8286, 4.8283],
        [4.8283, 5.7135, 6.0600],
        [4.8283, 4.8300, 4.8284],
        [4.8283, 4.8855, 4.8444]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.14182907342910767 
model_pd.l_d.mean(): -17.539857864379883 
model_pd.lagr.mean(): -17.398029327392578 
model_pd.lambdas: dict_items([('pout', tensor([1.1231], device='cuda:0')), ('power', tensor([0.9185], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4687], device='cuda:0')), ('power', tensor([-20.8658], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.14182907342910767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[4.8240, 5.9714, 6.6015],
        [4.8240, 4.9310, 4.8688],
        [4.8240, 5.2761, 5.2783],
        [4.8240, 5.1181, 5.0555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14224688708782196 
model_pd.l_d.mean(): -17.516475677490234 
model_pd.lagr.mean(): -17.374229431152344 
model_pd.lambdas: dict_items([('pout', tensor([1.1246], device='cuda:0')), ('power', tensor([0.9175], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4699], device='cuda:0')), ('power', tensor([-20.8678], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14224688708782196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8191, 4.8256, 4.8196],
        [4.8191, 5.6261, 5.8988],
        [4.8191, 4.8191, 4.8191],
        [4.8191, 4.8191, 4.8191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14273162186145782 
model_pd.l_d.mean(): -17.493139266967773 
model_pd.lagr.mean(): -17.35040855407715 
model_pd.lambdas: dict_items([('pout', tensor([1.1261], device='cuda:0')), ('power', tensor([0.9164], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4712], device='cuda:0')), ('power', tensor([-20.8702], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14273162186145782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8140, 5.6880, 6.0258],
        [4.8140, 4.9602, 4.8888],
        [4.8140, 4.8140, 4.8140],
        [4.8140, 5.1214, 5.0628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.14323905110359192 
model_pd.l_d.mean(): -17.469802856445312 
model_pd.lagr.mean(): -17.326562881469727 
model_pd.lambdas: dict_items([('pout', tensor([1.1276], device='cuda:0')), ('power', tensor([0.9154], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4726], device='cuda:0')), ('power', tensor([-20.8726], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.14323905110359192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8092, 4.8176, 4.8099],
        [4.8092, 4.8093, 4.8092],
        [4.8092, 4.8092, 4.8092],
        [4.8092, 5.4895, 5.6520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.14372451603412628 
model_pd.l_d.mean(): -17.44641876220703 
model_pd.lagr.mean(): -17.30269432067871 
model_pd.lambdas: dict_items([('pout', tensor([1.1290], device='cuda:0')), ('power', tensor([0.9143], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4740], device='cuda:0')), ('power', tensor([-20.8748], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.14372451603412628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8051, 5.2833, 5.3018],
        [4.8051, 4.8613, 4.8208],
        [4.8051, 5.2684, 5.2784],
        [4.8051, 4.8051, 4.8051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.1441454142332077 
model_pd.l_d.mean(): -17.422945022583008 
model_pd.lagr.mean(): -17.278799057006836 
model_pd.lambdas: dict_items([('pout', tensor([1.1305], device='cuda:0')), ('power', tensor([0.9133], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4751], device='cuda:0')), ('power', tensor([-20.8768], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.1441454142332077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8021, 6.0162, 6.7299],
        [4.8021, 4.8588, 4.8180],
        [4.8021, 5.0814, 5.0162],
        [4.8021, 4.8105, 4.8029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.14446547627449036 
model_pd.l_d.mean(): -17.39935874938965 
model_pd.lagr.mean(): -17.254892349243164 
model_pd.lambdas: dict_items([('pout', tensor([1.1320], device='cuda:0')), ('power', tensor([0.9122], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4759], device='cuda:0')), ('power', tensor([-20.8782], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.14446547627449036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8004, 4.8178, 4.8028],
        [4.8004, 6.0136, 6.7266],
        [4.8004, 4.8068, 4.8009],
        [4.8004, 5.9389, 6.5637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.14465713500976562 
model_pd.l_d.mean(): -17.375646591186523 
model_pd.lagr.mean(): -17.230989456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.1335], device='cuda:0')), ('power', tensor([0.9112], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4764], device='cuda:0')), ('power', tensor([-20.8791], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.14465713500976562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8001, 4.8001, 4.8001],
        [4.8001, 4.8335, 4.8068],
        [4.8001, 4.9827, 4.9080],
        [4.8001, 4.8002, 4.8001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.14470528066158295 
model_pd.l_d.mean(): -17.351797103881836 
model_pd.lagr.mean(): -17.20709228515625 
model_pd.lambdas: dict_items([('pout', tensor([1.1349], device='cuda:0')), ('power', tensor([0.9102], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4765], device='cuda:0')), ('power', tensor([-20.8794], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.14470528066158295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8013, 4.8016, 4.8013],
        [4.8013, 4.8013, 4.8013],
        [4.8013, 5.3090, 5.3462],
        [4.8013, 4.8029, 4.8014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.14460767805576324 
model_pd.l_d.mean(): -17.327817916870117 
model_pd.lagr.mean(): -17.183210372924805 
model_pd.lambdas: dict_items([('pout', tensor([1.1364], device='cuda:0')), ('power', tensor([0.9091], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4763], device='cuda:0')), ('power', tensor([-20.8790], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.14460767805576324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8038, 4.8038, 4.8038],
        [4.8038, 4.8084, 4.8041],
        [4.8038, 5.0825, 5.0174],
        [4.8038, 4.8044, 4.8038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.1443750113248825 
model_pd.l_d.mean(): -17.30372428894043 
model_pd.lagr.mean(): -17.15934944152832 
model_pd.lambdas: dict_items([('pout', tensor([1.1379], device='cuda:0')), ('power', tensor([0.9081], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4756], device='cuda:0')), ('power', tensor([-20.8781], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.1443750113248825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8074, 4.8642, 4.8234],
        [4.8074, 6.1657, 7.0582],
        [4.8074, 4.8077, 4.8074],
        [4.8074, 4.9618, 4.8895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.1440286636352539 
model_pd.l_d.mean(): -17.27953338623047 
model_pd.lagr.mean(): -17.13550567626953 
model_pd.lambdas: dict_items([('pout', tensor([1.1394], device='cuda:0')), ('power', tensor([0.9070], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4746], device='cuda:0')), ('power', tensor([-20.8766], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.1440286636352539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8120, 6.1713, 7.0643],
        [4.8120, 4.9945, 4.9198],
        [4.8120, 4.8126, 4.8120],
        [4.8120, 4.8294, 4.8143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.14359714090824127 
model_pd.l_d.mean(): -17.255260467529297 
model_pd.lagr.mean(): -17.111663818359375 
model_pd.lambdas: dict_items([('pout', tensor([1.1408], device='cuda:0')), ('power', tensor([0.9060], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4734], device='cuda:0')), ('power', tensor([-20.8748], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.14359714090824127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8172, 4.8175, 4.8172],
        [4.8172, 4.8174, 4.8172],
        [4.8172, 5.1229, 5.0644],
        [4.8172, 5.0222, 4.9474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.1431122124195099 
model_pd.l_d.mean(): -17.230941772460938 
model_pd.lagr.mean(): -17.08782958984375 
model_pd.lambdas: dict_items([('pout', tensor([1.1423], device='cuda:0')), ('power', tensor([0.9049], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4720], device='cuda:0')), ('power', tensor([-20.8726], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.1431122124195099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8226, 5.5400, 5.7337],
        [4.8226, 4.9286, 4.8669],
        [4.8226, 5.0053, 4.9305],
        [4.8226, 4.8400, 4.8250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.1426059901714325 
model_pd.l_d.mean(): -17.20660400390625 
model_pd.lagr.mean(): -17.063997268676758 
model_pd.lambdas: dict_items([('pout', tensor([1.1438], device='cuda:0')), ('power', tensor([0.9039], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4706], device='cuda:0')), ('power', tensor([-20.8704], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.1426059901714325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8281, 4.9827, 4.9103],
        [4.8281, 4.9735, 4.9025],
        [4.8281, 5.9706, 6.5968],
        [4.8281, 4.8281, 4.8281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.14210744202136993 
model_pd.l_d.mean(): -17.18226432800293 
model_pd.lagr.mean(): -17.040157318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.1452], device='cuda:0')), ('power', tensor([0.9029], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4691], device='cuda:0')), ('power', tensor([-20.8681], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.14210744202136993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8333, 4.9394, 4.8776],
        [4.8333, 4.8902, 4.8493],
        [4.8333, 4.9788, 4.9077],
        [4.8333, 6.3001, 7.3294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.1416412740945816 
model_pd.l_d.mean(): -17.157962799072266 
model_pd.lagr.mean(): -17.016321182250977 
model_pd.lambdas: dict_items([('pout', tensor([1.1467], device='cuda:0')), ('power', tensor([0.9018], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4678], device='cuda:0')), ('power', tensor([-20.8659], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.1416412740945816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8380, 4.8380, 4.8380],
        [4.8380, 4.8463, 4.8387],
        [4.8380, 4.8929, 4.8531],
        [4.8380, 5.3479, 5.3848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1412263959646225 
model_pd.l_d.mean(): -17.133710861206055 
model_pd.lagr.mean(): -16.99248504638672 
model_pd.lambdas: dict_items([('pout', tensor([1.1482], device='cuda:0')), ('power', tensor([0.9008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4665], device='cuda:0')), ('power', tensor([-20.8640], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1412263959646225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8421, 4.9877, 4.9165],
        [4.8421, 4.8421, 4.8421],
        [4.8421, 4.9968, 4.9243],
        [4.8421, 5.1343, 5.0717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.1408759504556656 
model_pd.l_d.mean(): -17.109527587890625 
model_pd.lagr.mean(): -16.968650817871094 
model_pd.lambdas: dict_items([('pout', tensor([1.1496], device='cuda:0')), ('power', tensor([0.8997], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4654], device='cuda:0')), ('power', tensor([-20.8623], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.1408759504556656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8454, 6.2123, 7.1092],
        [4.8454, 5.3243, 5.3421],
        [4.8454, 6.0662, 6.7823],
        [4.8454, 4.8459, 4.8454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.14059703052043915 
model_pd.l_d.mean(): -17.085418701171875 
model_pd.lagr.mean(): -16.944822311401367 
model_pd.lambdas: dict_items([('pout', tensor([1.1511], device='cuda:0')), ('power', tensor([0.8987], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4646], device='cuda:0')), ('power', tensor([-20.8610], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.14059703052043915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8479, 6.3179, 7.3487],
        [4.8479, 5.0534, 4.9782],
        [4.8479, 5.7295, 6.0730],
        [4.8479, 4.8479, 4.8479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.1403912603855133 
model_pd.l_d.mean(): -17.061389923095703 
model_pd.lagr.mean(): -16.92099952697754 
model_pd.lambdas: dict_items([('pout', tensor([1.1526], device='cuda:0')), ('power', tensor([0.8976], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4639], device='cuda:0')), ('power', tensor([-20.8600], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.1403912603855133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8496, 5.0550, 4.9799],
        [4.8496, 4.9951, 4.9239],
        [4.8496, 5.5691, 5.7627],
        [4.8496, 4.8497, 4.8496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.14025558531284332 
model_pd.l_d.mean(): -17.03744125366211 
model_pd.lagr.mean(): -16.897186279296875 
model_pd.lambdas: dict_items([('pout', tensor([1.1540], device='cuda:0')), ('power', tensor([0.8966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4635], device='cuda:0')), ('power', tensor([-20.8594], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.14025558531284332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[4.8506, 6.2178, 7.1145],
        [4.8506, 5.7249, 6.0611],
        [4.8506, 5.1427, 5.0799],
        [4.8506, 5.7320, 6.0752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.1401824802160263 
model_pd.l_d.mean(): -17.013561248779297 
model_pd.lagr.mean(): -16.87337875366211 
model_pd.lambdas: dict_items([('pout', tensor([1.1555], device='cuda:0')), ('power', tensor([0.8956], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4632], device='cuda:0')), ('power', tensor([-20.8591], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.1401824802160263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8511, 4.8844, 4.8577],
        [4.8511, 6.0188, 6.6717],
        [4.8511, 5.0056, 4.9331],
        [4.8511, 4.9059, 4.8661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.14016102254390717 
model_pd.l_d.mean(): -16.98973274230957 
model_pd.lagr.mean(): -16.849571228027344 
model_pd.lambdas: dict_items([('pout', tensor([1.1570], device='cuda:0')), ('power', tensor([0.8945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4632], device='cuda:0')), ('power', tensor([-20.8591], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.14016102254390717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8511, 4.8511, 4.8511],
        [4.8511, 4.8511, 4.8511],
        [4.8511, 6.3202, 7.3498],
        [4.8511, 4.8557, 4.8513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.1401781588792801 
model_pd.l_d.mean(): -16.9659481048584 
model_pd.lagr.mean(): -16.825769424438477 
model_pd.lambdas: dict_items([('pout', tensor([1.1584], device='cuda:0')), ('power', tensor([0.8935], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4632], device='cuda:0')), ('power', tensor([-20.8592], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.1401781588792801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8508, 4.9766, 4.9095],
        [4.8508, 4.8509, 4.8508],
        [4.8508, 5.5306, 5.6915],
        [4.8508, 5.7239, 6.0593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.14021886885166168 
model_pd.l_d.mean(): -16.942188262939453 
model_pd.lagr.mean(): -16.801969528198242 
model_pd.lambdas: dict_items([('pout', tensor([1.1599], device='cuda:0')), ('power', tensor([0.8924], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4633], device='cuda:0')), ('power', tensor([-20.8595], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.14021886885166168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8505, 4.9955, 4.9245],
        [4.8505, 5.0859, 5.0131],
        [4.8505, 5.2989, 5.2998],
        [4.8505, 4.8505, 4.8505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.14026795327663422 
model_pd.l_d.mean(): -16.91843605041504 
model_pd.lagr.mean(): -16.778167724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.1614], device='cuda:0')), ('power', tensor([0.8914], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4634], device='cuda:0')), ('power', tensor([-20.8598], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.14026795327663422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8502, 4.8504, 4.8502],
        [4.8502, 4.8506, 4.8502],
        [4.8502, 4.9060, 4.8657],
        [4.8502, 4.8503, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.1403106153011322 
model_pd.l_d.mean(): -16.894676208496094 
model_pd.lagr.mean(): -16.754365921020508 
model_pd.lambdas: dict_items([('pout', tensor([1.1628], device='cuda:0')), ('power', tensor([0.8903], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4635], device='cuda:0')), ('power', tensor([-20.8601], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.1403106153011322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8501, 4.8833, 4.8568],
        [4.8501, 4.8505, 4.8501],
        [4.8501, 5.3121, 5.3208],
        [4.8501, 4.8501, 4.8501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.14033357799053192 
model_pd.l_d.mean(): -16.87089729309082 
model_pd.lagr.mean(): -16.73056411743164 
model_pd.lambdas: dict_items([('pout', tensor([1.1643], device='cuda:0')), ('power', tensor([0.8893], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4636], device='cuda:0')), ('power', tensor([-20.8603], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.14033357799053192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8504, 5.7288, 6.0702],
        [4.8504, 4.8508, 4.8504],
        [4.8504, 4.8504, 4.8504],
        [4.8504, 4.8836, 4.8571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.1403261125087738 
model_pd.l_d.mean(): -16.847087860107422 
model_pd.lagr.mean(): -16.706762313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.1657], device='cuda:0')), ('power', tensor([0.8883], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4635], device='cuda:0')), ('power', tensor([-20.8604], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.1403261125087738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8512, 5.3587, 5.3946],
        [4.8512, 4.8558, 4.8514],
        [4.8512, 4.8512, 4.8512],
        [4.8512, 4.8512, 4.8512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.14028048515319824 
model_pd.l_d.mean(): -16.823244094848633 
model_pd.lagr.mean(): -16.682964324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.1672], device='cuda:0')), ('power', tensor([0.8872], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4633], device='cuda:0')), ('power', tensor([-20.8602], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.14028048515319824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8524, 4.8696, 4.8547],
        [4.8524, 4.8524, 4.8524],
        [4.8524, 4.8606, 4.8531],
        [4.8524, 5.2703, 5.2559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.14019270241260529 
model_pd.l_d.mean(): -16.799358367919922 
model_pd.lagr.mean(): -16.65916633605957 
model_pd.lambdas: dict_items([('pout', tensor([1.1687], device='cuda:0')), ('power', tensor([0.8862], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4631], device='cuda:0')), ('power', tensor([-20.8599], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.14019270241260529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8541, 5.0076, 4.9355],
        [4.8541, 4.8541, 4.8541],
        [4.8541, 4.8834, 4.8595],
        [4.8541, 5.0887, 5.0160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.14006204903125763 
model_pd.l_d.mean(): -16.775434494018555 
model_pd.lagr.mean(): -16.635372161865234 
model_pd.lambdas: dict_items([('pout', tensor([1.1701], device='cuda:0')), ('power', tensor([0.8851], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4626], device='cuda:0')), ('power', tensor([-20.8593], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.14006204903125763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8562, 4.8562, 4.8562],
        [4.8562, 5.2339, 5.2009],
        [4.8562, 6.3224, 7.3484],
        [4.8562, 4.8734, 4.8585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.13989150524139404 
model_pd.l_d.mean(): -16.751480102539062 
model_pd.lagr.mean(): -16.611589431762695 
model_pd.lambdas: dict_items([('pout', tensor([1.1716], device='cuda:0')), ('power', tensor([0.8841], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4621], device='cuda:0')), ('power', tensor([-20.8585], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.13989150524139404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8588, 4.9143, 4.8742],
        [4.8588, 4.8591, 4.8588],
        [4.8588, 4.8591, 4.8588],
        [4.8588, 5.2364, 5.2034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.13968655467033386 
model_pd.l_d.mean(): -16.727493286132812 
model_pd.lagr.mean(): -16.587806701660156 
model_pd.lambdas: dict_items([('pout', tensor([1.1731], device='cuda:0')), ('power', tensor([0.8830], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4614], device='cuda:0')), ('power', tensor([-20.8575], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.13968655467033386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8617, 5.2393, 5.2062],
        [4.8617, 4.9177, 4.8773],
        [4.8617, 4.8617, 4.8617],
        [4.8617, 4.8617, 4.8617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.1394551545381546 
model_pd.l_d.mean(): -16.7034854888916 
model_pd.lagr.mean(): -16.564029693603516 
model_pd.lambdas: dict_items([('pout', tensor([1.1745], device='cuda:0')), ('power', tensor([0.8820], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4607], device='cuda:0')), ('power', tensor([-20.8564], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.1394551545381546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8648, 4.8648, 4.8648],
        [4.8648, 5.3259, 5.3341],
        [4.8648, 4.8730, 4.8655],
        [4.8648, 4.8693, 4.8650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.13920602202415466 
model_pd.l_d.mean(): -16.679462432861328 
model_pd.lagr.mean(): -16.54025650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.1760], device='cuda:0')), ('power', tensor([0.8810], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4599], device='cuda:0')), ('power', tensor([-20.8551], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.13920602202415466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8680, 4.9243, 4.8837],
        [4.8680, 4.8686, 4.8680],
        [4.8680, 4.8680, 4.8680],
        [4.8680, 6.3361, 7.3627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.13894866406917572 
model_pd.l_d.mean(): -16.655437469482422 
model_pd.lagr.mean(): -16.516489028930664 
model_pd.lambdas: dict_items([('pout', tensor([1.1774], device='cuda:0')), ('power', tensor([0.8799], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4590], device='cuda:0')), ('power', tensor([-20.8539], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.13894866406917572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8712, 5.3324, 5.3405],
        [4.8712, 4.9963, 4.9294],
        [4.8712, 6.0901, 6.8024],
        [4.8712, 4.8712, 4.8712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.13869205117225647 
model_pd.l_d.mean(): -16.63141441345215 
model_pd.lagr.mean(): -16.492721557617188 
model_pd.lambdas: dict_items([('pout', tensor([1.1789], device='cuda:0')), ('power', tensor([0.8789], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4582], device='cuda:0')), ('power', tensor([-20.8526], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.13869205117225647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8744, 4.9287, 4.8892],
        [4.8744, 5.6773, 5.9454],
        [4.8744, 5.1786, 5.1195],
        [4.8744, 5.0558, 4.9811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.138444185256958 
model_pd.l_d.mean(): -16.607404708862305 
model_pd.lagr.mean(): -16.46895980834961 
model_pd.lambdas: dict_items([('pout', tensor([1.1804], device='cuda:0')), ('power', tensor([0.8778], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4574], device='cuda:0')), ('power', tensor([-20.8513], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.138444185256958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8774, 6.0969, 6.8094],
        [4.8774, 4.9329, 4.8928],
        [4.8774, 4.8946, 4.8797],
        [4.8774, 4.8774, 4.8774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.13821150362491608 
model_pd.l_d.mean(): -16.583415985107422 
model_pd.lagr.mean(): -16.44520378112793 
model_pd.lambdas: dict_items([('pout', tensor([1.1818], device='cuda:0')), ('power', tensor([0.8768], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4566], device='cuda:0')), ('power', tensor([-20.8501], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.13821150362491608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8802, 4.8865, 4.8806],
        [4.8802, 5.0615, 4.9868],
        [4.8802, 5.2577, 5.2243],
        [4.8802, 5.0334, 4.9613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.13799868524074554 
model_pd.l_d.mean(): -16.5594482421875 
model_pd.lagr.mean(): -16.421449661254883 
model_pd.lambdas: dict_items([('pout', tensor([1.1833], device='cuda:0')), ('power', tensor([0.8757], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4559], device='cuda:0')), ('power', tensor([-20.8490], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.13799868524074554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8827, 4.8909, 4.8834],
        [4.8827, 5.7546, 6.0878],
        [4.8827, 4.8827, 4.8827],
        [4.8827, 5.5993, 5.7903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.13780838251113892 
model_pd.l_d.mean(): -16.53550910949707 
model_pd.lagr.mean(): -16.397701263427734 
model_pd.lambdas: dict_items([('pout', tensor([1.1847], device='cuda:0')), ('power', tensor([0.8747], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4553], device='cuda:0')), ('power', tensor([-20.8481], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.13780838251113892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8850, 5.6880, 5.9557],
        [4.8850, 4.9142, 4.8904],
        [4.8850, 4.8850, 4.8850],
        [4.8850, 4.8852, 4.8850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.1376415640115738 
model_pd.l_d.mean(): -16.511594772338867 
model_pd.lagr.mean(): -16.373952865600586 
model_pd.lambdas: dict_items([('pout', tensor([1.1862], device='cuda:0')), ('power', tensor([0.8737], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4547], device='cuda:0')), ('power', tensor([-20.8472], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.1376415640115738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8869, 5.1767, 5.1137],
        [4.8869, 4.8951, 4.8876],
        [4.8869, 4.9918, 4.9306],
        [4.8869, 5.0680, 4.9934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.13749878108501434 
model_pd.l_d.mean(): -16.48770523071289 
model_pd.lagr.mean(): -16.35020637512207 
model_pd.lambdas: dict_items([('pout', tensor([1.1876], device='cuda:0')), ('power', tensor([0.8726], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4542], device='cuda:0')), ('power', tensor([-20.8465], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.13749878108501434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8886, 4.9215, 4.8952],
        [4.8886, 4.8889, 4.8886],
        [4.8886, 5.0697, 4.9950],
        [4.8886, 4.9447, 4.9043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.137378990650177 
model_pd.l_d.mean(): -16.463844299316406 
model_pd.lagr.mean(): -16.326465606689453 
model_pd.lambdas: dict_items([('pout', tensor([1.1891], device='cuda:0')), ('power', tensor([0.8716], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4538], device='cuda:0')), ('power', tensor([-20.8459], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.137378990650177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8901, 4.8946, 4.8903],
        [4.8901, 5.6062, 5.7967],
        [4.8901, 6.1100, 6.8217],
        [4.8901, 4.9462, 4.9057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13727836310863495 
model_pd.l_d.mean(): -16.44000244140625 
model_pd.lagr.mean(): -16.302724838256836 
model_pd.lambdas: dict_items([('pout', tensor([1.1905], device='cuda:0')), ('power', tensor([0.8705], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4534], device='cuda:0')), ('power', tensor([-20.8454], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.13727836310863495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8914, 4.9466, 4.9067],
        [4.8914, 5.3667, 5.3826],
        [4.8914, 5.3084, 5.2931],
        [4.8914, 5.0946, 5.0197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.1371922343969345 
model_pd.l_d.mean(): -16.41617774963379 
model_pd.lagr.mean(): -16.27898597717285 
model_pd.lambdas: dict_items([('pout', tensor([1.1920], device='cuda:0')), ('power', tensor([0.8695], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4531], device='cuda:0')), ('power', tensor([-20.8450], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.1371922343969345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8925, 5.0453, 4.9733],
        [4.8925, 5.3386, 5.3383],
        [4.8925, 4.8928, 4.8925],
        [4.8925, 5.5698, 5.7280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.13711488246917725 
model_pd.l_d.mean(): -16.392362594604492 
model_pd.lagr.mean(): -16.255247116088867 
model_pd.lambdas: dict_items([('pout', tensor([1.1934], device='cuda:0')), ('power', tensor([0.8684], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4528], device='cuda:0')), ('power', tensor([-20.8447], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.13711488246917725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8937, 4.9477, 4.9084],
        [4.8937, 5.3686, 5.3844],
        [4.8937, 5.7646, 6.0966],
        [4.8937, 5.3998, 5.4343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.1370404213666916 
model_pd.l_d.mean(): -16.368553161621094 
model_pd.lagr.mean(): -16.23151206970215 
model_pd.lambdas: dict_items([('pout', tensor([1.1949], device='cuda:0')), ('power', tensor([0.8674], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4526], device='cuda:0')), ('power', tensor([-20.8443], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.1370404213666916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8948, 4.8948, 4.8948],
        [4.8948, 6.3650, 7.3905],
        [4.8948, 4.8951, 4.8948],
        [4.8948, 5.0192, 4.9526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.13696308434009552 
model_pd.l_d.mean(): -16.344738006591797 
model_pd.lagr.mean(): -16.207775115966797 
model_pd.lambdas: dict_items([('pout', tensor([1.1963], device='cuda:0')), ('power', tensor([0.8664], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4523], device='cuda:0')), ('power', tensor([-20.8440], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.13696308434009552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[4.8961, 5.5268, 5.6482],
        [4.8961, 5.1294, 5.0566],
        [4.8961, 5.0486, 4.9767],
        [4.8961, 5.7738, 6.1123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.1368778645992279 
model_pd.l_d.mean(): -16.320919036865234 
model_pd.lagr.mean(): -16.18404197692871 
model_pd.lambdas: dict_items([('pout', tensor([1.1978], device='cuda:0')), ('power', tensor([0.8653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4520], device='cuda:0')), ('power', tensor([-20.8436], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.1368778645992279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8975, 6.0633, 6.7109],
        [4.8975, 5.6125, 5.8020],
        [4.8975, 4.9021, 4.8978],
        [4.8975, 5.0779, 5.0034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.1367807388305664 
model_pd.l_d.mean(): -16.297090530395508 
model_pd.lagr.mean(): -16.160308837890625 
model_pd.lambdas: dict_items([('pout', tensor([1.1992], device='cuda:0')), ('power', tensor([0.8643], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4516], device='cuda:0')), ('power', tensor([-20.8431], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.1367807388305664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8991, 5.1322, 5.0595],
        [4.8991, 6.0421, 6.6633],
        [4.8991, 4.9162, 4.9014],
        [4.8991, 5.0233, 4.9568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.13666905462741852 
model_pd.l_d.mean(): -16.27324676513672 
model_pd.lagr.mean(): -16.136577606201172 
model_pd.lambdas: dict_items([('pout', tensor([1.2007], device='cuda:0')), ('power', tensor([0.8632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4512], device='cuda:0')), ('power', tensor([-20.8425], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.13666905462741852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9009, 5.5772, 5.7346],
        [4.9009, 5.2036, 5.1442],
        [4.9009, 4.9010, 4.9009],
        [4.9009, 5.5313, 5.6523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.13654150068759918 
model_pd.l_d.mean(): -16.249391555786133 
model_pd.lagr.mean(): -16.112850189208984 
model_pd.lambdas: dict_items([('pout', tensor([1.2022], device='cuda:0')), ('power', tensor([0.8622], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4508], device='cuda:0')), ('power', tensor([-20.8418], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.13654150068759918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9029, 4.9030, 4.9029],
        [4.9029, 5.4083, 5.4423],
        [4.9029, 4.9356, 4.9095],
        [4.9029, 5.7731, 6.1041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.13639827072620392 
model_pd.l_d.mean(): -16.22552490234375 
model_pd.lagr.mean(): -16.089126586914062 
model_pd.lambdas: dict_items([('pout', tensor([1.2036], device='cuda:0')), ('power', tensor([0.8611], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4503], device='cuda:0')), ('power', tensor([-20.8411], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.13639827072620392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9051, 4.9590, 4.9198],
        [4.9051, 5.0853, 5.0108],
        [4.9051, 5.1811, 5.1152],
        [4.9051, 4.9054, 4.9051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.1362408846616745 
model_pd.l_d.mean(): -16.201644897460938 
model_pd.lagr.mean(): -16.065404891967773 
model_pd.lambdas: dict_items([('pout', tensor([1.2051], device='cuda:0')), ('power', tensor([0.8601], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4497], device='cuda:0')), ('power', tensor([-20.8402], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.1362408846616745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9075, 4.9079, 4.9075],
        [4.9075, 5.4127, 5.4466],
        [4.9075, 5.3667, 5.3735],
        [4.9075, 4.9075, 4.9075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13607177138328552 
model_pd.l_d.mean(): -16.17775535583496 
model_pd.lagr.mean(): -16.041683197021484 
model_pd.lambdas: dict_items([('pout', tensor([1.2065], device='cuda:0')), ('power', tensor([0.8591], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4491], device='cuda:0')), ('power', tensor([-20.8393], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13607177138328552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9100, 4.9181, 4.9107],
        [4.9100, 5.1123, 5.0375],
        [4.9100, 6.0762, 6.7232],
        [4.9100, 5.5861, 5.7431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.1358942836523056 
model_pd.l_d.mean(): -16.153860092163086 
model_pd.lagr.mean(): -16.01796531677246 
model_pd.lambdas: dict_items([('pout', tensor([1.2079], device='cuda:0')), ('power', tensor([0.8580], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4485], device='cuda:0')), ('power', tensor([-20.8383], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.1358942836523056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 5.2007, 5.1375],
        [4.9125, 6.1323, 6.8419],
        [4.9125, 4.9206, 4.9132],
        [4.9125, 4.9125, 4.9125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.13571228086948395 
model_pd.l_d.mean(): -16.12996482849121 
model_pd.lagr.mean(): -15.99425220489502 
model_pd.lambdas: dict_items([('pout', tensor([1.2094], device='cuda:0')), ('power', tensor([0.8570], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4478], device='cuda:0')), ('power', tensor([-20.8373], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.13571228086948395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9150, 4.9232, 4.9157],
        [4.9150, 5.2174, 5.1578],
        [4.9150, 5.0390, 4.9725],
        [4.9150, 5.5913, 5.7481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.13553087413311005 
model_pd.l_d.mean(): -16.106069564819336 
model_pd.lagr.mean(): -15.970539093017578 
model_pd.lambdas: dict_items([('pout', tensor([1.2108], device='cuda:0')), ('power', tensor([0.8559], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4472], device='cuda:0')), ('power', tensor([-20.8363], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.13553087413311005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9175, 4.9713, 4.9322],
        [4.9175, 6.0615, 6.6822],
        [4.9175, 5.7190, 5.9843],
        [4.9175, 4.9724, 4.9327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.1353541761636734 
model_pd.l_d.mean(): -16.082183837890625 
model_pd.lagr.mean(): -15.946829795837402 
model_pd.lambdas: dict_items([('pout', tensor([1.2123], device='cuda:0')), ('power', tensor([0.8549], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4466], device='cuda:0')), ('power', tensor([-20.8353], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.1353541761636734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9199, 4.9203, 4.9199],
        [4.9199, 5.3938, 5.4088],
        [4.9199, 4.9280, 4.9206],
        [4.9199, 6.2883, 7.1791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.13518570363521576 
model_pd.l_d.mean(): -16.058307647705078 
model_pd.lagr.mean(): -15.92312240600586 
model_pd.lambdas: dict_items([('pout', tensor([1.2137], device='cuda:0')), ('power', tensor([0.8539], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4460], device='cuda:0')), ('power', tensor([-20.8344], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.13518570363521576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9222, 5.2244, 5.1648],
        [4.9222, 5.7929, 6.1232],
        [4.9222, 4.9267, 4.9225],
        [4.9222, 5.2976, 5.2634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.13502717018127441 
model_pd.l_d.mean(): -16.034442901611328 
model_pd.lagr.mean(): -15.899415969848633 
model_pd.lambdas: dict_items([('pout', tensor([1.2152], device='cuda:0')), ('power', tensor([0.8528], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4454], device='cuda:0')), ('power', tensor([-20.8335], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.13502717018127441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9243, 4.9413, 4.9266],
        [4.9243, 4.9799, 4.9398],
        [4.9243, 5.1999, 5.1338],
        [4.9243, 5.8012, 6.1374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13487973809242249 
model_pd.l_d.mean(): -16.010595321655273 
model_pd.lagr.mean(): -15.875715255737305 
model_pd.lambdas: dict_items([('pout', tensor([1.2166], device='cuda:0')), ('power', tensor([0.8518], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4448], device='cuda:0')), ('power', tensor([-20.8326], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.13487973809242249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9263, 5.4001, 5.4148],
        [4.9263, 4.9553, 4.9317],
        [4.9263, 4.9266, 4.9263],
        [4.9263, 4.9325, 4.9268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.1347430795431137 
model_pd.l_d.mean(): -15.9867582321167 
model_pd.lagr.mean(): -15.852015495300293 
model_pd.lambdas: dict_items([('pout', tensor([1.2181], device='cuda:0')), ('power', tensor([0.8507], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4443], device='cuda:0')), ('power', tensor([-20.8319], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.1347430795431137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9282, 4.9363, 4.9289],
        [4.9282, 4.9607, 4.9347],
        [4.9282, 5.8050, 6.1411],
        [4.9282, 4.9327, 4.9285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.13461598753929138 
model_pd.l_d.mean(): -15.962934494018555 
model_pd.lagr.mean(): -15.82831859588623 
model_pd.lambdas: dict_items([('pout', tensor([1.2195], device='cuda:0')), ('power', tensor([0.8497], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4439], device='cuda:0')), ('power', tensor([-20.8312], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.13461598753929138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9300, 4.9305, 4.9300],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9315, 4.9300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13449640572071075 
model_pd.l_d.mean(): -15.939117431640625 
model_pd.lagr.mean(): -15.804620742797852 
model_pd.lambdas: dict_items([('pout', tensor([1.2210], device='cuda:0')), ('power', tensor([0.8486], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4434], device='cuda:0')), ('power', tensor([-20.8305], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.13449640572071075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9317, 4.9317, 4.9317],
        [4.9317, 4.9853, 4.9463],
        [4.9317, 6.3009, 7.1913],
        [4.9317, 5.0744, 5.0040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.13438169658184052 
model_pd.l_d.mean(): -15.915312767028809 
model_pd.lagr.mean(): -15.78093147277832 
model_pd.lambdas: dict_items([('pout', tensor([1.2224], device='cuda:0')), ('power', tensor([0.8476], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4430], device='cuda:0')), ('power', tensor([-20.8298], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.13438169658184052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9334, 4.9415, 4.9341],
        [4.9334, 4.9503, 4.9356],
        [4.9334, 4.9623, 4.9387],
        [4.9334, 5.7347, 5.9991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.13426879048347473 
model_pd.l_d.mean(): -15.891508102416992 
model_pd.lagr.mean(): -15.75723934173584 
model_pd.lambdas: dict_items([('pout', tensor([1.2238], device='cuda:0')), ('power', tensor([0.8466], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4426], device='cuda:0')), ('power', tensor([-20.8292], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.13426879048347473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9351, 6.0797, 6.6995],
        [4.9351, 4.9903, 4.9504],
        [4.9351, 5.6495, 5.8372],
        [4.9351, 4.9354, 4.9351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13415493071079254 
model_pd.l_d.mean(): -15.867706298828125 
model_pd.lagr.mean(): -15.733551025390625 
model_pd.lambdas: dict_items([('pout', tensor([1.2253], device='cuda:0')), ('power', tensor([0.8455], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4422], device='cuda:0')), ('power', tensor([-20.8285], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.13415493071079254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]], device='cuda:0')
 pt:tensor([[4.9369, 5.3117, 5.2771],
        [4.9369, 6.3063, 7.1964],
        [4.9369, 5.0405, 4.9798],
        [4.9369, 5.0884, 5.0167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.13403752446174622 
model_pd.l_d.mean(): -15.843900680541992 
model_pd.lagr.mean(): -15.70986270904541 
model_pd.lambdas: dict_items([('pout', tensor([1.2267], device='cuda:0')), ('power', tensor([0.8445], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4417], device='cuda:0')), ('power', tensor([-20.8279], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.13403752446174622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9387, 4.9387, 4.9387],
        [4.9387, 4.9390, 4.9387],
        [4.9387, 4.9403, 4.9388],
        [4.9387, 4.9922, 4.9533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.13391439616680145 
model_pd.l_d.mean(): -15.820096969604492 
model_pd.lagr.mean(): -15.686182975769043 
model_pd.lambdas: dict_items([('pout', tensor([1.2282], device='cuda:0')), ('power', tensor([0.8434], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4413], device='cuda:0')), ('power', tensor([-20.8272], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.13391439616680145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9406, 5.4138, 5.4280],
        [4.9406, 5.6549, 5.8423],
        [4.9406, 4.9407, 4.9406],
        [4.9406, 5.8172, 6.1524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13378417491912842 
model_pd.l_d.mean(): -15.796285629272461 
model_pd.lagr.mean(): -15.662501335144043 
model_pd.lambdas: dict_items([('pout', tensor([1.2296], device='cuda:0')), ('power', tensor([0.8424], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4408], device='cuda:0')), ('power', tensor([-20.8264], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13378417491912842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9427, 4.9430, 4.9427],
        [4.9427, 4.9433, 4.9427],
        [4.9427, 4.9715, 4.9480],
        [4.9427, 5.0661, 4.9998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13364629447460175 
model_pd.l_d.mean(): -15.772470474243164 
model_pd.lagr.mean(): -15.638824462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.2310], device='cuda:0')), ('power', tensor([0.8414], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4403], device='cuda:0')), ('power', tensor([-20.8256], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.13364629447460175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9449, 6.4197, 7.4441],
        [4.9449, 5.4179, 5.4320],
        [4.9449, 5.3888, 5.3869],
        [4.9449, 5.1768, 5.1038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.13350063562393188 
model_pd.l_d.mean(): -15.748648643493652 
model_pd.lagr.mean(): -15.615147590637207 
model_pd.lambdas: dict_items([('pout', tensor([1.2325], device='cuda:0')), ('power', tensor([0.8403], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4397], device='cuda:0')), ('power', tensor([-20.8247], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.13350063562393188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9471, 4.9471, 4.9471],
        [4.9471, 4.9471, 4.9471],
        [4.9471, 5.6228, 5.7781],
        [4.9471, 5.0017, 4.9622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.13334809243679047 
model_pd.l_d.mean(): -15.724825859069824 
model_pd.lagr.mean(): -15.591477394104004 
model_pd.lambdas: dict_items([('pout', tensor([1.2339], device='cuda:0')), ('power', tensor([0.8393], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4391], device='cuda:0')), ('power', tensor([-20.8238], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.13334809243679047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9495, 6.0949, 6.7143],
        [4.9495, 5.0918, 5.0216],
        [4.9495, 4.9495, 4.9495],
        [4.9495, 6.3203, 7.2103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.133189857006073 
model_pd.l_d.mean(): -15.701001167297363 
model_pd.lagr.mean(): -15.567811012268066 
model_pd.lambdas: dict_items([('pout', tensor([1.2354], device='cuda:0')), ('power', tensor([0.8382], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4385], device='cuda:0')), ('power', tensor([-20.8228], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.133189857006073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9519, 5.8298, 6.1656],
        [4.9519, 5.0752, 5.0089],
        [4.9519, 5.4561, 5.4886],
        [4.9519, 5.0073, 4.9673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.13302788138389587 
model_pd.l_d.mean(): -15.67717170715332 
model_pd.lagr.mean(): -15.544143676757812 
model_pd.lambdas: dict_items([('pout', tensor([1.2368], device='cuda:0')), ('power', tensor([0.8372], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4379], device='cuda:0')), ('power', tensor([-20.8218], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.13302788138389587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9544, 5.4125, 5.4180],
        [4.9544, 5.5840, 5.7029],
        [4.9544, 5.0966, 5.0264],
        [4.9544, 4.9544, 4.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.132863849401474 
model_pd.l_d.mean(): -15.653347969055176 
model_pd.lagr.mean(): -15.52048397064209 
model_pd.lambdas: dict_items([('pout', tensor([1.2382], device='cuda:0')), ('power', tensor([0.8362], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4373], device='cuda:0')), ('power', tensor([-20.8208], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.132863849401474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9569, 4.9569, 4.9569],
        [4.9569, 5.8278, 6.1565],
        [4.9569, 6.1029, 6.7223],
        [4.9569, 5.0991, 5.0288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.13269974291324615 
model_pd.l_d.mean(): -15.629524230957031 
model_pd.lagr.mean(): -15.496824264526367 
model_pd.lambdas: dict_items([('pout', tensor([1.2397], device='cuda:0')), ('power', tensor([0.8351], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4367], device='cuda:0')), ('power', tensor([-20.8198], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.13269974291324615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9593, 4.9594, 4.9593],
        [4.9593, 5.0627, 5.0021],
        [4.9593, 5.0825, 5.0163],
        [4.9593, 5.0146, 4.9747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.13253723084926605 
model_pd.l_d.mean(): -15.605706214904785 
model_pd.lagr.mean(): -15.473169326782227 
model_pd.lambdas: dict_items([('pout', tensor([1.2411], device='cuda:0')), ('power', tensor([0.8341], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4361], device='cuda:0')), ('power', tensor([-20.8188], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.13253723084926605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9617, 4.9623, 4.9617],
        [4.9617, 5.3361, 5.3011],
        [4.9617, 5.5914, 5.7101],
        [4.9617, 4.9617, 4.9617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.1323777288198471 
model_pd.l_d.mean(): -15.581897735595703 
model_pd.lagr.mean(): -15.449520111083984 
model_pd.lambdas: dict_items([('pout', tensor([1.2426], device='cuda:0')), ('power', tensor([0.8330], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4354], device='cuda:0')), ('power', tensor([-20.8178], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.1323777288198471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 5.4222, 5.4275],
        [4.9641, 4.9657, 4.9642],
        [4.9641, 5.1153, 5.0436],
        [4.9641, 6.1338, 6.7795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.13222211599349976 
model_pd.l_d.mean(): -15.55809211730957 
model_pd.lagr.mean(): -15.425869941711426 
model_pd.lambdas: dict_items([('pout', tensor([1.2440], device='cuda:0')), ('power', tensor([0.8320], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4348], device='cuda:0')), ('power', tensor([-20.8168], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.13222211599349976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9664, 6.3395, 7.2299],
        [4.9664, 5.0895, 5.0233],
        [4.9664, 4.9664, 4.9664],
        [4.9664, 4.9668, 4.9664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.1320708841085434 
model_pd.l_d.mean(): -15.534297943115234 
model_pd.lagr.mean(): -15.402227401733398 
model_pd.lambdas: dict_items([('pout', tensor([1.2454], device='cuda:0')), ('power', tensor([0.8309], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4343], device='cuda:0')), ('power', tensor([-20.8158], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.1320708841085434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9687, 5.4416, 5.4551],
        [4.9687, 5.1476, 5.0732],
        [4.9687, 4.9688, 4.9687],
        [4.9687, 4.9974, 4.9740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.13192397356033325 
model_pd.l_d.mean(): -15.510507583618164 
model_pd.lagr.mean(): -15.378583908081055 
model_pd.lambdas: dict_items([('pout', tensor([1.2469], device='cuda:0')), ('power', tensor([0.8299], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4337], device='cuda:0')), ('power', tensor([-20.8149], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.13192397356033325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9709, 4.9725, 4.9710],
        [4.9709, 4.9754, 4.9712],
        [4.9709, 5.2578, 5.1941],
        [4.9709, 4.9996, 4.9762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.13178083300590515 
model_pd.l_d.mean(): -15.486724853515625 
model_pd.lagr.mean(): -15.354944229125977 
model_pd.lambdas: dict_items([('pout', tensor([1.2483], device='cuda:0')), ('power', tensor([0.8289], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4331], device='cuda:0')), ('power', tensor([-20.8140], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.13178083300590515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9731, 5.0264, 4.9876],
        [4.9731, 5.2741, 5.2139],
        [4.9731, 5.2046, 5.1315],
        [4.9731, 5.3875, 5.3702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.13164068758487701 
model_pd.l_d.mean(): -15.462950706481934 
model_pd.lagr.mean(): -15.331310272216797 
model_pd.lambdas: dict_items([('pout', tensor([1.2497], device='cuda:0')), ('power', tensor([0.8278], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4326], device='cuda:0')), ('power', tensor([-20.8131], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.13164068758487701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9753, 6.3494, 7.2399],
        [4.9753, 5.8541, 6.1892],
        [4.9753, 4.9756, 4.9753],
        [4.9753, 6.4547, 7.4802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13150237500667572 
model_pd.l_d.mean(): -15.43918228149414 
model_pd.lagr.mean(): -15.307680130004883 
model_pd.lambdas: dict_items([('pout', tensor([1.2512], device='cuda:0')), ('power', tensor([0.8268], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4320], device='cuda:0')), ('power', tensor([-20.8122], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13150237500667572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9775, 5.8563, 6.1914],
        [4.9775, 4.9778, 4.9775],
        [4.9775, 4.9774, 4.9774],
        [4.9775, 4.9774, 4.9774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.13136467337608337 
model_pd.l_d.mean(): -15.415414810180664 
model_pd.lagr.mean(): -15.284049987792969 
model_pd.lambdas: dict_items([('pout', tensor([1.2526], device='cuda:0')), ('power', tensor([0.8257], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4315], device='cuda:0')), ('power', tensor([-20.8113], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.13136467337608337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9796, 4.9877, 4.9803],
        [4.9796, 5.0119, 4.9860],
        [4.9796, 6.1275, 6.7466],
        [4.9796, 4.9796, 4.9796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13122640550136566 
model_pd.l_d.mean(): -15.391651153564453 
model_pd.lagr.mean(): -15.260424613952637 
model_pd.lambdas: dict_items([('pout', tensor([1.2540], device='cuda:0')), ('power', tensor([0.8247], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4309], device='cuda:0')), ('power', tensor([-20.8104], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13122640550136566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9818, 5.2685, 5.2047],
        [4.9818, 5.0141, 4.9883],
        [4.9818, 4.9834, 4.9819],
        [4.9818, 5.4254, 5.4226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.13108661770820618 
model_pd.l_d.mean(): -15.367890357971191 
model_pd.lagr.mean(): -15.236804008483887 
model_pd.lambdas: dict_items([('pout', tensor([1.2554], device='cuda:0')), ('power', tensor([0.8237], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4304], device='cuda:0')), ('power', tensor([-20.8095], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.13108661770820618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9841, 5.0389, 4.9992],
        [4.9841, 5.6601, 5.8142],
        [4.9841, 4.9841, 4.9841],
        [4.9841, 5.0372, 4.9985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13094601035118103 
model_pd.l_d.mean(): -15.344131469726562 
model_pd.lagr.mean(): -15.21318531036377 
model_pd.lambdas: dict_items([('pout', tensor([1.2569], device='cuda:0')), ('power', tensor([0.8226], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4298], device='cuda:0')), ('power', tensor([-20.8085], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13094601035118103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9863, 5.1871, 5.1123],
        [4.9863, 5.0150, 4.9916],
        [4.9863, 5.0414, 5.0016],
        [4.9863, 5.7886, 6.0511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.13080523908138275 
model_pd.l_d.mean(): -15.320374488830566 
model_pd.lagr.mean(): -15.189569473266602 
model_pd.lambdas: dict_items([('pout', tensor([1.2583], device='cuda:0')), ('power', tensor([0.8216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4293], device='cuda:0')), ('power', tensor([-20.8076], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.13080523908138275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9886, 5.2893, 5.2289],
        [4.9886, 5.1671, 5.0928],
        [4.9886, 5.2751, 5.2112],
        [4.9886, 4.9947, 4.9890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.1306643933057785 
model_pd.l_d.mean(): -15.296618461608887 
model_pd.lagr.mean(): -15.165953636169434 
model_pd.lambdas: dict_items([('pout', tensor([1.2597], device='cuda:0')), ('power', tensor([0.8205], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4287], device='cuda:0')), ('power', tensor([-20.8067], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.1306643933057785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9908, 5.0456, 5.0060],
        [4.9908, 4.9908, 4.9908],
        [4.9908, 5.2650, 5.1983],
        [4.9908, 5.7058, 5.8915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.13052354753017426 
model_pd.l_d.mean(): -15.272867202758789 
model_pd.lagr.mean(): -15.142343521118164 
model_pd.lambdas: dict_items([('pout', tensor([1.2612], device='cuda:0')), ('power', tensor([0.8195], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4281], device='cuda:0')), ('power', tensor([-20.8057], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.13052354753017426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9931, 4.9934, 4.9931],
        [4.9931, 5.0481, 5.0084],
        [4.9931, 4.9992, 4.9935],
        [4.9931, 5.0011, 4.9938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.1303824782371521 
model_pd.l_d.mean(): -15.24911880493164 
model_pd.lagr.mean(): -15.118736267089844 
model_pd.lambdas: dict_items([('pout', tensor([1.2626], device='cuda:0')), ('power', tensor([0.8185], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4276], device='cuda:0')), ('power', tensor([-20.8048], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.1303824782371521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9954, 4.9969, 4.9954],
        [4.9954, 5.2960, 5.2356],
        [4.9954, 6.3719, 7.2629],
        [4.9954, 6.4777, 7.5038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.13024106621742249 
model_pd.l_d.mean(): -15.225374221801758 
model_pd.lagr.mean(): -15.095132827758789 
model_pd.lambdas: dict_items([('pout', tensor([1.2640], device='cuda:0')), ('power', tensor([0.8174], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4270], device='cuda:0')), ('power', tensor([-20.8038], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.13024106621742249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9977, 5.4411, 5.4380],
        [4.9977, 4.9977, 4.9977],
        [4.9977, 5.0527, 5.0129],
        [4.9977, 5.2841, 5.2201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.13009847700595856 
model_pd.l_d.mean(): -15.20163345336914 
model_pd.lagr.mean(): -15.071535110473633 
model_pd.lambdas: dict_items([('pout', tensor([1.2654], device='cuda:0')), ('power', tensor([0.8164], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4264], device='cuda:0')), ('power', tensor([-20.8028], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.13009847700595856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0000, 5.0015, 5.0000],
        [5.0000, 5.6763, 5.8299],
        [5.0000, 5.8788, 6.2122],
        [5.0000, 5.0001, 5.0000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.1299545019865036 
model_pd.l_d.mean(): -15.177889823913574 
model_pd.lagr.mean(): -15.047935485839844 
model_pd.lambdas: dict_items([('pout', tensor([1.2669], device='cuda:0')), ('power', tensor([0.8153], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4258], device='cuda:0')), ('power', tensor([-20.8018], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.1299545019865036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0024, 5.2334, 5.1602],
        [5.0024, 5.8052, 6.0672],
        [5.0024, 5.4601, 5.4644],
        [5.0024, 5.0024, 5.0024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.12980864942073822 
model_pd.l_d.mean(): -15.154154777526855 
model_pd.lagr.mean(): -15.024346351623535 
model_pd.lambdas: dict_items([('pout', tensor([1.2683], device='cuda:0')), ('power', tensor([0.8143], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4252], device='cuda:0')), ('power', tensor([-20.8008], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.12980864942073822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0048, 5.0595, 5.0199],
        [5.0048, 5.5088, 5.5398],
        [5.0048, 5.0109, 5.0052],
        [5.0048, 5.4774, 5.4900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.1296607404947281 
model_pd.l_d.mean(): -15.130415916442871 
model_pd.lagr.mean(): -15.000755310058594 
model_pd.lambdas: dict_items([('pout', tensor([1.2697], device='cuda:0')), ('power', tensor([0.8133], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4246], device='cuda:0')), ('power', tensor([-20.7998], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.1296607404947281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0072, 5.5112, 5.5422],
        [5.0072, 6.4914, 7.5182],
        [5.0072, 5.0072, 5.0072],
        [5.0072, 5.0072, 5.0072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.12951064109802246 
model_pd.l_d.mean(): -15.106683731079102 
model_pd.lagr.mean(): -14.9771728515625 
model_pd.lambdas: dict_items([('pout', tensor([1.2711], device='cuda:0')), ('power', tensor([0.8122], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4240], device='cuda:0')), ('power', tensor([-20.7987], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.12951064109802246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0097, 5.2102, 5.1353],
        [5.0097, 5.2407, 5.1674],
        [5.0097, 5.0177, 5.0104],
        [5.0097, 5.0097, 5.0097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.1293584257364273 
model_pd.l_d.mean(): -15.082951545715332 
model_pd.lagr.mean(): -14.953593254089355 
model_pd.lambdas: dict_items([('pout', tensor([1.2726], device='cuda:0')), ('power', tensor([0.8112], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4234], device='cuda:0')), ('power', tensor([-20.7977], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.1293584257364273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0122, 5.1906, 5.1161],
        [5.0122, 5.3861, 5.3502],
        [5.0122, 5.0408, 5.0175],
        [5.0122, 5.6888, 5.8421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.12920436263084412 
model_pd.l_d.mean(): -15.059220314025879 
model_pd.lagr.mean(): -14.930015563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.2740], device='cuda:0')), ('power', tensor([0.8101], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4228], device='cuda:0')), ('power', tensor([-20.7966], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.12920436263084412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0148, 5.0433, 5.0201],
        [5.0148, 5.1374, 5.0713],
        [5.0148, 5.1177, 5.0572],
        [5.0148, 5.3887, 5.3527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.129048690199852 
model_pd.l_d.mean(): -15.035491943359375 
model_pd.lagr.mean(): -14.90644359588623 
model_pd.lambdas: dict_items([('pout', tensor([1.2754], device='cuda:0')), ('power', tensor([0.8091], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4221], device='cuda:0')), ('power', tensor([-20.7955], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.129048690199852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0174, 5.0254, 5.0181],
        [5.0174, 5.0175, 5.0174],
        [5.0174, 5.0174, 5.0174],
        [5.0174, 5.1589, 5.0888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12889181077480316 
model_pd.l_d.mean(): -15.01176643371582 
model_pd.lagr.mean(): -14.882874488830566 
model_pd.lambdas: dict_items([('pout', tensor([1.2768], device='cuda:0')), ('power', tensor([0.8081], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4215], device='cuda:0')), ('power', tensor([-20.7943], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12889181077480316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0200, 5.3939, 5.3578],
        [5.0200, 5.4928, 5.5051],
        [5.0200, 5.0746, 5.0351],
        [5.0200, 5.0261, 5.0204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.12873411178588867 
model_pd.l_d.mean(): -14.988045692443848 
model_pd.lagr.mean(): -14.859312057495117 
model_pd.lambdas: dict_items([('pout', tensor([1.2783], device='cuda:0')), ('power', tensor([0.8070], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4208], device='cuda:0')), ('power', tensor([-20.7932], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.12873411178588867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0226, 5.0230, 5.0226],
        [5.0226, 6.1746, 6.7938],
        [5.0226, 5.2231, 5.1481],
        [5.0226, 5.3231, 5.2623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12857604026794434 
model_pd.l_d.mean(): -14.964326858520508 
model_pd.lagr.mean(): -14.835750579833984 
model_pd.lambdas: dict_items([('pout', tensor([1.2797], device='cuda:0')), ('power', tensor([0.8060], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4202], device='cuda:0')), ('power', tensor([-20.7921], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12857604026794434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0253, 5.7413, 5.9261],
        [5.0253, 5.0802, 5.0405],
        [5.0253, 5.2992, 5.2322],
        [5.0253, 5.3116, 5.2473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.1284179985523224 
model_pd.l_d.mean(): -14.94061279296875 
model_pd.lagr.mean(): -14.81219482421875 
model_pd.lambdas: dict_items([('pout', tensor([1.2811], device='cuda:0')), ('power', tensor([0.8049], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4195], device='cuda:0')), ('power', tensor([-20.7909], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.1284179985523224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0279, 5.0282, 5.0279],
        [5.0279, 5.9020, 6.2290],
        [5.0279, 5.4018, 5.3656],
        [5.0279, 5.4715, 5.4677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.12826016545295715 
model_pd.l_d.mean(): -14.916902542114258 
model_pd.lagr.mean(): -14.788641929626465 
model_pd.lambdas: dict_items([('pout', tensor([1.2825], device='cuda:0')), ('power', tensor([0.8039], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4188], device='cuda:0')), ('power', tensor([-20.7897], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.12826016545295715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0306, 5.0306, 5.0306],
        [5.0306, 5.7078, 5.8606],
        [5.0306, 5.0854, 5.0458],
        [5.0306, 5.5349, 5.5653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.12810276448726654 
model_pd.l_d.mean(): -14.89319896697998 
model_pd.lagr.mean(): -14.765096664428711 
model_pd.lambdas: dict_items([('pout', tensor([1.2839], device='cuda:0')), ('power', tensor([0.8029], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4182], device='cuda:0')), ('power', tensor([-20.7886], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.12810276448726654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0332, 6.4154, 7.3080],
        [5.0332, 5.0335, 5.0332],
        [5.0332, 5.0881, 5.0484],
        [5.0332, 5.0617, 5.0385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.1279458850622177 
model_pd.l_d.mean(): -14.869499206542969 
model_pd.lagr.mean(): -14.74155330657959 
model_pd.lambdas: dict_items([('pout', tensor([1.2853], device='cuda:0')), ('power', tensor([0.8018], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4175], device='cuda:0')), ('power', tensor([-20.7874], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.1279458850622177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0359, 5.6666, 5.7832],
        [5.0359, 5.0680, 5.0422],
        [5.0359, 5.8403, 6.1017],
        [5.0359, 5.9168, 6.2497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.12778951227664948 
model_pd.l_d.mean(): -14.845805168151855 
model_pd.lagr.mean(): -14.718015670776367 
model_pd.lambdas: dict_items([('pout', tensor([1.2868], device='cuda:0')), ('power', tensor([0.8008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4168], device='cuda:0')), ('power', tensor([-20.7863], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.12778951227664948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0386, 5.3390, 5.2780],
        [5.0386, 5.0386, 5.0386],
        [5.0386, 5.0447, 5.0390],
        [5.0386, 5.4965, 5.5001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.12763343751430511 
model_pd.l_d.mean(): -14.822113990783691 
model_pd.lagr.mean(): -14.694480895996094 
model_pd.lambdas: dict_items([('pout', tensor([1.2882], device='cuda:0')), ('power', tensor([0.7997], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4162], device='cuda:0')), ('power', tensor([-20.7851], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.12763343751430511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0412, 5.2416, 5.1665],
        [5.0412, 5.0412, 5.0412],
        [5.0412, 5.0415, 5.0412],
        [5.0412, 5.0958, 5.0563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.1274775117635727 
model_pd.l_d.mean(): -14.798432350158691 
model_pd.lagr.mean(): -14.670954704284668 
model_pd.lambdas: dict_items([('pout', tensor([1.2896], device='cuda:0')), ('power', tensor([0.7987], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4155], device='cuda:0')), ('power', tensor([-20.7839], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.1274775117635727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0439, 5.0441, 5.0439],
        [5.0439, 5.0979, 5.0587],
        [5.0439, 5.2220, 5.1475],
        [5.0439, 5.0439, 5.0439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.1273214966058731 
model_pd.l_d.mean(): -14.774747848510742 
model_pd.lagr.mean(): -14.64742660522461 
model_pd.lambdas: dict_items([('pout', tensor([1.2910], device='cuda:0')), ('power', tensor([0.7977], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4148], device='cuda:0')), ('power', tensor([-20.7827], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.1273214966058731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0466, 5.4609, 5.4421],
        [5.0466, 5.0472, 5.0466],
        [5.0466, 5.1690, 5.1029],
        [5.0466, 5.5511, 5.5812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.12716521322727203 
model_pd.l_d.mean(): -14.751070022583008 
model_pd.lagr.mean(): -14.623905181884766 
model_pd.lambdas: dict_items([('pout', tensor([1.2924], device='cuda:0')), ('power', tensor([0.7966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4141], device='cuda:0')), ('power', tensor([-20.7815], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.12716521322727203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0493, 6.2041, 6.8238],
        [5.0493, 5.1520, 5.0916],
        [5.0493, 5.0494, 5.0493],
        [5.0493, 5.0497, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.12700840830802917 
model_pd.l_d.mean(): -14.727397918701172 
model_pd.lagr.mean(): -14.60038948059082 
model_pd.lambdas: dict_items([('pout', tensor([1.2938], device='cuda:0')), ('power', tensor([0.7956], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4135], device='cuda:0')), ('power', tensor([-20.7803], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.12700840830802917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0521, 5.1061, 5.0669],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 6.2854, 6.9951],
        [5.0521, 5.4663, 5.4474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.1268509030342102 
model_pd.l_d.mean(): -14.703726768493652 
model_pd.lagr.mean(): -14.576875686645508 
model_pd.lambdas: dict_items([('pout', tensor([1.2952], device='cuda:0')), ('power', tensor([0.7945], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4128], device='cuda:0')), ('power', tensor([-20.7791], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.1268509030342102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0548, 5.1093, 5.0699],
        [5.0548, 6.4403, 7.3339],
        [5.0548, 6.2885, 6.9983],
        [5.0548, 5.9369, 6.2696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.12669259309768677 
model_pd.l_d.mean(): -14.680062294006348 
model_pd.lagr.mean(): -14.553369522094727 
model_pd.lambdas: dict_items([('pout', tensor([1.2967], device='cuda:0')), ('power', tensor([0.7935], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4121], device='cuda:0')), ('power', tensor([-20.7779], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.12669259309768677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]], device='cuda:0')
 pt:tensor([[5.0576, 6.4436, 7.3373],
        [5.0576, 5.3580, 5.2968],
        [5.0576, 6.2134, 6.8332],
        [5.0576, 5.9335, 6.2601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.12653346359729767 
model_pd.l_d.mean(): -14.656400680541992 
model_pd.lagr.mean(): -14.529867172241211 
model_pd.lambdas: dict_items([('pout', tensor([1.2981], device='cuda:0')), ('power', tensor([0.7925], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4114], device='cuda:0')), ('power', tensor([-20.7767], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.12653346359729767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0604, 5.9438, 6.2774],
        [5.0604, 5.1152, 5.0756],
        [5.0604, 5.0607, 5.0604],
        [5.0604, 5.9428, 6.2755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.12637357413768768 
model_pd.l_d.mean(): -14.632740020751953 
model_pd.lagr.mean(): -14.506366729736328 
model_pd.lambdas: dict_items([('pout', tensor([1.2995], device='cuda:0')), ('power', tensor([0.7914], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4107], device='cuda:0')), ('power', tensor([-20.7754], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.12637357413768768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]], device='cuda:0')
 pt:tensor([[5.0632, 6.5573, 7.5878],
        [5.0632, 5.9395, 6.2660],
        [5.0632, 5.9458, 6.2784],
        [5.0632, 5.1855, 5.1194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.12621568143367767 
model_pd.l_d.mean(): -14.609085083007812 
model_pd.lagr.mean(): -14.482869148254395 
model_pd.lambdas: dict_items([('pout', tensor([1.3009], device='cuda:0')), ('power', tensor([0.7904], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4100], device='cuda:0')), ('power', tensor([-20.7742], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.12621568143367767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0660, 5.1199, 5.0807],
        [5.0660, 5.2162, 5.1445],
        [5.0660, 5.0659, 5.0660],
        [5.0660, 5.3397, 5.2723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.12606143951416016 
model_pd.l_d.mean(): -14.585440635681152 
model_pd.lagr.mean(): -14.459379196166992 
model_pd.lambdas: dict_items([('pout', tensor([1.3023], device='cuda:0')), ('power', tensor([0.7894], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4093], device='cuda:0')), ('power', tensor([-20.7729], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.12606143951416016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0686, 5.5734, 5.6030],
        [5.0686, 5.0686, 5.0686],
        [5.0686, 5.0766, 5.0693],
        [5.0686, 5.1006, 5.0750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.12591144442558289 
model_pd.l_d.mean(): -14.56180477142334 
model_pd.lagr.mean(): -14.435893058776855 
model_pd.lambdas: dict_items([('pout', tensor([1.3037], device='cuda:0')), ('power', tensor([0.7883], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4087], device='cuda:0')), ('power', tensor([-20.7718], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.12591144442558289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0712, 5.1240, 5.0855],
        [5.0712, 5.0718, 5.0713],
        [5.0712, 5.0997, 5.0765],
        [5.0712, 5.8774, 6.1382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.1257653832435608 
model_pd.l_d.mean(): -14.538176536560059 
model_pd.lagr.mean(): -14.412410736083984 
model_pd.lambdas: dict_items([('pout', tensor([1.3051], device='cuda:0')), ('power', tensor([0.7873], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4080], device='cuda:0')), ('power', tensor([-20.7706], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.1257653832435608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0738, 5.9580, 6.2914],
        [5.0738, 5.3600, 5.2951],
        [5.0738, 5.0818, 5.0745],
        [5.0738, 5.0738, 5.0738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.1256222277879715 
model_pd.l_d.mean(): -14.51455020904541 
model_pd.lagr.mean(): -14.388928413391113 
model_pd.lambdas: dict_items([('pout', tensor([1.3065], device='cuda:0')), ('power', tensor([0.7862], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4074], device='cuda:0')), ('power', tensor([-20.7694], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.1256222277879715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]], device='cuda:0')
 pt:tensor([[5.0764, 5.7550, 5.9068],
        [5.0764, 5.2265, 5.1548],
        [5.0764, 5.9597, 6.2921],
        [5.0764, 5.3501, 5.2826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.125480517745018 
model_pd.l_d.mean(): -14.490937232971191 
model_pd.lagr.mean(): -14.365456581115723 
model_pd.lambdas: dict_items([('pout', tensor([1.3079], device='cuda:0')), ('power', tensor([0.7852], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4067], device='cuda:0')), ('power', tensor([-20.7683], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.125480517745018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0790, 5.0790, 5.0790],
        [5.0790, 5.0790, 5.0790],
        [5.0790, 5.5229, 5.5180],
        [5.0790, 5.3095, 5.2358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.12533840537071228 
model_pd.l_d.mean(): -14.467324256896973 
model_pd.lagr.mean(): -14.341985702514648 
model_pd.lambdas: dict_items([('pout', tensor([1.3093], device='cuda:0')), ('power', tensor([0.7842], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4061], device='cuda:0')), ('power', tensor([-20.7672], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.12533840537071228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0816, 5.0816, 5.0816],
        [5.0816, 5.9589, 6.2851],
        [5.0816, 6.2634, 6.9102],
        [5.0816, 5.5398, 5.5425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.12519417703151703 
model_pd.l_d.mean(): -14.443714141845703 
model_pd.lagr.mean(): -14.318519592285156 
model_pd.lambdas: dict_items([('pout', tensor([1.3107], device='cuda:0')), ('power', tensor([0.7831], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4054], device='cuda:0')), ('power', tensor([-20.7660], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.12519417703151703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0843, 5.0887, 5.0845],
        [5.0843, 5.3148, 5.2410],
        [5.0843, 5.0858, 5.0843],
        [5.0843, 5.8024, 5.9860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.125046506524086 
model_pd.l_d.mean(): -14.420106887817383 
model_pd.lagr.mean(): -14.295060157775879 
model_pd.lambdas: dict_items([('pout', tensor([1.3121], device='cuda:0')), ('power', tensor([0.7821], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4048], device='cuda:0')), ('power', tensor([-20.7648], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.125046506524086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0870, 5.0873, 5.0871],
        [5.0870, 5.2649, 5.1902],
        [5.0870, 5.7191, 5.8344],
        [5.0870, 5.0871, 5.0870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.12489421665668488 
model_pd.l_d.mean(): -14.396496772766113 
model_pd.lagr.mean(): -14.271602630615234 
model_pd.lambdas: dict_items([('pout', tensor([1.3136], device='cuda:0')), ('power', tensor([0.7810], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4041], device='cuda:0')), ('power', tensor([-20.7635], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.12489421665668488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0899, 5.1065, 5.0921],
        [5.0899, 5.0899, 5.0899],
        [5.0899, 5.0979, 5.0906],
        [5.0899, 6.2727, 6.9197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.12473676353693008 
model_pd.l_d.mean(): -14.372888565063477 
model_pd.lagr.mean(): -14.248151779174805 
model_pd.lambdas: dict_items([('pout', tensor([1.3150], device='cuda:0')), ('power', tensor([0.7800], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4034], device='cuda:0')), ('power', tensor([-20.7622], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.12473676353693008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0929, 5.5074, 5.4877],
        [5.0929, 5.0944, 5.0930],
        [5.0929, 5.0929, 5.0929],
        [5.0929, 5.0929, 5.0929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.12457409501075745 
model_pd.l_d.mean(): -14.349274635314941 
model_pd.lagr.mean(): -14.224700927734375 
model_pd.lambdas: dict_items([('pout', tensor([1.3164], device='cuda:0')), ('power', tensor([0.7790], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4026], device='cuda:0')), ('power', tensor([-20.7608], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.12457409501075745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0960, 6.2795, 6.9267],
        [5.0960, 5.1243, 5.1012],
        [5.0960, 5.0960, 5.0960],
        [5.0960, 5.2181, 5.1520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.12440679222345352 
model_pd.l_d.mean(): -14.325667381286621 
model_pd.lagr.mean(): -14.201260566711426 
model_pd.lambdas: dict_items([('pout', tensor([1.3178], device='cuda:0')), ('power', tensor([0.7779], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4019], device='cuda:0')), ('power', tensor([-20.7594], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.12440679222345352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0991, 5.1529, 5.1138],
        [5.0991, 5.9777, 6.3038],
        [5.0991, 5.1052, 5.0996],
        [5.0991, 5.0991, 5.0991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12423568218946457 
model_pd.l_d.mean(): -14.302057266235352 
model_pd.lagr.mean(): -14.177821159362793 
model_pd.lambdas: dict_items([('pout', tensor([1.3192], device='cuda:0')), ('power', tensor([0.7769], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4011], device='cuda:0')), ('power', tensor([-20.7580], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12423568218946457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1023, 5.4764, 5.4390],
        [5.1023, 5.1561, 5.1170],
        [5.1023, 6.2868, 6.9342],
        [5.1023, 5.5609, 5.5632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.12406197935342789 
model_pd.l_d.mean(): -14.278451919555664 
model_pd.lagr.mean(): -14.154390335083008 
model_pd.lambdas: dict_items([('pout', tensor([1.3206], device='cuda:0')), ('power', tensor([0.7759], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.4003], device='cuda:0')), ('power', tensor([-20.7565], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.12406197935342789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1056, 5.7384, 5.8534],
        [5.1056, 5.1056, 5.1055],
        [5.1056, 5.9910, 6.3232],
        [5.1056, 5.1599, 5.1205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.12388698011636734 
model_pd.l_d.mean(): -14.25484848022461 
model_pd.lagr.mean(): -14.130961418151855 
model_pd.lambdas: dict_items([('pout', tensor([1.3220], device='cuda:0')), ('power', tensor([0.7748], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3995], device='cuda:0')), ('power', tensor([-20.7550], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.12388698011636734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1088, 5.1149, 5.1092],
        [5.1088, 5.1094, 5.1088],
        [5.1088, 5.1088, 5.1088],
        [5.1088, 5.4830, 5.4455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.12371186912059784 
model_pd.l_d.mean(): -14.231247901916504 
model_pd.lagr.mean(): -14.107536315917969 
model_pd.lambdas: dict_items([('pout', tensor([1.3234], device='cuda:0')), ('power', tensor([0.7738], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3987], device='cuda:0')), ('power', tensor([-20.7535], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.12371186912059784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1120, 5.1123, 5.1121],
        [5.1120, 5.9917, 6.3179],
        [5.1120, 5.1440, 5.1183],
        [5.1120, 5.8317, 6.0149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.12353774160146713 
model_pd.l_d.mean(): -14.207656860351562 
model_pd.lagr.mean(): -14.084118843078613 
model_pd.lambdas: dict_items([('pout', tensor([1.3248], device='cuda:0')), ('power', tensor([0.7727], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3979], device='cuda:0')), ('power', tensor([-20.7520], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.12353774160146713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1153, 5.7486, 5.8635],
        [5.1153, 5.1153, 5.1153],
        [5.1153, 5.1318, 5.1175],
        [5.1153, 5.7957, 5.9468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12336540222167969 
model_pd.l_d.mean(): -14.184067726135254 
model_pd.lagr.mean(): -14.060702323913574 
model_pd.lambdas: dict_items([('pout', tensor([1.3262], device='cuda:0')), ('power', tensor([0.7717], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3971], device='cuda:0')), ('power', tensor([-20.7505], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12336540222167969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1185, 5.1187, 5.1185],
        [5.1185, 5.5775, 5.5795],
        [5.1185, 5.1185, 5.1185],
        [5.1185, 5.7990, 5.9502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.12319514900445938 
model_pd.l_d.mean(): -14.160489082336426 
model_pd.lagr.mean(): -14.037294387817383 
model_pd.lambdas: dict_items([('pout', tensor([1.3276], device='cuda:0')), ('power', tensor([0.7707], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3963], device='cuda:0')), ('power', tensor([-20.7490], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.12319514900445938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1216, 5.5664, 5.5608],
        [5.1216, 5.4961, 5.4584],
        [5.1216, 5.3955, 5.3275],
        [5.1216, 5.1216, 5.1216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.12302741408348083 
model_pd.l_d.mean(): -14.136916160583496 
model_pd.lagr.mean(): -14.013888359069824 
model_pd.lambdas: dict_items([('pout', tensor([1.3289], device='cuda:0')), ('power', tensor([0.7696], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3955], device='cuda:0')), ('power', tensor([-20.7475], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.12302741408348083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1248, 6.3125, 6.9608],
        [5.1248, 5.3987, 5.3306],
        [5.1248, 5.1794, 5.1398],
        [5.1248, 5.2469, 5.1807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.12286179512739182 
model_pd.l_d.mean(): -14.113350868225098 
model_pd.lagr.mean(): -13.99048900604248 
model_pd.lambdas: dict_items([('pout', tensor([1.3303], device='cuda:0')), ('power', tensor([0.7686], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3947], device='cuda:0')), ('power', tensor([-20.7460], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.12286179512739182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[5.1279, 5.8090, 5.9600],
        [5.1279, 5.3281, 5.2525],
        [5.1279, 6.0090, 6.3352],
        [5.1279, 6.0163, 6.3496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.12269780784845352 
model_pd.l_d.mean(): -14.089791297912598 
model_pd.lagr.mean(): -13.967093467712402 
model_pd.lambdas: dict_items([('pout', tensor([1.3317], device='cuda:0')), ('power', tensor([0.7676], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3939], device='cuda:0')), ('power', tensor([-20.7446], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.12269780784845352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1310, 5.2334, 5.1730],
        [5.1310, 5.1310, 5.1310],
        [5.1310, 5.1389, 5.1317],
        [5.1310, 6.3752, 7.0878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.1225348636507988 
model_pd.l_d.mean(): -14.06623649597168 
model_pd.lagr.mean(): -13.94370174407959 
model_pd.lambdas: dict_items([('pout', tensor([1.3331], device='cuda:0')), ('power', tensor([0.7665], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3932], device='cuda:0')), ('power', tensor([-20.7431], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.1225348636507988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1341, 5.1879, 5.1488],
        [5.1341, 5.1625, 5.1393],
        [5.1341, 5.1884, 5.1491],
        [5.1341, 6.5331, 7.4317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12237229943275452 
model_pd.l_d.mean(): -14.042686462402344 
model_pd.lagr.mean(): -13.920313835144043 
model_pd.lambdas: dict_items([('pout', tensor([1.3345], device='cuda:0')), ('power', tensor([0.7655], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3924], device='cuda:0')), ('power', tensor([-20.7417], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12237229943275452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1373, 5.1692, 5.1436],
        [5.1373, 5.5120, 5.4740],
        [5.1373, 6.0191, 6.3454],
        [5.1373, 5.4380, 5.3758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.12220937758684158 
model_pd.l_d.mean(): -14.019144058227539 
model_pd.lagr.mean(): -13.896934509277344 
model_pd.lambdas: dict_items([('pout', tensor([1.3359], device='cuda:0')), ('power', tensor([0.7644], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3916], device='cuda:0')), ('power', tensor([-20.7402], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.12220937758684158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1404, 5.2815, 5.2111],
        [5.1404, 5.2625, 5.1963],
        [5.1404, 5.1723, 5.1467],
        [5.1404, 5.3712, 5.2969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12204565852880478 
model_pd.l_d.mean(): -13.995600700378418 
model_pd.lagr.mean(): -13.873555183410645 
model_pd.lambdas: dict_items([('pout', tensor([1.3373], device='cuda:0')), ('power', tensor([0.7634], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3908], device='cuda:0')), ('power', tensor([-20.7387], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.12204565852880478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1436, 5.1515, 5.1443],
        [5.1436, 6.0261, 6.3523],
        [5.1436, 5.1602, 5.1458],
        [5.1436, 5.6185, 5.6283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.12188062071800232 
model_pd.l_d.mean(): -13.972063064575195 
model_pd.lagr.mean(): -13.85018253326416 
model_pd.lambdas: dict_items([('pout', tensor([1.3387], device='cuda:0')), ('power', tensor([0.7624], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3900], device='cuda:0')), ('power', tensor([-20.7372], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.12188062071800232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1469, 5.1787, 5.1531],
        [5.1469, 5.1634, 5.1490],
        [5.1469, 6.5483, 7.4478],
        [5.1469, 5.1752, 5.1520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.12171409279108047 
model_pd.l_d.mean(): -13.948530197143555 
model_pd.lagr.mean(): -13.82681655883789 
model_pd.lambdas: dict_items([('pout', tensor([1.3401], device='cuda:0')), ('power', tensor([0.7613], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3892], device='cuda:0')), ('power', tensor([-20.7356], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.12171409279108047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1501, 5.2044, 5.1650],
        [5.1501, 5.1501, 5.1501],
        [5.1501, 5.2039, 5.1648],
        [5.1501, 6.0396, 6.3720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.12154614180326462 
model_pd.l_d.mean(): -13.924997329711914 
model_pd.lagr.mean(): -13.803451538085938 
model_pd.lambdas: dict_items([('pout', tensor([1.3415], device='cuda:0')), ('power', tensor([0.7603], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3884], device='cuda:0')), ('power', tensor([-20.7341], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.12154614180326462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1534, 6.0432, 6.3757],
        [5.1534, 5.8360, 5.9867],
        [5.1534, 5.5285, 5.4903],
        [5.1534, 5.1550, 5.1535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.12137699127197266 
model_pd.l_d.mean(): -13.901471138000488 
model_pd.lagr.mean(): -13.780094146728516 
model_pd.lambdas: dict_items([('pout', tensor([1.3429], device='cuda:0')), ('power', tensor([0.7593], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3876], device='cuda:0')), ('power', tensor([-20.7325], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.12137699127197266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1567, 5.2592, 5.1987],
        [5.1567, 6.0479, 6.3814],
        [5.1567, 5.6321, 5.6417],
        [5.1567, 5.2105, 5.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12120699882507324 
model_pd.l_d.mean(): -13.877946853637695 
model_pd.lagr.mean(): -13.756739616394043 
model_pd.lambdas: dict_items([('pout', tensor([1.3442], device='cuda:0')), ('power', tensor([0.7582], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3868], device='cuda:0')), ('power', tensor([-20.7309], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12120699882507324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 5.1601, 5.1601],
        [5.1601, 5.1601, 5.1601],
        [5.1601, 5.3910, 5.3166],
        [5.1601, 5.9726, 6.2327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12103715538978577 
model_pd.l_d.mean(): -13.854426383972168 
model_pd.lagr.mean(): -13.733388900756836 
model_pd.lambdas: dict_items([('pout', tensor([1.3456], device='cuda:0')), ('power', tensor([0.7572], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3860], device='cuda:0')), ('power', tensor([-20.7293], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12103715538978577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1633, 6.0478, 6.3742],
        [5.1633, 5.3045, 5.2340],
        [5.1633, 5.3414, 5.2663],
        [5.1633, 5.1633, 5.1633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.12087094038724899 
model_pd.l_d.mean(): -13.830910682678223 
model_pd.lagr.mean(): -13.710040092468262 
model_pd.lambdas: dict_items([('pout', tensor([1.3470], device='cuda:0')), ('power', tensor([0.7562], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3852], device='cuda:0')), ('power', tensor([-20.7277], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.12087094038724899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1665, 5.3168, 5.2447],
        [5.1665, 5.2192, 5.1807],
        [5.1665, 5.2690, 5.2085],
        [5.1665, 5.1668, 5.1665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1207100972533226 
model_pd.l_d.mean(): -13.807406425476074 
model_pd.lagr.mean(): -13.68669605255127 
model_pd.lambdas: dict_items([('pout', tensor([1.3484], device='cuda:0')), ('power', tensor([0.7551], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3844], device='cuda:0')), ('power', tensor([-20.7262], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1207100972533226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 5.4441, 5.3756],
        [5.1696, 5.3199, 5.2478],
        [5.1696, 5.8932, 6.0760],
        [5.1696, 5.1697, 5.1696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.12055507302284241 
model_pd.l_d.mean(): -13.783907890319824 
model_pd.lagr.mean(): -13.663352966308594 
model_pd.lambdas: dict_items([('pout', tensor([1.3498], device='cuda:0')), ('power', tensor([0.7541], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3836], device='cuda:0')), ('power', tensor([-20.7247], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.12055507302284241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1726, 5.3229, 5.2507],
        [5.1726, 5.1728, 5.1726],
        [5.1726, 5.2009, 5.1777],
        [5.1726, 5.2751, 5.2145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.12040513753890991 
model_pd.l_d.mean(): -13.760418891906738 
model_pd.lagr.mean(): -13.640013694763184 
model_pd.lambdas: dict_items([('pout', tensor([1.3512], device='cuda:0')), ('power', tensor([0.7530], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3829], device='cuda:0')), ('power', tensor([-20.7232], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.12040513753890991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1755, 5.2977, 5.2313],
        [5.1755, 5.8598, 6.0104],
        [5.1755, 6.5828, 7.4851],
        [5.1755, 5.1920, 5.1776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.12025897949934006 
model_pd.l_d.mean(): -13.736936569213867 
model_pd.lagr.mean(): -13.616677284240723 
model_pd.lambdas: dict_items([('pout', tensor([1.3526], device='cuda:0')), ('power', tensor([0.7520], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3822], device='cuda:0')), ('power', tensor([-20.7218], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.12025897949934006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1783, 6.5863, 7.4889],
        [5.1783, 5.1783, 5.1783],
        [5.1783, 5.2322, 5.1930],
        [5.1783, 5.1844, 5.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.12011491507291794 
model_pd.l_d.mean(): -13.713459968566895 
model_pd.lagr.mean(): -13.593344688415527 
model_pd.lambdas: dict_items([('pout', tensor([1.3539], device='cuda:0')), ('power', tensor([0.7510], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3815], device='cuda:0')), ('power', tensor([-20.7204], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.12011491507291794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1812, 5.8660, 6.0166],
        [5.1812, 5.5981, 5.5771],
        [5.1812, 5.1827, 5.1813],
        [5.1812, 5.2339, 5.1954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.11997098475694656 
model_pd.l_d.mean(): -13.68998908996582 
model_pd.lagr.mean(): -13.57001781463623 
model_pd.lambdas: dict_items([('pout', tensor([1.3553], device='cuda:0')), ('power', tensor([0.7499], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3807], device='cuda:0')), ('power', tensor([-20.7190], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.11997098475694656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1841, 5.2368, 5.1983],
        [5.1841, 5.1841, 5.1841],
        [5.1841, 5.1841, 5.1841],
        [5.1841, 5.4155, 5.3408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.11982551217079163 
model_pd.l_d.mean(): -13.666524887084961 
model_pd.lagr.mean(): -13.546699523925781 
model_pd.lambdas: dict_items([('pout', tensor([1.3567], device='cuda:0')), ('power', tensor([0.7489], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3800], device='cuda:0')), ('power', tensor([-20.7176], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.11982551217079163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1871, 6.4411, 7.1573],
        [5.1871, 5.4186, 5.3438],
        [5.1871, 6.0024, 6.2628],
        [5.1871, 5.1871, 5.1871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.11967702955007553 
model_pd.l_d.mean(): -13.643057823181152 
model_pd.lagr.mean(): -13.523381233215332 
model_pd.lambdas: dict_items([('pout', tensor([1.3581], device='cuda:0')), ('power', tensor([0.7479], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3793], device='cuda:0')), ('power', tensor([-20.7161], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.11967702955007553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]], device='cuda:0')
 pt:tensor([[5.1902, 6.0058, 6.2663],
        [5.1902, 5.4778, 5.4115],
        [5.1902, 5.8757, 6.0264],
        [5.1902, 5.9156, 6.0985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.1195245310664177 
model_pd.l_d.mean(): -13.619595527648926 
model_pd.lagr.mean(): -13.500070571899414 
model_pd.lambdas: dict_items([('pout', tensor([1.3595], device='cuda:0')), ('power', tensor([0.7468], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3785], device='cuda:0')), ('power', tensor([-20.7146], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.1195245310664177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1934, 5.8315, 5.9457],
        [5.1934, 6.0888, 6.4230],
        [5.1934, 5.2253, 5.1996],
        [5.1934, 5.3719, 5.2965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.1193673238158226 
model_pd.l_d.mean(): -13.5961332321167 
model_pd.lagr.mean(): -13.476765632629395 
model_pd.lambdas: dict_items([('pout', tensor([1.3608], device='cuda:0')), ('power', tensor([0.7458], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3777], device='cuda:0')), ('power', tensor([-20.7130], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.1193673238158226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1966, 6.0915, 6.4248],
        [5.1966, 5.3752, 5.2998],
        [5.1966, 5.2132, 5.1988],
        [5.1966, 6.0850, 6.4122]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.11920569837093353 
model_pd.l_d.mean(): -13.572669982910156 
model_pd.lagr.mean(): -13.45346450805664 
model_pd.lambdas: dict_items([('pout', tensor([1.3622], device='cuda:0')), ('power', tensor([0.7448], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3769], device='cuda:0')), ('power', tensor([-20.7114], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.11920569837093353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2000, 5.2000, 5.2000],
        [5.2000, 6.4001, 7.0527],
        [5.2000, 5.2166, 5.2022],
        [5.2000, 5.2000, 5.2000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.11904003471136093 
model_pd.l_d.mean(): -13.549209594726562 
model_pd.lagr.mean(): -13.430169105529785 
model_pd.lambdas: dict_items([('pout', tensor([1.3636], device='cuda:0')), ('power', tensor([0.7437], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3761], device='cuda:0')), ('power', tensor([-20.7097], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.11904003471136093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2034, 5.3259, 5.2594],
        [5.2034, 5.2035, 5.2034],
        [5.2034, 6.0992, 6.4327],
        [5.2034, 5.6513, 5.6446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.11887113004922867 
model_pd.l_d.mean(): -13.525749206542969 
model_pd.lagr.mean(): -13.406878471374512 
model_pd.lambdas: dict_items([('pout', tensor([1.3650], device='cuda:0')), ('power', tensor([0.7427], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3752], device='cuda:0')), ('power', tensor([-20.7080], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.11887113004922867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[5.2069, 5.8462, 5.9604],
        [5.2069, 5.4080, 5.3318],
        [5.2069, 6.6215, 7.5273],
        [5.2069, 6.1042, 6.4387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.11870013177394867 
model_pd.l_d.mean(): -13.502296447753906 
model_pd.lagr.mean(): -13.383596420288086 
model_pd.lambdas: dict_items([('pout', tensor([1.3663], device='cuda:0')), ('power', tensor([0.7417], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3744], device='cuda:0')), ('power', tensor([-20.7062], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.11870013177394867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2104, 5.8500, 5.9642],
        [5.2104, 5.2148, 5.2107],
        [5.2104, 6.7364, 7.7823],
        [5.2104, 5.3132, 5.2524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.11852800101041794 
model_pd.l_d.mean(): -13.478845596313477 
model_pd.lagr.mean(): -13.36031723022461 
model_pd.lambdas: dict_items([('pout', tensor([1.3677], device='cuda:0')), ('power', tensor([0.7406], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3735], device='cuda:0')), ('power', tensor([-20.7045], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.11852800101041794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2140, 5.2145, 5.2140],
        [5.2140, 5.2200, 5.2144],
        [5.2140, 5.2142, 5.2140],
        [5.2140, 5.3648, 5.2923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.11835572868585587 
model_pd.l_d.mean(): -13.455404281616211 
model_pd.lagr.mean(): -13.337048530578613 
model_pd.lambdas: dict_items([('pout', tensor([1.3691], device='cuda:0')), ('power', tensor([0.7396], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3726], device='cuda:0')), ('power', tensor([-20.7027], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.11835572868585587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2175, 5.2190, 5.2175],
        [5.2175, 6.0363, 6.2974],
        [5.2175, 5.2175, 5.2175],
        [5.2175, 6.7454, 7.7923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.11818410456180573 
model_pd.l_d.mean(): -13.431964874267578 
model_pd.lagr.mean(): -13.313780784606934 
model_pd.lambdas: dict_items([('pout', tensor([1.3704], device='cuda:0')), ('power', tensor([0.7385], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3718], device='cuda:0')), ('power', tensor([-20.7009], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.11818410456180573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2210, 5.2214, 5.2210],
        [5.2210, 5.2226, 5.2211],
        [5.2210, 6.4253, 7.0797],
        [5.2210, 5.2210, 5.2210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.11801370233297348 
model_pd.l_d.mean(): -13.408535957336426 
model_pd.lagr.mean(): -13.290522575378418 
model_pd.lambdas: dict_items([('pout', tensor([1.3718], device='cuda:0')), ('power', tensor([0.7375], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3709], device='cuda:0')), ('power', tensor([-20.6991], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.11801370233297348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2245, 5.4260, 5.3495],
        [5.2245, 5.2774, 5.2387],
        [5.2245, 5.2790, 5.2395],
        [5.2245, 5.9133, 6.0642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.11784470081329346 
model_pd.l_d.mean(): -13.385111808776855 
model_pd.lagr.mean(): -13.267267227172852 
model_pd.lambdas: dict_items([('pout', tensor([1.3732], device='cuda:0')), ('power', tensor([0.7365], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3700], device='cuda:0')), ('power', tensor([-20.6973], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.11784470081329346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2280, 6.6477, 7.5563],
        [5.2280, 5.2280, 5.2280],
        [5.2280, 5.2324, 5.2283],
        [5.2280, 5.4604, 5.3852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.11767704784870148 
model_pd.l_d.mean(): -13.361698150634766 
model_pd.lagr.mean(): -13.24402141571045 
model_pd.lambdas: dict_items([('pout', tensor([1.3746], device='cuda:0')), ('power', tensor([0.7354], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3692], device='cuda:0')), ('power', tensor([-20.6956], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.11767704784870148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2315, 5.2481, 5.2337],
        [5.2315, 5.3826, 5.3099],
        [5.2315, 5.7432, 5.7704],
        [5.2315, 5.2315, 5.2315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.11751046031713486 
model_pd.l_d.mean(): -13.338287353515625 
model_pd.lagr.mean(): -13.220776557922363 
model_pd.lambdas: dict_items([('pout', tensor([1.3759], device='cuda:0')), ('power', tensor([0.7344], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3683], device='cuda:0')), ('power', tensor([-20.6938], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.11751046031713486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2350, 6.6563, 7.5658],
        [5.2350, 5.2898, 5.2500],
        [5.2350, 5.6846, 5.6775],
        [5.2350, 5.7146, 5.7234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.11734452843666077 
model_pd.l_d.mean(): -13.314888000488281 
model_pd.lagr.mean(): -13.197543144226074 
model_pd.lambdas: dict_items([('pout', tensor([1.3773], device='cuda:0')), ('power', tensor([0.7334], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3675], device='cuda:0')), ('power', tensor([-20.6921], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.11734452843666077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2385, 6.1325, 6.4609],
        [5.2385, 5.2385, 5.2385],
        [5.2385, 5.2446, 5.2389],
        [5.2385, 5.2385, 5.2385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.11717870086431503 
model_pd.l_d.mean(): -13.29149055480957 
model_pd.lagr.mean(): -13.174311637878418 
model_pd.lambdas: dict_items([('pout', tensor([1.3787], device='cuda:0')), ('power', tensor([0.7323], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3666], device='cuda:0')), ('power', tensor([-20.6903], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.11717870086431503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2420, 5.5311, 5.4642],
        [5.2420, 6.1364, 6.4650],
        [5.2420, 6.1430, 6.4777],
        [5.2420, 5.2704, 5.2472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.11701236665248871 
model_pd.l_d.mean(): -13.26810073852539 
model_pd.lagr.mean(): -13.15108871459961 
model_pd.lambdas: dict_items([('pout', tensor([1.3800], device='cuda:0')), ('power', tensor([0.7313], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3657], device='cuda:0')), ('power', tensor([-20.6885], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.11701236665248871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2455, 5.2499, 5.2458],
        [5.2455, 5.2535, 5.2462],
        [5.2455, 6.6694, 7.5802],
        [5.2455, 5.2455, 5.2455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.11684507131576538 
model_pd.l_d.mean(): -13.244714736938477 
model_pd.lagr.mean(): -13.127869606018066 
model_pd.lambdas: dict_items([('pout', tensor([1.3814], device='cuda:0')), ('power', tensor([0.7303], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3649], device='cuda:0')), ('power', tensor([-20.6867], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.11684507131576538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2491, 5.3021, 5.2633],
        [5.2491, 6.1510, 6.4860],
        [5.2491, 5.2491, 5.2491],
        [5.2491, 5.2811, 5.2554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11667641252279282 
model_pd.l_d.mean(): -13.221336364746094 
model_pd.lagr.mean(): -13.104660034179688 
model_pd.lambdas: dict_items([('pout', tensor([1.3828], device='cuda:0')), ('power', tensor([0.7292], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3640], device='cuda:0')), ('power', tensor([-20.6849], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11667641252279282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2527, 5.2543, 5.2528],
        [5.2527, 5.2527, 5.2527],
        [5.2527, 5.6731, 5.6513],
        [5.2527, 5.3950, 5.3237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.11650614440441132 
model_pd.l_d.mean(): -13.197957992553711 
model_pd.lagr.mean(): -13.081451416015625 
model_pd.lambdas: dict_items([('pout', tensor([1.3841], device='cuda:0')), ('power', tensor([0.7282], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3631], device='cuda:0')), ('power', tensor([-20.6830], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.11650614440441132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2564, 5.3113, 5.2715],
        [5.2564, 6.1604, 6.4966],
        [5.2564, 5.2564, 5.2564],
        [5.2564, 5.3094, 5.2706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.11633401364088058 
model_pd.l_d.mean(): -13.174589157104492 
model_pd.lagr.mean(): -13.058255195617676 
model_pd.lambdas: dict_items([('pout', tensor([1.3855], device='cuda:0')), ('power', tensor([0.7272], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3622], device='cuda:0')), ('power', tensor([-20.6811], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.11633401364088058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]], device='cuda:0')
 pt:tensor([[5.2601, 5.4116, 5.3387],
        [5.2601, 5.4024, 5.3312],
        [5.2601, 5.9928, 6.1764],
        [5.2601, 5.4623, 5.3854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.11616022139787674 
model_pd.l_d.mean(): -13.15122127532959 
model_pd.lagr.mean(): -13.03506088256836 
model_pd.lambdas: dict_items([('pout', tensor([1.3868], device='cuda:0')), ('power', tensor([0.7261], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3613], device='cuda:0')), ('power', tensor([-20.6792], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.11616022139787674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2639, 5.2654, 5.2639],
        [5.2639, 5.4972, 5.4215],
        [5.2639, 5.2639, 5.2639],
        [5.2639, 5.3181, 5.2786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.11598470062017441 
model_pd.l_d.mean(): -13.127859115600586 
model_pd.lagr.mean(): -13.011874198913574 
model_pd.lambdas: dict_items([('pout', tensor([1.3882], device='cuda:0')), ('power', tensor([0.7251], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3604], device='cuda:0')), ('power', tensor([-20.6773], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.11598470062017441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2677, 6.4812, 7.1396],
        [5.2677, 5.3224, 5.2827],
        [5.2677, 6.1657, 6.4951],
        [5.2677, 5.2677, 5.2677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.11580778658390045 
model_pd.l_d.mean(): -13.104504585266113 
model_pd.lagr.mean(): -12.988697052001953 
model_pd.lambdas: dict_items([('pout', tensor([1.3896], device='cuda:0')), ('power', tensor([0.7241], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3594], device='cuda:0')), ('power', tensor([-20.6753], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.11580778658390045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2715, 5.5762, 5.5123],
        [5.2715, 5.9649, 6.1161],
        [5.2715, 5.3000, 5.2767],
        [5.2715, 5.3749, 5.3137]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11562971770763397 
model_pd.l_d.mean(): -13.081151962280273 
model_pd.lagr.mean(): -12.965521812438965 
model_pd.lambdas: dict_items([('pout', tensor([1.3909], device='cuda:0')), ('power', tensor([0.7230], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3585], device='cuda:0')), ('power', tensor([-20.6733], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11562971770763397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2754, 5.3285, 5.2896],
        [5.2754, 6.0097, 6.1936],
        [5.2754, 5.7274, 5.7199],
        [5.2754, 5.3301, 5.2904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.1154508963227272 
model_pd.l_d.mean(): -13.057804107666016 
model_pd.lagr.mean(): -12.942353248596191 
model_pd.lambdas: dict_items([('pout', tensor([1.3923], device='cuda:0')), ('power', tensor([0.7220], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3575], device='cuda:0')), ('power', tensor([-20.6713], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.1154508963227272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2793, 5.2796, 5.2793],
        [5.2793, 5.7012, 5.6790],
        [5.2793, 5.2793, 5.2793],
        [5.2793, 5.3827, 5.3215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.11527158319950104 
model_pd.l_d.mean(): -13.034465789794922 
model_pd.lagr.mean(): -12.919194221496582 
model_pd.lambdas: dict_items([('pout', tensor([1.3936], device='cuda:0')), ('power', tensor([0.7210], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3566], device='cuda:0')), ('power', tensor([-20.6693], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.11527158319950104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2832, 5.2832, 5.2832],
        [5.2832, 6.1910, 6.5282],
        [5.2832, 6.8287, 7.8861],
        [5.2832, 5.2835, 5.2832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.11509204655885696 
model_pd.l_d.mean(): -13.01113224029541 
model_pd.lagr.mean(): -12.896039962768555 
model_pd.lambdas: dict_items([('pout', tensor([1.3950], device='cuda:0')), ('power', tensor([0.7199], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3556], device='cuda:0')), ('power', tensor([-20.6673], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.11509204655885696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.2871, 6.5620, 7.2880],
        [5.2871, 5.3906, 5.3294],
        [5.2871, 5.9821, 6.1335],
        [5.2871, 5.5778, 5.5103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.11491242796182632 
model_pd.l_d.mean(): -12.987804412841797 
model_pd.lagr.mean(): -12.872892379760742 
model_pd.lambdas: dict_items([('pout', tensor([1.3963], device='cuda:0')), ('power', tensor([0.7189], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3547], device='cuda:0')), ('power', tensor([-20.6652], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.11491242796182632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2911, 5.4713, 5.3949],
        [5.2911, 5.4940, 5.4167],
        [5.2911, 6.8388, 7.8974],
        [5.2911, 5.2911, 5.2911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.11473274230957031 
model_pd.l_d.mean(): -12.96448802947998 
model_pd.lagr.mean(): -12.84975528717041 
model_pd.lambdas: dict_items([('pout', tensor([1.3977], device='cuda:0')), ('power', tensor([0.7179], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3537], device='cuda:0')), ('power', tensor([-20.6632], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.11473274230957031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2950, 5.3272, 5.3013],
        [5.2950, 5.2950, 5.2950],
        [5.2950, 5.3494, 5.3098],
        [5.2950, 5.2950, 5.2950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.11455308645963669 
model_pd.l_d.mean(): -12.941170692443848 
model_pd.lagr.mean(): -12.826617240905762 
model_pd.lambdas: dict_items([('pout', tensor([1.3991], device='cuda:0')), ('power', tensor([0.7168], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3528], device='cuda:0')), ('power', tensor([-20.6611], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.11455308645963669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2990, 6.0359, 6.2201],
        [5.2990, 6.2014, 6.5320],
        [5.2990, 5.2990, 5.2990],
        [5.2990, 6.5765, 7.3037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.11437337845563889 
model_pd.l_d.mean(): -12.917862892150879 
model_pd.lagr.mean(): -12.803489685058594 
model_pd.lambdas: dict_items([('pout', tensor([1.4004], device='cuda:0')), ('power', tensor([0.7158], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3518], device='cuda:0')), ('power', tensor([-20.6591], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.11437337845563889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3030, 5.8195, 5.8463],
        [5.3030, 5.3030, 5.3030],
        [5.3030, 5.3578, 5.3180],
        [5.3030, 5.4066, 5.3452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.11419352889060974 
model_pd.l_d.mean(): -12.894563674926758 
model_pd.lagr.mean(): -12.780369758605957 
model_pd.lambdas: dict_items([('pout', tensor([1.4018], device='cuda:0')), ('power', tensor([0.7148], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3508], device='cuda:0')), ('power', tensor([-20.6570], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.11419352889060974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3070, 5.9553, 6.0698],
        [5.3070, 5.3618, 5.3220],
        [5.3070, 5.3075, 5.3070],
        [5.3070, 5.7756, 5.7754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.11401339620351791 
model_pd.l_d.mean(): -12.871270179748535 
model_pd.lagr.mean(): -12.757256507873535 
model_pd.lambdas: dict_items([('pout', tensor([1.4031], device='cuda:0')), ('power', tensor([0.7137], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3498], device='cuda:0')), ('power', tensor([-20.6549], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.11401339620351791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3110, 5.3112, 5.3110],
        [5.3110, 5.7651, 5.7572],
        [5.3110, 5.4916, 5.4150],
        [5.3110, 6.0084, 6.1600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.11383281648159027 
model_pd.l_d.mean(): -12.847983360290527 
model_pd.lagr.mean(): -12.734150886535645 
model_pd.lambdas: dict_items([('pout', tensor([1.4045], device='cuda:0')), ('power', tensor([0.7127], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3489], device='cuda:0')), ('power', tensor([-20.6528], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.11383281648159027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3150, 5.3153, 5.3150],
        [5.3150, 5.3472, 5.3213],
        [5.3150, 6.5960, 7.3249],
        [5.3150, 5.3211, 5.3155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.11365168541669846 
model_pd.l_d.mean(): -12.824701309204102 
model_pd.lagr.mean(): -12.711050033569336 
model_pd.lambdas: dict_items([('pout', tensor([1.4058], device='cuda:0')), ('power', tensor([0.7117], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3479], device='cuda:0')), ('power', tensor([-20.6506], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.11365168541669846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3191, 5.3191, 5.3191],
        [5.3191, 5.3358, 5.3213],
        [5.3191, 6.2244, 6.5557],
        [5.3191, 5.7432, 5.7205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.11346990615129471 
model_pd.l_d.mean(): -12.801426887512207 
model_pd.lagr.mean(): -12.687956809997559 
model_pd.lambdas: dict_items([('pout', tensor([1.4071], device='cuda:0')), ('power', tensor([0.7106], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3469], device='cuda:0')), ('power', tensor([-20.6485], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.11346990615129471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3232, 5.4471, 5.3796],
        [5.3232, 5.3232, 5.3232],
        [5.3232, 5.7780, 5.7701],
        [5.3232, 6.6060, 7.3358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.11328727751970291 
model_pd.l_d.mean(): -12.778157234191895 
model_pd.lagr.mean(): -12.664870262145996 
model_pd.lambdas: dict_items([('pout', tensor([1.4085], device='cuda:0')), ('power', tensor([0.7096], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3459], device='cuda:0')), ('power', tensor([-20.6463], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.11328727751970291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]], device='cuda:0')
 pt:tensor([[5.3273, 5.7100, 5.6696],
        [5.3273, 5.6066, 5.5360],
        [5.3273, 6.2414, 6.5804],
        [5.3273, 5.4513, 5.3837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.11310379952192307 
model_pd.l_d.mean(): -12.75489616394043 
model_pd.lagr.mean(): -12.641792297363281 
model_pd.lambdas: dict_items([('pout', tensor([1.4098], device='cuda:0')), ('power', tensor([0.7086], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3449], device='cuda:0')), ('power', tensor([-20.6442], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.11310379952192307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3314, 6.6161, 7.3468],
        [5.3314, 5.3320, 5.3315],
        [5.3314, 6.5334, 7.1701],
        [5.3314, 6.5581, 7.2226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.1129194125533104 
model_pd.l_d.mean(): -12.731639862060547 
model_pd.lagr.mean(): -12.618720054626465 
model_pd.lambdas: dict_items([('pout', tensor([1.4112], device='cuda:0')), ('power', tensor([0.7075], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3439], device='cuda:0')), ('power', tensor([-20.6420], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.1129194125533104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3356, 5.3890, 5.3499],
        [5.3356, 5.3524, 5.3378],
        [5.3356, 6.2510, 6.5903],
        [5.3356, 5.9867, 6.1014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.1127341166138649 
model_pd.l_d.mean(): -12.708391189575195 
model_pd.lagr.mean(): -12.595657348632812 
model_pd.lambdas: dict_items([('pout', tensor([1.4125], device='cuda:0')), ('power', tensor([0.7065], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3429], device='cuda:0')), ('power', tensor([-20.6398], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.1127341166138649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3398, 5.3399, 5.3398],
        [5.3398, 5.6324, 5.5642],
        [5.3398, 6.2548, 6.5933],
        [5.3398, 6.1741, 6.4382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.11254792660474777 
model_pd.l_d.mean(): -12.685144424438477 
model_pd.lagr.mean(): -12.572596549987793 
model_pd.lambdas: dict_items([('pout', tensor([1.4139], device='cuda:0')), ('power', tensor([0.7055], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3419], device='cuda:0')), ('power', tensor([-20.6375], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.11254792660474777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3441, 5.9960, 6.1108],
        [5.3441, 6.2607, 6.6004],
        [5.3441, 5.3521, 5.3448],
        [5.3441, 5.3987, 5.3589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.11236079782247543 
model_pd.l_d.mean(): -12.661908149719238 
model_pd.lagr.mean(): -12.54954719543457 
model_pd.lambdas: dict_items([('pout', tensor([1.4152], device='cuda:0')), ('power', tensor([0.7044], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3408], device='cuda:0')), ('power', tensor([-20.6352], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.11236079782247543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3484, 5.5013, 5.4275],
        [5.3484, 5.6284, 5.5575],
        [5.3484, 6.6368, 7.3694],
        [5.3484, 5.3484, 5.3484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.11217276006937027 
model_pd.l_d.mean(): -12.638677597045898 
model_pd.lagr.mean(): -12.526504516601562 
model_pd.lambdas: dict_items([('pout', tensor([1.4165], device='cuda:0')), ('power', tensor([0.7034], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3398], device='cuda:0')), ('power', tensor([-20.6330], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.11217276006937027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3527, 5.6329, 5.5619],
        [5.3527, 6.0957, 6.2807],
        [5.3527, 5.4081, 5.3679],
        [5.3527, 6.2629, 6.5956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.1119839996099472 
model_pd.l_d.mean(): -12.615452766418457 
model_pd.lagr.mean(): -12.50346851348877 
model_pd.lambdas: dict_items([('pout', tensor([1.4179], device='cuda:0')), ('power', tensor([0.7024], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3388], device='cuda:0')), ('power', tensor([-20.6307], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.1119839996099472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3570, 6.9231, 7.9931],
        [5.3570, 6.5891, 7.2561],
        [5.3570, 5.8140, 5.8057],
        [5.3570, 5.3650, 5.3577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.11179438978433609 
model_pd.l_d.mean(): -12.592235565185547 
model_pd.lagr.mean(): -12.480441093444824 
model_pd.lambdas: dict_items([('pout', tensor([1.4192], device='cuda:0')), ('power', tensor([0.7013], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3377], device='cuda:0')), ('power', tensor([-20.6283], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.11179438978433609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3614, 5.5145, 5.4406],
        [5.3614, 5.8493, 5.8571],
        [5.3614, 5.3614, 5.3614],
        [5.3614, 5.4168, 5.3766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.11160402745008469 
model_pd.l_d.mean(): -12.569024085998535 
model_pd.lagr.mean(): -12.457420349121094 
model_pd.lambdas: dict_items([('pout', tensor([1.4206], device='cuda:0')), ('power', tensor([0.7003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3367], device='cuda:0')), ('power', tensor([-20.6260], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.11160402745008469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3658, 5.4903, 5.4224],
        [5.3658, 5.3661, 5.3658],
        [5.3658, 5.3719, 5.3662],
        [5.3658, 6.8200, 7.7480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.11141294240951538 
model_pd.l_d.mean(): -12.545820236206055 
model_pd.lagr.mean(): -12.434407234191895 
model_pd.lambdas: dict_items([('pout', tensor([1.4219], device='cuda:0')), ('power', tensor([0.6993], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3356], device='cuda:0')), ('power', tensor([-20.6237], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.11141294240951538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3702, 5.8587, 5.8664],
        [5.3702, 5.3717, 5.3702],
        [5.3702, 5.5750, 5.4968],
        [5.3702, 5.8430, 5.8423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11122111976146698 
model_pd.l_d.mean(): -12.522624015808105 
model_pd.lagr.mean(): -12.411402702331543 
model_pd.lambdas: dict_items([('pout', tensor([1.4232], device='cuda:0')), ('power', tensor([0.6983], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3345], device='cuda:0')), ('power', tensor([-20.6213], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11122111976146698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3746, 5.3826, 5.3753],
        [5.3746, 6.8312, 7.7605],
        [5.3746, 5.6686, 5.5998],
        [5.3746, 6.0789, 6.2312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.11102858185768127 
model_pd.l_d.mean(): -12.499432563781738 
model_pd.lagr.mean(): -12.38840389251709 
model_pd.lambdas: dict_items([('pout', tensor([1.4246], device='cuda:0')), ('power', tensor([0.6972], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3335], device='cuda:0')), ('power', tensor([-20.6189], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.11102858185768127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3791, 6.9515, 8.0254],
        [5.3791, 5.5612, 5.4838],
        [5.3791, 5.6156, 5.5385],
        [5.3791, 5.3959, 5.3813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.11083528399467468 
model_pd.l_d.mean(): -12.476250648498535 
model_pd.lagr.mean(): -12.365415573120117 
model_pd.lambdas: dict_items([('pout', tensor([1.4259], device='cuda:0')), ('power', tensor([0.6962], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3324], device='cuda:0')), ('power', tensor([-20.6165], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.11083528399467468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3836, 5.4124, 5.3888],
        [5.3836, 6.2984, 6.6324],
        [5.3836, 5.8115, 5.7882],
        [5.3836, 5.3851, 5.3836]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.11064128577709198 
model_pd.l_d.mean(): -12.453071594238281 
model_pd.lagr.mean(): -12.342430114746094 
model_pd.lambdas: dict_items([('pout', tensor([1.4272], device='cuda:0')), ('power', tensor([0.6952], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3313], device='cuda:0')), ('power', tensor([-20.6141], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.11064128577709198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3881, 5.4429, 5.4030],
        [5.3881, 5.3883, 5.3881],
        [5.3881, 5.4418, 5.4025],
        [5.3881, 6.0939, 6.2464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.11044645309448242 
model_pd.l_d.mean(): -12.429903030395508 
model_pd.lagr.mean(): -12.319456100463867 
model_pd.lambdas: dict_items([('pout', tensor([1.4286], device='cuda:0')), ('power', tensor([0.6941], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3302], device='cuda:0')), ('power', tensor([-20.6116], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.11044645309448242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3926, 5.7788, 5.7376],
        [5.3926, 5.7021, 5.6365],
        [5.3926, 5.3926, 5.3926],
        [5.3926, 6.1404, 6.3261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.1102508157491684 
model_pd.l_d.mean(): -12.40674114227295 
model_pd.lagr.mean(): -12.296490669250488 
model_pd.lambdas: dict_items([('pout', tensor([1.4299], device='cuda:0')), ('power', tensor([0.6931], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3291], device='cuda:0')), ('power', tensor([-20.6091], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.1102508157491684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3972, 5.4521, 5.4121],
        [5.3972, 5.3988, 5.3973],
        [5.3972, 5.3972, 5.3972],
        [5.3972, 5.5511, 5.4768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.11005420237779617 
model_pd.l_d.mean(): -12.38358211517334 
model_pd.lagr.mean(): -12.273528099060059 
model_pd.lambdas: dict_items([('pout', tensor([1.4312], device='cuda:0')), ('power', tensor([0.6921], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3280], device='cuda:0')), ('power', tensor([-20.6066], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.11005420237779617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4018, 5.6839, 5.6123],
        [5.4018, 6.3262, 6.6674],
        [5.4018, 5.4575, 5.4171],
        [5.4018, 5.4021, 5.4018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.10985664278268814 
model_pd.l_d.mean(): -12.360434532165527 
model_pd.lagr.mean(): -12.250577926635742 
model_pd.lambdas: dict_items([('pout', tensor([1.4325], device='cuda:0')), ('power', tensor([0.6910], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3269], device='cuda:0')), ('power', tensor([-20.6041], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.10985664278268814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4065, 6.2498, 6.5160],
        [5.4065, 5.4065, 5.4065],
        [5.4065, 6.6493, 7.3214],
        [5.4065, 6.3315, 6.6729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.10965804010629654 
model_pd.l_d.mean(): -12.337292671203613 
model_pd.lagr.mean(): -12.22763442993164 
model_pd.lambdas: dict_items([('pout', tensor([1.4339], device='cuda:0')), ('power', tensor([0.6900], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3258], device='cuda:0')), ('power', tensor([-20.6016], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.10965804010629654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4111, 5.4112, 5.4111],
        [5.4111, 5.4111, 5.4111],
        [5.4111, 5.5559, 5.4831],
        [5.4111, 5.4111, 5.4111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.10945827513933182 
model_pd.l_d.mean(): -12.314157485961914 
model_pd.lagr.mean(): -12.204699516296387 
model_pd.lambdas: dict_items([('pout', tensor([1.4352], device='cuda:0')), ('power', tensor([0.6890], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3247], device='cuda:0')), ('power', tensor([-20.5991], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.10945827513933182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4158, 5.4161, 5.4158],
        [5.4158, 5.4159, 5.4158],
        [5.4158, 5.4158, 5.4158],
        [5.4158, 5.4327, 5.4181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.10925740003585815 
model_pd.l_d.mean(): -12.291027069091797 
model_pd.lagr.mean(): -12.181769371032715 
model_pd.lambdas: dict_items([('pout', tensor([1.4365], device='cuda:0')), ('power', tensor([0.6880], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3236], device='cuda:0')), ('power', tensor([-20.5965], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.10925740003585815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4206, 6.7256, 7.4665],
        [5.4206, 5.6585, 5.5807],
        [5.4206, 5.4375, 5.4228],
        [5.4206, 5.6268, 5.5479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.10905531048774719 
model_pd.l_d.mean(): -12.267904281616211 
model_pd.lagr.mean(): -12.158848762512207 
model_pd.lambdas: dict_items([('pout', tensor([1.4378], device='cuda:0')), ('power', tensor([0.6869], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3224], device='cuda:0')), ('power', tensor([-20.5939], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.10905531048774719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]], device='cuda:0')
 pt:tensor([[5.4254, 5.5704, 5.4975],
        [5.4254, 6.3534, 6.6957],
        [5.4254, 5.7214, 5.6520],
        [5.4254, 5.5798, 5.5052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.10885188728570938 
model_pd.l_d.mean(): -12.244791030883789 
model_pd.lagr.mean(): -12.135939598083496 
model_pd.lambdas: dict_items([('pout', tensor([1.4392], device='cuda:0')), ('power', tensor([0.6859], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3213], device='cuda:0')), ('power', tensor([-20.5913], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.10885188728570938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4302, 5.6135, 5.5355],
        [5.4302, 5.4861, 5.4455],
        [5.4302, 5.4364, 5.4306],
        [5.4302, 5.5557, 5.4872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.10864713788032532 
model_pd.l_d.mean(): -12.22168254852295 
model_pd.lagr.mean(): -12.113035202026367 
model_pd.lambdas: dict_items([('pout', tensor([1.4405], device='cuda:0')), ('power', tensor([0.6849], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3201], device='cuda:0')), ('power', tensor([-20.5886], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.10864713788032532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4350, 6.0963, 6.2118],
        [5.4350, 5.9125, 5.9113],
        [5.4350, 5.4520, 5.4373],
        [5.4350, 5.4902, 5.4500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.10844101011753082 
model_pd.l_d.mean(): -12.198580741882324 
model_pd.lagr.mean(): -12.090139389038086 
model_pd.lambdas: dict_items([('pout', tensor([1.4418], device='cuda:0')), ('power', tensor([0.6838], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3190], device='cuda:0')), ('power', tensor([-20.5859], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.10844101011753082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4399, 5.5946, 5.5199],
        [5.4399, 5.9178, 5.9165],
        [5.4399, 5.4399, 5.4399],
        [5.4399, 5.4399, 5.4399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.10823342949151993 
model_pd.l_d.mean(): -12.175488471984863 
model_pd.lagr.mean(): -12.067255020141602 
model_pd.lambdas: dict_items([('pout', tensor([1.4431], device='cuda:0')), ('power', tensor([0.6828], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3178], device='cuda:0')), ('power', tensor([-20.5832], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.10823342949151993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4449, 5.5006, 5.4601],
        [5.4449, 5.9079, 5.8989],
        [5.4449, 5.4449, 5.4449],
        [5.4449, 7.0365, 8.1224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.10802442580461502 
model_pd.l_d.mean(): -12.152400970458984 
model_pd.lagr.mean(): -12.044376373291016 
model_pd.lambdas: dict_items([('pout', tensor([1.4444], device='cuda:0')), ('power', tensor([0.6818], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3166], device='cuda:0')), ('power', tensor([-20.5805], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.10802442580461502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4498, 5.5553, 5.4927],
        [5.4498, 5.4499, 5.4498],
        [5.4498, 5.4501, 5.4498],
        [5.4498, 5.6569, 5.5776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.10781380534172058 
model_pd.l_d.mean(): -12.12932014465332 
model_pd.lagr.mean(): -12.021506309509277 
model_pd.lambdas: dict_items([('pout', tensor([1.4457], device='cuda:0')), ('power', tensor([0.6807], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3155], device='cuda:0')), ('power', tensor([-20.5777], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.10781380534172058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4549, 5.6388, 5.5604],
        [5.4549, 5.7521, 5.6823],
        [5.4549, 5.4552, 5.4549],
        [5.4549, 5.5109, 5.4702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.10760162025690079 
model_pd.l_d.mean(): -12.106247901916504 
model_pd.lagr.mean(): -11.99864673614502 
model_pd.lambdas: dict_items([('pout', tensor([1.4471], device='cuda:0')), ('power', tensor([0.6797], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3143], device='cuda:0')), ('power', tensor([-20.5750], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.10760162025690079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4599, 6.1239, 6.2397],
        [5.4599, 5.4599, 5.4599],
        [5.4599, 5.4599, 5.4599],
        [5.4599, 5.4599, 5.4599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.10738781839609146 
model_pd.l_d.mean(): -12.083181381225586 
model_pd.lagr.mean(): -11.975793838500977 
model_pd.lambdas: dict_items([('pout', tensor([1.4484], device='cuda:0')), ('power', tensor([0.6787], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3131], device='cuda:0')), ('power', tensor([-20.5721], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.10738781839609146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4650, 6.3926, 6.7304],
        [5.4650, 5.4650, 5.4650],
        [5.4650, 5.4651, 5.4650],
        [5.4650, 5.7497, 5.6771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.10717230290174484 
model_pd.l_d.mean(): -12.0601224899292 
model_pd.lagr.mean(): -11.952950477600098 
model_pd.lambdas: dict_items([('pout', tensor([1.4497], device='cuda:0')), ('power', tensor([0.6777], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3118], device='cuda:0')), ('power', tensor([-20.5693], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.10717230290174484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4702, 5.6255, 5.5504],
        [5.4702, 5.4704, 5.4702],
        [5.4702, 6.7273, 7.4064],
        [5.4702, 6.2275, 6.4149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.10695501416921616 
model_pd.l_d.mean(): -12.037069320678711 
model_pd.lagr.mean(): -11.93011474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.4510], device='cuda:0')), ('power', tensor([0.6766], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3106], device='cuda:0')), ('power', tensor([-20.5664], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.10695501416921616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4754, 6.7082, 7.3594],
        [5.4754, 5.4816, 5.4758],
        [5.4754, 5.6599, 5.5812],
        [5.4754, 5.6831, 5.6035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.10673587769269943 
model_pd.l_d.mean(): -12.014025688171387 
model_pd.lagr.mean(): -11.907289505004883 
model_pd.lambdas: dict_items([('pout', tensor([1.4523], device='cuda:0')), ('power', tensor([0.6756], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3094], device='cuda:0')), ('power', tensor([-20.5635], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.10673587769269943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4806, 5.5361, 5.4956],
        [5.4806, 5.9775, 5.9845],
        [5.4806, 5.4809, 5.4806],
        [5.4806, 5.4806, 5.4806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.10651477426290512 
model_pd.l_d.mean(): -11.990987777709961 
model_pd.lagr.mean(): -11.884472846984863 
model_pd.lambdas: dict_items([('pout', tensor([1.4536], device='cuda:0')), ('power', tensor([0.6746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3081], device='cuda:0')), ('power', tensor([-20.5606], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.10651477426290512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4859, 6.9725, 7.9194],
        [5.4859, 5.4865, 5.4859],
        [5.4859, 5.5029, 5.4881],
        [5.4859, 5.4859, 5.4859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.10629166662693024 
model_pd.l_d.mean(): -11.967958450317383 
model_pd.lagr.mean(): -11.861666679382324 
model_pd.lambdas: dict_items([('pout', tensor([1.4549], device='cuda:0')), ('power', tensor([0.6735], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3069], device='cuda:0')), ('power', tensor([-20.5576], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.10629166662693024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4912, 5.5973, 5.5343],
        [5.4912, 5.6762, 5.5973],
        [5.4912, 5.5473, 5.5065],
        [5.4912, 5.4912, 5.4912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.10606637597084045 
model_pd.l_d.mean(): -11.94493579864502 
model_pd.lagr.mean(): -11.838869094848633 
model_pd.lambdas: dict_items([('pout', tensor([1.4562], device='cuda:0')), ('power', tensor([0.6725], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3056], device='cuda:0')), ('power', tensor([-20.5546], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.10606637597084045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.4966, 6.3530, 6.6225],
        [5.4966, 5.9320, 5.9076],
        [5.4966, 5.6817, 5.6028],
        [5.4966, 6.8198, 7.5702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.10583887994289398 
model_pd.l_d.mean(): -11.921918869018555 
model_pd.lagr.mean(): -11.816080093383789 
model_pd.lambdas: dict_items([('pout', tensor([1.4575], device='cuda:0')), ('power', tensor([0.6715], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3044], device='cuda:0')), ('power', tensor([-20.5516], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.10583887994289398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5021, 5.6873, 5.6083],
        [5.5021, 5.7107, 5.6307],
        [5.5021, 6.7666, 7.4493],
        [5.5021, 5.5191, 5.5043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.10560894012451172 
model_pd.l_d.mean(): -11.898908615112305 
model_pd.lagr.mean(): -11.793299674987793 
model_pd.lambdas: dict_items([('pout', tensor([1.4588], device='cuda:0')), ('power', tensor([0.6705], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3031], device='cuda:0')), ('power', tensor([-20.5485], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.10560894012451172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5076, 5.7941, 5.7209],
        [5.5076, 5.6930, 5.6139],
        [5.5076, 5.5092, 5.5076],
        [5.5076, 5.5077, 5.5076]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.10537642985582352 
model_pd.l_d.mean(): -11.875907897949219 
model_pd.lagr.mean(): -11.77053165435791 
model_pd.lambdas: dict_items([('pout', tensor([1.4601], device='cuda:0')), ('power', tensor([0.6694], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3018], device='cuda:0')), ('power', tensor([-20.5454], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.10537642985582352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5131, 5.9065, 5.8639],
        [5.5131, 5.5697, 5.5286],
        [5.5131, 5.5194, 5.5136],
        [5.5131, 6.4486, 6.7888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.10514117032289505 
model_pd.l_d.mean(): -11.852911949157715 
model_pd.lagr.mean(): -11.747770309448242 
model_pd.lambdas: dict_items([('pout', tensor([1.4614], device='cuda:0')), ('power', tensor([0.6684], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3005], device='cuda:0')), ('power', tensor([-20.5422], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.10514117032289505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5188, 6.0527, 6.0786],
        [5.5188, 5.6459, 5.5764],
        [5.5188, 5.5250, 5.5192],
        [5.5188, 5.5188, 5.5188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.10490306466817856 
model_pd.l_d.mean(): -11.829923629760742 
model_pd.lagr.mean(): -11.725020408630371 
model_pd.lambdas: dict_items([('pout', tensor([1.4627], device='cuda:0')), ('power', tensor([0.6674], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2991], device='cuda:0')), ('power', tensor([-20.5390], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.10490306466817856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5245, 6.4698, 6.8182],
        [5.5245, 5.5260, 5.5245],
        [5.5245, 6.0250, 6.0318],
        [5.5245, 5.6517, 5.5821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.10466182231903076 
model_pd.l_d.mean(): -11.806941986083984 
model_pd.lagr.mean(): -11.702280044555664 
model_pd.lambdas: dict_items([('pout', tensor([1.4640], device='cuda:0')), ('power', tensor([0.6664], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2978], device='cuda:0')), ('power', tensor([-20.5358], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.10466182231903076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5302, 6.4755, 6.8231],
        [5.5302, 5.8461, 5.7786],
        [5.5302, 5.5302, 5.5302],
        [5.5302, 5.5303, 5.5302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.1044173613190651 
model_pd.l_d.mean(): -11.783967018127441 
model_pd.lagr.mean(): -11.679549217224121 
model_pd.lambdas: dict_items([('pout', tensor([1.4653], device='cuda:0')), ('power', tensor([0.6653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2964], device='cuda:0')), ('power', tensor([-20.5325], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.1044173613190651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5361, 5.5361, 5.5361],
        [5.5361, 6.4754, 6.8169],
        [5.5361, 5.5361, 5.5361],
        [5.5361, 5.6834, 5.6092]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.10416940599679947 
model_pd.l_d.mean(): -11.760998725891113 
model_pd.lagr.mean(): -11.656828880310059 
model_pd.lambdas: dict_items([('pout', tensor([1.4666], device='cuda:0')), ('power', tensor([0.6643], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2951], device='cuda:0')), ('power', tensor([-20.5291], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.10416940599679947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5420, 5.5420, 5.5420],
        [5.5420, 5.5502, 5.5427],
        [5.5420, 5.5751, 5.5485],
        [5.5420, 5.8433, 5.7722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.10391774773597717 
model_pd.l_d.mean(): -11.738039016723633 
model_pd.lagr.mean(): -11.63412094116211 
model_pd.lambdas: dict_items([('pout', tensor([1.4679], device='cuda:0')), ('power', tensor([0.6633], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2937], device='cuda:0')), ('power', tensor([-20.5258], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.10391774773597717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5480, 5.9871, 5.9622],
        [5.5480, 5.6040, 5.5631],
        [5.5480, 6.0846, 6.1105],
        [5.5480, 5.5774, 5.5533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.10366220027208328 
model_pd.l_d.mean(): -11.715084075927734 
model_pd.lagr.mean(): -11.611421585083008 
model_pd.lambdas: dict_items([('pout', tensor([1.4692], device='cuda:0')), ('power', tensor([0.6623], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2923], device='cuda:0')), ('power', tensor([-20.5223], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.10366220027208328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5541, 6.8916, 7.6495],
        [5.5541, 5.5557, 5.5541],
        [5.5541, 5.6611, 5.5975],
        [5.5541, 5.5544, 5.5541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.10340240597724915 
model_pd.l_d.mean(): -11.69213581085205 
model_pd.lagr.mean(): -11.588733673095703 
model_pd.lambdas: dict_items([('pout', tensor([1.4705], device='cuda:0')), ('power', tensor([0.6612], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2908], device='cuda:0')), ('power', tensor([-20.5188], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.10340240597724915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5602, 6.0002, 5.9753],
        [5.5602, 6.5107, 6.8600],
        [5.5602, 5.5603, 5.5602],
        [5.5602, 5.5605, 5.5602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.10313815623521805 
model_pd.l_d.mean(): -11.669194221496582 
model_pd.lagr.mean(): -11.566056251525879 
model_pd.lambdas: dict_items([('pout', tensor([1.4718], device='cuda:0')), ('power', tensor([0.6602], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2894], device='cuda:0')), ('power', tensor([-20.5153], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.10313815623521805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5665, 5.5665, 5.5665],
        [5.5665, 6.2939, 6.4496],
        [5.5665, 5.5681, 5.5666],
        [5.5665, 5.7146, 5.6400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.10286904871463776 
model_pd.l_d.mean(): -11.646258354187012 
model_pd.lagr.mean(): -11.543389320373535 
model_pd.lambdas: dict_items([('pout', tensor([1.4731], device='cuda:0')), ('power', tensor([0.6592], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2879], device='cuda:0')), ('power', tensor([-20.5116], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.10286904871463776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5729, 5.5901, 5.5751],
        [5.5729, 5.5731, 5.5729],
        [5.5729, 6.0613, 6.0593],
        [5.5729, 5.6024, 5.5782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.1025947853922844 
model_pd.l_d.mean(): -11.623330116271973 
model_pd.lagr.mean(): -11.520735740661621 
model_pd.lambdas: dict_items([('pout', tensor([1.4744], device='cuda:0')), ('power', tensor([0.6582], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2864], device='cuda:0')), ('power', tensor([-20.5080], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.1025947853922844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5794, 5.8826, 5.8109],
        [5.5794, 5.7668, 5.6868],
        [5.5794, 5.7373, 5.6608],
        [5.5794, 5.5839, 5.5796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.10231497883796692 
model_pd.l_d.mean(): -11.6004056930542 
model_pd.lagr.mean(): -11.498090744018555 
model_pd.lambdas: dict_items([('pout', tensor([1.4757], device='cuda:0')), ('power', tensor([0.6571], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2849], device='cuda:0')), ('power', tensor([-20.5042], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.10231497883796692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5860, 5.7973, 5.7161],
        [5.5860, 6.5420, 6.8938],
        [5.5860, 5.6431, 5.6015],
        [5.5860, 5.8761, 5.8018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.10202910006046295 
model_pd.l_d.mean(): -11.57748794555664 
model_pd.lagr.mean(): -11.475459098815918 
model_pd.lambdas: dict_items([('pout', tensor([1.4769], device='cuda:0')), ('power', tensor([0.6561], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2834], device='cuda:0')), ('power', tensor([-20.5004], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.10202910006046295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5927, 6.2722, 6.3897],
        [5.5927, 5.5930, 5.5927],
        [5.5927, 5.6223, 5.5980],
        [5.5927, 5.8043, 5.7230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.10173661261796951 
model_pd.l_d.mean(): -11.554574012756348 
model_pd.lagr.mean(): -11.452836990356445 
model_pd.lambdas: dict_items([('pout', tensor([1.4782], device='cuda:0')), ('power', tensor([0.6551], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2818], device='cuda:0')), ('power', tensor([-20.4965], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.10173661261796951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5995, 5.5998, 5.5995],
        [5.5995, 5.7073, 5.6432],
        [5.5995, 5.6548, 5.6143],
        [5.5995, 6.5569, 6.9085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.10143694281578064 
model_pd.l_d.mean(): -11.531665802001953 
model_pd.lagr.mean(): -11.430229187011719 
model_pd.lambdas: dict_items([('pout', tensor([1.4795], device='cuda:0')), ('power', tensor([0.6541], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2802], device='cuda:0')), ('power', tensor([-20.4925], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.10143694281578064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6065, 6.2877, 6.4055],
        [5.6065, 5.8977, 5.8231],
        [5.6065, 5.6066, 5.6065],
        [5.6065, 5.6635, 5.6220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.101129449903965 
model_pd.l_d.mean(): -11.508764266967773 
model_pd.lagr.mean(): -11.407634735107422 
model_pd.lambdas: dict_items([('pout', tensor([1.4808], device='cuda:0')), ('power', tensor([0.6530], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2786], device='cuda:0')), ('power', tensor([-20.4884], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.101129449903965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6137, 5.6139, 5.6137],
        [5.6137, 5.6690, 5.6284],
        [5.6137, 6.9665, 7.7327],
        [5.6137, 6.4884, 6.7629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.10081323236227036 
model_pd.l_d.mean(): -11.485864639282227 
model_pd.lagr.mean(): -11.385051727294922 
model_pd.lambdas: dict_items([('pout', tensor([1.4821], device='cuda:0')), ('power', tensor([0.6520], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2769], device='cuda:0')), ('power', tensor([-20.4842], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.10081323236227036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[5.6210, 6.1136, 6.1114],
        [5.6210, 5.9264, 5.8542],
        [5.6210, 5.7292, 5.6649],
        [5.6210, 5.8664, 5.7857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10048744082450867 
model_pd.l_d.mean(): -11.462966918945312 
model_pd.lagr.mean(): -11.362479209899902 
model_pd.lambdas: dict_items([('pout', tensor([1.4833], device='cuda:0')), ('power', tensor([0.6510], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2752], device='cuda:0')), ('power', tensor([-20.4799], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.10048744082450867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6285, 5.6285, 5.6285],
        [5.6285, 6.1218, 6.1195],
        [5.6285, 5.6459, 5.6308],
        [5.6285, 5.6291, 5.6285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.10015100240707397 
model_pd.l_d.mean(): -11.440074920654297 
model_pd.lagr.mean(): -11.339923858642578 
model_pd.lambdas: dict_items([('pout', tensor([1.4846], device='cuda:0')), ('power', tensor([0.6500], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2735], device='cuda:0')), ('power', tensor([-20.4754], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.10015100240707397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6362, 6.5933, 6.9405],
        [5.6362, 5.6362, 5.6362],
        [5.6362, 5.6362, 5.6362],
        [5.6362, 5.6368, 5.6362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.0998026505112648 
model_pd.l_d.mean(): -11.417182922363281 
model_pd.lagr.mean(): -11.31737995147705 
model_pd.lambdas: dict_items([('pout', tensor([1.4859], device='cuda:0')), ('power', tensor([0.6489], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2717], device='cuda:0')), ('power', tensor([-20.4709], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.0998026505112648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6441, 6.0466, 6.0025],
        [5.6441, 6.6098, 6.9641],
        [5.6441, 6.0908, 6.0651],
        [5.6441, 5.6441, 5.6441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.09944086521863937 
model_pd.l_d.mean(): -11.394293785095215 
model_pd.lagr.mean(): -11.294853210449219 
model_pd.lambdas: dict_items([('pout', tensor([1.4871], device='cuda:0')), ('power', tensor([0.6479], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2698], device='cuda:0')), ('power', tensor([-20.4662], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.09944086521863937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6523, 5.6523, 5.6523],
        [5.6523, 5.6529, 5.6524],
        [5.6523, 6.3914, 6.5492],
        [5.6523, 5.6523, 5.6523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.0990639254450798 
model_pd.l_d.mean(): -11.371404647827148 
model_pd.lagr.mean(): -11.272340774536133 
model_pd.lambdas: dict_items([('pout', tensor([1.4884], device='cuda:0')), ('power', tensor([0.6469], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2679], device='cuda:0')), ('power', tensor([-20.4613], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.0990639254450798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6608, 5.6609, 5.6608],
        [5.6608, 7.3201, 8.4500],
        [5.6608, 5.7909, 5.7197],
        [5.6608, 5.6783, 5.6631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.09866970032453537 
model_pd.l_d.mean(): -11.348514556884766 
model_pd.lagr.mean(): -11.249844551086426 
model_pd.lambdas: dict_items([('pout', tensor([1.4897], device='cuda:0')), ('power', tensor([0.6459], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2660], device='cuda:0')), ('power', tensor([-20.4562], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.09866970032453537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6697, 6.9754, 7.6791],
        [5.6697, 6.1510, 6.1406],
        [5.6697, 5.6702, 5.6697],
        [5.6697, 5.6697, 5.6697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.09825552999973297 
model_pd.l_d.mean(): -11.3256196975708 
model_pd.lagr.mean(): -11.227364540100098 
model_pd.lambdas: dict_items([('pout', tensor([1.4909], device='cuda:0')), ('power', tensor([0.6448], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2639], device='cuda:0')), ('power', tensor([-20.4510], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.09825552999973297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6789, 6.3695, 6.4886],
        [5.6789, 5.7089, 5.6843],
        [5.6789, 5.6794, 5.6789],
        [5.6789, 5.6805, 5.6789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.09781816601753235 
model_pd.l_d.mean(): -11.302718162536621 
model_pd.lagr.mean(): -11.204899787902832 
model_pd.lambdas: dict_items([('pout', tensor([1.4922], device='cuda:0')), ('power', tensor([0.6438], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2618], device='cuda:0')), ('power', tensor([-20.4455], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.09781816601753235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]], device='cuda:0')
 pt:tensor([[5.6885, 5.9841, 5.9083],
        [5.6885, 5.8796, 5.7979],
        [5.6885, 6.6625, 7.0199],
        [5.6885, 6.9725, 7.6491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.09735363721847534 
model_pd.l_d.mean(): -11.27980899810791 
model_pd.lagr.mean(): -11.182455062866211 
model_pd.lambdas: dict_items([('pout', tensor([1.4935], device='cuda:0')), ('power', tensor([0.6428], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2596], device='cuda:0')), ('power', tensor([-20.4397], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.09735363721847534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.6986, 7.2480, 8.2331],
        [5.6986, 7.0119, 7.7196],
        [5.6986, 5.7566, 5.7144],
        [5.6986, 6.1827, 6.1721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.0968567281961441 
model_pd.l_d.mean(): -11.256890296936035 
model_pd.lagr.mean(): -11.160033226013184 
model_pd.lambdas: dict_items([('pout', tensor([1.4947], device='cuda:0')), ('power', tensor([0.6418], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2573], device='cuda:0')), ('power', tensor([-20.4336], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.0968567281961441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7094, 6.4567, 6.6162],
        [5.7094, 5.7094, 5.7094],
        [5.7094, 5.8710, 5.7926],
        [5.7094, 5.7094, 5.7094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.09632080048322678 
model_pd.l_d.mean(): -11.233951568603516 
model_pd.lagr.mean(): -11.137630462646484 
model_pd.lambdas: dict_items([('pout', tensor([1.4960], device='cuda:0')), ('power', tensor([0.6408], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2548], device='cuda:0')), ('power', tensor([-20.4271], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.09632080048322678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7208, 6.5141, 6.7091],
        [5.7208, 5.8524, 5.7804],
        [5.7208, 5.7785, 5.7364],
        [5.7208, 7.2771, 8.2667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.09573695063591003 
model_pd.l_d.mean(): -11.210991859436035 
model_pd.lagr.mean(): -11.115255355834961 
model_pd.lambdas: dict_items([('pout', tensor([1.4972], device='cuda:0')), ('power', tensor([0.6397], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2522], device='cuda:0')), ('power', tensor([-20.4201], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.09573695063591003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7331, 5.9260, 5.8435],
        [5.7331, 6.0609, 5.9904],
        [5.7331, 6.4313, 6.5516],
        [5.7331, 5.7897, 5.7482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.09509298950433731 
model_pd.l_d.mean(): -11.188000679016113 
model_pd.lagr.mean(): -11.092907905578613 
model_pd.lambdas: dict_items([('pout', tensor([1.4985], device='cuda:0')), ('power', tensor([0.6387], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2494], device='cuda:0')), ('power', tensor([-20.4126], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.09509298950433731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7465, 5.7465, 5.7465],
        [5.7465, 6.7248, 7.0795],
        [5.7465, 5.7466, 5.7465],
        [5.7465, 6.2682, 6.2746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.09437215328216553 
model_pd.l_d.mean(): -11.16496467590332 
model_pd.lagr.mean(): -11.070592880249023 
model_pd.lambdas: dict_items([('pout', tensor([1.4997], device='cuda:0')), ('power', tensor([0.6377], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2463], device='cuda:0')), ('power', tensor([-20.4045], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.09437215328216553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7613, 5.8203, 5.7774],
        [5.7613, 5.7629, 5.7613],
        [5.7613, 5.7957, 5.7680],
        [5.7613, 6.4635, 6.5845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.0935497060418129 
model_pd.l_d.mean(): -11.14186954498291 
model_pd.lagr.mean(): -11.048319816589355 
model_pd.lambdas: dict_items([('pout', tensor([1.5010], device='cuda:0')), ('power', tensor([0.6367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2429], device='cuda:0')), ('power', tensor([-20.3954], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.0935497060418129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7778, 5.7780, 5.7778],
        [5.7778, 5.7782, 5.7778],
        [5.7778, 5.7778, 5.7778],
        [5.7778, 6.1910, 6.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.09258829802274704 
model_pd.l_d.mean(): -11.118685722351074 
model_pd.lagr.mean(): -11.026097297668457 
model_pd.lambdas: dict_items([('pout', tensor([1.5022], device='cuda:0')), ('power', tensor([0.6357], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2392], device='cuda:0')), ('power', tensor([-20.3852], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.09258829802274704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7967, 6.3238, 6.3303],
        [5.7967, 6.1129, 6.0380],
        [5.7967, 5.8562, 5.8129],
        [5.7967, 5.7973, 5.7967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.09142772108316422 
model_pd.l_d.mean(): -11.095378875732422 
model_pd.lagr.mean(): -11.003951072692871 
model_pd.lambdas: dict_items([('pout', tensor([1.5034], device='cuda:0')), ('power', tensor([0.6346], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2349], device='cuda:0')), ('power', tensor([-20.3735], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.09142772108316422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8189, 5.8370, 5.8213],
        [5.8189, 5.8537, 5.8257],
        [5.8189, 5.8189, 5.8189],
        [5.8189, 6.3314, 6.3289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.08996396511793137 
model_pd.l_d.mean(): -11.071880340576172 
model_pd.lagr.mean(): -10.981916427612305 
model_pd.lambdas: dict_items([('pout', tensor([1.5047], device='cuda:0')), ('power', tensor([0.6336], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2299], device='cuda:0')), ('power', tensor([-20.3597], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.08996396511793137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8460, 5.8461, 5.8460],
        [5.8460, 5.8460, 5.8460],
        [5.8460, 7.4432, 8.4589],
        [5.8460, 5.8810, 5.8528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.08799762278795242 
model_pd.l_d.mean(): -11.04808521270752 
model_pd.lagr.mean(): -10.960087776184082 
model_pd.lambdas: dict_items([('pout', tensor([1.5059], device='cuda:0')), ('power', tensor([0.6326], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2238], device='cuda:0')), ('power', tensor([-20.3427], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.08799762278795242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8806, 5.8808, 5.8806],
        [5.8806, 5.8989, 5.8830],
        [5.8806, 6.0796, 5.9945],
        [5.8806, 5.8806, 5.8806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.08508506417274475 
model_pd.l_d.mean(): -11.023782730102539 
model_pd.lagr.mean(): -10.938697814941406 
model_pd.lambdas: dict_items([('pout', tensor([1.5071], device='cuda:0')), ('power', tensor([0.6316], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2160], device='cuda:0')), ('power', tensor([-20.3209], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.08508506417274475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.9280, 6.7105, 6.8777],
        [5.9280, 6.2697, 6.1964],
        [5.9280, 6.0974, 6.0153],
        [5.9280, 6.2533, 6.1763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.07995450496673584 
model_pd.l_d.mean(): -10.99852466583252 
model_pd.lagr.mean(): -10.918570518493652 
model_pd.lambdas: dict_items([('pout', tensor([1.5083], device='cuda:0')), ('power', tensor([0.6306], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.2054], device='cuda:0')), ('power', tensor([-20.2907], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.07995450496673584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[6.0015, 6.2316, 6.1432],
        [6.0015, 6.3320, 6.2539],
        [6.0015, 6.1413, 6.0648],
        [6.0015, 6.4827, 6.4552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.06632125377655029 
model_pd.l_d.mean(): -10.971108436584473 
model_pd.lagr.mean(): -10.904787063598633 
model_pd.lambdas: dict_items([('pout', tensor([1.5095], device='cuda:0')), ('power', tensor([0.6296], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1892], device='cuda:0')), ('power', tensor([-20.2433], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.06632125377655029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1375, 6.1379, 6.1375],
        [6.1375, 6.1378, 6.1375],
        [6.1375, 7.2069, 7.6003],
        [6.1375, 6.1426, 6.1378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 1.5653427839279175 
model_pd.l_d.mean(): -10.937539100646973 
model_pd.lagr.mean(): -9.372196197509766 
model_pd.lambdas: dict_items([('pout', tensor([1.5107], device='cuda:0')), ('power', tensor([0.6285], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1597], device='cuda:0')), ('power', tensor([-20.1540], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:1.5653427839279175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[6.3255, 7.8889, 8.7764],
        [6.3255, 6.6100, 6.5169],
        [6.3255, 6.6972, 6.6180],
        [6.3255, 6.6794, 6.5962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.11818484961986542 
model_pd.l_d.mean(): -10.896232604980469 
model_pd.lagr.mean(): -10.778047561645508 
model_pd.lambdas: dict_items([('pout', tensor([1.5118], device='cuda:0')), ('power', tensor([0.6275], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1199], device='cuda:0')), ('power', tensor([-20.0272], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.11818484961986542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5079, 6.5772, 6.5269],
        [6.5079, 6.8933, 6.8115],
        [6.5079, 6.5079, 6.5079],
        [6.5079, 6.6382, 6.5609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1038711816072464 
model_pd.l_d.mean(): -10.852234840393066 
model_pd.lagr.mean(): -10.748363494873047 
model_pd.lambdas: dict_items([('pout', tensor([1.5129], device='cuda:0')), ('power', tensor([0.6266], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0825], device='cuda:0')), ('power', tensor([-19.9009], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1038711816072464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6291, 7.5298, 7.7245],
        [6.6291, 8.1780, 8.9979],
        [6.6291, 7.1216, 7.0690],
        [6.6291, 6.6506, 6.6319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.09922666102647781 
model_pd.l_d.mean(): -10.814348220825195 
model_pd.lagr.mean(): -10.715121269226074 
model_pd.lambdas: dict_items([('pout', tensor([1.5139], device='cuda:0')), ('power', tensor([0.6256], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0582], device='cuda:0')), ('power', tensor([-19.8153], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.09922666102647781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6829, 7.6460, 7.8856],
        [6.6829, 8.2793, 9.1437],
        [6.6829, 6.6829, 6.6829],
        [6.6829, 7.8715, 8.3109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.0976223349571228 
model_pd.l_d.mean(): -10.785673141479492 
model_pd.lagr.mean(): -10.688051223754883 
model_pd.lambdas: dict_items([('pout', tensor([1.5150], device='cuda:0')), ('power', tensor([0.6246], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0476], device='cuda:0')), ('power', tensor([-19.7769], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.0976223349571228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6748, 6.9797, 6.8802],
        [6.6748, 6.8095, 6.7296],
        [6.6748, 6.6755, 6.6748],
        [6.6748, 8.2367, 9.0636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.09785059094429016 
model_pd.l_d.mean(): -10.766220092773438 
model_pd.lagr.mean(): -10.66836929321289 
model_pd.lambdas: dict_items([('pout', tensor([1.5160], device='cuda:0')), ('power', tensor([0.6236], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0492], device='cuda:0')), ('power', tensor([-19.7826], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.09785059094429016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6151, 6.6152, 6.6151],
        [6.6151, 8.2664, 9.2050],
        [6.6151, 6.6855, 6.6343],
        [6.6151, 7.4505, 7.5970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.0996817946434021 
model_pd.l_d.mean(): -10.754178047180176 
model_pd.lagr.mean(): -10.654496192932129 
model_pd.lambdas: dict_items([('pout', tensor([1.5171], device='cuda:0')), ('power', tensor([0.6226], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0610], device='cuda:0')), ('power', tensor([-19.8252], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.0996817946434021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5162, 7.6599, 8.0773],
        [6.5162, 6.5852, 6.5350],
        [6.5162, 6.5239, 6.5168],
        [6.5162, 8.4843, 9.8284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.10349246859550476 
model_pd.l_d.mean(): -10.746817588806152 
model_pd.lagr.mean(): -10.643324851989746 
model_pd.lambdas: dict_items([('pout', tensor([1.5182], device='cuda:0')), ('power', tensor([0.6216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0808], device='cuda:0')), ('power', tensor([-19.8951], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.10349246859550476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3930, 7.5194, 7.9354],
        [6.3930, 6.3930, 6.3930],
        [6.3930, 6.6429, 6.5472],
        [6.3930, 6.3930, 6.3930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.1109699010848999 
model_pd.l_d.mean(): -10.741034507751465 
model_pd.lagr.mean(): -10.630064964294434 
model_pd.lambdas: dict_items([('pout', tensor([1.5193], device='cuda:0')), ('power', tensor([0.6206], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1060], device='cuda:0')), ('power', tensor([-19.9809], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.1109699010848999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2691, 6.2764, 6.2696],
        [6.2691, 6.2786, 6.2699],
        [6.2691, 6.5502, 6.4581],
        [6.2691, 7.3683, 7.7739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.12906886637210846 
model_pd.l_d.mean(): -10.733317375183105 
model_pd.lagr.mean(): -10.604248046875 
model_pd.lambdas: dict_items([('pout', tensor([1.5204], device='cuda:0')), ('power', tensor([0.6196], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1317], device='cuda:0')), ('power', tensor([-20.0657], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.12906886637210846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2242, 6.2248, 6.2242],
        [6.2242, 6.2885, 6.2416],
        [6.2242, 6.4385, 6.3471],
        [6.2242, 6.5554, 6.4708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.1466909497976303 
model_pd.l_d.mean(): -10.716318130493164 
model_pd.lagr.mean(): -10.569626808166504 
model_pd.lambdas: dict_items([('pout', tensor([1.5215], device='cuda:0')), ('power', tensor([0.6186], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1412], device='cuda:0')), ('power', tensor([-20.0961], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.1466909497976303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]], device='cuda:0')
 pt:tensor([[6.2456, 6.4160, 6.3302],
        [6.2456, 6.8064, 6.8046],
        [6.2456, 6.6111, 6.5331],
        [6.2456, 6.5782, 6.4933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.1365911215543747 
model_pd.l_d.mean(): -10.692769050598145 
model_pd.lagr.mean(): -10.556178092956543 
model_pd.lambdas: dict_items([('pout', tensor([1.5227], device='cuda:0')), ('power', tensor([0.6176], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1367], device='cuda:0')), ('power', tensor([-20.0817], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.1365911215543747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]], device='cuda:0')
 pt:tensor([[6.3235, 7.1129, 7.2505],
        [6.3235, 6.6949, 6.6158],
        [6.3235, 7.3352, 7.6540],
        [6.3235, 6.4733, 6.3914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.11849655210971832 
model_pd.l_d.mean(): -10.663461685180664 
model_pd.lagr.mean(): -10.544964790344238 
model_pd.lambdas: dict_items([('pout', tensor([1.5238], device='cuda:0')), ('power', tensor([0.6166], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1203], device='cuda:0')), ('power', tensor([-20.0287], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.11849655210971832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4079, 6.4285, 6.4106],
        [6.4079, 6.6306, 6.5358],
        [6.4079, 8.3364, 9.6529],
        [6.4079, 6.4749, 6.4261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.10977786779403687 
model_pd.l_d.mean(): -10.632946968078613 
model_pd.lagr.mean(): -10.52316951751709 
model_pd.lambdas: dict_items([('pout', tensor([1.5249], device='cuda:0')), ('power', tensor([0.6156], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1029], device='cuda:0')), ('power', tensor([-19.9706], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.10977786779403687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[6.4699, 6.7238, 6.6266],
        [6.4699, 6.8342, 6.7487],
        [6.4699, 7.5109, 7.8396],
        [6.4699, 6.7628, 6.6670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.10579761117696762 
model_pd.l_d.mean(): -10.604608535766602 
model_pd.lagr.mean(): -10.498810768127441 
model_pd.lambdas: dict_items([('pout', tensor([1.5260], device='cuda:0')), ('power', tensor([0.6146], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0902], device='cuda:0')), ('power', tensor([-19.9275], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.10579761117696762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5011, 6.5018, 6.5011],
        [6.5011, 6.5011, 6.5011],
        [6.5011, 6.5088, 6.5017],
        [6.5011, 6.5011, 6.5011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.10419363528490067 
model_pd.l_d.mean(): -10.579815864562988 
model_pd.lagr.mean(): -10.475622177124023 
model_pd.lambdas: dict_items([('pout', tensor([1.5271], device='cuda:0')), ('power', tensor([0.6136], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0839], device='cuda:0')), ('power', tensor([-19.9057], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.10419363528490067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5013, 6.5020, 6.5013],
        [6.5013, 6.8863, 6.8046],
        [6.5013, 6.7282, 6.6316],
        [6.5013, 7.4321, 7.6631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.10418389737606049 
model_pd.l_d.mean(): -10.558805465698242 
model_pd.lagr.mean(): -10.454621315002441 
model_pd.lambdas: dict_items([('pout', tensor([1.5282], device='cuda:0')), ('power', tensor([0.6126], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0838], device='cuda:0')), ('power', tensor([-19.9055], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.10418389737606049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4741, 6.9522, 6.9009],
        [6.4741, 6.4748, 6.4742],
        [6.4741, 6.4950, 6.4769],
        [6.4741, 8.2844, 9.4391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.10556364059448242 
model_pd.l_d.mean(): -10.541032791137695 
model_pd.lagr.mean(): -10.435468673706055 
model_pd.lambdas: dict_items([('pout', tensor([1.5292], device='cuda:0')), ('power', tensor([0.6116], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0893], device='cuda:0')), ('power', tensor([-19.9245], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.10556364059448242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4262, 7.0080, 7.0065],
        [6.4262, 6.4361, 6.4271],
        [6.4262, 7.3436, 7.5711],
        [6.4262, 6.4615, 6.4327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.10845991224050522 
model_pd.l_d.mean(): -10.525430679321289 
model_pd.lagr.mean(): -10.416971206665039 
model_pd.lambdas: dict_items([('pout', tensor([1.5303], device='cuda:0')), ('power', tensor([0.6106], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0991], device='cuda:0')), ('power', tensor([-19.9579], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.10845991224050522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3674, 6.8354, 6.7850],
        [6.3674, 6.3674, 6.3674],
        [6.3674, 6.6544, 6.5605],
        [6.3674, 8.2811, 9.5874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11327485740184784 
model_pd.l_d.mean(): -10.510590553283691 
model_pd.lagr.mean(): -10.397315979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.5315], device='cuda:0')), ('power', tensor([0.6096], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1112], device='cuda:0')), ('power', tensor([-19.9985], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11327485740184784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3128, 6.6836, 6.6046],
        [6.3128, 6.9005, 6.9089],
        [6.3128, 7.4206, 7.8288],
        [6.3128, 6.3202, 6.3134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.12007804960012436 
model_pd.l_d.mean(): -10.494796752929688 
model_pd.lagr.mean(): -10.37471866607666 
model_pd.lambdas: dict_items([('pout', tensor([1.5326], device='cuda:0')), ('power', tensor([0.6086], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1226], device='cuda:0')), ('power', tensor([-20.0359], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.12007804960012436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.2866, 6.7974, 6.7689],
        [6.2866, 6.2868, 6.2866],
        [6.2866, 6.4353, 6.3540],
        [6.2866, 6.6222, 6.5366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.12488536536693573 
model_pd.l_d.mean(): -10.47594165802002 
model_pd.lagr.mean(): -10.351056098937988 
model_pd.lambdas: dict_items([('pout', tensor([1.5337], device='cuda:0')), ('power', tensor([0.6076], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1280], device='cuda:0')), ('power', tensor([-20.0538], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.12488536536693573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[6.3063, 8.1976, 9.4882],
        [6.3063, 6.6765, 6.5976],
        [6.3063, 7.3147, 7.6324],
        [6.3063, 7.8638, 8.7478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.12115456908941269 
model_pd.l_d.mean(): -10.452730178833008 
model_pd.lagr.mean(): -10.331575393676758 
model_pd.lambdas: dict_items([('pout', tensor([1.5348], device='cuda:0')), ('power', tensor([0.6066], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1239], device='cuda:0')), ('power', tensor([-20.0404], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.12115456908941269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3511, 6.4159, 6.3684],
        [6.3511, 6.9834, 7.0147],
        [6.3511, 7.4592, 7.8630],
        [6.3511, 6.4774, 6.4024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.11498630046844482 
model_pd.l_d.mean(): -10.427092552185059 
model_pd.lagr.mean(): -10.312106132507324 
model_pd.lambdas: dict_items([('pout', tensor([1.5359], device='cuda:0')), ('power', tensor([0.6056], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1146], device='cuda:0')), ('power', tensor([-20.0097], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.11498630046844482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3967, 6.4042, 6.3972],
        [6.3967, 7.2582, 7.4439],
        [6.3967, 6.3967, 6.3967],
        [6.3967, 6.3967, 6.3967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.11065208911895752 
model_pd.l_d.mean(): -10.401288032531738 
model_pd.lagr.mean(): -10.29063606262207 
model_pd.lambdas: dict_items([('pout', tensor([1.5371], device='cuda:0')), ('power', tensor([0.6046], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1052], device='cuda:0')), ('power', tensor([-19.9783], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.11065208911895752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4301, 7.0123, 7.0109],
        [6.4301, 6.4356, 6.4304],
        [6.4301, 6.9939, 6.9830],
        [6.4301, 6.7757, 6.6877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.10819673538208008 
model_pd.l_d.mean(): -10.376665115356445 
model_pd.lagr.mean(): -10.268468856811523 
model_pd.lambdas: dict_items([('pout', tensor([1.5382], device='cuda:0')), ('power', tensor([0.6036], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0983], device='cuda:0')), ('power', tensor([-19.9552], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.10819673538208008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4462, 7.5846, 8.0053],
        [6.4462, 6.4538, 6.4468],
        [6.4462, 6.7379, 6.6425],
        [6.4462, 6.8089, 6.7238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.10716284811496735 
model_pd.l_d.mean(): -10.35385513305664 
model_pd.lagr.mean(): -10.246692657470703 
model_pd.lambdas: dict_items([('pout', tensor([1.5392], device='cuda:0')), ('power', tensor([0.6026], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0950], device='cuda:0')), ('power', tensor([-19.9439], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.10716284811496735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4446, 6.5126, 6.4631],
        [6.4446, 6.7912, 6.7030],
        [6.4446, 7.0100, 6.9991],
        [6.4446, 6.4465, 6.4446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.10726599395275116 
model_pd.l_d.mean(): -10.3329439163208 
model_pd.lagr.mean(): -10.225677490234375 
model_pd.lambdas: dict_items([('pout', tensor([1.5403], device='cuda:0')), ('power', tensor([0.6016], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0954], device='cuda:0')), ('power', tensor([-19.9451], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.10726599395275116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4275, 6.4628, 6.4339],
        [6.4275, 6.4278, 6.4275],
        [6.4275, 6.4275, 6.4275],
        [6.4275, 6.6511, 6.5558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.1083729937672615 
model_pd.l_d.mean(): -10.313610076904297 
model_pd.lagr.mean(): -10.20523738861084 
model_pd.lambdas: dict_items([('pout', tensor([1.5414], device='cuda:0')), ('power', tensor([0.6006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0989], device='cuda:0')), ('power', tensor([-19.9570], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.1083729937672615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3996, 6.8707, 6.8200],
        [6.3996, 6.6500, 6.5541],
        [6.3996, 6.7431, 6.6557],
        [6.3996, 6.4001, 6.3996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.11041494458913803 
model_pd.l_d.mean(): -10.295258522033691 
model_pd.lagr.mean(): -10.184844017028809 
model_pd.lambdas: dict_items([('pout', tensor([1.5425], device='cuda:0')), ('power', tensor([0.5996], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1046], device='cuda:0')), ('power', tensor([-19.9763], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.11041494458913803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3679, 6.3682, 6.3679],
        [6.3679, 6.3698, 6.3680],
        [6.3679, 6.3679, 6.3679],
        [6.3679, 7.3887, 7.7106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.1132216826081276 
model_pd.l_d.mean(): -10.277109146118164 
model_pd.lagr.mean(): -10.163887023925781 
model_pd.lambdas: dict_items([('pout', tensor([1.5437], device='cuda:0')), ('power', tensor([0.5986], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1111], device='cuda:0')), ('power', tensor([-19.9981], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.1132216826081276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3412, 7.1336, 7.2718],
        [6.3412, 6.3411, 6.3412],
        [6.3412, 6.3418, 6.3412],
        [6.3412, 6.3412, 6.3412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.11614713072776794 
model_pd.l_d.mean(): -10.258317947387695 
model_pd.lagr.mean(): -10.142170906066895 
model_pd.lambdas: dict_items([('pout', tensor([1.5448], device='cuda:0')), ('power', tensor([0.5976], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1167], device='cuda:0')), ('power', tensor([-20.0165], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.11614713072776794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3287, 8.2283, 9.5247],
        [6.3287, 6.3290, 6.3287],
        [6.3287, 7.8931, 8.7811],
        [6.3287, 6.7006, 6.6214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.117750383913517 
model_pd.l_d.mean(): -10.238138198852539 
model_pd.lagr.mean(): -10.12038803100586 
model_pd.lambdas: dict_items([('pout', tensor([1.5459], device='cuda:0')), ('power', tensor([0.5966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1193], device='cuda:0')), ('power', tensor([-20.0251], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.117750383913517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3350, 6.6201, 6.5267],
        [6.3350, 6.9652, 6.9963],
        [6.3350, 6.3350, 6.3350],
        [6.3350, 6.3740, 6.3426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.11691942065954208 
model_pd.l_d.mean(): -10.216288566589355 
model_pd.lagr.mean(): -10.099369049072266 
model_pd.lambdas: dict_items([('pout', tensor([1.5470], device='cuda:0')), ('power', tensor([0.5956], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1179], device='cuda:0')), ('power', tensor([-20.0207], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.11691942065954208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3555, 6.3556, 6.3555],
        [6.3555, 6.4224, 6.3738],
        [6.3555, 6.3630, 6.3561],
        [6.3555, 6.5412, 6.4514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.11449690908193588 
model_pd.l_d.mean(): -10.193198204040527 
model_pd.lagr.mean(): -10.07870101928711 
model_pd.lambdas: dict_items([('pout', tensor([1.5481], device='cuda:0')), ('power', tensor([0.5946], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1137], device='cuda:0')), ('power', tensor([-20.0066], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.11449690908193588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3812, 7.8596, 8.6411],
        [6.3812, 6.7392, 6.6551],
        [6.3812, 6.3831, 6.3813],
        [6.3812, 6.9396, 6.9287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.11196793615818024 
model_pd.l_d.mean(): -10.169656753540039 
model_pd.lagr.mean(): -10.05768871307373 
model_pd.lambdas: dict_items([('pout', tensor([1.5492], device='cuda:0')), ('power', tensor([0.5936], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1084], device='cuda:0')), ('power', tensor([-19.9890], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.11196793615818024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4042, 7.3176, 7.5441],
        [6.4042, 7.2669, 7.4528],
        [6.4042, 7.5320, 7.9479],
        [6.4042, 6.4140, 6.4050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.11005963385105133 
model_pd.l_d.mean(): -10.146371841430664 
model_pd.lagr.mean(): -10.036312103271484 
model_pd.lambdas: dict_items([('pout', tensor([1.5503], device='cuda:0')), ('power', tensor([0.5926], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1036], device='cuda:0')), ('power', tensor([-19.9731], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.11005963385105133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4195, 6.7643, 6.6766],
        [6.4195, 6.4195, 6.4195],
        [6.4195, 7.0198, 7.0287],
        [6.4195, 7.5424, 7.9518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.1089254766702652 
model_pd.l_d.mean(): -10.123799324035645 
model_pd.lagr.mean(): -10.014873504638672 
model_pd.lambdas: dict_items([('pout', tensor([1.5514], device='cuda:0')), ('power', tensor([0.5916], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1005], device='cuda:0')), ('power', tensor([-19.9625], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.1089254766702652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4253, 6.4253, 6.4253],
        [6.4253, 6.8987, 6.8478],
        [6.4253, 6.4253, 6.4253],
        [6.4253, 7.3425, 7.5699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.10852449387311935 
model_pd.l_d.mean(): -10.102132797241211 
model_pd.lagr.mean(): -9.993608474731445 
model_pd.lambdas: dict_items([('pout', tensor([1.5525], device='cuda:0')), ('power', tensor([0.5906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0993], device='cuda:0')), ('power', tensor([-19.9585], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.10852449387311935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4217, 7.0223, 7.0311],
        [6.4217, 6.5746, 6.4911],
        [6.4217, 6.4894, 6.4401],
        [6.4217, 8.3553, 9.6755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.10877269506454468 
model_pd.l_d.mean(): -10.08133602142334 
model_pd.lagr.mean(): -9.972563743591309 
model_pd.lambdas: dict_items([('pout', tensor([1.5536], device='cuda:0')), ('power', tensor([0.5896], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1000], device='cuda:0')), ('power', tensor([-19.9610], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.10877269506454468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4107, 6.4107, 6.4107],
        [6.4107, 7.9280, 8.7484],
        [6.4107, 7.4399, 7.7646],
        [6.4107, 6.4107, 6.4107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.10956815630197525 
model_pd.l_d.mean(): -10.06119155883789 
model_pd.lagr.mean(): -9.95162296295166 
model_pd.lambdas: dict_items([('pout', tensor([1.5547], device='cuda:0')), ('power', tensor([0.5886], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1023], device='cuda:0')), ('power', tensor([-19.9687], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.10956815630197525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3954, 7.5225, 7.9388],
        [6.3954, 8.1789, 9.3160],
        [6.3954, 6.4631, 6.4139],
        [6.3954, 7.8777, 8.6614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.11076009273529053 
model_pd.l_d.mean(): -10.041372299194336 
model_pd.lagr.mean(): -9.930612564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.5558], device='cuda:0')), ('power', tensor([0.5876], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1054], device='cuda:0')), ('power', tensor([-19.9792], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.11076009273529053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3799, 7.4941, 7.9002],
        [6.3799, 6.3853, 6.3802],
        [6.3799, 6.3803, 6.3799],
        [6.3799, 8.1582, 9.2918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11209262907505035 
model_pd.l_d.mean(): -10.021505355834961 
model_pd.lagr.mean(): -9.909412384033203 
model_pd.lambdas: dict_items([('pout', tensor([1.5570], device='cuda:0')), ('power', tensor([0.5866], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1086], device='cuda:0')), ('power', tensor([-19.9899], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11209262907505035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3684, 6.3888, 6.3711],
        [6.3684, 6.4357, 6.3868],
        [6.3684, 6.3684, 6.3684],
        [6.3684, 6.4334, 6.3858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.1131768599152565 
model_pd.l_d.mean(): -10.001241683959961 
model_pd.lagr.mean(): -9.88806438446045 
model_pd.lambdas: dict_items([('pout', tensor([1.5581], device='cuda:0')), ('power', tensor([0.5856], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1110], device='cuda:0')), ('power', tensor([-19.9978], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.1131768599152565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3641, 6.3660, 6.3642],
        [6.3641, 6.3845, 6.3668],
        [6.3641, 6.3641, 6.3641],
        [6.3641, 6.5388, 6.4509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.11360467970371246 
model_pd.l_d.mean(): -9.980359077453613 
model_pd.lagr.mean(): -9.866754531860352 
model_pd.lambdas: dict_items([('pout', tensor([1.5592], device='cuda:0')), ('power', tensor([0.5846], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1119], device='cuda:0')), ('power', tensor([-20.0008], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.11360467970371246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3680, 6.3778, 6.3689],
        [6.3680, 6.6550, 6.5611],
        [6.3680, 6.3684, 6.3680],
        [6.3680, 6.7092, 6.6223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.11321655660867691 
model_pd.l_d.mean(): -9.95881175994873 
model_pd.lagr.mean(): -9.845595359802246 
model_pd.lambdas: dict_items([('pout', tensor([1.5603], device='cuda:0')), ('power', tensor([0.5836], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1111], device='cuda:0')), ('power', tensor([-19.9981], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.11321655660867691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3784, 7.4012, 7.7236],
        [6.3784, 6.3803, 6.3785],
        [6.3784, 6.3882, 6.3793],
        [6.3784, 6.3784, 6.3784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.11223047226667404 
model_pd.l_d.mean(): -9.936755180358887 
model_pd.lagr.mean(): -9.824524879455566 
model_pd.lambdas: dict_items([('pout', tensor([1.5614], device='cuda:0')), ('power', tensor([0.5826], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1089], device='cuda:0')), ('power', tensor([-19.9909], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.11223047226667404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3921, 7.3032, 7.5289],
        [6.3921, 6.3920, 6.3920],
        [6.3921, 6.4593, 6.4104],
        [6.3921, 6.7506, 6.6664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.11104122549295425 
model_pd.l_d.mean(): -9.914459228515625 
model_pd.lagr.mean(): -9.803418159484863 
model_pd.lambdas: dict_items([('pout', tensor([1.5625], device='cuda:0')), ('power', tensor([0.5816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1061], device='cuda:0')), ('power', tensor([-19.9816], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.11104122549295425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4054, 6.5816, 6.4930],
        [6.4054, 7.5251, 7.9332],
        [6.4054, 6.5929, 6.5023],
        [6.4054, 6.4055, 6.4054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10996988415718079 
model_pd.l_d.mean(): -9.892195701599121 
model_pd.lagr.mean(): -9.782225608825684 
model_pd.lambdas: dict_items([('pout', tensor([1.5636], device='cuda:0')), ('power', tensor([0.5806], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1034], device='cuda:0')), ('power', tensor([-19.9723], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10996988415718079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4159, 6.9779, 6.9670],
        [6.4159, 6.4235, 6.4165],
        [6.4159, 6.4160, 6.4159],
        [6.4159, 7.5474, 7.9654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.10918845981359482 
model_pd.l_d.mean(): -9.870187759399414 
model_pd.lagr.mean(): -9.76099967956543 
model_pd.lambdas: dict_items([('pout', tensor([1.5647], device='cuda:0')), ('power', tensor([0.5796], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1012], device='cuda:0')), ('power', tensor([-19.9650], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.10918845981359482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4220, 6.4220, 6.4220],
        [6.4220, 6.6101, 6.5192],
        [6.4220, 6.4224, 6.4220],
        [6.4220, 6.9847, 6.9737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.10875601321458817 
model_pd.l_d.mean(): -9.848554611206055 
model_pd.lagr.mean(): -9.739798545837402 
model_pd.lambdas: dict_items([('pout', tensor([1.5658], device='cuda:0')), ('power', tensor([0.5786], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1000], device='cuda:0')), ('power', tensor([-19.9608], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.10875601321458817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]], device='cuda:0')
 pt:tensor([[6.4234, 7.0239, 7.0327],
        [6.4234, 7.3400, 7.5672],
        [6.4234, 6.9862, 6.9752],
        [6.4234, 7.0646, 7.0963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.10866543650627136 
model_pd.l_d.mean(): -9.827320098876953 
model_pd.lagr.mean(): -9.71865463256836 
model_pd.lambdas: dict_items([('pout', tensor([1.5669], device='cuda:0')), ('power', tensor([0.5776], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0997], device='cuda:0')), ('power', tensor([-19.9599], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.10866543650627136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4205, 7.3365, 7.5636],
        [6.4205, 7.9404, 8.7622],
        [6.4205, 6.6717, 6.5755],
        [6.4205, 6.4875, 6.4386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.1088690534234047 
model_pd.l_d.mean(): -9.806421279907227 
model_pd.lagr.mean(): -9.697552680969238 
model_pd.lambdas: dict_items([('pout', tensor([1.5680], device='cuda:0')), ('power', tensor([0.5766], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1003], device='cuda:0')), ('power', tensor([-19.9619], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.1088690534234047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]], device='cuda:0')
 pt:tensor([[6.4146, 6.6656, 6.5695],
        [6.4146, 6.6024, 6.5116],
        [6.4146, 7.3297, 7.5564],
        [6.4146, 6.7589, 6.6713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.10928705334663391 
model_pd.l_d.mean(): -9.785741806030273 
model_pd.lagr.mean(): -9.676454544067383 
model_pd.lambdas: dict_items([('pout', tensor([1.5691], device='cuda:0')), ('power', tensor([0.5756], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1015], device='cuda:0')), ('power', tensor([-19.9660], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.10928705334663391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4076, 6.5353, 6.4595],
        [6.4076, 6.4152, 6.4082],
        [6.4076, 7.5372, 7.9543],
        [6.4076, 6.7515, 6.6639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.10980776697397232 
model_pd.l_d.mean(): -9.765130043029785 
model_pd.lagr.mean(): -9.655322074890137 
model_pd.lambdas: dict_items([('pout', tensor([1.5702], device='cuda:0')), ('power', tensor([0.5746], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1029], device='cuda:0')), ('power', tensor([-19.9708], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.10980776697397232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4014, 6.4667, 6.4189],
        [6.4014, 8.1867, 9.3248],
        [6.4014, 7.5295, 7.9461],
        [6.4014, 6.9239, 6.8949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.11029387265443802 
model_pd.l_d.mean(): -9.744441986083984 
model_pd.lagr.mean(): -9.634147644042969 
model_pd.lambdas: dict_items([('pout', tensor([1.5713], device='cuda:0')), ('power', tensor([0.5736], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1042], device='cuda:0')), ('power', tensor([-19.9752], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.11029387265443802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3975, 6.4050, 6.3980],
        [6.3975, 8.3219, 9.6354],
        [6.3975, 6.9756, 6.9740],
        [6.3975, 6.9196, 6.8905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.11060810089111328 
model_pd.l_d.mean(): -9.72356128692627 
model_pd.lagr.mean(): -9.612953186035156 
model_pd.lambdas: dict_items([('pout', tensor([1.5724], device='cuda:0')), ('power', tensor([0.5726], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1050], device='cuda:0')), ('power', tensor([-19.9779], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.11060810089111328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3969, 7.5240, 7.9402],
        [6.3969, 6.4023, 6.3972],
        [6.3969, 8.1806, 9.3176],
        [6.3969, 6.3969, 6.3969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.11065520346164703 
model_pd.l_d.mean(): -9.702425956726074 
model_pd.lagr.mean(): -9.591771125793457 
model_pd.lambdas: dict_items([('pout', tensor([1.5735], device='cuda:0')), ('power', tensor([0.5716], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1052], device='cuda:0')), ('power', tensor([-19.9783], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.11065520346164703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3998, 7.4265, 7.7502],
        [6.3998, 6.4203, 6.4025],
        [6.3998, 6.5870, 6.4965],
        [6.3998, 6.4676, 6.4184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.11041992157697678 
model_pd.l_d.mean(): -9.681041717529297 
model_pd.lagr.mean(): -9.570621490478516 
model_pd.lambdas: dict_items([('pout', tensor([1.5746], device='cuda:0')), ('power', tensor([0.5706], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1046], device='cuda:0')), ('power', tensor([-19.9763], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.11041992157697678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4056, 6.4731, 6.4240],
        [6.4056, 7.5345, 7.9515],
        [6.4056, 6.7492, 6.6617],
        [6.4056, 7.4335, 7.7575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.10996860265731812 
model_pd.l_d.mean(): -9.65946102142334 
model_pd.lagr.mean(): -9.549492835998535 
model_pd.lambdas: dict_items([('pout', tensor([1.5757], device='cuda:0')), ('power', tensor([0.5696], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1034], device='cuda:0')), ('power', tensor([-19.9723], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.10996860265731812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4131, 7.4424, 7.7669],
        [6.4131, 6.4135, 6.4131],
        [6.4131, 6.7911, 6.7106],
        [6.4131, 6.4800, 6.4312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.10941130667924881 
model_pd.l_d.mean(): -9.637777328491211 
model_pd.lagr.mean(): -9.528366088867188 
model_pd.lambdas: dict_items([('pout', tensor([1.5768], device='cuda:0')), ('power', tensor([0.5686], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1018], device='cuda:0')), ('power', tensor([-19.9671], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.10941130667924881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[6.4208, 6.9452, 6.9161],
        [6.4208, 7.5517, 7.9686],
        [6.4208, 6.7654, 6.6776],
        [6.4208, 6.5974, 6.5085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.10885760933160782 
model_pd.l_d.mean(): -9.616086959838867 
model_pd.lagr.mean(): -9.507229804992676 
model_pd.lambdas: dict_items([('pout', tensor([1.5779], device='cuda:0')), ('power', tensor([0.5676], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.1003], device='cuda:0')), ('power', tensor([-19.9618], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.10885760933160782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4276, 6.6509, 6.5558],
        [6.4276, 6.4628, 6.4340],
        [6.4276, 6.4482, 6.4303],
        [6.4276, 6.9527, 6.9235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.10838863998651505 
model_pd.l_d.mean(): -9.594477653503418 
model_pd.lagr.mean(): -9.486088752746582 
model_pd.lambdas: dict_items([('pout', tensor([1.5790], device='cuda:0')), ('power', tensor([0.5666], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0989], device='cuda:0')), ('power', tensor([-19.9571], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.10838863998651505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4327, 6.4346, 6.4327],
        [6.4327, 6.5008, 6.4513],
        [6.4327, 6.4328, 6.4327],
        [6.4327, 6.4334, 6.4327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.10805007815361023 
model_pd.l_d.mean(): -9.573002815246582 
model_pd.lagr.mean(): -9.46495246887207 
model_pd.lambdas: dict_items([('pout', tensor([1.5801], device='cuda:0')), ('power', tensor([0.5656], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0978], device='cuda:0')), ('power', tensor([-19.9536], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.10805007815361023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4356, 8.3738, 9.6968],
        [6.4356, 6.5035, 6.4541],
        [6.4356, 6.4359, 6.4356],
        [6.4356, 7.0374, 7.0461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.10785671323537827 
model_pd.l_d.mean(): -9.551687240600586 
model_pd.lagr.mean(): -9.443830490112305 
model_pd.lambdas: dict_items([('pout', tensor([1.5812], device='cuda:0')), ('power', tensor([0.5646], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0972], device='cuda:0')), ('power', tensor([-19.9515], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.10785671323537827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4366, 6.4368, 6.4366],
        [6.4366, 7.9299, 8.7194],
        [6.4366, 6.5649, 6.4887],
        [6.4366, 6.4372, 6.4366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.10779890418052673 
model_pd.l_d.mean(): -9.530518531799316 
model_pd.lagr.mean(): -9.422719955444336 
model_pd.lambdas: dict_items([('pout', tensor([1.5823], device='cuda:0')), ('power', tensor([0.5636], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0970], device='cuda:0')), ('power', tensor([-19.9509], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.10779890418052673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4358, 6.4358, 6.4358],
        [6.4358, 6.9997, 6.9886],
        [6.4358, 6.4711, 6.4422],
        [6.4358, 7.5698, 7.9879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.1078488677740097 
model_pd.l_d.mean(): -9.509466171264648 
model_pd.lagr.mean(): -9.401617050170898 
model_pd.lambdas: dict_items([('pout', tensor([1.5834], device='cuda:0')), ('power', tensor([0.5627], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0972], device='cuda:0')), ('power', tensor([-19.9514], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.1078488677740097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4340, 7.0762, 7.1079],
        [6.4340, 7.9266, 8.7157],
        [6.4340, 6.9977, 6.9866],
        [6.4340, 6.4341, 6.4340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.10796564072370529 
model_pd.l_d.mean(): -9.488479614257812 
model_pd.lagr.mean(): -9.380514144897461 
model_pd.lambdas: dict_items([('pout', tensor([1.5845], device='cuda:0')), ('power', tensor([0.5617], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0976], device='cuda:0')), ('power', tensor([-19.9526], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.10796564072370529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4320, 6.4327, 6.4320],
        [6.4320, 6.5001, 6.4506],
        [6.4320, 6.4526, 6.4347],
        [6.4320, 8.3687, 9.6906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1081007868051529 
model_pd.l_d.mean(): -9.467506408691406 
model_pd.lagr.mean(): -9.359405517578125 
model_pd.lambdas: dict_items([('pout', tensor([1.5856], device='cuda:0')), ('power', tensor([0.5607], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0980], device='cuda:0')), ('power', tensor([-19.9541], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1081007868051529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4304, 6.6187, 6.5277],
        [6.4304, 6.4304, 6.4305],
        [6.4304, 6.9038, 6.8528],
        [6.4304, 7.5632, 7.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.108205646276474 
model_pd.l_d.mean(): -9.446495056152344 
model_pd.lagr.mean(): -9.338289260864258 
model_pd.lambdas: dict_items([('pout', tensor([1.5867], device='cuda:0')), ('power', tensor([0.5597], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0983], device='cuda:0')), ('power', tensor([-19.9552], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.108205646276474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]], device='cuda:0')
 pt:tensor([[6.4300, 6.9931, 6.9820],
        [6.4300, 6.6533, 6.5581],
        [6.4300, 6.8091, 6.7283],
        [6.4300, 6.5829, 6.4993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.10824010521173477 
model_pd.l_d.mean(): -9.42541217803955 
model_pd.lagr.mean(): -9.317172050476074 
model_pd.lambdas: dict_items([('pout', tensor([1.5878], device='cuda:0')), ('power', tensor([0.5587], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0984], device='cuda:0')), ('power', tensor([-19.9555], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.10824010521173477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4309, 6.4309, 6.4309],
        [6.4309, 6.4980, 6.4491],
        [6.4309, 6.4312, 6.4309],
        [6.4309, 6.4310, 6.4309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.10818076133728027 
model_pd.l_d.mean(): -9.404239654541016 
model_pd.lagr.mean(): -9.296058654785156 
model_pd.lambdas: dict_items([('pout', tensor([1.5889], device='cuda:0')), ('power', tensor([0.5577], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0982], device='cuda:0')), ('power', tensor([-19.9549], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.10818076133728027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[6.4332, 7.3000, 7.4867],
        [6.4332, 7.0752, 7.1068],
        [6.4332, 8.3702, 9.6923],
        [6.4332, 6.9967, 6.9856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.10802503675222397 
model_pd.l_d.mean(): -9.382979393005371 
model_pd.lagr.mean(): -9.274954795837402 
model_pd.lambdas: dict_items([('pout', tensor([1.5900], device='cuda:0')), ('power', tensor([0.5567], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0977], device='cuda:0')), ('power', tensor([-19.9533], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.10802503675222397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4369, 6.4372, 6.4369],
        [6.4369, 6.4423, 6.4372],
        [6.4369, 6.4370, 6.4369],
        [6.4369, 6.4575, 6.4396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.10778976231813431 
model_pd.l_d.mean(): -9.361649513244629 
model_pd.lagr.mean(): -9.253859519958496 
model_pd.lambdas: dict_items([('pout', tensor([1.5911], device='cuda:0')), ('power', tensor([0.5557], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0970], device='cuda:0')), ('power', tensor([-19.9507], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.10778976231813431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4414, 6.5072, 6.4590],
        [6.4414, 6.9156, 6.8645],
        [6.4414, 7.5680, 7.9786],
        [6.4414, 7.0241, 7.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.1075039803981781 
model_pd.l_d.mean(): -9.340271949768066 
model_pd.lagr.mean(): -9.232768058776855 
model_pd.lambdas: dict_items([('pout', tensor([1.5922], device='cuda:0')), ('power', tensor([0.5547], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0961], device='cuda:0')), ('power', tensor([-19.9476], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.1075039803981781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4463, 6.4467, 6.4463],
        [6.4463, 6.9209, 6.8698],
        [6.4463, 6.4463, 6.4463],
        [6.4463, 7.9420, 8.7327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1072009801864624 
model_pd.l_d.mean(): -9.318883895874023 
model_pd.lagr.mean(): -9.21168327331543 
model_pd.lambdas: dict_items([('pout', tensor([1.5933], device='cuda:0')), ('power', tensor([0.5537], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0951], device='cuda:0')), ('power', tensor([-19.9442], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1072009801864624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4511, 6.5184, 6.4693],
        [6.4511, 6.4510, 6.4511],
        [6.4511, 7.0163, 7.0052],
        [6.4511, 6.7035, 6.6068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.10691051185131073 
model_pd.l_d.mean(): -9.297511100769043 
model_pd.lagr.mean(): -9.190600395202637 
model_pd.lambdas: dict_items([('pout', tensor([1.5944], device='cuda:0')), ('power', tensor([0.5527], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0941], device='cuda:0')), ('power', tensor([-19.9409], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.10691051185131073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4554, 7.0396, 7.0380],
        [6.4554, 7.0999, 7.1318],
        [6.4554, 6.4609, 6.4557],
        [6.4554, 7.0211, 7.0100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.10665477812290192 
model_pd.l_d.mean(): -9.276176452636719 
model_pd.lagr.mean(): -9.16952133178711 
model_pd.lambdas: dict_items([('pout', tensor([1.5955], device='cuda:0')), ('power', tensor([0.5517], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0932], device='cuda:0')), ('power', tensor([-19.9379], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.10665477812290192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4590, 6.5274, 6.4777],
        [6.4590, 6.4609, 6.4590],
        [6.4590, 6.5250, 6.4766],
        [6.4590, 7.2689, 7.4101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.10644654929637909 
model_pd.l_d.mean(): -9.254897117614746 
model_pd.lagr.mean(): -9.14845085144043 
model_pd.lambdas: dict_items([('pout', tensor([1.5966], device='cuda:0')), ('power', tensor([0.5507], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0925], device='cuda:0')), ('power', tensor([-19.9354], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.10644654929637909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4617, 6.4617, 6.4617],
        [6.4617, 7.0467, 7.0450],
        [6.4617, 6.5298, 6.4803],
        [6.4617, 6.4622, 6.4617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10628952085971832 
model_pd.l_d.mean(): -9.233674049377441 
model_pd.lagr.mean(): -9.127384185791016 
model_pd.lambdas: dict_items([('pout', tensor([1.5977], device='cuda:0')), ('power', tensor([0.5497], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0919], device='cuda:0')), ('power', tensor([-19.9335], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10628952085971832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4637, 7.2742, 7.4156],
        [6.4637, 6.6175, 6.5335],
        [6.4637, 6.4692, 6.4640],
        [6.4637, 6.6416, 6.5521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.106179378926754 
model_pd.l_d.mean(): -9.212503433227539 
model_pd.lagr.mean(): -9.106324195861816 
model_pd.lambdas: dict_items([('pout', tensor([1.5988], device='cuda:0')), ('power', tensor([0.5487], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0915], device='cuda:0')), ('power', tensor([-19.9322], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.106179378926754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4650, 8.0695, 8.9803],
        [6.4650, 6.6544, 6.5628],
        [6.4650, 6.8282, 6.7429],
        [6.4650, 6.4669, 6.4651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.10610619187355042 
model_pd.l_d.mean(): -9.191375732421875 
model_pd.lagr.mean(): -9.085269927978516 
model_pd.lambdas: dict_items([('pout', tensor([1.5999], device='cuda:0')), ('power', tensor([0.5477], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0913], device='cuda:0')), ('power', tensor([-19.9312], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.10610619187355042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4659, 6.9945, 6.9651],
        [6.4659, 6.4659, 6.4659],
        [6.4659, 6.4714, 6.4663],
        [6.4659, 6.4662, 6.4659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.10605596005916595 
model_pd.l_d.mean(): -9.170272827148438 
model_pd.lagr.mean(): -9.064216613769531 
model_pd.lambdas: dict_items([('pout', tensor([1.6009], device='cuda:0')), ('power', tensor([0.5467], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0911], device='cuda:0')), ('power', tensor([-19.9306], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.10605596005916595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4667, 7.0521, 7.0504],
        [6.4667, 7.9679, 8.7615],
        [6.4667, 6.6915, 6.5957],
        [6.4667, 6.4722, 6.4670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.10601310431957245 
model_pd.l_d.mean(): -9.149179458618164 
model_pd.lagr.mean(): -9.043166160583496 
model_pd.lambdas: dict_items([('pout', tensor([1.6020], device='cuda:0')), ('power', tensor([0.5457], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0909], device='cuda:0')), ('power', tensor([-19.9301], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.10601310431957245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4676, 6.5031, 6.4741],
        [6.4676, 8.2743, 9.4260],
        [6.4676, 6.5358, 6.4862],
        [6.4676, 6.7208, 6.6239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.10596290230751038 
model_pd.l_d.mean(): -9.128080368041992 
model_pd.lagr.mean(): -9.022117614746094 
model_pd.lambdas: dict_items([('pout', tensor([1.6031], device='cuda:0')), ('power', tensor([0.5447], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0907], device='cuda:0')), ('power', tensor([-19.9294], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.10596290230751038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4689, 6.7611, 6.6655],
        [6.4689, 6.9455, 6.8941],
        [6.4689, 6.6584, 6.5668],
        [6.4689, 6.4765, 6.4695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.10589306801557541 
model_pd.l_d.mean(): -9.106965065002441 
model_pd.lagr.mean(): -9.00107192993164 
model_pd.lambdas: dict_items([('pout', tensor([1.6042], device='cuda:0')), ('power', tensor([0.5437], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0905], device='cuda:0')), ('power', tensor([-19.9286], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.10589306801557541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4707, 6.4712, 6.4707],
        [6.4707, 6.4708, 6.4707],
        [6.4707, 6.6488, 6.5592],
        [6.4707, 6.5998, 6.5231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.10579577833414078 
model_pd.l_d.mean(): -9.08582878112793 
model_pd.lagr.mean(): -8.980032920837402 
model_pd.lambdas: dict_items([('pout', tensor([1.6053], device='cuda:0')), ('power', tensor([0.5427], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0901], device='cuda:0')), ('power', tensor([-19.9273], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.10579577833414078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4731, 6.4731, 6.4731],
        [6.4731, 6.7655, 6.6698],
        [6.4731, 7.3461, 7.5340],
        [6.4731, 6.6981, 6.6022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.10566789656877518 
model_pd.l_d.mean(): -9.06466293334961 
model_pd.lagr.mean(): -8.95899486541748 
model_pd.lambdas: dict_items([('pout', tensor([1.6064], device='cuda:0')), ('power', tensor([0.5417], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0896], device='cuda:0')), ('power', tensor([-19.9257], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.10566789656877518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4760, 8.0107, 8.8402],
        [6.4760, 6.4760, 6.4760],
        [6.4760, 7.4008, 7.6298],
        [6.4760, 6.6657, 6.5740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.10551130026578903 
model_pd.l_d.mean(): -9.043476104736328 
model_pd.lagr.mean(): -8.93796443939209 
model_pd.lambdas: dict_items([('pout', tensor([1.6075], device='cuda:0')), ('power', tensor([0.5407], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0890], device='cuda:0')), ('power', tensor([-19.9236], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.10551130026578903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4794, 6.4798, 6.4794],
        [6.4794, 6.4797, 6.4794],
        [6.4794, 6.4799, 6.4794],
        [6.4794, 7.9840, 8.7792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.10533139854669571 
model_pd.l_d.mean(): -9.022270202636719 
model_pd.lagr.mean(): -8.916938781738281 
model_pd.lambdas: dict_items([('pout', tensor([1.6086], device='cuda:0')), ('power', tensor([0.5397], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0883], device='cuda:0')), ('power', tensor([-19.9213], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.10533139854669571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]], device='cuda:0')
 pt:tensor([[6.4832, 8.0198, 8.8505],
        [6.4832, 6.9609, 6.9094],
        [6.4832, 7.1308, 7.1628],
        [6.4832, 7.6279, 8.0505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10513637214899063 
model_pd.l_d.mean(): -9.001057624816895 
model_pd.lagr.mean(): -8.89592170715332 
model_pd.lambdas: dict_items([('pout', tensor([1.6097], device='cuda:0')), ('power', tensor([0.5387], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0876], device='cuda:0')), ('power', tensor([-19.9187], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10513637214899063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]], device='cuda:0')
 pt:tensor([[6.4871, 8.3001, 9.4558],
        [6.4871, 7.9938, 8.7901],
        [6.4871, 7.5300, 7.8588],
        [6.4871, 7.0942, 7.1029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.10493478178977966 
model_pd.l_d.mean(): -8.979843139648438 
model_pd.lagr.mean(): -8.874908447265625 
model_pd.lambdas: dict_items([('pout', tensor([1.6107], device='cuda:0')), ('power', tensor([0.5377], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0868], device='cuda:0')), ('power', tensor([-19.9159], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.10493478178977966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4911, 6.5595, 6.5097],
        [6.4911, 6.7169, 6.6207],
        [6.4911, 6.5267, 6.4976],
        [6.4911, 7.9988, 8.7958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.10473472625017166 
model_pd.l_d.mean(): -8.958635330200195 
model_pd.lagr.mean(): -8.853900909423828 
model_pd.lambdas: dict_items([('pout', tensor([1.6118], device='cuda:0')), ('power', tensor([0.5367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0860], device='cuda:0')), ('power', tensor([-19.9132], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.10473472625017166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4950, 8.0349, 8.8673],
        [6.4950, 7.0833, 7.0816],
        [6.4950, 6.7885, 6.6924],
        [6.4950, 6.5306, 6.5015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.10454278439283371 
model_pd.l_d.mean(): -8.937444686889648 
model_pd.lagr.mean(): -8.832901954650879 
model_pd.lambdas: dict_items([('pout', tensor([1.6129], device='cuda:0')), ('power', tensor([0.5357], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0852], device='cuda:0')), ('power', tensor([-19.9105], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.10454278439283371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4987, 7.1481, 7.1801],
        [6.4987, 8.0084, 8.8065],
        [6.4987, 6.4986, 6.4986],
        [6.4987, 7.4272, 7.6572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.1043633446097374 
model_pd.l_d.mean(): -8.916268348693848 
model_pd.lagr.mean(): -8.811904907226562 
model_pd.lambdas: dict_items([('pout', tensor([1.6140], device='cuda:0')), ('power', tensor([0.5347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0845], device='cuda:0')), ('power', tensor([-19.9079], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.1043633446097374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5021, 6.5021, 6.5021],
        [6.5021, 6.5021, 6.5021],
        [6.5021, 6.7568, 6.6593],
        [6.5021, 6.5707, 6.5208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.1041983962059021 
model_pd.l_d.mean(): -8.89511489868164 
model_pd.lagr.mean(): -8.790916442871094 
model_pd.lambdas: dict_items([('pout', tensor([1.6151], device='cuda:0')), ('power', tensor([0.5337], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0838], device='cuda:0')), ('power', tensor([-19.9055], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.1041983962059021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5053, 8.0480, 8.8819],
        [6.5053, 6.6351, 6.5580],
        [6.5053, 6.5053, 6.5053],
        [6.5053, 7.1555, 7.1875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.10404808074235916 
model_pd.l_d.mean(): -8.873982429504395 
model_pd.lagr.mean(): -8.76993465423584 
model_pd.lambdas: dict_items([('pout', tensor([1.6162], device='cuda:0')), ('power', tensor([0.5328], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0831], device='cuda:0')), ('power', tensor([-19.9033], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.10404808074235916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[6.5082, 7.6580, 8.0826],
        [6.5082, 8.0518, 8.8861],
        [6.5082, 6.9881, 6.9363],
        [6.5082, 7.0979, 7.0962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.10391052812337875 
model_pd.l_d.mean(): -8.852869033813477 
model_pd.lagr.mean(): -8.748958587646484 
model_pd.lambdas: dict_items([('pout', tensor([1.6173], device='cuda:0')), ('power', tensor([0.5318], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0825], device='cuda:0')), ('power', tensor([-19.9012], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.10391052812337875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5110, 6.5111, 6.5110],
        [6.5110, 8.1285, 9.0468],
        [6.5110, 7.3899, 7.5791],
        [6.5110, 6.8610, 6.7718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.10378272831439972 
model_pd.l_d.mean(): -8.831769943237305 
model_pd.lagr.mean(): -8.727987289428711 
model_pd.lambdas: dict_items([('pout', tensor([1.6183], device='cuda:0')), ('power', tensor([0.5308], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0820], device='cuda:0')), ('power', tensor([-19.8993], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.10378272831439972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5136, 6.5213, 6.5142],
        [6.5136, 6.6437, 6.5665],
        [6.5136, 6.8984, 6.8164],
        [6.5136, 6.5140, 6.5136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.10366076976060867 
model_pd.l_d.mean(): -8.810681343078613 
model_pd.lagr.mean(): -8.70702075958252 
model_pd.lambdas: dict_items([('pout', tensor([1.6194], device='cuda:0')), ('power', tensor([0.5298], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0814], device='cuda:0')), ('power', tensor([-19.8975], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.10366076976060867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5163, 6.9969, 6.9450],
        [6.5163, 7.1068, 7.1051],
        [6.5163, 6.5240, 6.5168],
        [6.5163, 6.6464, 6.5691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.10354074090719223 
model_pd.l_d.mean(): -8.789600372314453 
model_pd.lagr.mean(): -8.686059951782227 
model_pd.lambdas: dict_items([('pout', tensor([1.6205], device='cuda:0')), ('power', tensor([0.5288], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0809], device='cuda:0')), ('power', tensor([-19.8956], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.10354074090719223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5190, 6.5191, 6.5190],
        [6.5190, 7.6710, 8.0964],
        [6.5190, 7.1098, 7.1081],
        [6.5190, 6.5209, 6.5191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.10341876745223999 
model_pd.l_d.mean(): -8.768524169921875 
model_pd.lagr.mean(): -8.665105819702148 
model_pd.lambdas: dict_items([('pout', tensor([1.6216], device='cuda:0')), ('power', tensor([0.5278], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0804], device='cuda:0')), ('power', tensor([-19.8937], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.10341876745223999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5219, 6.5886, 6.5397],
        [6.5219, 7.0556, 7.0259],
        [6.5219, 6.7775, 6.6796],
        [6.5219, 6.5219, 6.5219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.10329188406467438 
model_pd.l_d.mean(): -8.74744701385498 
model_pd.lagr.mean(): -8.644155502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.6227], device='cuda:0')), ('power', tensor([0.5268], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0798], device='cuda:0')), ('power', tensor([-19.8917], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.10329188406467438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5249, 6.7163, 6.6237],
        [6.5249, 7.6781, 8.1039],
        [6.5249, 6.5256, 6.5249],
        [6.5249, 6.5250, 6.5249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.10315782576799393 
model_pd.l_d.mean(): -8.726370811462402 
model_pd.lagr.mean(): -8.623212814331055 
model_pd.lambdas: dict_items([('pout', tensor([1.6237], device='cuda:0')), ('power', tensor([0.5258], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0792], device='cuda:0')), ('power', tensor([-19.8896], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.10315782576799393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5282, 6.5282, 6.5282],
        [6.5282, 7.1397, 7.1484],
        [6.5282, 6.5359, 6.5288],
        [6.5282, 6.8793, 6.7898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.1030154600739479 
model_pd.l_d.mean(): -8.705289840698242 
model_pd.lagr.mean(): -8.602273941040039 
model_pd.lambdas: dict_items([('pout', tensor([1.6248], device='cuda:0')), ('power', tensor([0.5248], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0785], device='cuda:0')), ('power', tensor([-19.8873], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.1030154600739479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5317, 6.5318, 6.5317],
        [6.5317, 6.7591, 6.6622],
        [6.5317, 6.5317, 6.5317],
        [6.5317, 7.1238, 7.1221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.10286475718021393 
model_pd.l_d.mean(): -8.684208869934082 
model_pd.lagr.mean(): -8.581343650817871 
model_pd.lambdas: dict_items([('pout', tensor([1.6259], device='cuda:0')), ('power', tensor([0.5238], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0778], device='cuda:0')), ('power', tensor([-19.8848], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.10286475718021393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5354, 6.7917, 6.6935],
        [6.5354, 6.6044, 6.5542],
        [6.5354, 6.9032, 6.8167],
        [6.5354, 7.5873, 7.9188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.10270635038614273 
model_pd.l_d.mean(): -8.663125038146973 
model_pd.lagr.mean(): -8.560419082641602 
model_pd.lambdas: dict_items([('pout', tensor([1.6270], device='cuda:0')), ('power', tensor([0.5228], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0771], device='cuda:0')), ('power', tensor([-19.8822], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.10270635038614273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5394, 7.1935, 7.2257],
        [6.5394, 6.5397, 6.5394],
        [6.5394, 6.6088, 6.5584],
        [6.5394, 7.4227, 7.6128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.10254176706075668 
model_pd.l_d.mean(): -8.642045021057129 
model_pd.lagr.mean(): -8.53950309753418 
model_pd.lambdas: dict_items([('pout', tensor([1.6280], device='cuda:0')), ('power', tensor([0.5218], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0763], device='cuda:0')), ('power', tensor([-19.8795], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.10254176706075668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5435, 6.6129, 6.5625],
        [6.5435, 6.7713, 6.6742],
        [6.5435, 6.5840, 6.5514],
        [6.5435, 7.1981, 7.2303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.10237272083759308 
model_pd.l_d.mean(): -8.620966911315918 
model_pd.lagr.mean(): -8.518593788146973 
model_pd.lambdas: dict_items([('pout', tensor([1.6291], device='cuda:0')), ('power', tensor([0.5208], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0755], device='cuda:0')), ('power', tensor([-19.8766], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.10237272083759308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5477, 6.8045, 6.7061],
        [6.5477, 6.6147, 6.5656],
        [6.5477, 6.6162, 6.5663],
        [6.5477, 6.5477, 6.5477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.10220117121934891 
model_pd.l_d.mean(): -8.599891662597656 
model_pd.lagr.mean(): -8.497690200805664 
model_pd.lambdas: dict_items([('pout', tensor([1.6302], device='cuda:0')), ('power', tensor([0.5198], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0746], device='cuda:0')), ('power', tensor([-19.8736], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.10220117121934891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5520, 6.5520, 6.5520],
        [6.5520, 7.2076, 7.2399],
        [6.5520, 8.3859, 9.5549],
        [6.5520, 6.5524, 6.5520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.1020289957523346 
model_pd.l_d.mean(): -8.578825950622559 
model_pd.lagr.mean(): -8.476797103881836 
model_pd.lambdas: dict_items([('pout', tensor([1.6313], device='cuda:0')), ('power', tensor([0.5188], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0737], device='cuda:0')), ('power', tensor([-19.8706], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.1020289957523346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5563, 7.0933, 7.0634],
        [6.5563, 7.3802, 7.5238],
        [6.5563, 6.5773, 6.5591],
        [6.5563, 8.1130, 8.9545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.10185783356428146 
model_pd.l_d.mean(): -8.557766914367676 
model_pd.lagr.mean(): -8.45590877532959 
model_pd.lambdas: dict_items([('pout', tensor([1.6323], device='cuda:0')), ('power', tensor([0.5178], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0729], device='cuda:0')), ('power', tensor([-19.8676], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.10185783356428146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5606, 6.5609, 6.5606],
        [6.5606, 8.0870, 8.8939],
        [6.5606, 6.5707, 6.5615],
        [6.5606, 7.3851, 7.5289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.10168879479169846 
model_pd.l_d.mean(): -8.53671932220459 
model_pd.lagr.mean(): -8.435030937194824 
model_pd.lambdas: dict_items([('pout', tensor([1.6334], device='cuda:0')), ('power', tensor([0.5168], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0720], device='cuda:0')), ('power', tensor([-19.8646], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.10168879479169846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5649, 6.5653, 6.5649],
        [6.5649, 7.1606, 7.1588],
        [6.5649, 6.7577, 6.6645],
        [6.5649, 6.5750, 6.5658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.10152271389961243 
model_pd.l_d.mean(): -8.515680313110352 
model_pd.lagr.mean(): -8.41415786743164 
model_pd.lambdas: dict_items([('pout', tensor([1.6345], device='cuda:0')), ('power', tensor([0.5159], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0712], device='cuda:0')), ('power', tensor([-19.8615], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.10152271389961243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5692, 6.7981, 6.7006],
        [6.5692, 6.5903, 6.5720],
        [6.5692, 7.1464, 7.1350],
        [6.5692, 6.7621, 6.6688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.10135993361473083 
model_pd.l_d.mean(): -8.494653701782227 
model_pd.lagr.mean(): -8.393293380737305 
model_pd.lambdas: dict_items([('pout', tensor([1.6356], device='cuda:0')), ('power', tensor([0.5149], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0703], device='cuda:0')), ('power', tensor([-19.8585], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.10135993361473083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[6.5735, 7.1122, 7.0822],
        [6.5735, 7.2316, 7.2640],
        [6.5735, 6.8315, 6.7326],
        [6.5735, 6.7548, 6.6636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.1012004092335701 
model_pd.l_d.mean(): -8.473638534545898 
model_pd.lagr.mean(): -8.372438430786133 
model_pd.lambdas: dict_items([('pout', tensor([1.6366], device='cuda:0')), ('power', tensor([0.5139], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0694], device='cuda:0')), ('power', tensor([-19.8555], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.1012004092335701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5777, 6.6466, 6.5964],
        [6.5777, 6.6451, 6.5957],
        [6.5777, 7.6375, 7.9716],
        [6.5777, 6.5777, 6.5777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.1010437160730362 
model_pd.l_d.mean(): -8.452630996704102 
model_pd.lagr.mean(): -8.351587295532227 
model_pd.lambdas: dict_items([('pout', tensor([1.6377], device='cuda:0')), ('power', tensor([0.5129], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0686], device='cuda:0')), ('power', tensor([-19.8525], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.1010437160730362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5820, 7.0681, 7.0157],
        [6.5820, 6.7136, 6.6354],
        [6.5820, 7.7371, 8.1580],
        [6.5820, 6.5820, 6.5820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.10088920593261719 
model_pd.l_d.mean(): -8.431636810302734 
model_pd.lagr.mean(): -8.330747604370117 
model_pd.lambdas: dict_items([('pout', tensor([1.6388], device='cuda:0')), ('power', tensor([0.5119], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0678], device='cuda:0')), ('power', tensor([-19.8495], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.10088920593261719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5862, 6.5869, 6.5862],
        [6.5862, 7.1261, 7.0961],
        [6.5862, 6.5862, 6.5862],
        [6.5862, 7.5293, 7.7628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.10073617845773697 
model_pd.l_d.mean(): -8.410652160644531 
model_pd.lagr.mean(): -8.309915542602539 
model_pd.lambdas: dict_items([('pout', tensor([1.6398], device='cuda:0')), ('power', tensor([0.5109], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0669], device='cuda:0')), ('power', tensor([-19.8466], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.10073617845773697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5905, 6.9455, 6.8550],
        [6.5905, 8.2309, 9.1622],
        [6.5905, 6.6006, 6.5913],
        [6.5905, 6.7478, 6.6619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.1005837619304657 
model_pd.l_d.mean(): -8.38967514038086 
model_pd.lagr.mean(): -8.289091110229492 
model_pd.lambdas: dict_items([('pout', tensor([1.6409], device='cuda:0')), ('power', tensor([0.5099], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0661], device='cuda:0')), ('power', tensor([-19.8435], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.1005837619304657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5948, 6.6357, 6.6028],
        [6.5948, 7.5393, 7.7732],
        [6.5948, 7.2554, 7.2880],
        [6.5948, 6.5948, 6.5948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.10043124109506607 
model_pd.l_d.mean(): -8.368705749511719 
model_pd.lagr.mean(): -8.268274307250977 
model_pd.lambdas: dict_items([('pout', tensor([1.6420], device='cuda:0')), ('power', tensor([0.5089], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0652], device='cuda:0')), ('power', tensor([-19.8405], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.10043124109506607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5992, 6.9712, 6.8838],
        [6.5992, 6.8983, 6.8004],
        [6.5992, 6.7312, 6.6529],
        [6.5992, 6.6690, 6.6183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.10027781128883362 
model_pd.l_d.mean(): -8.34774398803711 
model_pd.lagr.mean(): -8.247466087341309 
model_pd.lambdas: dict_items([('pout', tensor([1.6430], device='cuda:0')), ('power', tensor([0.5079], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0643], device='cuda:0')), ('power', tensor([-19.8374], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.10027781128883362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6038, 8.2480, 9.1815],
        [6.6038, 6.6057, 6.6038],
        [6.6038, 6.7978, 6.7040],
        [6.6038, 6.9596, 6.8689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.10012298077344894 
model_pd.l_d.mean(): -8.326790809631348 
model_pd.lagr.mean(): -8.226667404174805 
model_pd.lambdas: dict_items([('pout', tensor([1.6441], device='cuda:0')), ('power', tensor([0.5069], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0634], device='cuda:0')), ('power', tensor([-19.8342], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.10012298077344894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6084, 6.9997, 6.9163],
        [6.6084, 8.4607, 9.6415],
        [6.6084, 8.2540, 9.1883],
        [6.6084, 6.6786, 6.6276]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.0999663844704628 
model_pd.l_d.mean(): -8.305842399597168 
model_pd.lagr.mean(): -8.205876350402832 
model_pd.lambdas: dict_items([('pout', tensor([1.6452], device='cuda:0')), ('power', tensor([0.5059], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0625], device='cuda:0')), ('power', tensor([-19.8309], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.0999663844704628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6131, 7.7831, 8.2144],
        [6.6131, 6.6810, 6.6313],
        [6.6131, 6.6131, 6.6131],
        [6.6131, 6.6131, 6.6131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.09980777651071548 
model_pd.l_d.mean(): -8.28490161895752 
model_pd.lagr.mean(): -8.185093879699707 
model_pd.lambdas: dict_items([('pout', tensor([1.6462], device='cuda:0')), ('power', tensor([0.5049], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0616], device='cuda:0')), ('power', tensor([-19.8275], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.09980777651071548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6180, 7.2393, 7.2482],
        [6.6180, 6.8489, 6.7505],
        [6.6180, 7.1073, 7.0545],
        [6.6180, 6.6859, 6.6361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.0996471419930458 
model_pd.l_d.mean(): -8.263967514038086 
model_pd.lagr.mean(): -8.16431999206543 
model_pd.lambdas: dict_items([('pout', tensor([1.6473], device='cuda:0')), ('power', tensor([0.5039], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0606], device='cuda:0')), ('power', tensor([-19.8241], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.0996471419930458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6230, 6.6230, 6.6230],
        [6.6230, 7.1127, 7.0599],
        [6.6230, 8.2728, 9.2095],
        [6.6230, 6.6237, 6.6230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.09948458522558212 
model_pd.l_d.mean(): -8.2430419921875 
model_pd.lagr.mean(): -8.14355754852295 
model_pd.lambdas: dict_items([('pout', tensor([1.6483], device='cuda:0')), ('power', tensor([0.5030], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0596], device='cuda:0')), ('power', tensor([-19.8206], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.09948458522558212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6281, 6.9287, 6.8303],
        [6.6281, 6.6288, 6.6281],
        [6.6281, 6.6281, 6.6281],
        [6.6281, 7.2113, 7.1998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.09932034462690353 
model_pd.l_d.mean(): -8.222122192382812 
model_pd.lagr.mean(): -8.122801780700684 
model_pd.lambdas: dict_items([('pout', tensor([1.6494], device='cuda:0')), ('power', tensor([0.5020], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0586], device='cuda:0')), ('power', tensor([-19.8169], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.09932034462690353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6333, 7.1240, 7.0711],
        [6.6333, 6.7035, 6.6524],
        [6.6333, 7.0075, 6.9196],
        [6.6333, 7.7989, 8.2237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.09915472567081451 
model_pd.l_d.mean(): -8.201210975646973 
model_pd.lagr.mean(): -8.102056503295898 
model_pd.lambdas: dict_items([('pout', tensor([1.6505], device='cuda:0')), ('power', tensor([0.5010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0576], device='cuda:0')), ('power', tensor([-19.8132], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.09915472567081451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6386, 6.6752, 6.6453],
        [6.6386, 7.2230, 7.2115],
        [6.6386, 6.7973, 6.7106],
        [6.6386, 6.6386, 6.6386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.09898810088634491 
model_pd.l_d.mean(): -8.18030834197998 
model_pd.lagr.mean(): -8.081319808959961 
model_pd.lambdas: dict_items([('pout', tensor([1.6515], device='cuda:0')), ('power', tensor([0.5000], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0565], device='cuda:0')), ('power', tensor([-19.8095], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.09898810088634491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6440, 6.6443, 6.6440],
        [6.6440, 6.7771, 6.6981],
        [6.6440, 6.6444, 6.6440],
        [6.6440, 7.5968, 7.8328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.09882083535194397 
model_pd.l_d.mean(): -8.159416198730469 
model_pd.lagr.mean(): -8.060595512390137 
model_pd.lambdas: dict_items([('pout', tensor([1.6526], device='cuda:0')), ('power', tensor([0.4990], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0554], device='cuda:0')), ('power', tensor([-19.8056], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.09882083535194397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6495, 7.8271, 8.2612],
        [6.6495, 6.6496, 6.6495],
        [6.6495, 6.7203, 6.6689],
        [6.6495, 6.9110, 6.8109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.09865324944257736 
model_pd.l_d.mean(): -8.138531684875488 
model_pd.lagr.mean(): -8.039878845214844 
model_pd.lambdas: dict_items([('pout', tensor([1.6536], device='cuda:0')), ('power', tensor([0.4980], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0544], device='cuda:0')), ('power', tensor([-19.8017], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.09865324944257736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6551, 7.1477, 7.0946],
        [6.6551, 7.0308, 6.9425],
        [6.6551, 6.6765, 6.6579],
        [6.6551, 6.6551, 6.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.09848567843437195 
model_pd.l_d.mean(): -8.117659568786621 
model_pd.lagr.mean(): -8.019173622131348 
model_pd.lambdas: dict_items([('pout', tensor([1.6547], device='cuda:0')), ('power', tensor([0.4970], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0533], device='cuda:0')), ('power', tensor([-19.7978], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.09848567843437195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6607, 6.6822, 6.6635],
        [6.6607, 7.2078, 7.1774],
        [6.6607, 6.6975, 6.6674],
        [6.6607, 6.6611, 6.6607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.09831840544939041 
model_pd.l_d.mean(): -8.096797943115234 
model_pd.lagr.mean(): -7.99847936630249 
model_pd.lambdas: dict_items([('pout', tensor([1.6557], device='cuda:0')), ('power', tensor([0.4960], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0521], device='cuda:0')), ('power', tensor([-19.7937], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.09831840544939041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6664, 6.7370, 6.6857],
        [6.6664, 7.7430, 8.0824],
        [6.6664, 7.2141, 7.1837],
        [6.6664, 6.6668, 6.6664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.09815157949924469 
model_pd.l_d.mean(): -8.075946807861328 
model_pd.lagr.mean(): -7.977795124053955 
model_pd.lambdas: dict_items([('pout', tensor([1.6568], device='cuda:0')), ('power', tensor([0.4950], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0510], device='cuda:0')), ('power', tensor([-19.7897], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.09815157949924469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6722, 6.6725, 6.6722],
        [6.6722, 6.8318, 6.7446],
        [6.6722, 6.7090, 6.6789],
        [6.6722, 7.2995, 7.3086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.09798528254032135 
model_pd.l_d.mean(): -8.055106163024902 
model_pd.lagr.mean(): -7.957120895385742 
model_pd.lambdas: dict_items([('pout', tensor([1.6578], device='cuda:0')), ('power', tensor([0.4940], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0499], device='cuda:0')), ('power', tensor([-19.7856], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.09798528254032135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6780, 6.6787, 6.6780],
        [6.6780, 7.8628, 8.3005],
        [6.6780, 7.8529, 8.2811],
        [6.6780, 8.2685, 9.1284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09781961143016815 
model_pd.l_d.mean(): -8.034276962280273 
model_pd.lagr.mean(): -7.93645715713501 
model_pd.lambdas: dict_items([('pout', tensor([1.6589], device='cuda:0')), ('power', tensor([0.4931], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0488], device='cuda:0')), ('power', tensor([-19.7814], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.09781961143016815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6838, 7.2922, 7.2905],
        [6.6838, 6.6838, 6.6838],
        [6.6838, 7.8599, 8.2886],
        [6.6838, 6.6845, 6.6838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.09765459597110748 
model_pd.l_d.mean(): -8.013457298278809 
model_pd.lagr.mean(): -7.915802478790283 
model_pd.lambdas: dict_items([('pout', tensor([1.6599], device='cuda:0')), ('power', tensor([0.4921], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0476], device='cuda:0')), ('power', tensor([-19.7773], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.09765459597110748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6897, 6.6897, 6.6897],
        [6.6897, 6.9531, 6.8523],
        [6.6897, 7.0867, 7.0022],
        [6.6897, 6.9937, 6.8942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.0974901095032692 
model_pd.l_d.mean(): -7.992651462554932 
model_pd.lagr.mean(): -7.895161151885986 
model_pd.lambdas: dict_items([('pout', tensor([1.6610], device='cuda:0')), ('power', tensor([0.4911], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0464], device='cuda:0')), ('power', tensor([-19.7730], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.0974901095032692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6957, 6.7671, 6.7152],
        [6.6957, 6.7060, 6.6966],
        [6.6957, 7.3683, 7.4015],
        [6.6957, 6.6958, 6.6957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.0973261147737503 
model_pd.l_d.mean(): -7.97185754776001 
model_pd.lagr.mean(): -7.874531269073486 
model_pd.lambdas: dict_items([('pout', tensor([1.6620], device='cuda:0')), ('power', tensor([0.4901], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0453], device='cuda:0')), ('power', tensor([-19.7688], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.0973261147737503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7017, 7.2529, 7.2223],
        [6.7017, 7.6643, 7.9028],
        [6.7017, 6.7022, 6.7018],
        [6.7017, 6.8873, 6.7939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.09716247022151947 
model_pd.l_d.mean(): -7.951074123382568 
model_pd.lagr.mean(): -7.853911876678467 
model_pd.lambdas: dict_items([('pout', tensor([1.6631], device='cuda:0')), ('power', tensor([0.4891], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0441], device='cuda:0')), ('power', tensor([-19.7645], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.09716247022151947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7079, 7.1061, 7.0214],
        [6.7079, 6.9056, 6.8100],
        [6.7079, 6.7079, 6.7078],
        [6.7079, 7.3392, 7.3483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.09699904918670654 
model_pd.l_d.mean(): -7.930301666259766 
model_pd.lagr.mean(): -7.8333024978637695 
model_pd.lambdas: dict_items([('pout', tensor([1.6641], device='cuda:0')), ('power', tensor([0.4881], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0429], device='cuda:0')), ('power', tensor([-19.7601], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.09699904918670654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.7140, 6.7140, 6.7140],
        [6.7140, 6.8488, 6.7688],
        [6.7140, 7.0770, 6.9845],
        [6.7140, 7.0193, 6.9194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.09683578461408615 
model_pd.l_d.mean(): -7.909541130065918 
model_pd.lagr.mean(): -7.8127055168151855 
model_pd.lambdas: dict_items([('pout', tensor([1.6651], device='cuda:0')), ('power', tensor([0.4871], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.0417], device='cuda:0')), ('power', tensor([-19.7557], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.09683578461408615
Traceback (most recent call last):
  File "g:\CINT\Data_model_drive_direct\main\main.py", line 207, in <module>
    train()
  File "g:\CINT\Data_model_drive_direct\main\main.py", line 199, in train
    plt.show()
  File "F:\Miniconda\envs\HARQ_GCN\lib\site-packages\matplotlib\pyplot.py", line 612, in show
    return _get_backend_mod().show(*args, **kwargs)
  File "F:\Miniconda\envs\HARQ_GCN\lib\site-packages\matplotlib\backend_bases.py", line 3553, in show
    cls.mainloop()
  File "F:\Miniconda\envs\HARQ_GCN\lib\site-packages\matplotlib\backends\_backend_tk.py", line 520, in start_main_loop
    first_manager.window.mainloop()
  File "F:\Miniconda\envs\HARQ_GCN\lib\tkinter\__init__.py", line 1429, in mainloop
    self.tk.mainloop(n)
KeyboardInterrupt
