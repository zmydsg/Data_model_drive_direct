
bounds:tensor([-2.])	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.]), 'power': tensor([1.])},
vars:{'pout': tensor([0.]), 'power': tensor([0.])}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[2.3753, 2.7571, 2.9162],
        [2.3753, 2.6743, 2.7534],
        [2.3753, 2.3910, 2.3785],
        [2.3753, 2.5079, 2.4824]], grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.16718415915966034 
model_pd.l_d.mean(): -18.154653549194336 
model_pd.lagr.mean(): -18.32183837890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0024])), ('power', tensor([0.9990]))]) 
model_pd.vars: dict_items([('pout', tensor([2.3992])), ('power', tensor([-20.5539]))])
epoch：0	 i:0 	 global-step:0	 l-p:-0.16718415915966034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[2.4887, 2.4890, 2.4887],
        [2.4887, 2.4974, 2.4899],
        [2.4887, 3.1458, 3.6241],
        [2.4887, 3.1025, 3.5211]], grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): -0.2121804654598236 
model_pd.l_d.mean(): -18.32259178161621 
model_pd.lagr.mean(): -18.534772872924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0047])), ('power', tensor([0.9979]))]) 
model_pd.vars: dict_items([('pout', tensor([2.3378])), ('power', tensor([-20.6873]))])
epoch：1	 i:0 	 global-step:20	 l-p:-0.2121804654598236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[2.6034, 3.1887, 3.5454],
        [2.6034, 3.3010, 3.8072],
        [2.6034, 2.6582, 2.6268],
        [2.6034, 2.6828, 2.6466]], grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): -0.2828591465950012 
model_pd.l_d.mean(): -18.468114852905273 
model_pd.lagr.mean(): -18.750974655151367 
model_pd.lambdas: dict_items([('pout', tensor([1.0070])), ('power', tensor([0.9969]))]) 
model_pd.vars: dict_items([('pout', tensor([2.2786])), ('power', tensor([-20.8004]))])
epoch：2	 i:0 	 global-step:40	 l-p:-0.2828591465950012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[2.7192, 2.7192, 2.7192],
        [2.7192, 2.8459, 2.8087],
        [2.7192, 3.1695, 3.3521],
        [2.7192, 3.1356, 3.2842]], grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): -0.4108627736568451 
model_pd.l_d.mean(): -18.59357452392578 
model_pd.lagr.mean(): -19.004436492919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0092])), ('power', tensor([0.9959]))]) 
model_pd.vars: dict_items([('pout', tensor([2.2213])), ('power', tensor([-20.8953]))])
epoch：3	 i:0 	 global-step:60	 l-p:-0.4108627736568451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[2.8362, 3.0859, 3.0912],
        [2.8362, 2.8366, 2.8362],
        [2.8362, 2.8533, 2.8395],
        [2.8362, 2.8362, 2.8362]], grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): -0.7185405492782593 
model_pd.l_d.mean(): -18.700969696044922 
model_pd.lagr.mean(): -19.419509887695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0114])), ('power', tensor([0.9948]))]) 
model_pd.vars: dict_items([('pout', tensor([2.1659])), ('power', tensor([-20.9738]))])
epoch：4	 i:0 	 global-step:80	 l-p:-0.7185405492782593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[2.9546, 2.9887, 2.9643],
        [2.9546, 2.9584, 2.9548],
        [2.9546, 2.9883, 2.9641],
        [2.9546, 2.9546, 2.9546]], grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): -2.6707847118377686 
model_pd.l_d.mean(): -18.79201316833496 
model_pd.lagr.mean(): -21.462797164916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0135])), ('power', tensor([0.9938]))]) 
model_pd.vars: dict_items([('pout', tensor([2.1122])), ('power', tensor([-21.0376]))])
epoch：5	 i:0 	 global-step:100	 l-p:-2.6707847118377686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[3.0742, 3.1099, 3.0844],
        [3.0742, 3.0742, 3.0742],
        [3.0742, 3.0742, 3.0742],
        [3.0742, 3.2543, 3.2181]], grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 1.3372321128845215 
model_pd.l_d.mean(): -18.868227005004883 
model_pd.lagr.mean(): -17.530994415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0156])), ('power', tensor([0.9927]))]) 
model_pd.vars: dict_items([('pout', tensor([2.0600])), ('power', tensor([-21.0878]))])
epoch：6	 i:0 	 global-step:120	 l-p:1.3372321128845215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[3.1951, 3.5977, 3.6829],
        [3.1951, 3.5008, 3.5166],
        [3.1951, 3.1982, 3.1953],
        [3.1951, 3.4642, 3.4591]], grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.6733180284500122 
model_pd.l_d.mean(): -18.930919647216797 
model_pd.lagr.mean(): -18.257600784301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0176])), ('power', tensor([0.9916]))]) 
model_pd.vars: dict_items([('pout', tensor([2.0093])), ('power', tensor([-21.1258]))])
epoch：7	 i:0 	 global-step:140	 l-p:0.6733180284500122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[3.3175, 4.2036, 4.7973],
        [3.3175, 3.5987, 3.5930],
        [3.3175, 3.4565, 3.4070],
        [3.3175, 4.2680, 4.9481]], grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.19232414662837982 
model_pd.l_d.mean(): -18.981233596801758 
model_pd.lagr.mean(): -18.788909912109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0195])), ('power', tensor([0.9906]))]) 
model_pd.vars: dict_items([('pout', tensor([1.9600])), ('power', tensor([-21.1525]))])
epoch：8	 i:0 	 global-step:160	 l-p:0.19232414662837982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.4413, 3.5862, 3.5345],
        [3.4413, 3.4817, 3.4527],
        [3.4413, 3.4813, 3.4526],
        [3.4413, 3.4417, 3.4413]], grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.442516028881073 
model_pd.l_d.mean(): -19.020164489746094 
model_pd.lagr.mean(): -18.577648162841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0215])), ('power', tensor([0.9895]))]) 
model_pd.vars: dict_items([('pout', tensor([1.9119])), ('power', tensor([-21.1687]))])
epoch：9	 i:0 	 global-step:180	 l-p:0.442516028881073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[3.5667, 4.1451, 4.3462],
        [3.5667, 3.9364, 3.9672],
        [3.5667, 3.6598, 3.6107],
        [3.5667, 3.5671, 3.5667]], grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.3055328130722046 
model_pd.l_d.mean(): -19.04859161376953 
model_pd.lagr.mean(): -18.743059158325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0233])), ('power', tensor([0.9885]))]) 
model_pd.vars: dict_items([('pout', tensor([1.8649])), ('power', tensor([-21.1753]))])
epoch：10	 i:0 	 global-step:200	 l-p:0.3055328130722046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[3.6937, 3.8122, 3.7574],
        [3.6937, 4.0338, 4.0383],
        [3.6937, 3.7166, 3.6980],
        [3.6937, 3.7002, 3.6943]], grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.2397688329219818 
model_pd.l_d.mean(): -19.067293167114258 
model_pd.lagr.mean(): -18.827524185180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0251])), ('power', tensor([0.9874]))]) 
model_pd.vars: dict_items([('pout', tensor([1.8190])), ('power', tensor([-21.1729]))])
epoch：11	 i:0 	 global-step:220	 l-p:0.2397688329219818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[3.8225, 4.7249, 5.2381],
        [3.8225, 3.8292, 3.8231],
        [3.8225, 4.1762, 4.1806],
        [3.8225, 3.8228, 3.8225]], grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.1927950084209442 
model_pd.l_d.mean(): -19.076913833618164 
model_pd.lagr.mean(): -18.884119033813477 
model_pd.lambdas: dict_items([('pout', tensor([1.0269])), ('power', tensor([0.9864]))]) 
model_pd.vars: dict_items([('pout', tensor([1.7740])), ('power', tensor([-21.1620]))])
epoch：12	 i:0 	 global-step:240	 l-p:0.1927950084209442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[3.9530, 3.9530, 3.9530],
        [3.9530, 4.2644, 4.2402],
        [3.9530, 4.1476, 4.0888],
        [3.9530, 4.3440, 4.3620]], grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.1415387988090515 
model_pd.l_d.mean(): -19.07802963256836 
model_pd.lagr.mean(): -18.936491012573242 
model_pd.lambdas: dict_items([('pout', tensor([1.0286])), ('power', tensor([0.9853]))]) 
model_pd.vars: dict_items([('pout', tensor([1.7300])), ('power', tensor([-21.1432]))])
epoch：13	 i:0 	 global-step:260	 l-p:0.1415387988090515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.0854, 4.1337, 4.0990],
        [4.0854, 4.8200, 5.1084],
        [4.0854, 4.2180, 4.1565],
        [4.0854, 4.1344, 4.0993]], grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.3138750493526459 
model_pd.l_d.mean(): -19.071134567260742 
model_pd.lagr.mean(): -18.757259368896484 
model_pd.lambdas: dict_items([('pout', tensor([1.0303])), ('power', tensor([0.9842]))]) 
model_pd.vars: dict_items([('pout', tensor([1.6868])), ('power', tensor([-21.1168]))])
epoch：14	 i:0 	 global-step:280	 l-p:0.3138750493526459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.2197, 4.2197, 4.2197],
        [4.2197, 4.9875, 5.2916],
        [4.2197, 4.5548, 4.5284],
        [4.2197, 4.2702, 4.2339]], grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.24692291021347046 
model_pd.l_d.mean(): -19.05667495727539 
model_pd.lagr.mean(): -18.809751510620117 
model_pd.lambdas: dict_items([('pout', tensor([1.0320])), ('power', tensor([0.9832]))]) 
model_pd.vars: dict_items([('pout', tensor([1.6444])), ('power', tensor([-21.0833]))])
epoch：15	 i:0 	 global-step:300	 l-p:0.24692291021347046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.3558, 4.5726, 4.5068],
        [4.3558, 4.3618, 4.3563],
        [4.3558, 4.3561, 4.3558],
        [4.3558, 4.3558, 4.3558]], grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.14882910251617432 
model_pd.l_d.mean(): -19.03505516052246 
model_pd.lagr.mean(): -18.886226654052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0336])), ('power', tensor([0.9821]))]) 
model_pd.vars: dict_items([('pout', tensor([1.6028])), ('power', tensor([-21.0429]))])
epoch：16	 i:0 	 global-step:320	 l-p:0.14882910251617432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.4940, 4.4985, 4.4943],
        [4.4940, 5.7680, 6.6110],
        [4.4940, 4.4940, 4.4940],
        [4.4940, 4.6143, 4.5505]], grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.33151066303253174 
model_pd.l_d.mean(): -19.006633758544922 
model_pd.lagr.mean(): -18.67512321472168 
model_pd.lambdas: dict_items([('pout', tensor([1.0351])), ('power', tensor([0.9811]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5618])), ('power', tensor([-20.9961]))])
epoch：17	 i:0 	 global-step:340	 l-p:0.33151066303253174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[4.6342, 5.1024, 5.1223],
        [4.6342, 5.2534, 5.3786],
        [4.6342, 5.2977, 5.4587],
        [4.6342, 4.7867, 4.7157]], grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.1678912341594696 
model_pd.l_d.mean(): -18.971725463867188 
model_pd.lagr.mean(): -18.803834915161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0367])), ('power', tensor([0.9800]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5215])), ('power', tensor([-20.9430]))])
epoch：18	 i:0 	 global-step:360	 l-p:0.1678912341594696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[4.7737, 4.9598, 4.8842],
        [4.7737, 5.6585, 6.0068],
        [4.7737, 4.9826, 4.9070],
        [4.7737, 5.2286, 5.2321]], grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.1466757357120514 
model_pd.l_d.mean(): -18.931013107299805 
model_pd.lagr.mean(): -18.784337997436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0381])), ('power', tensor([0.9790]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4827])), ('power', tensor([-20.8850]))])
epoch：19	 i:0 	 global-step:380	 l-p:0.1466757357120514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8994, 5.0519, 4.9778],
        [4.8994, 5.8118, 6.1711],
        [4.8994, 5.8045, 6.1566],
        [4.8994, 4.8994, 4.8994]], grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.1353941559791565 
model_pd.l_d.mean(): -18.88738250732422 
model_pd.lagr.mean(): -18.75198745727539 
model_pd.lambdas: dict_items([('pout', tensor([1.0396])), ('power', tensor([0.9779]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4486])), ('power', tensor([-20.8289]))])
epoch：20	 i:0 	 global-step:400	 l-p:0.1353941559791565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.0047, 5.0047, 5.0047],
        [5.0047, 5.1706, 5.0932],
        [5.0047, 5.0650, 5.0215],
        [5.0047, 5.4992, 5.5111]], grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.12814810872077942 
model_pd.l_d.mean(): -18.84393310546875 
model_pd.lagr.mean(): -18.715784072875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0410])), ('power', tensor([0.9769]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4208])), ('power', tensor([-20.7792]))])
epoch：21	 i:0 	 global-step:420	 l-p:0.12814810872077942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[5.0860, 5.2019, 5.1346],
        [5.0860, 6.0379, 6.4121],
        [5.0860, 5.0860, 5.0860],
        [5.0860, 5.0911, 5.0863]], grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.12335197627544403 
model_pd.l_d.mean(): -18.803268432617188 
model_pd.lagr.mean(): -18.679916381835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0424])), ('power', tensor([0.9759]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3997])), ('power', tensor([-20.7392]))])
epoch：22	 i:0 	 global-step:440	 l-p:0.12335197627544403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.1423, 5.1427, 5.1423],
        [5.1423, 6.4163, 7.1329],
        [5.1423, 5.6682, 5.6897],
        [5.1423, 5.1423, 5.1423]], grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.12031979858875275 
model_pd.l_d.mean(): -18.767099380493164 
model_pd.lagr.mean(): -18.646780014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0438])), ('power', tensor([0.9748]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3853])), ('power', tensor([-20.7108]))])
epoch：23	 i:0 	 global-step:460	 l-p:0.12031979858875275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.1753, 5.1753, 5.1753],
        [5.1753, 5.1753, 5.1753],
        [5.1753, 5.7393, 5.7819],
        [5.1753, 5.4376, 5.3574]], grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.11862902343273163 
model_pd.l_d.mean(): -18.735977172851562 
model_pd.lagr.mean(): -18.617347717285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0452])), ('power', tensor([0.9738]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3769])), ('power', tensor([-20.6939]))])
epoch：24	 i:0 	 global-step:480	 l-p:0.11862902343273163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[5.1877, 6.6914, 7.6824],
        [5.1877, 6.1610, 6.5433],
        [5.1877, 5.3602, 5.2797],
        [5.1877, 5.6543, 5.6404]], grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.11800716072320938 
model_pd.l_d.mean(): -18.709707260131836 
model_pd.lagr.mean(): -18.591699600219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0465])), ('power', tensor([0.9728]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3737])), ('power', tensor([-20.6875]))])
epoch：25	 i:0 	 global-step:500	 l-p:0.11800716072320938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.1821, 5.4106, 5.3277],
        [5.1821, 5.1839, 5.1821],
        [5.1821, 5.1821, 5.1821],
        [5.1821, 6.0712, 6.3730]], grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.11828970164060593 
model_pd.l_d.mean(): -18.687789916992188 
model_pd.lagr.mean(): -18.569499969482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0479])), ('power', tensor([0.9717]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3752])), ('power', tensor([-20.6904]))])
epoch：26	 i:0 	 global-step:520	 l-p:0.11828970164060593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.1607, 5.1607, 5.1607],
        [5.1607, 5.4699, 5.3983],
        [5.1607, 5.6245, 5.6107],
        [5.1607, 5.1610, 5.1607]], grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.11937478184700012 
model_pd.l_d.mean(): -18.669565200805664 
model_pd.lagr.mean(): -18.550189971923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0493])), ('power', tensor([0.9707]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3806])), ('power', tensor([-20.7015]))])
epoch：27	 i:0 	 global-step:540	 l-p:0.11937478184700012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.1256, 5.1448, 5.1282],
        [5.1256, 5.1256, 5.1256],
        [5.1256, 5.1274, 5.1257],
        [5.1256, 5.1259, 5.1256]], grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.12120769172906876 
model_pd.l_d.mean(): -18.654277801513672 
model_pd.lagr.mean(): -18.533069610595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0507])), ('power', tensor([0.9697]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3895])), ('power', tensor([-20.7194]))])
epoch：28	 i:0 	 global-step:560	 l-p:0.12120769172906876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.0788, 5.8553, 6.0672],
        [5.0788, 5.0858, 5.0793],
        [5.0788, 5.3825, 5.3122],
        [5.0788, 5.0788, 5.0788]], grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.12376958876848221 
model_pd.l_d.mean(): -18.641101837158203 
model_pd.lagr.mean(): -18.517332077026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0521])), ('power', tensor([0.9686]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4016])), ('power', tensor([-20.7430]))])
epoch：29	 i:0 	 global-step:580	 l-p:0.12376958876848221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[5.0231, 5.2439, 5.1638],
        [5.0231, 5.1589, 5.0868],
        [5.0231, 5.4302, 5.3968],
        [5.0231, 5.3517, 5.2898]], grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.12702752649784088 
model_pd.l_d.mean(): -18.629024505615234 
model_pd.lagr.mean(): -18.501996994018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0535])), ('power', tensor([0.9676]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4160])), ('power', tensor([-20.7705]))])
epoch：30	 i:0 	 global-step:600	 l-p:0.12702752649784088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.9614, 4.9617, 4.9614],
        [4.9614, 5.7175, 5.9241],
        [4.9614, 5.8861, 6.2500],
        [4.9614, 5.1255, 5.0490]], grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13098230957984924 
model_pd.l_d.mean(): -18.617124557495117 
model_pd.lagr.mean(): -18.486143112182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0549])), ('power', tensor([0.9665]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4322])), ('power', tensor([-20.8002]))])
epoch：31	 i:0 	 global-step:620	 l-p:0.13098230957984924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[4.8963, 4.9011, 4.8965],
        [4.8963, 5.8070, 6.1656],
        [4.8963, 4.9550, 4.9127],
        [4.8963, 5.2155, 5.1554]], grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13566803932189941 
model_pd.l_d.mean(): -18.604625701904297 
model_pd.lagr.mean(): -18.468957901000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0564])), ('power', tensor([0.9655]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4495])), ('power', tensor([-20.8307]))])
epoch：32	 i:0 	 global-step:640	 l-p:0.13566803932189941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.8303, 4.8306, 4.8304],
        [4.8303, 4.8320, 4.8304],
        [4.8303, 4.8303, 4.8303],
        [4.8303, 4.8351, 4.8306]], grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.14115208387374878 
model_pd.l_d.mean(): -18.590923309326172 
model_pd.lagr.mean(): -18.449771881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0579])), ('power', tensor([0.9645]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4673])), ('power', tensor([-20.8606]))])
epoch：33	 i:0 	 global-step:660	 l-p:0.14115208387374878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.7665, 4.7665, 4.7665],
        [4.7665, 4.8240, 4.7826],
        [4.7665, 5.0762, 5.0180],
        [4.7665, 5.2489, 5.2691]], grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.14753203094005585 
model_pd.l_d.mean(): -18.57557487487793 
model_pd.lagr.mean(): -18.428043365478516 
model_pd.lambdas: dict_items([('pout', tensor([1.0593])), ('power', tensor([0.9634]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4848])), ('power', tensor([-20.8886]))])
epoch：34	 i:0 	 global-step:680	 l-p:0.14753203094005585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.7077, 4.7093, 4.7077],
        [4.7077, 4.7412, 4.7144],
        [4.7077, 5.1257, 5.1139],
        [4.7077, 4.7648, 4.7238]], grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.15489670634269714 
model_pd.l_d.mean(): -18.55834197998047 
model_pd.lagr.mean(): -18.403446197509766 
model_pd.lambdas: dict_items([('pout', tensor([1.0608])), ('power', tensor([0.9624]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5010])), ('power', tensor([-20.9135]))])
epoch：35	 i:0 	 global-step:700	 l-p:0.15489670634269714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[4.6578, 4.9596, 4.9030],
        [4.6578, 5.7717, 6.3869],
        [4.6578, 5.7934, 6.4341],
        [4.6578, 5.3611, 5.5541]], grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.16313253343105316 
model_pd.l_d.mean(): -18.53913688659668 
model_pd.lagr.mean(): -18.376005172729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0624])), ('power', tensor([0.9613]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5150])), ('power', tensor([-20.9340]))])
epoch：36	 i:0 	 global-step:720	 l-p:0.16313253343105316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.6215, 5.4011, 5.6672],
        [4.6215, 4.6261, 4.6218],
        [4.6215, 4.6219, 4.6215],
        [4.6215, 5.3185, 5.5098]], grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.17121988534927368 
model_pd.l_d.mean(): -18.518003463745117 
model_pd.lagr.mean(): -18.346782684326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0639])), ('power', tensor([0.9603]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5253])), ('power', tensor([-20.9486]))])
epoch：37	 i:0 	 global-step:740	 l-p:0.17121988534927368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.6044, 5.0538, 5.0650],
        [4.6044, 5.2177, 5.3416],
        [4.6044, 4.7080, 4.6479],
        [4.6044, 4.6044, 4.6044]], grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.17611326277256012 
model_pd.l_d.mean(): -18.495058059692383 
model_pd.lagr.mean(): -18.318944931030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0654])), ('power', tensor([0.9592]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5301])), ('power', tensor([-20.9553]))])
epoch：38	 i:0 	 global-step:760	 l-p:0.17611326277256012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.6049, 4.6131, 4.6056],
        [4.6049, 5.4463, 5.7743],
        [4.6049, 4.6051, 4.6049],
        [4.6049, 5.2182, 5.3420]], grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.17599815130233765 
model_pd.l_d.mean(): -18.470775604248047 
model_pd.lagr.mean(): -18.294776916503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0669])), ('power', tensor([0.9582]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5300])), ('power', tensor([-20.9552]))])
epoch：39	 i:0 	 global-step:780	 l-p:0.17599815130233765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.6210, 4.6210, 4.6210],
        [4.6210, 5.4001, 5.6659],
        [4.6210, 4.6256, 4.6213],
        [4.6210, 4.6761, 4.6364]], grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.17143841087818146 
model_pd.l_d.mean(): -18.445354461669922 
model_pd.lagr.mean(): -18.273916244506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0685])), ('power', tensor([0.9571]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5254])), ('power', tensor([-20.9490]))])
epoch：40	 i:0 	 global-step:800	 l-p:0.17143841087818146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.6508, 5.0911, 5.0946],
        [4.6508, 4.6802, 4.6563],
        [4.6508, 4.8310, 4.7578],
        [4.6508, 4.6508, 4.6508]], grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.16459977626800537 
model_pd.l_d.mean(): -18.418781280517578 
model_pd.lagr.mean(): -18.254180908203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0700])), ('power', tensor([0.9561]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5170])), ('power', tensor([-20.9372]))])
epoch：41	 i:0 	 global-step:820	 l-p:0.16459977626800537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[4.6878, 4.6878, 4.6878],
        [4.6878, 5.1320, 5.1354],
        [4.6878, 4.6879, 4.6878],
        [4.6878, 5.1605, 5.1803]], grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.15798352658748627 
model_pd.l_d.mean(): -18.391340255737305 
model_pd.lagr.mean(): -18.233356475830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0715])), ('power', tensor([0.9550]))]) 
model_pd.vars: dict_items([('pout', tensor([1.5067])), ('power', tensor([-20.9222]))])
epoch：42	 i:0 	 global-step:840	 l-p:0.15798352658748627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.7271, 5.0333, 4.9757],
        [4.7271, 4.7287, 4.7271],
        [4.7271, 5.2349, 5.2739],
        [4.7271, 4.7271, 4.7271]], grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.15235237777233124 
model_pd.l_d.mean(): -18.363344192504883 
model_pd.lagr.mean(): -18.21099090576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0730])), ('power', tensor([0.9540]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4957])), ('power', tensor([-20.9059]))])
epoch：43	 i:0 	 global-step:860	 l-p:0.15235237777233124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.7651, 4.9123, 4.8407],
        [4.7651, 5.4019, 5.5299],
        [4.7651, 4.9500, 4.8748],
        [4.7651, 4.8208, 4.7804]], grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.14777790009975433 
model_pd.l_d.mean(): -18.335132598876953 
model_pd.lagr.mean(): -18.187355041503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0745])), ('power', tensor([0.9530]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4853])), ('power', tensor([-20.8898]))])
epoch：44	 i:0 	 global-step:880	 l-p:0.14777790009975433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.7995, 4.8012, 4.7996],
        [4.7995, 5.3159, 5.3553],
        [4.7995, 5.9508, 6.5857],
        [4.7995, 4.8080, 4.8002]], grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.1441640704870224 
model_pd.l_d.mean(): -18.307058334350586 
model_pd.lagr.mean(): -18.16289520263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0760])), ('power', tensor([0.9519]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4758])), ('power', tensor([-20.8749]))])
epoch：45	 i:0 	 global-step:900	 l-p:0.1441640704870224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.8287, 5.4747, 5.6044],
        [4.8287, 4.8334, 4.8290],
        [4.8287, 4.8293, 4.8287],
        [4.8287, 5.7219, 6.0728]], grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.14139850437641144 
model_pd.l_d.mean(): -18.279428482055664 
model_pd.lagr.mean(): -18.138029098510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0774])), ('power', tensor([0.9509]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4679])), ('power', tensor([-20.8621]))])
epoch：46	 i:0 	 global-step:920	 l-p:0.14139850437641144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[4.8516, 4.8516, 4.8516],
        [4.8516, 5.0946, 5.0204],
        [4.8516, 6.0928, 6.8251],
        [4.8516, 5.1664, 5.1070]], grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.13938350975513458 
model_pd.l_d.mean(): -18.25250244140625 
model_pd.lagr.mean(): -18.11311912536621 
model_pd.lambdas: dict_items([('pout', tensor([1.0789])), ('power', tensor([0.9498]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4617])), ('power', tensor([-20.8520]))])
epoch：47	 i:0 	 global-step:940	 l-p:0.13938350975513458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[4.8677, 4.8985, 4.8734],
        [4.8677, 4.9982, 4.9289],
        [4.8677, 5.1689, 5.1053],
        [4.8677, 5.1115, 5.0370]], grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.1380385458469391 
model_pd.l_d.mean(): -18.22645378112793 
model_pd.lagr.mean(): -18.088415145874023 
model_pd.lambdas: dict_items([('pout', tensor([1.0803])), ('power', tensor([0.9488]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4574])), ('power', tensor([-20.8448]))])
epoch：48	 i:0 	 global-step:960	 l-p:0.1380385458469391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.8769, 4.9339, 4.8926],
        [4.8769, 5.3548, 5.3661],
        [4.8769, 4.8836, 4.8774],
        [4.8769, 4.9077, 4.8827]], grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.13729660212993622 
model_pd.l_d.mean(): -18.20136833190918 
model_pd.lagr.mean(): -18.064071655273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0818])), ('power', tensor([0.9477]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4549])), ('power', tensor([-20.8407]))])
epoch：49	 i:0 	 global-step:980	 l-p:0.13729660212993622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[4.8795, 4.9365, 4.8952],
        [4.8795, 4.8811, 4.8795],
        [4.8795, 4.8798, 4.8795],
        [4.8795, 6.1281, 6.8645]], grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.13709931075572968 
model_pd.l_d.mean(): -18.17723846435547 
model_pd.lagr.mean(): -18.040138244628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0832])), ('power', tensor([0.9467]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4543])), ('power', tensor([-20.8396]))])
epoch：50	 i:0 	 global-step:1000	 l-p:0.13709931075572968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.8759, 5.1646, 5.0977],
        [4.8759, 5.3088, 5.2960],
        [4.8759, 5.1200, 5.0453],
        [4.8759, 4.8762, 4.8759]], grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.13739393651485443 
model_pd.l_d.mean(): -18.153976440429688 
model_pd.lagr.mean(): -18.016582489013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0847])), ('power', tensor([0.9457]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4552])), ('power', tensor([-20.8413]))])
epoch：51	 i:0 	 global-step:1020	 l-p:0.13739393651485443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.8670, 4.9766, 4.9130],
        [4.8670, 4.9259, 4.8836],
        [4.8670, 5.2989, 5.2861],
        [4.8670, 4.8670, 4.8670]], grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.1381298005580902 
model_pd.l_d.mean(): -18.131441116333008 
model_pd.lagr.mean(): -17.993310928344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0862])), ('power', tensor([0.9446]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4577])), ('power', tensor([-20.8454]))])
epoch：52	 i:0 	 global-step:1040	 l-p:0.1381298005580902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.8537, 4.8844, 4.8595],
        [4.8537, 5.1535, 5.0901],
        [4.8537, 5.3140, 5.3170],
        [4.8537, 6.0180, 6.6592]], grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.13925468921661377 
model_pd.l_d.mean(): -18.10944938659668 
model_pd.lagr.mean(): -17.97019386291504 
model_pd.lambdas: dict_items([('pout', tensor([1.0876])), ('power', tensor([0.9436]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4612])), ('power', tensor([-20.8515]))])
epoch：53	 i:0 	 global-step:1060	 l-p:0.13925468921661377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[4.8372, 6.3215, 7.3675],
        [4.8372, 6.0722, 6.8006],
        [4.8372, 5.1230, 5.0568],
        [4.8372, 5.2249, 5.1931]], grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1407119333744049 
model_pd.l_d.mean(): -18.087791442871094 
model_pd.lagr.mean(): -17.947078704833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0891])), ('power', tensor([0.9425]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4657])), ('power', tensor([-20.8591]))])
epoch：54	 i:0 	 global-step:1080	 l-p:0.1407119333744049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8185, 5.0049, 4.9290],
        [4.8185, 5.4613, 5.5900],
        [4.8185, 4.8191, 4.8185],
        [4.8185, 4.8185, 4.8185]], grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.14243485033512115 
model_pd.l_d.mean(): -18.066274642944336 
model_pd.lagr.mean(): -17.923839569091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0906])), ('power', tensor([0.9415]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4708])), ('power', tensor([-20.8675]))])
epoch：55	 i:0 	 global-step:1100	 l-p:0.14243485033512115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.7990, 4.8330, 4.8058],
        [4.7990, 5.6090, 5.8842],
        [4.7990, 5.2822, 5.3019],
        [4.7990, 4.9559, 4.8827]], grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.14434199035167694 
model_pd.l_d.mean(): -18.04471206665039 
model_pd.lagr.mean(): -17.90036964416504 
model_pd.lambdas: dict_items([('pout', tensor([1.0920])), ('power', tensor([0.9404]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4762])), ('power', tensor([-20.8762]))])
epoch：56	 i:0 	 global-step:1120	 l-p:0.14434199035167694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.7799, 5.6541, 5.9936],
        [4.7799, 4.7799, 4.7799],
        [4.7799, 5.9449, 6.6006],
        [4.7799, 5.4164, 5.5439]], grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.14633050560951233 
model_pd.l_d.mean(): -18.02294921875 
model_pd.lagr.mean(): -17.876619338989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0935])), ('power', tensor([0.9394]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4814])), ('power', tensor([-20.8846]))])
epoch：57	 i:0 	 global-step:1140	 l-p:0.14633050560951233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[4.7624, 5.6388, 5.9827],
        [4.7624, 4.8691, 4.8072],
        [4.7624, 5.2721, 5.3107],
        [4.7624, 5.6397, 5.9846]], grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.1482725739479065 
model_pd.l_d.mean(): -18.000869750976562 
model_pd.lagr.mean(): -17.852596282958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0950])), ('power', tensor([0.9384]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4862])), ('power', tensor([-20.8923]))])
epoch：58	 i:0 	 global-step:1160	 l-p:0.1482725739479065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.7477, 4.8741, 4.8069],
        [4.7477, 4.7477, 4.7477],
        [4.7477, 4.7812, 4.7544],
        [4.7477, 4.8934, 4.8225]], grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.1500166356563568 
model_pd.l_d.mean(): -17.9783878326416 
model_pd.lagr.mean(): -17.828371047973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0965])), ('power', tensor([0.9373]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4903])), ('power', tensor([-20.8987]))])
epoch：59	 i:0 	 global-step:1180	 l-p:0.1500166356563568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.7367, 4.7367, 4.7367],
        [4.7367, 5.4489, 5.6432],
        [4.7367, 4.9721, 4.9001],
        [4.7367, 5.0275, 4.9660]], grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.15140163898468018 
model_pd.l_d.mean(): -17.955463409423828 
model_pd.lagr.mean(): -17.804061889648438 
model_pd.lambdas: dict_items([('pout', tensor([1.0980])), ('power', tensor([0.9363]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4934])), ('power', tensor([-20.9035]))])
epoch：60	 i:0 	 global-step:1200	 l-p:0.15140163898468018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.7300, 5.4407, 5.6347],
        [4.7300, 5.2043, 5.2236],
        [4.7300, 5.4031, 5.5651],
        [4.7300, 4.9649, 4.8930]], grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.152284637093544 
model_pd.l_d.mean(): -17.932090759277344 
model_pd.lagr.mean(): -17.77980613708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0995])), ('power', tensor([0.9352]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4953])), ('power', tensor([-20.9064]))])
epoch：61	 i:0 	 global-step:1220	 l-p:0.152284637093544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.7279, 5.1872, 5.1979],
        [4.7279, 6.0705, 6.9559],
        [4.7279, 4.7575, 4.7334],
        [4.7279, 5.1443, 5.1319]], grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.15257881581783295 
model_pd.l_d.mean(): -17.90827178955078 
model_pd.lagr.mean(): -17.755693435668945 
model_pd.lambdas: dict_items([('pout', tensor([1.1010])), ('power', tensor([0.9342]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4959])), ('power', tensor([-20.9074]))])
epoch：62	 i:0 	 global-step:1240	 l-p:0.15257881581783295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.7303, 4.8842, 4.8124],
        [4.7303, 5.4406, 5.6343],
        [4.7303, 4.7304, 4.7303],
        [4.7303, 4.9122, 4.8381]], grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.15228094160556793 
model_pd.l_d.mean(): -17.884056091308594 
model_pd.lagr.mean(): -17.731775283813477 
model_pd.lambdas: dict_items([('pout', tensor([1.1025])), ('power', tensor([0.9331]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4952])), ('power', tensor([-20.9065]))])
epoch：63	 i:0 	 global-step:1260	 l-p:0.15228094160556793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.7368, 4.7368, 4.7368],
        [4.7368, 5.5326, 5.8028],
        [4.7368, 4.7935, 4.7527],
        [4.7368, 4.9188, 4.8446]], grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.15147250890731812 
model_pd.l_d.mean(): -17.85947608947754 
model_pd.lagr.mean(): -17.708003997802734 
model_pd.lambdas: dict_items([('pout', tensor([1.1040])), ('power', tensor([0.9321]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4934])), ('power', tensor([-20.9039]))])
epoch：64	 i:0 	 global-step:1280	 l-p:0.15147250890731812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.7464, 4.7798, 4.7531],
        [4.7464, 4.7464, 4.7464],
        [4.7464, 4.7480, 4.7465],
        [4.7464, 4.7464, 4.7464]], grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.15029162168502808 
model_pd.l_d.mean(): -17.834611892700195 
model_pd.lagr.mean(): -17.6843204498291 
model_pd.lambdas: dict_items([('pout', tensor([1.1055])), ('power', tensor([0.9310]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4908])), ('power', tensor([-20.9000]))])
epoch：65	 i:0 	 global-step:1300	 l-p:0.15029162168502808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.7583, 6.1095, 6.9999],
        [4.7583, 5.0497, 4.9880],
        [4.7583, 4.7587, 4.7583],
        [4.7583, 4.7589, 4.7583]], grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14889530837535858 
model_pd.l_d.mean(): -17.809532165527344 
model_pd.lagr.mean(): -17.66063690185547 
model_pd.lambdas: dict_items([('pout', tensor([1.1069])), ('power', tensor([0.9300]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4876])), ('power', tensor([-20.8950]))])
epoch：66	 i:0 	 global-step:1320	 l-p:0.14889530837535858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.7714, 4.8012, 4.7769],
        [4.7714, 5.6478, 5.9918],
        [4.7714, 5.4496, 5.6125],
        [4.7714, 4.7714, 4.7714]], grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.14742740988731384 
model_pd.l_d.mean(): -17.784324645996094 
model_pd.lagr.mean(): -17.636898040771484 
model_pd.lambdas: dict_items([('pout', tensor([1.1084])), ('power', tensor([0.9289]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4840])), ('power', tensor([-20.8895]))])
epoch：67	 i:0 	 global-step:1340	 l-p:0.14742740988731384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.7847, 4.7847, 4.7847],
        [4.7847, 5.2949, 5.3331],
        [4.7847, 5.4192, 5.5457],
        [4.7847, 6.2453, 7.2735]], grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.14600330591201782 
model_pd.l_d.mean(): -17.759082794189453 
model_pd.lagr.mean(): -17.613079071044922 
model_pd.lambdas: dict_items([('pout', tensor([1.1099])), ('power', tensor([0.9279]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4803])), ('power', tensor([-20.8838]))])
epoch：68	 i:0 	 global-step:1360	 l-p:0.14600330591201782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.7974, 5.4792, 5.6428],
        [4.7974, 4.8020, 4.7976],
        [4.7974, 5.2777, 5.2968],
        [4.7974, 4.8539, 4.8131]], grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.14470668137073517 
model_pd.l_d.mean(): -17.733888626098633 
model_pd.lagr.mean(): -17.589181900024414 
model_pd.lambdas: dict_items([('pout', tensor([1.1114])), ('power', tensor([0.9269]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4769])), ('power', tensor([-20.8784]))])
epoch：69	 i:0 	 global-step:1380	 l-p:0.14470668137073517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.8087, 4.8134, 4.8090],
        [4.8087, 4.8087, 4.8087],
        [4.8087, 4.8087, 4.8087],
        [4.8087, 4.8093, 4.8087]], grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.14359283447265625 
model_pd.l_d.mean(): -17.70882225036621 
model_pd.lagr.mean(): -17.565229415893555 
model_pd.lambdas: dict_items([('pout', tensor([1.1129])), ('power', tensor([0.9258]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4738])), ('power', tensor([-20.8736]))])
epoch：70	 i:0 	 global-step:1400	 l-p:0.14359283447265625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.8182, 6.2894, 7.3247],
        [4.8182, 5.6275, 5.9016],
        [4.8182, 4.8482, 4.8238],
        [4.8182, 4.8188, 4.8182]], grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.14269404113292694 
model_pd.l_d.mean(): -17.683948516845703 
model_pd.lagr.mean(): -17.5412540435791 
model_pd.lambdas: dict_items([('pout', tensor([1.1143])), ('power', tensor([0.9248]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4713])), ('power', tensor([-20.8695]))])
epoch：71	 i:0 	 global-step:1420	 l-p:0.14269404113292694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.8254, 6.2989, 7.3355],
        [4.8254, 4.8831, 4.8416],
        [4.8254, 4.8258, 4.8254],
        [4.8254, 4.8431, 4.8278]], grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.14202505350112915 
model_pd.l_d.mean(): -17.65931510925293 
model_pd.lagr.mean(): -17.517290115356445 
model_pd.lambdas: dict_items([('pout', tensor([1.1158])), ('power', tensor([0.9237]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4693])), ('power', tensor([-20.8664]))])
epoch：72	 i:0 	 global-step:1440	 l-p:0.14202505350112915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.8303, 5.2841, 5.2865],
        [4.8303, 5.2547, 5.2415],
        [4.8303, 4.9377, 4.8753],
        [4.8303, 4.8303, 4.8303]], grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.14158685505390167 
model_pd.l_d.mean(): -17.63494873046875 
model_pd.lagr.mean(): -17.493362426757812 
model_pd.lambdas: dict_items([('pout', tensor([1.1173])), ('power', tensor([0.9227]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4680])), ('power', tensor([-20.8644]))])
epoch：73	 i:0 	 global-step:1460	 l-p:0.14158685505390167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[4.8328, 5.0180, 4.9424],
        [4.8328, 4.9895, 4.9162],
        [4.8328, 5.7128, 6.0533],
        [4.8328, 5.7199, 6.0674]], grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.14136996865272522 
model_pd.l_d.mean(): -17.61084747314453 
model_pd.lagr.mean(): -17.4694766998291 
model_pd.lambdas: dict_items([('pout', tensor([1.1187])), ('power', tensor([0.9216]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4674])), ('power', tensor([-20.8634]))])
epoch：74	 i:0 	 global-step:1480	 l-p:0.14136996865272522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.8332, 4.8908, 4.8494],
        [4.8332, 6.0067, 6.6655],
        [4.8332, 5.1157, 5.0499],
        [4.8332, 4.8905, 4.8492]], grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.14135608077049255 
model_pd.l_d.mean(): -17.586992263793945 
model_pd.lagr.mean(): -17.445636749267578 
model_pd.lambdas: dict_items([('pout', tensor([1.1202])), ('power', tensor([0.9206]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4673])), ('power', tensor([-20.8634]))])
epoch：75	 i:0 	 global-step:1500	 l-p:0.14135608077049255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[4.8315, 4.8888, 4.8476],
        [4.8315, 4.8362, 4.8318],
        [4.8315, 5.7106, 6.0505],
        [4.8315, 6.0571, 6.7780]], grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.141519695520401 
model_pd.l_d.mean(): -17.563343048095703 
model_pd.lagr.mean(): -17.421823501586914 
model_pd.lambdas: dict_items([('pout', tensor([1.1217])), ('power', tensor([0.9196]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4678])), ('power', tensor([-20.8642]))])
epoch：76	 i:0 	 global-step:1520	 l-p:0.141519695520401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[4.8283, 4.8286, 4.8284],
        [4.8283, 5.7135, 6.0600],
        [4.8283, 4.8300, 4.8284],
        [4.8283, 4.8855, 4.8444]], grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.14182907342910767 
model_pd.l_d.mean(): -17.539857864379883 
model_pd.lagr.mean(): -17.398029327392578 
model_pd.lambdas: dict_items([('pout', tensor([1.1231])), ('power', tensor([0.9185]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4687])), ('power', tensor([-20.8658]))])
epoch：77	 i:0 	 global-step:1540	 l-p:0.14182907342910767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[4.8240, 5.9714, 6.6015],
        [4.8240, 4.9310, 4.8688],
        [4.8240, 5.2761, 5.2783],
        [4.8240, 5.1181, 5.0555]], grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.142246812582016 
model_pd.l_d.mean(): -17.516475677490234 
model_pd.lagr.mean(): -17.374229431152344 
model_pd.lambdas: dict_items([('pout', tensor([1.1246])), ('power', tensor([0.9175]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4699])), ('power', tensor([-20.8678]))])
epoch：78	 i:0 	 global-step:1560	 l-p:0.142246812582016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8191, 4.8256, 4.8196],
        [4.8191, 5.6261, 5.8988],
        [4.8191, 4.8191, 4.8191],
        [4.8191, 4.8191, 4.8191]], grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14273160696029663 
model_pd.l_d.mean(): -17.49314308166504 
model_pd.lagr.mean(): -17.350412368774414 
model_pd.lambdas: dict_items([('pout', tensor([1.1261])), ('power', tensor([0.9164]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4712])), ('power', tensor([-20.8702]))])
epoch：79	 i:0 	 global-step:1580	 l-p:0.14273160696029663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[4.8140, 5.6880, 6.0258],
        [4.8140, 4.9602, 4.8888],
        [4.8140, 4.8140, 4.8140],
        [4.8140, 5.1214, 5.0628]], grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.14323903620243073 
model_pd.l_d.mean(): -17.469804763793945 
model_pd.lagr.mean(): -17.32656478881836 
model_pd.lambdas: dict_items([('pout', tensor([1.1276])), ('power', tensor([0.9154]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4726])), ('power', tensor([-20.8726]))])
epoch：80	 i:0 	 global-step:1600	 l-p:0.14323903620243073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.8092, 4.8176, 4.8099],
        [4.8092, 4.8093, 4.8092],
        [4.8092, 4.8092, 4.8092],
        [4.8092, 5.4895, 5.6520]], grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.14372453093528748 
model_pd.l_d.mean(): -17.44641876220703 
model_pd.lagr.mean(): -17.30269432067871 
model_pd.lambdas: dict_items([('pout', tensor([1.1290])), ('power', tensor([0.9143]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4740])), ('power', tensor([-20.8748]))])
epoch：81	 i:0 	 global-step:1620	 l-p:0.14372453093528748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[4.8051, 5.2833, 5.3018],
        [4.8051, 4.8613, 4.8208],
        [4.8051, 5.2684, 5.2784],
        [4.8051, 4.8051, 4.8051]], grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.14414545893669128 
model_pd.l_d.mean(): -17.422945022583008 
model_pd.lagr.mean(): -17.278799057006836 
model_pd.lambdas: dict_items([('pout', tensor([1.1305])), ('power', tensor([0.9133]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4751])), ('power', tensor([-20.8768]))])
epoch：82	 i:0 	 global-step:1640	 l-p:0.14414545893669128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.8021, 6.0162, 6.7299],
        [4.8021, 4.8588, 4.8180],
        [4.8021, 5.0814, 5.0162],
        [4.8021, 4.8105, 4.8029]], grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.14446546137332916 
model_pd.l_d.mean(): -17.39935874938965 
model_pd.lagr.mean(): -17.254892349243164 
model_pd.lambdas: dict_items([('pout', tensor([1.1320])), ('power', tensor([0.9122]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4759])), ('power', tensor([-20.8782]))])
epoch：83	 i:0 	 global-step:1660	 l-p:0.14446546137332916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[4.8004, 4.8178, 4.8028],
        [4.8004, 6.0136, 6.7266],
        [4.8004, 4.8068, 4.8009],
        [4.8004, 5.9388, 6.5637]], grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.1446571797132492 
model_pd.l_d.mean(): -17.375646591186523 
model_pd.lagr.mean(): -17.230989456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.1335])), ('power', tensor([0.9112]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4764])), ('power', tensor([-20.8791]))])
epoch：84	 i:0 	 global-step:1680	 l-p:0.1446571797132492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.8001, 4.8001, 4.8002],
        [4.8001, 4.8335, 4.8068],
        [4.8001, 4.9827, 4.9080],
        [4.8001, 4.8002, 4.8002]], grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.14470534026622772 
model_pd.l_d.mean(): -17.351797103881836 
model_pd.lagr.mean(): -17.20709228515625 
model_pd.lambdas: dict_items([('pout', tensor([1.1349])), ('power', tensor([0.9102]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4765])), ('power', tensor([-20.8794]))])
epoch：85	 i:0 	 global-step:1700	 l-p:0.14470534026622772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.8013, 4.8016, 4.8013],
        [4.8013, 4.8013, 4.8013],
        [4.8013, 5.3090, 5.3462],
        [4.8013, 4.8029, 4.8014]], grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.14460766315460205 
model_pd.l_d.mean(): -17.327817916870117 
model_pd.lagr.mean(): -17.183210372924805 
model_pd.lambdas: dict_items([('pout', tensor([1.1364])), ('power', tensor([0.9091]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4763])), ('power', tensor([-20.8790]))])
epoch：86	 i:0 	 global-step:1720	 l-p:0.14460766315460205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.8038, 4.8038, 4.8038],
        [4.8038, 4.8084, 4.8041],
        [4.8038, 5.0825, 5.0174],
        [4.8038, 4.8044, 4.8038]], grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.1443749964237213 
model_pd.l_d.mean(): -17.30372428894043 
model_pd.lagr.mean(): -17.15934944152832 
model_pd.lambdas: dict_items([('pout', tensor([1.1379])), ('power', tensor([0.9081]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4756])), ('power', tensor([-20.8781]))])
epoch：87	 i:0 	 global-step:1740	 l-p:0.1443749964237213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.8074, 4.8642, 4.8234],
        [4.8074, 6.1657, 7.0582],
        [4.8074, 4.8077, 4.8074],
        [4.8074, 4.9618, 4.8895]], grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.1440287083387375 
model_pd.l_d.mean(): -17.27953338623047 
model_pd.lagr.mean(): -17.1355037689209 
model_pd.lambdas: dict_items([('pout', tensor([1.1394])), ('power', tensor([0.9070]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4746])), ('power', tensor([-20.8766]))])
epoch：88	 i:0 	 global-step:1760	 l-p:0.1440287083387375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.8120, 6.1713, 7.0643],
        [4.8120, 4.9945, 4.9198],
        [4.8120, 4.8126, 4.8120],
        [4.8120, 4.8294, 4.8143]], grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.14359714090824127 
model_pd.l_d.mean(): -17.255260467529297 
model_pd.lagr.mean(): -17.111663818359375 
model_pd.lambdas: dict_items([('pout', tensor([1.1408])), ('power', tensor([0.9060]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4734])), ('power', tensor([-20.8748]))])
epoch：89	 i:0 	 global-step:1780	 l-p:0.14359714090824127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.8172, 4.8175, 4.8172],
        [4.8172, 4.8174, 4.8172],
        [4.8172, 5.1229, 5.0644],
        [4.8172, 5.0222, 4.9474]], grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.1431121975183487 
model_pd.l_d.mean(): -17.230941772460938 
model_pd.lagr.mean(): -17.08782958984375 
model_pd.lambdas: dict_items([('pout', tensor([1.1423])), ('power', tensor([0.9049]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4720])), ('power', tensor([-20.8726]))])
epoch：90	 i:0 	 global-step:1800	 l-p:0.1431121975183487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.8226, 5.5400, 5.7337],
        [4.8226, 4.9286, 4.8669],
        [4.8226, 5.0053, 4.9305],
        [4.8226, 4.8400, 4.8250]], grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.14260590076446533 
model_pd.l_d.mean(): -17.206602096557617 
model_pd.lagr.mean(): -17.063995361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.1438])), ('power', tensor([0.9039]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4706])), ('power', tensor([-20.8704]))])
epoch：91	 i:0 	 global-step:1820	 l-p:0.14260590076446533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.8281, 4.9827, 4.9103],
        [4.8281, 4.9735, 4.9025],
        [4.8281, 5.9706, 6.5968],
        [4.8281, 4.8281, 4.8281]], grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.14210742712020874 
model_pd.l_d.mean(): -17.182268142700195 
model_pd.lagr.mean(): -17.0401611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.1452])), ('power', tensor([0.9029]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4691])), ('power', tensor([-20.8681]))])
epoch：92	 i:0 	 global-step:1840	 l-p:0.14210742712020874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.8333, 4.9394, 4.8776],
        [4.8333, 4.8902, 4.8493],
        [4.8333, 4.9788, 4.9077],
        [4.8333, 6.3001, 7.3294]], grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.14164122939109802 
model_pd.l_d.mean(): -17.157962799072266 
model_pd.lagr.mean(): -17.016321182250977 
model_pd.lambdas: dict_items([('pout', tensor([1.1467])), ('power', tensor([0.9018]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4678])), ('power', tensor([-20.8659]))])
epoch：93	 i:0 	 global-step:1860	 l-p:0.14164122939109802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[4.8380, 4.8380, 4.8380],
        [4.8380, 4.8463, 4.8387],
        [4.8380, 4.8929, 4.8531],
        [4.8380, 5.3479, 5.3848]], grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.14122644066810608 
model_pd.l_d.mean(): -17.133712768554688 
model_pd.lagr.mean(): -16.99248695373535 
model_pd.lambdas: dict_items([('pout', tensor([1.1482])), ('power', tensor([0.9008]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4665])), ('power', tensor([-20.8640]))])
epoch：94	 i:0 	 global-step:1880	 l-p:0.14122644066810608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.8421, 4.9877, 4.9165],
        [4.8421, 4.8421, 4.8421],
        [4.8421, 4.9968, 4.9243],
        [4.8421, 5.1343, 5.0717]], grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.1408759355545044 
model_pd.l_d.mean(): -17.109525680541992 
model_pd.lagr.mean(): -16.96864891052246 
model_pd.lambdas: dict_items([('pout', tensor([1.1496])), ('power', tensor([0.8997]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4654])), ('power', tensor([-20.8623]))])
epoch：95	 i:0 	 global-step:1900	 l-p:0.1408759355545044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[4.8454, 6.2123, 7.1092],
        [4.8454, 5.3243, 5.3421],
        [4.8454, 6.0662, 6.7823],
        [4.8454, 4.8459, 4.8454]], grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.14059701561927795 
model_pd.l_d.mean(): -17.085416793823242 
model_pd.lagr.mean(): -16.944820404052734 
model_pd.lambdas: dict_items([('pout', tensor([1.1511])), ('power', tensor([0.8987]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4646])), ('power', tensor([-20.8610]))])
epoch：96	 i:0 	 global-step:1920	 l-p:0.14059701561927795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.8479, 6.3179, 7.3487],
        [4.8479, 5.0534, 4.9782],
        [4.8479, 5.7295, 6.0730],
        [4.8479, 4.8479, 4.8479]], grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.1403913050889969 
model_pd.l_d.mean(): -17.061391830444336 
model_pd.lagr.mean(): -16.921001434326172 
model_pd.lambdas: dict_items([('pout', tensor([1.1526])), ('power', tensor([0.8976]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4639])), ('power', tensor([-20.8600]))])
epoch：97	 i:0 	 global-step:1940	 l-p:0.1403913050889969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.8496, 5.0550, 4.9799],
        [4.8496, 4.9951, 4.9239],
        [4.8496, 5.5691, 5.7627],
        [4.8496, 4.8497, 4.8496]], grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.14025557041168213 
model_pd.l_d.mean(): -17.03744125366211 
model_pd.lagr.mean(): -16.897186279296875 
model_pd.lambdas: dict_items([('pout', tensor([1.1540])), ('power', tensor([0.8966]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4635])), ('power', tensor([-20.8594]))])
epoch：98	 i:0 	 global-step:1960	 l-p:0.14025557041168213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[4.8506, 6.2178, 7.1145],
        [4.8506, 5.7249, 6.0611],
        [4.8506, 5.1427, 5.0799],
        [4.8506, 5.7320, 6.0752]], grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.14018245041370392 
model_pd.l_d.mean(): -17.013561248779297 
model_pd.lagr.mean(): -16.87337875366211 
model_pd.lambdas: dict_items([('pout', tensor([1.1555])), ('power', tensor([0.8956]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4632])), ('power', tensor([-20.8591]))])
epoch：99	 i:0 	 global-step:1980	 l-p:0.14018245041370392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.8511, 4.8844, 4.8577],
        [4.8511, 6.0188, 6.6717],
        [4.8511, 5.0056, 4.9331],
        [4.8511, 4.9059, 4.8661]], grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.14016102254390717 
model_pd.l_d.mean(): -16.98973274230957 
model_pd.lagr.mean(): -16.849571228027344 
model_pd.lambdas: dict_items([('pout', tensor([1.1570])), ('power', tensor([0.8945]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4632])), ('power', tensor([-20.8591]))])
epoch：100	 i:0 	 global-step:2000	 l-p:0.14016102254390717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.8511, 4.8511, 4.8511],
        [4.8511, 4.8511, 4.8511],
        [4.8511, 6.3202, 7.3498],
        [4.8511, 4.8557, 4.8513]], grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.14017808437347412 
model_pd.l_d.mean(): -16.96595001220703 
model_pd.lagr.mean(): -16.82577133178711 
model_pd.lambdas: dict_items([('pout', tensor([1.1584])), ('power', tensor([0.8935]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4632])), ('power', tensor([-20.8592]))])
epoch：101	 i:0 	 global-step:2020	 l-p:0.14017808437347412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.8508, 4.9766, 4.9095],
        [4.8508, 4.8509, 4.8508],
        [4.8508, 5.5306, 5.6915],
        [4.8508, 5.7239, 6.0593]], grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.1402188539505005 
model_pd.l_d.mean(): -16.942188262939453 
model_pd.lagr.mean(): -16.801969528198242 
model_pd.lambdas: dict_items([('pout', tensor([1.1599])), ('power', tensor([0.8924]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4633])), ('power', tensor([-20.8595]))])
epoch：102	 i:0 	 global-step:2040	 l-p:0.1402188539505005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8505, 4.9955, 4.9245],
        [4.8505, 5.0859, 5.0131],
        [4.8505, 5.2989, 5.2999],
        [4.8505, 4.8505, 4.8505]], grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.1402679681777954 
model_pd.l_d.mean(): -16.918434143066406 
model_pd.lagr.mean(): -16.778165817260742 
model_pd.lambdas: dict_items([('pout', tensor([1.1614])), ('power', tensor([0.8914]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4634])), ('power', tensor([-20.8598]))])
epoch：103	 i:0 	 global-step:2060	 l-p:0.1402679681777954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[4.8502, 4.8504, 4.8502],
        [4.8502, 4.8506, 4.8502],
        [4.8502, 4.9060, 4.8657],
        [4.8502, 4.8503, 4.8502]], grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.140310600399971 
model_pd.l_d.mean(): -16.894678115844727 
model_pd.lagr.mean(): -16.75436782836914 
model_pd.lambdas: dict_items([('pout', tensor([1.1628])), ('power', tensor([0.8903]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4635])), ('power', tensor([-20.8601]))])
epoch：104	 i:0 	 global-step:2080	 l-p:0.140310600399971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.8501, 4.8833, 4.8568],
        [4.8501, 4.8505, 4.8501],
        [4.8501, 5.3121, 5.3208],
        [4.8501, 4.8501, 4.8501]], grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.1403336226940155 
model_pd.l_d.mean(): -16.870895385742188 
model_pd.lagr.mean(): -16.730562210083008 
model_pd.lambdas: dict_items([('pout', tensor([1.1643])), ('power', tensor([0.8893]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4636])), ('power', tensor([-20.8603]))])
epoch：105	 i:0 	 global-step:2100	 l-p:0.1403336226940155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.8504, 5.7288, 6.0702],
        [4.8504, 4.8508, 4.8504],
        [4.8504, 4.8504, 4.8504],
        [4.8504, 4.8836, 4.8571]], grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.14032608270645142 
model_pd.l_d.mean(): -16.847089767456055 
model_pd.lagr.mean(): -16.706764221191406 
model_pd.lambdas: dict_items([('pout', tensor([1.1657])), ('power', tensor([0.8883]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4635])), ('power', tensor([-20.8604]))])
epoch：106	 i:0 	 global-step:2120	 l-p:0.14032608270645142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.8512, 5.3587, 5.3946],
        [4.8512, 4.8558, 4.8514],
        [4.8512, 4.8512, 4.8512],
        [4.8512, 4.8512, 4.8512]], grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.14028051495552063 
model_pd.l_d.mean(): -16.823244094848633 
model_pd.lagr.mean(): -16.682964324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.1672])), ('power', tensor([0.8872]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4633])), ('power', tensor([-20.8602]))])
epoch：107	 i:0 	 global-step:2140	 l-p:0.14028051495552063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.8524, 4.8696, 4.8547],
        [4.8524, 4.8524, 4.8524],
        [4.8524, 4.8606, 4.8531],
        [4.8524, 5.2703, 5.2559]], grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.1401926577091217 
model_pd.l_d.mean(): -16.799358367919922 
model_pd.lagr.mean(): -16.65916633605957 
model_pd.lambdas: dict_items([('pout', tensor([1.1687])), ('power', tensor([0.8862]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4631])), ('power', tensor([-20.8599]))])
epoch：108	 i:0 	 global-step:2160	 l-p:0.1401926577091217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[4.8541, 5.0076, 4.9355],
        [4.8541, 4.8541, 4.8541],
        [4.8541, 4.8834, 4.8595],
        [4.8541, 5.0887, 5.0160]], grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.14006203413009644 
model_pd.l_d.mean(): -16.775432586669922 
model_pd.lagr.mean(): -16.6353702545166 
model_pd.lambdas: dict_items([('pout', tensor([1.1701])), ('power', tensor([0.8851]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4626])), ('power', tensor([-20.8593]))])
epoch：109	 i:0 	 global-step:2180	 l-p:0.14006203413009644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[4.8562, 4.8562, 4.8562],
        [4.8562, 5.2339, 5.2009],
        [4.8562, 6.3224, 7.3484],
        [4.8562, 4.8734, 4.8585]], grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.13989144563674927 
model_pd.l_d.mean(): -16.751480102539062 
model_pd.lagr.mean(): -16.611589431762695 
model_pd.lambdas: dict_items([('pout', tensor([1.1716])), ('power', tensor([0.8841]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4621])), ('power', tensor([-20.8585]))])
epoch：110	 i:0 	 global-step:2200	 l-p:0.13989144563674927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[4.8588, 4.9143, 4.8742],
        [4.8588, 4.8591, 4.8588],
        [4.8588, 4.8592, 4.8588],
        [4.8588, 5.2364, 5.2034]], grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.13968661427497864 
model_pd.l_d.mean(): -16.727497100830078 
model_pd.lagr.mean(): -16.587810516357422 
model_pd.lambdas: dict_items([('pout', tensor([1.1731])), ('power', tensor([0.8830]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4614])), ('power', tensor([-20.8575]))])
epoch：111	 i:0 	 global-step:2220	 l-p:0.13968661427497864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.8617, 5.2393, 5.2062],
        [4.8617, 4.9177, 4.8773],
        [4.8617, 4.8617, 4.8617],
        [4.8617, 4.8617, 4.8617]], grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.1394551396369934 
model_pd.l_d.mean(): -16.7034854888916 
model_pd.lagr.mean(): -16.564029693603516 
model_pd.lambdas: dict_items([('pout', tensor([1.1745])), ('power', tensor([0.8820]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4607])), ('power', tensor([-20.8564]))])
epoch：112	 i:0 	 global-step:2240	 l-p:0.1394551396369934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.8648, 4.8648, 4.8648],
        [4.8648, 5.3259, 5.3341],
        [4.8648, 4.8730, 4.8655],
        [4.8648, 4.8693, 4.8650]], grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.13920600712299347 
model_pd.l_d.mean(): -16.679462432861328 
model_pd.lagr.mean(): -16.54025650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.1760])), ('power', tensor([0.8810]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4599])), ('power', tensor([-20.8551]))])
epoch：113	 i:0 	 global-step:2260	 l-p:0.13920600712299347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.8680, 4.9243, 4.8838],
        [4.8680, 4.8686, 4.8680],
        [4.8680, 4.8680, 4.8680],
        [4.8680, 6.3361, 7.3627]], grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.1389486938714981 
model_pd.l_d.mean(): -16.65543556213379 
model_pd.lagr.mean(): -16.51648712158203 
model_pd.lambdas: dict_items([('pout', tensor([1.1774])), ('power', tensor([0.8799]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4590])), ('power', tensor([-20.8539]))])
epoch：114	 i:0 	 global-step:2280	 l-p:0.1389486938714981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.8712, 5.3324, 5.3405],
        [4.8712, 4.9963, 4.9294],
        [4.8712, 6.0901, 6.8024],
        [4.8712, 4.8712, 4.8712]], grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.13869209587574005 
model_pd.l_d.mean(): -16.631412506103516 
model_pd.lagr.mean(): -16.492719650268555 
model_pd.lambdas: dict_items([('pout', tensor([1.1789])), ('power', tensor([0.8789]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4582])), ('power', tensor([-20.8526]))])
epoch：115	 i:0 	 global-step:2300	 l-p:0.13869209587574005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.8744, 4.9287, 4.8893],
        [4.8744, 5.6773, 5.9454],
        [4.8744, 5.1786, 5.1195],
        [4.8744, 5.0558, 4.9811]], grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.13844417035579681 
model_pd.l_d.mean(): -16.607406616210938 
model_pd.lagr.mean(): -16.468961715698242 
model_pd.lambdas: dict_items([('pout', tensor([1.1803])), ('power', tensor([0.8778]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4574])), ('power', tensor([-20.8513]))])
epoch：116	 i:0 	 global-step:2320	 l-p:0.13844417035579681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.8774, 6.0969, 6.8094],
        [4.8774, 4.9329, 4.8928],
        [4.8774, 4.8946, 4.8797],
        [4.8774, 4.8774, 4.8774]], grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.13821148872375488 
model_pd.l_d.mean(): -16.58341407775879 
model_pd.lagr.mean(): -16.445201873779297 
model_pd.lambdas: dict_items([('pout', tensor([1.1818])), ('power', tensor([0.8768]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4566])), ('power', tensor([-20.8501]))])
epoch：117	 i:0 	 global-step:2340	 l-p:0.13821148872375488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[4.8802, 4.8865, 4.8806],
        [4.8802, 5.0615, 4.9868],
        [4.8802, 5.2577, 5.2243],
        [4.8802, 5.0334, 4.9613]], grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.13799867033958435 
model_pd.l_d.mean(): -16.5594482421875 
model_pd.lagr.mean(): -16.421449661254883 
model_pd.lambdas: dict_items([('pout', tensor([1.1833])), ('power', tensor([0.8757]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4559])), ('power', tensor([-20.8490]))])
epoch：118	 i:0 	 global-step:2360	 l-p:0.13799867033958435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.8827, 4.8909, 4.8834],
        [4.8827, 5.7546, 6.0878],
        [4.8827, 4.8827, 4.8827],
        [4.8827, 5.5993, 5.7903]], grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.1378084123134613 
model_pd.l_d.mean(): -16.535507202148438 
model_pd.lagr.mean(): -16.3976993560791 
model_pd.lambdas: dict_items([('pout', tensor([1.1847])), ('power', tensor([0.8747]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4553])), ('power', tensor([-20.8481]))])
epoch：119	 i:0 	 global-step:2380	 l-p:0.1378084123134613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[4.8850, 5.6880, 5.9557],
        [4.8850, 4.9142, 4.8904],
        [4.8850, 4.8850, 4.8850],
        [4.8850, 4.8852, 4.8850]], grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.13764162361621857 
model_pd.l_d.mean(): -16.511594772338867 
model_pd.lagr.mean(): -16.373952865600586 
model_pd.lambdas: dict_items([('pout', tensor([1.1862])), ('power', tensor([0.8737]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4547])), ('power', tensor([-20.8472]))])
epoch：120	 i:0 	 global-step:2400	 l-p:0.13764162361621857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.8869, 5.1767, 5.1137],
        [4.8869, 4.8951, 4.8876],
        [4.8869, 4.9918, 4.9306],
        [4.8869, 5.0680, 4.9934]], grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.13749881088733673 
model_pd.l_d.mean(): -16.487707138061523 
model_pd.lagr.mean(): -16.350208282470703 
model_pd.lambdas: dict_items([('pout', tensor([1.1876])), ('power', tensor([0.8726]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4542])), ('power', tensor([-20.8465]))])
epoch：121	 i:0 	 global-step:2420	 l-p:0.13749881088733673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.8886, 4.9215, 4.8952],
        [4.8886, 4.8889, 4.8886],
        [4.8886, 5.0697, 4.9950],
        [4.8886, 4.9447, 4.9043]], grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.13737896084785461 
model_pd.l_d.mean(): -16.463844299316406 
model_pd.lagr.mean(): -16.326465606689453 
model_pd.lambdas: dict_items([('pout', tensor([1.1891])), ('power', tensor([0.8716]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4538])), ('power', tensor([-20.8459]))])
epoch：122	 i:0 	 global-step:2440	 l-p:0.13737896084785461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.8901, 4.8946, 4.8903],
        [4.8901, 5.6062, 5.7967],
        [4.8901, 6.1100, 6.8217],
        [4.8901, 4.9462, 4.9058]], grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13727834820747375 
model_pd.l_d.mean(): -16.440000534057617 
model_pd.lagr.mean(): -16.302722930908203 
model_pd.lambdas: dict_items([('pout', tensor([1.1905])), ('power', tensor([0.8705]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4534])), ('power', tensor([-20.8454]))])
epoch：123	 i:0 	 global-step:2460	 l-p:0.13727834820747375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[4.8914, 4.9466, 4.9067],
        [4.8914, 5.3667, 5.3826],
        [4.8914, 5.3084, 5.2931],
        [4.8914, 5.0946, 5.0197]], grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.13719218969345093 
model_pd.l_d.mean(): -16.41617774963379 
model_pd.lagr.mean(): -16.27898597717285 
model_pd.lambdas: dict_items([('pout', tensor([1.1920])), ('power', tensor([0.8695]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4531])), ('power', tensor([-20.8450]))])
epoch：124	 i:0 	 global-step:2480	 l-p:0.13719218969345093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.8925, 5.0453, 4.9733],
        [4.8925, 5.3386, 5.3383],
        [4.8925, 4.8928, 4.8925],
        [4.8925, 5.5698, 5.7280]], grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.13711482286453247 
model_pd.l_d.mean(): -16.392362594604492 
model_pd.lagr.mean(): -16.255247116088867 
model_pd.lambdas: dict_items([('pout', tensor([1.1934])), ('power', tensor([0.8684]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4528])), ('power', tensor([-20.8447]))])
epoch：125	 i:0 	 global-step:2500	 l-p:0.13711482286453247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[4.8937, 4.9477, 4.9084],
        [4.8937, 5.3686, 5.3844],
        [4.8937, 5.7646, 6.0966],
        [4.8937, 5.3998, 5.4343]], grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.13704043626785278 
model_pd.l_d.mean(): -16.36855125427246 
model_pd.lagr.mean(): -16.231510162353516 
model_pd.lambdas: dict_items([('pout', tensor([1.1949])), ('power', tensor([0.8674]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4526])), ('power', tensor([-20.8443]))])
epoch：126	 i:0 	 global-step:2520	 l-p:0.13704043626785278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.8948, 4.8948, 4.8948],
        [4.8948, 6.3650, 7.3905],
        [4.8948, 4.8951, 4.8948],
        [4.8948, 5.0192, 4.9526]], grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.1369631439447403 
model_pd.l_d.mean(): -16.344738006591797 
model_pd.lagr.mean(): -16.207775115966797 
model_pd.lambdas: dict_items([('pout', tensor([1.1963])), ('power', tensor([0.8664]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4523])), ('power', tensor([-20.8440]))])
epoch：127	 i:0 	 global-step:2540	 l-p:0.1369631439447403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[4.8961, 5.5268, 5.6482],
        [4.8961, 5.1294, 5.0566],
        [4.8961, 5.0486, 4.9767],
        [4.8961, 5.7738, 6.1123]], grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.1368778795003891 
model_pd.l_d.mean(): -16.320919036865234 
model_pd.lagr.mean(): -16.18404197692871 
model_pd.lambdas: dict_items([('pout', tensor([1.1978])), ('power', tensor([0.8653]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4520])), ('power', tensor([-20.8435]))])
epoch：128	 i:0 	 global-step:2560	 l-p:0.1368778795003891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[4.8975, 6.0633, 6.7109],
        [4.8975, 5.6125, 5.8020],
        [4.8975, 4.9021, 4.8978],
        [4.8975, 5.0779, 5.0034]], grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.1367807239294052 
model_pd.l_d.mean(): -16.29709243774414 
model_pd.lagr.mean(): -16.16031265258789 
model_pd.lambdas: dict_items([('pout', tensor([1.1992])), ('power', tensor([0.8643]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4516])), ('power', tensor([-20.8431]))])
epoch：129	 i:0 	 global-step:2580	 l-p:0.1367807239294052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.8991, 5.1323, 5.0595],
        [4.8991, 6.0421, 6.6633],
        [4.8991, 4.9162, 4.9014],
        [4.8991, 5.0233, 4.9568]], grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.13666900992393494 
model_pd.l_d.mean(): -16.27324867248535 
model_pd.lagr.mean(): -16.136579513549805 
model_pd.lambdas: dict_items([('pout', tensor([1.2007])), ('power', tensor([0.8632]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4512])), ('power', tensor([-20.8425]))])
epoch：130	 i:0 	 global-step:2600	 l-p:0.13666900992393494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[4.9009, 5.5772, 5.7346],
        [4.9009, 5.2036, 5.1442],
        [4.9009, 4.9010, 4.9009],
        [4.9009, 5.5313, 5.6523]], grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.13654151558876038 
model_pd.l_d.mean(): -16.249391555786133 
model_pd.lagr.mean(): -16.112850189208984 
model_pd.lambdas: dict_items([('pout', tensor([1.2022])), ('power', tensor([0.8622]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4508])), ('power', tensor([-20.8418]))])
epoch：131	 i:0 	 global-step:2620	 l-p:0.13654151558876038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[4.9029, 4.9030, 4.9029],
        [4.9029, 5.4083, 5.4423],
        [4.9029, 4.9356, 4.9095],
        [4.9029, 5.7731, 6.1041]], grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.13639825582504272 
model_pd.l_d.mean(): -16.22552490234375 
model_pd.lagr.mean(): -16.089126586914062 
model_pd.lambdas: dict_items([('pout', tensor([1.2036])), ('power', tensor([0.8611]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4503])), ('power', tensor([-20.8411]))])
epoch：132	 i:0 	 global-step:2640	 l-p:0.13639825582504272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.9051, 4.9590, 4.9198],
        [4.9051, 5.0853, 5.0108],
        [4.9051, 5.1811, 5.1152],
        [4.9051, 4.9054, 4.9052]], grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.1362408548593521 
model_pd.l_d.mean(): -16.201642990112305 
model_pd.lagr.mean(): -16.06540298461914 
model_pd.lambdas: dict_items([('pout', tensor([1.2051])), ('power', tensor([0.8601]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4497])), ('power', tensor([-20.8402]))])
epoch：133	 i:0 	 global-step:2660	 l-p:0.1362408548593521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[4.9075, 4.9079, 4.9075],
        [4.9075, 5.4127, 5.4466],
        [4.9075, 5.3667, 5.3735],
        [4.9075, 4.9075, 4.9075]], grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13607169687747955 
model_pd.l_d.mean(): -16.177757263183594 
model_pd.lagr.mean(): -16.041685104370117 
model_pd.lambdas: dict_items([('pout', tensor([1.2065])), ('power', tensor([0.8591]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4491])), ('power', tensor([-20.8393]))])
epoch：134	 i:0 	 global-step:2680	 l-p:0.13607169687747955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.9100, 4.9181, 4.9107],
        [4.9100, 5.1123, 5.0375],
        [4.9100, 6.0762, 6.7232],
        [4.9100, 5.5861, 5.7431]], grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.13589425384998322 
model_pd.l_d.mean(): -16.15386199951172 
model_pd.lagr.mean(): -16.017967224121094 
model_pd.lambdas: dict_items([('pout', tensor([1.2079])), ('power', tensor([0.8580]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4485])), ('power', tensor([-20.8384]))])
epoch：135	 i:0 	 global-step:2700	 l-p:0.13589425384998322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.9125, 5.2007, 5.1375],
        [4.9125, 6.1323, 6.8419],
        [4.9125, 4.9206, 4.9132],
        [4.9125, 4.9125, 4.9125]], grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.13571226596832275 
model_pd.l_d.mean(): -16.129966735839844 
model_pd.lagr.mean(): -15.994254112243652 
model_pd.lambdas: dict_items([('pout', tensor([1.2094])), ('power', tensor([0.8570]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4478])), ('power', tensor([-20.8373]))])
epoch：136	 i:0 	 global-step:2720	 l-p:0.13571226596832275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[4.9150, 4.9232, 4.9157],
        [4.9150, 5.2174, 5.1578],
        [4.9150, 5.0390, 4.9725],
        [4.9150, 5.5913, 5.7481]], grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.13553091883659363 
model_pd.l_d.mean(): -16.106067657470703 
model_pd.lagr.mean(): -15.970537185668945 
model_pd.lambdas: dict_items([('pout', tensor([1.2108])), ('power', tensor([0.8559]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4472])), ('power', tensor([-20.8363]))])
epoch：137	 i:0 	 global-step:2740	 l-p:0.13553091883659363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.9175, 4.9713, 4.9322],
        [4.9175, 6.0615, 6.6822],
        [4.9175, 5.7190, 5.9843],
        [4.9175, 4.9724, 4.9327]], grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13535425066947937 
model_pd.l_d.mean(): -16.082185745239258 
model_pd.lagr.mean(): -15.946831703186035 
model_pd.lambdas: dict_items([('pout', tensor([1.2123])), ('power', tensor([0.8549]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4466])), ('power', tensor([-20.8353]))])
epoch：138	 i:0 	 global-step:2760	 l-p:0.13535425066947937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.9199, 4.9203, 4.9199],
        [4.9199, 5.3938, 5.4088],
        [4.9199, 4.9280, 4.9206],
        [4.9199, 6.2883, 7.1791]], grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.13518570363521576 
model_pd.l_d.mean(): -16.058305740356445 
model_pd.lagr.mean(): -15.923120498657227 
model_pd.lambdas: dict_items([('pout', tensor([1.2137])), ('power', tensor([0.8539]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4460])), ('power', tensor([-20.8344]))])
epoch：139	 i:0 	 global-step:2780	 l-p:0.13518570363521576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[4.9222, 5.2244, 5.1648],
        [4.9222, 5.7928, 6.1232],
        [4.9222, 4.9267, 4.9225],
        [4.9222, 5.2976, 5.2634]], grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.135027214884758 
model_pd.l_d.mean(): -16.034442901611328 
model_pd.lagr.mean(): -15.899415969848633 
model_pd.lambdas: dict_items([('pout', tensor([1.2152])), ('power', tensor([0.8528]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4454])), ('power', tensor([-20.8335]))])
epoch：140	 i:0 	 global-step:2800	 l-p:0.135027214884758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.9243, 4.9413, 4.9266],
        [4.9243, 4.9799, 4.9398],
        [4.9243, 5.1999, 5.1338],
        [4.9243, 5.8012, 6.1374]], grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13487975299358368 
model_pd.l_d.mean(): -16.01059341430664 
model_pd.lagr.mean(): -15.875713348388672 
model_pd.lambdas: dict_items([('pout', tensor([1.2166])), ('power', tensor([0.8518]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4448])), ('power', tensor([-20.8326]))])
epoch：141	 i:0 	 global-step:2820	 l-p:0.13487975299358368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.9263, 5.4001, 5.4148],
        [4.9263, 4.9553, 4.9317],
        [4.9263, 4.9266, 4.9263],
        [4.9263, 4.9326, 4.9268]], grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.1347431242465973 
model_pd.l_d.mean(): -15.986757278442383 
model_pd.lagr.mean(): -15.852014541625977 
model_pd.lambdas: dict_items([('pout', tensor([1.2181])), ('power', tensor([0.8507]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4443])), ('power', tensor([-20.8319]))])
epoch：142	 i:0 	 global-step:2840	 l-p:0.1347431242465973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[4.9282, 4.9363, 4.9289],
        [4.9282, 4.9607, 4.9347],
        [4.9282, 5.8050, 6.1411],
        [4.9282, 4.9327, 4.9285]], grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.13461598753929138 
model_pd.l_d.mean(): -15.962930679321289 
model_pd.lagr.mean(): -15.828314781188965 
model_pd.lambdas: dict_items([('pout', tensor([1.2195])), ('power', tensor([0.8497]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4439])), ('power', tensor([-20.8312]))])
epoch：143	 i:0 	 global-step:2860	 l-p:0.13461598753929138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[4.9300, 4.9305, 4.9300],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9315, 4.9300]], grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13449639081954956 
model_pd.l_d.mean(): -15.939116477966309 
model_pd.lagr.mean(): -15.804619789123535 
model_pd.lambdas: dict_items([('pout', tensor([1.2210])), ('power', tensor([0.8486]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4434])), ('power', tensor([-20.8305]))])
epoch：144	 i:0 	 global-step:2880	 l-p:0.13449639081954956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.9317, 4.9317, 4.9317],
        [4.9317, 4.9853, 4.9463],
        [4.9317, 6.3009, 7.1913],
        [4.9317, 5.0743, 5.0040]], grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.13438165187835693 
model_pd.l_d.mean(): -15.915310859680176 
model_pd.lagr.mean(): -15.780929565429688 
model_pd.lambdas: dict_items([('pout', tensor([1.2224])), ('power', tensor([0.8476]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4430])), ('power', tensor([-20.8298]))])
epoch：145	 i:0 	 global-step:2900	 l-p:0.13438165187835693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.9334, 4.9415, 4.9341],
        [4.9334, 4.9503, 4.9356],
        [4.9334, 4.9623, 4.9387],
        [4.9334, 5.7347, 5.9991]], grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.13426874577999115 
model_pd.l_d.mean(): -15.89150619506836 
model_pd.lagr.mean(): -15.757237434387207 
model_pd.lambdas: dict_items([('pout', tensor([1.2238])), ('power', tensor([0.8466]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4426])), ('power', tensor([-20.8292]))])
epoch：146	 i:0 	 global-step:2920	 l-p:0.13426874577999115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[4.9351, 6.0797, 6.6995],
        [4.9351, 4.9903, 4.9504],
        [4.9351, 5.6495, 5.8372],
        [4.9351, 4.9354, 4.9351]], grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13415499031543732 
model_pd.l_d.mean(): -15.867704391479492 
model_pd.lagr.mean(): -15.733549118041992 
model_pd.lambdas: dict_items([('pout', tensor([1.2253])), ('power', tensor([0.8455]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4422])), ('power', tensor([-20.8285]))])
epoch：147	 i:0 	 global-step:2940	 l-p:0.13415499031543732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[4.9369, 5.3117, 5.2771],
        [4.9369, 6.3063, 7.1964],
        [4.9369, 5.0405, 4.9798],
        [4.9369, 5.0884, 5.0167]], grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.13403749465942383 
model_pd.l_d.mean(): -15.843902587890625 
model_pd.lagr.mean(): -15.70986557006836 
model_pd.lambdas: dict_items([('pout', tensor([1.2267])), ('power', tensor([0.8445]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4417])), ('power', tensor([-20.8279]))])
epoch：148	 i:0 	 global-step:2960	 l-p:0.13403749465942383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.9387, 4.9387, 4.9387],
        [4.9387, 4.9390, 4.9387],
        [4.9387, 4.9403, 4.9388],
        [4.9387, 4.9922, 4.9533]], grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.13391442596912384 
model_pd.l_d.mean(): -15.820096969604492 
model_pd.lagr.mean(): -15.686182975769043 
model_pd.lambdas: dict_items([('pout', tensor([1.2282])), ('power', tensor([0.8434]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4413])), ('power', tensor([-20.8272]))])
epoch：149	 i:0 	 global-step:2980	 l-p:0.13391442596912384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[4.9406, 5.4138, 5.4280],
        [4.9406, 5.6549, 5.8423],
        [4.9406, 4.9407, 4.9407],
        [4.9406, 5.8172, 6.1524]], grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.1337842047214508 
model_pd.l_d.mean(): -15.796283721923828 
model_pd.lagr.mean(): -15.66249942779541 
model_pd.lambdas: dict_items([('pout', tensor([1.2296])), ('power', tensor([0.8424]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4408])), ('power', tensor([-20.8264]))])
epoch：150	 i:0 	 global-step:3000	 l-p:0.1337842047214508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[4.9427, 4.9430, 4.9427],
        [4.9427, 4.9433, 4.9427],
        [4.9427, 4.9715, 4.9480],
        [4.9427, 5.0661, 4.9998]], grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13364629447460175 
model_pd.l_d.mean(): -15.772470474243164 
model_pd.lagr.mean(): -15.638824462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.2310])), ('power', tensor([0.8414]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4403])), ('power', tensor([-20.8256]))])
epoch：151	 i:0 	 global-step:3020	 l-p:0.13364629447460175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[4.9449, 6.4197, 7.4441],
        [4.9449, 5.4179, 5.4320],
        [4.9449, 5.3888, 5.3869],
        [4.9449, 5.1768, 5.1038]], grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.1335005909204483 
model_pd.l_d.mean(): -15.748649597167969 
model_pd.lagr.mean(): -15.615148544311523 
model_pd.lambdas: dict_items([('pout', tensor([1.2325])), ('power', tensor([0.8403]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4397])), ('power', tensor([-20.8247]))])
epoch：152	 i:0 	 global-step:3040	 l-p:0.1335005909204483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[4.9472, 4.9472, 4.9472],
        [4.9472, 4.9472, 4.9472],
        [4.9472, 5.6228, 5.7781],
        [4.9472, 5.0017, 4.9622]], grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.1333480030298233 
model_pd.l_d.mean(): -15.724824905395508 
model_pd.lagr.mean(): -15.591476440429688 
model_pd.lambdas: dict_items([('pout', tensor([1.2339])), ('power', tensor([0.8393]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4391])), ('power', tensor([-20.8238]))])
epoch：153	 i:0 	 global-step:3060	 l-p:0.1333480030298233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[4.9495, 6.0949, 6.7143],
        [4.9495, 5.0918, 5.0216],
        [4.9495, 4.9495, 4.9495],
        [4.9495, 6.3203, 7.2103]], grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.133189857006073 
model_pd.l_d.mean(): -15.701000213623047 
model_pd.lagr.mean(): -15.56781005859375 
model_pd.lambdas: dict_items([('pout', tensor([1.2354])), ('power', tensor([0.8382]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4385])), ('power', tensor([-20.8228]))])
epoch：154	 i:0 	 global-step:3080	 l-p:0.133189857006073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.9519, 5.8298, 6.1656],
        [4.9519, 5.0752, 5.0089],
        [4.9519, 5.4561, 5.4886],
        [4.9519, 5.0073, 4.9673]], grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.13302788138389587 
model_pd.l_d.mean(): -15.677172660827637 
model_pd.lagr.mean(): -15.544144630432129 
model_pd.lambdas: dict_items([('pout', tensor([1.2368])), ('power', tensor([0.8372]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4379])), ('power', tensor([-20.8218]))])
epoch：155	 i:0 	 global-step:3100	 l-p:0.13302788138389587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[4.9544, 5.4125, 5.4180],
        [4.9544, 5.5840, 5.7029],
        [4.9544, 5.0966, 5.0264],
        [4.9544, 4.9544, 4.9544]], grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.13286390900611877 
model_pd.l_d.mean(): -15.653347969055176 
model_pd.lagr.mean(): -15.52048397064209 
model_pd.lambdas: dict_items([('pout', tensor([1.2382])), ('power', tensor([0.8362]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4373])), ('power', tensor([-20.8208]))])
epoch：156	 i:0 	 global-step:3120	 l-p:0.13286390900611877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[4.9569, 4.9569, 4.9569],
        [4.9569, 5.8278, 6.1565],
        [4.9569, 6.1029, 6.7223],
        [4.9569, 5.0991, 5.0288]], grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.13269975781440735 
model_pd.l_d.mean(): -15.629524230957031 
model_pd.lagr.mean(): -15.496824264526367 
model_pd.lambdas: dict_items([('pout', tensor([1.2397])), ('power', tensor([0.8351]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4367])), ('power', tensor([-20.8198]))])
epoch：157	 i:0 	 global-step:3140	 l-p:0.13269975781440735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[4.9593, 4.9594, 4.9593],
        [4.9593, 5.0627, 5.0021],
        [4.9593, 5.0825, 5.0163],
        [4.9593, 5.0146, 4.9747]], grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.13253723084926605 
model_pd.l_d.mean(): -15.605707168579102 
model_pd.lagr.mean(): -15.473170280456543 
model_pd.lambdas: dict_items([('pout', tensor([1.2411])), ('power', tensor([0.8341]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4361])), ('power', tensor([-20.8188]))])
epoch：158	 i:0 	 global-step:3160	 l-p:0.13253723084926605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[4.9617, 4.9623, 4.9617],
        [4.9617, 5.3361, 5.3011],
        [4.9617, 5.5914, 5.7101],
        [4.9617, 4.9617, 4.9617]], grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.1323777586221695 
model_pd.l_d.mean(): -15.581897735595703 
model_pd.lagr.mean(): -15.449520111083984 
model_pd.lambdas: dict_items([('pout', tensor([1.2425])), ('power', tensor([0.8330]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4354])), ('power', tensor([-20.8178]))])
epoch：159	 i:0 	 global-step:3180	 l-p:0.1323777586221695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[4.9641, 5.4222, 5.4275],
        [4.9641, 4.9657, 4.9642],
        [4.9641, 5.1153, 5.0436],
        [4.9641, 6.1338, 6.7795]], grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.13222205638885498 
model_pd.l_d.mean(): -15.558091163635254 
model_pd.lagr.mean(): -15.42586898803711 
model_pd.lambdas: dict_items([('pout', tensor([1.2440])), ('power', tensor([0.8320]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4348])), ('power', tensor([-20.8168]))])
epoch：160	 i:0 	 global-step:3200	 l-p:0.13222205638885498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[4.9664, 6.3395, 7.2299],
        [4.9664, 5.0895, 5.0233],
        [4.9664, 4.9664, 4.9664],
        [4.9664, 4.9668, 4.9664]], grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.13207083940505981 
model_pd.l_d.mean(): -15.534296035766602 
model_pd.lagr.mean(): -15.402225494384766 
model_pd.lambdas: dict_items([('pout', tensor([1.2454])), ('power', tensor([0.8309]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4343])), ('power', tensor([-20.8158]))])
epoch：161	 i:0 	 global-step:3220	 l-p:0.13207083940505981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.9687, 5.4416, 5.4551],
        [4.9687, 5.1476, 5.0732],
        [4.9687, 4.9688, 4.9687],
        [4.9687, 4.9974, 4.9740]], grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.13192392885684967 
model_pd.l_d.mean(): -15.51050853729248 
model_pd.lagr.mean(): -15.378584861755371 
model_pd.lambdas: dict_items([('pout', tensor([1.2469])), ('power', tensor([0.8299]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4337])), ('power', tensor([-20.8149]))])
epoch：162	 i:0 	 global-step:3240	 l-p:0.13192392885684967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[4.9709, 4.9725, 4.9710],
        [4.9709, 4.9754, 4.9712],
        [4.9709, 5.2578, 5.1941],
        [4.9709, 4.9996, 4.9762]], grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.13178084790706635 
model_pd.l_d.mean(): -15.486726760864258 
model_pd.lagr.mean(): -15.35494613647461 
model_pd.lambdas: dict_items([('pout', tensor([1.2483])), ('power', tensor([0.8289]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4331])), ('power', tensor([-20.8140]))])
epoch：163	 i:0 	 global-step:3260	 l-p:0.13178084790706635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[4.9731, 5.0264, 4.9876],
        [4.9731, 5.2741, 5.2139],
        [4.9731, 5.2046, 5.1315],
        [4.9731, 5.3875, 5.3702]], grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1316407173871994 
model_pd.l_d.mean(): -15.462950706481934 
model_pd.lagr.mean(): -15.331310272216797 
model_pd.lambdas: dict_items([('pout', tensor([1.2497])), ('power', tensor([0.8278]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4326])), ('power', tensor([-20.8131]))])
epoch：164	 i:0 	 global-step:3280	 l-p:0.1316407173871994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.9753, 6.3494, 7.2399],
        [4.9753, 5.8541, 6.1892],
        [4.9753, 4.9756, 4.9753],
        [4.9753, 6.4547, 7.4802]], grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13150234520435333 
model_pd.l_d.mean(): -15.43918228149414 
model_pd.lagr.mean(): -15.307680130004883 
model_pd.lambdas: dict_items([('pout', tensor([1.2512])), ('power', tensor([0.8268]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4320])), ('power', tensor([-20.8122]))])
epoch：165	 i:0 	 global-step:3300	 l-p:0.13150234520435333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[4.9775, 5.8563, 6.1914],
        [4.9775, 4.9778, 4.9775],
        [4.9775, 4.9774, 4.9775],
        [4.9775, 4.9775, 4.9775]], grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.13136467337608337 
model_pd.l_d.mean(): -15.415414810180664 
model_pd.lagr.mean(): -15.284049987792969 
model_pd.lambdas: dict_items([('pout', tensor([1.2526])), ('power', tensor([0.8257]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4315])), ('power', tensor([-20.8113]))])
epoch：166	 i:0 	 global-step:3320	 l-p:0.13136467337608337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[4.9796, 4.9877, 4.9803],
        [4.9796, 5.0119, 4.9860],
        [4.9796, 6.1275, 6.7466],
        [4.9796, 4.9796, 4.9796]], grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13122650980949402 
model_pd.l_d.mean(): -15.391653060913086 
model_pd.lagr.mean(): -15.26042652130127 
model_pd.lambdas: dict_items([('pout', tensor([1.2540])), ('power', tensor([0.8247]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4309])), ('power', tensor([-20.8104]))])
epoch：167	 i:0 	 global-step:3340	 l-p:0.13122650980949402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[4.9818, 5.2685, 5.2047],
        [4.9818, 5.0141, 4.9883],
        [4.9818, 4.9834, 4.9819],
        [4.9818, 5.4254, 5.4226]], grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.13108663260936737 
model_pd.l_d.mean(): -15.367890357971191 
model_pd.lagr.mean(): -15.236804008483887 
model_pd.lambdas: dict_items([('pout', tensor([1.2554])), ('power', tensor([0.8237]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4304])), ('power', tensor([-20.8095]))])
epoch：168	 i:0 	 global-step:3360	 l-p:0.13108663260936737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[4.9841, 5.0389, 4.9992],
        [4.9841, 5.6601, 5.8142],
        [4.9841, 4.9841, 4.9841],
        [4.9841, 5.0372, 4.9985]], grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13094596564769745 
model_pd.l_d.mean(): -15.344131469726562 
model_pd.lagr.mean(): -15.21318531036377 
model_pd.lambdas: dict_items([('pout', tensor([1.2569])), ('power', tensor([0.8226]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4298])), ('power', tensor([-20.8085]))])
epoch：169	 i:0 	 global-step:3380	 l-p:0.13094596564769745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[4.9863, 5.1871, 5.1123],
        [4.9863, 5.0150, 4.9916],
        [4.9863, 5.0414, 5.0016],
        [4.9863, 5.7886, 6.0511]], grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.13080516457557678 
model_pd.l_d.mean(): -15.320374488830566 
model_pd.lagr.mean(): -15.189569473266602 
model_pd.lambdas: dict_items([('pout', tensor([1.2583])), ('power', tensor([0.8216]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4293])), ('power', tensor([-20.8076]))])
epoch：170	 i:0 	 global-step:3400	 l-p:0.13080516457557678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[4.9886, 5.2893, 5.2289],
        [4.9886, 5.1671, 5.0928],
        [4.9886, 5.2751, 5.2112],
        [4.9886, 4.9947, 4.9890]], grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.13066434860229492 
model_pd.l_d.mean(): -15.296616554260254 
model_pd.lagr.mean(): -15.165952682495117 
model_pd.lambdas: dict_items([('pout', tensor([1.2597])), ('power', tensor([0.8205]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4287])), ('power', tensor([-20.8067]))])
epoch：171	 i:0 	 global-step:3420	 l-p:0.13066434860229492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[4.9908, 5.0456, 5.0060],
        [4.9908, 4.9908, 4.9908],
        [4.9908, 5.2650, 5.1983],
        [4.9908, 5.7058, 5.8915]], grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.13052351772785187 
model_pd.l_d.mean(): -15.272867202758789 
model_pd.lagr.mean(): -15.142343521118164 
model_pd.lambdas: dict_items([('pout', tensor([1.2612])), ('power', tensor([0.8195]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4281])), ('power', tensor([-20.8057]))])
epoch：172	 i:0 	 global-step:3440	 l-p:0.13052351772785187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[4.9931, 4.9934, 4.9931],
        [4.9931, 5.0481, 5.0084],
        [4.9931, 4.9992, 4.9935],
        [4.9931, 5.0011, 4.9938]], grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.13038258254528046 
model_pd.l_d.mean(): -15.24911880493164 
model_pd.lagr.mean(): -15.118736267089844 
model_pd.lambdas: dict_items([('pout', tensor([1.2626])), ('power', tensor([0.8185]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4276])), ('power', tensor([-20.8048]))])
epoch：173	 i:0 	 global-step:3460	 l-p:0.13038258254528046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[4.9954, 4.9969, 4.9954],
        [4.9954, 5.2960, 5.2356],
        [4.9954, 6.3719, 7.2629],
        [4.9954, 6.4777, 7.5038]], grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.13024109601974487 
model_pd.l_d.mean(): -15.22537612915039 
model_pd.lagr.mean(): -15.095134735107422 
model_pd.lambdas: dict_items([('pout', tensor([1.2640])), ('power', tensor([0.8174]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4270])), ('power', tensor([-20.8038]))])
epoch：174	 i:0 	 global-step:3480	 l-p:0.13024109601974487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[4.9977, 5.4411, 5.4380],
        [4.9977, 4.9977, 4.9977],
        [4.9977, 5.0527, 5.0129],
        [4.9977, 5.2841, 5.2201]], grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.13009849190711975 
model_pd.l_d.mean(): -15.201634407043457 
model_pd.lagr.mean(): -15.07153606414795 
model_pd.lambdas: dict_items([('pout', tensor([1.2654])), ('power', tensor([0.8164]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4264])), ('power', tensor([-20.8028]))])
epoch：175	 i:0 	 global-step:3500	 l-p:0.13009849190711975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.0000, 5.0015, 5.0000],
        [5.0000, 5.6763, 5.8299],
        [5.0000, 5.8788, 6.2122],
        [5.0000, 5.0001, 5.0000]], grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.129954531788826 
model_pd.l_d.mean(): -15.177892684936523 
model_pd.lagr.mean(): -15.047938346862793 
model_pd.lambdas: dict_items([('pout', tensor([1.2669])), ('power', tensor([0.8153]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4258])), ('power', tensor([-20.8018]))])
epoch：176	 i:0 	 global-step:3520	 l-p:0.129954531788826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[5.0024, 5.2334, 5.1602],
        [5.0024, 5.8052, 6.0672],
        [5.0024, 5.4601, 5.4644],
        [5.0024, 5.0024, 5.0024]], grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.1298086792230606 
model_pd.l_d.mean(): -15.154152870178223 
model_pd.lagr.mean(): -15.024344444274902 
model_pd.lambdas: dict_items([('pout', tensor([1.2683])), ('power', tensor([0.8143]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4252])), ('power', tensor([-20.8008]))])
epoch：177	 i:0 	 global-step:3540	 l-p:0.1298086792230606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.0048, 5.0595, 5.0199],
        [5.0048, 5.5088, 5.5398],
        [5.0048, 5.0109, 5.0052],
        [5.0048, 5.4774, 5.4900]], grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.12966077029705048 
model_pd.l_d.mean(): -15.130415916442871 
model_pd.lagr.mean(): -15.000755310058594 
model_pd.lambdas: dict_items([('pout', tensor([1.2697])), ('power', tensor([0.8133]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4246])), ('power', tensor([-20.7998]))])
epoch：178	 i:0 	 global-step:3560	 l-p:0.12966077029705048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[5.0072, 5.5113, 5.5422],
        [5.0072, 6.4914, 7.5182],
        [5.0072, 5.0072, 5.0072],
        [5.0072, 5.0072, 5.0072]], grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.12951058149337769 
model_pd.l_d.mean(): -15.106681823730469 
model_pd.lagr.mean(): -14.977170944213867 
model_pd.lambdas: dict_items([('pout', tensor([1.2711])), ('power', tensor([0.8122]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4240])), ('power', tensor([-20.7987]))])
epoch：179	 i:0 	 global-step:3580	 l-p:0.12951058149337769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.0097, 5.2102, 5.1353],
        [5.0097, 5.2407, 5.1674],
        [5.0097, 5.0177, 5.0104],
        [5.0097, 5.0097, 5.0097]], grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12935839593410492 
model_pd.l_d.mean(): -15.082950592041016 
model_pd.lagr.mean(): -14.953592300415039 
model_pd.lambdas: dict_items([('pout', tensor([1.2726])), ('power', tensor([0.8112]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4234])), ('power', tensor([-20.7977]))])
epoch：180	 i:0 	 global-step:3600	 l-p:0.12935839593410492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.0122, 5.1906, 5.1161],
        [5.0122, 5.3861, 5.3502],
        [5.0122, 5.0408, 5.0175],
        [5.0122, 5.6888, 5.8421]], grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.12920436263084412 
model_pd.l_d.mean(): -15.059220314025879 
model_pd.lagr.mean(): -14.930015563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.2740])), ('power', tensor([0.8101]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4228])), ('power', tensor([-20.7966]))])
epoch：181	 i:0 	 global-step:3620	 l-p:0.12920436263084412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[5.0148, 5.0434, 5.0201],
        [5.0148, 5.1374, 5.0713],
        [5.0148, 5.1177, 5.0572],
        [5.0148, 5.3887, 5.3527]], grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.1290486454963684 
model_pd.l_d.mean(): -15.035492897033691 
model_pd.lagr.mean(): -14.906444549560547 
model_pd.lambdas: dict_items([('pout', tensor([1.2754])), ('power', tensor([0.8091]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4221])), ('power', tensor([-20.7955]))])
epoch：182	 i:0 	 global-step:3640	 l-p:0.1290486454963684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[5.0174, 5.0254, 5.0181],
        [5.0174, 5.0175, 5.0174],
        [5.0174, 5.0174, 5.0174],
        [5.0174, 5.1589, 5.0888]], grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12889178097248077 
model_pd.l_d.mean(): -15.011771202087402 
model_pd.lagr.mean(): -14.882879257202148 
model_pd.lambdas: dict_items([('pout', tensor([1.2768])), ('power', tensor([0.8081]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4215])), ('power', tensor([-20.7943]))])
epoch：183	 i:0 	 global-step:3660	 l-p:0.12889178097248077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[5.0200, 5.3939, 5.3578],
        [5.0200, 5.4928, 5.5051],
        [5.0200, 5.0746, 5.0351],
        [5.0200, 5.0261, 5.0205]], grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.12873411178588867 
model_pd.l_d.mean(): -14.98804759979248 
model_pd.lagr.mean(): -14.85931396484375 
model_pd.lambdas: dict_items([('pout', tensor([1.2783])), ('power', tensor([0.8070]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4208])), ('power', tensor([-20.7932]))])
epoch：184	 i:0 	 global-step:3680	 l-p:0.12873411178588867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[5.0226, 5.0230, 5.0226],
        [5.0226, 6.1746, 6.7938],
        [5.0226, 5.2231, 5.1481],
        [5.0226, 5.3231, 5.2623]], grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12857605516910553 
model_pd.l_d.mean(): -14.964326858520508 
model_pd.lagr.mean(): -14.835750579833984 
model_pd.lambdas: dict_items([('pout', tensor([1.2797])), ('power', tensor([0.8060]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4202])), ('power', tensor([-20.7921]))])
epoch：185	 i:0 	 global-step:3700	 l-p:0.12857605516910553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[5.0253, 5.7413, 5.9261],
        [5.0253, 5.0802, 5.0405],
        [5.0253, 5.2992, 5.2322],
        [5.0253, 5.3116, 5.2473]], grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.1284179836511612 
model_pd.l_d.mean(): -14.9406156539917 
model_pd.lagr.mean(): -14.8121976852417 
model_pd.lambdas: dict_items([('pout', tensor([1.2811])), ('power', tensor([0.8049]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4195])), ('power', tensor([-20.7909]))])
epoch：186	 i:0 	 global-step:3720	 l-p:0.1284179836511612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[5.0279, 5.0282, 5.0279],
        [5.0279, 5.9020, 6.2290],
        [5.0279, 5.4018, 5.3656],
        [5.0279, 5.4715, 5.4677]], grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.12826019525527954 
model_pd.l_d.mean(): -14.91690444946289 
model_pd.lagr.mean(): -14.788643836975098 
model_pd.lambdas: dict_items([('pout', tensor([1.2825])), ('power', tensor([0.8039]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4188])), ('power', tensor([-20.7897]))])
epoch：187	 i:0 	 global-step:3740	 l-p:0.12826019525527954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[5.0306, 5.0306, 5.0306],
        [5.0306, 5.7078, 5.8606],
        [5.0306, 5.0854, 5.0458],
        [5.0306, 5.5349, 5.5653]], grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.12810279428958893 
model_pd.l_d.mean(): -14.893200874328613 
model_pd.lagr.mean(): -14.765097618103027 
model_pd.lambdas: dict_items([('pout', tensor([1.2839])), ('power', tensor([0.8029]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4182])), ('power', tensor([-20.7886]))])
epoch：188	 i:0 	 global-step:3760	 l-p:0.12810279428958893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[5.0332, 6.4154, 7.3080],
        [5.0332, 5.0335, 5.0332],
        [5.0332, 5.0881, 5.0484],
        [5.0332, 5.0617, 5.0385]], grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.1279458850622177 
model_pd.l_d.mean(): -14.869501113891602 
model_pd.lagr.mean(): -14.741555213928223 
model_pd.lambdas: dict_items([('pout', tensor([1.2853])), ('power', tensor([0.8018]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4175])), ('power', tensor([-20.7874]))])
epoch：189	 i:0 	 global-step:3780	 l-p:0.1279458850622177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.0359, 5.6666, 5.7832],
        [5.0359, 5.0680, 5.0422],
        [5.0359, 5.8403, 6.1017],
        [5.0359, 5.9168, 6.2497]], grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.1277894824743271 
model_pd.l_d.mean(): -14.845806121826172 
model_pd.lagr.mean(): -14.718016624450684 
model_pd.lambdas: dict_items([('pout', tensor([1.2868])), ('power', tensor([0.8008]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4168])), ('power', tensor([-20.7863]))])
epoch：190	 i:0 	 global-step:3800	 l-p:0.1277894824743271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.0386, 5.3390, 5.2780],
        [5.0386, 5.0386, 5.0386],
        [5.0386, 5.0447, 5.0390],
        [5.0386, 5.4965, 5.5001]], grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.1276334524154663 
model_pd.l_d.mean(): -14.82211685180664 
model_pd.lagr.mean(): -14.694483757019043 
model_pd.lambdas: dict_items([('pout', tensor([1.2882])), ('power', tensor([0.7997]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4162])), ('power', tensor([-20.7851]))])
epoch：191	 i:0 	 global-step:3820	 l-p:0.1276334524154663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.0412, 5.2416, 5.1665],
        [5.0412, 5.0412, 5.0412],
        [5.0412, 5.0415, 5.0412],
        [5.0412, 5.0958, 5.0563]], grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.1274775117635727 
model_pd.l_d.mean(): -14.798432350158691 
model_pd.lagr.mean(): -14.670954704284668 
model_pd.lambdas: dict_items([('pout', tensor([1.2896])), ('power', tensor([0.7987]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4155])), ('power', tensor([-20.7839]))])
epoch：192	 i:0 	 global-step:3840	 l-p:0.1274775117635727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.0439, 5.0441, 5.0439],
        [5.0439, 5.0979, 5.0587],
        [5.0439, 5.2220, 5.1475],
        [5.0439, 5.0439, 5.0439]], grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.1273215115070343 
model_pd.l_d.mean(): -14.774747848510742 
model_pd.lagr.mean(): -14.64742660522461 
model_pd.lambdas: dict_items([('pout', tensor([1.2910])), ('power', tensor([0.7977]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4148])), ('power', tensor([-20.7827]))])
epoch：193	 i:0 	 global-step:3860	 l-p:0.1273215115070343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[5.0466, 5.4609, 5.4421],
        [5.0466, 5.0472, 5.0466],
        [5.0466, 5.1690, 5.1029],
        [5.0466, 5.5511, 5.5812]], grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.1271652728319168 
model_pd.l_d.mean(): -14.751073837280273 
model_pd.lagr.mean(): -14.623908996582031 
model_pd.lambdas: dict_items([('pout', tensor([1.2924])), ('power', tensor([0.7966]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4141])), ('power', tensor([-20.7815]))])
epoch：194	 i:0 	 global-step:3880	 l-p:0.1271652728319168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[5.0493, 6.2041, 6.8238],
        [5.0493, 5.1520, 5.0916],
        [5.0493, 5.0494, 5.0493],
        [5.0493, 5.0497, 5.0493]], grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.12700839340686798 
model_pd.l_d.mean(): -14.727398872375488 
model_pd.lagr.mean(): -14.600390434265137 
model_pd.lambdas: dict_items([('pout', tensor([1.2938])), ('power', tensor([0.7956]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4135])), ('power', tensor([-20.7803]))])
epoch：195	 i:0 	 global-step:3900	 l-p:0.12700839340686798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[5.0521, 5.1061, 5.0669],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 6.2854, 6.9951],
        [5.0521, 5.4663, 5.4474]], grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.1268509179353714 
model_pd.l_d.mean(): -14.703728675842285 
model_pd.lagr.mean(): -14.57687759399414 
model_pd.lambdas: dict_items([('pout', tensor([1.2952])), ('power', tensor([0.7945]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4128])), ('power', tensor([-20.7791]))])
epoch：196	 i:0 	 global-step:3920	 l-p:0.1268509179353714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.0548, 5.1093, 5.0699],
        [5.0548, 6.4403, 7.3339],
        [5.0548, 6.2885, 6.9983],
        [5.0548, 5.9369, 6.2696]], grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.12669262290000916 
model_pd.l_d.mean(): -14.68006420135498 
model_pd.lagr.mean(): -14.55337142944336 
model_pd.lambdas: dict_items([('pout', tensor([1.2967])), ('power', tensor([0.7935]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4121])), ('power', tensor([-20.7779]))])
epoch：197	 i:0 	 global-step:3940	 l-p:0.12669262290000916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[5.0576, 6.4436, 7.3373],
        [5.0576, 5.3580, 5.2968],
        [5.0576, 6.2134, 6.8332],
        [5.0576, 5.9335, 6.2601]], grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.12653349339962006 
model_pd.l_d.mean(): -14.656399726867676 
model_pd.lagr.mean(): -14.529866218566895 
model_pd.lambdas: dict_items([('pout', tensor([1.2981])), ('power', tensor([0.7925]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4114])), ('power', tensor([-20.7767]))])
epoch：198	 i:0 	 global-step:3960	 l-p:0.12653349339962006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.0604, 5.9438, 6.2774],
        [5.0604, 5.1152, 5.0756],
        [5.0604, 5.0607, 5.0604],
        [5.0604, 5.9428, 6.2755]], grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.12637358903884888 
model_pd.l_d.mean(): -14.632742881774902 
model_pd.lagr.mean(): -14.506369590759277 
model_pd.lambdas: dict_items([('pout', tensor([1.2995])), ('power', tensor([0.7914]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4107])), ('power', tensor([-20.7754]))])
epoch：199	 i:0 	 global-step:3980	 l-p:0.12637358903884888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[5.0632, 6.5573, 7.5878],
        [5.0632, 5.9395, 6.2660],
        [5.0632, 5.9458, 6.2784],
        [5.0632, 5.1855, 5.1194]], grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.12621572613716125 
model_pd.l_d.mean(): -14.609087944030762 
model_pd.lagr.mean(): -14.482872009277344 
model_pd.lambdas: dict_items([('pout', tensor([1.3009])), ('power', tensor([0.7904]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4100])), ('power', tensor([-20.7742]))])
epoch：200	 i:0 	 global-step:4000	 l-p:0.12621572613716125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.0660, 5.1199, 5.0807],
        [5.0660, 5.2162, 5.1445],
        [5.0660, 5.0660, 5.0660],
        [5.0660, 5.3397, 5.2723]], grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.12606148421764374 
model_pd.l_d.mean(): -14.585442543029785 
model_pd.lagr.mean(): -14.459381103515625 
model_pd.lambdas: dict_items([('pout', tensor([1.3023])), ('power', tensor([0.7894]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4093])), ('power', tensor([-20.7729]))])
epoch：201	 i:0 	 global-step:4020	 l-p:0.12606148421764374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[5.0686, 5.5734, 5.6030],
        [5.0686, 5.0686, 5.0686],
        [5.0686, 5.0766, 5.0693],
        [5.0686, 5.1006, 5.0750]], grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.1259114295244217 
model_pd.l_d.mean(): -14.56180477142334 
model_pd.lagr.mean(): -14.435893058776855 
model_pd.lambdas: dict_items([('pout', tensor([1.3037])), ('power', tensor([0.7883]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4087])), ('power', tensor([-20.7718]))])
epoch：202	 i:0 	 global-step:4040	 l-p:0.1259114295244217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.0712, 5.1240, 5.0855],
        [5.0712, 5.0718, 5.0713],
        [5.0712, 5.0997, 5.0765],
        [5.0712, 5.8774, 6.1382]], grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.12576541304588318 
model_pd.l_d.mean(): -14.538176536560059 
model_pd.lagr.mean(): -14.412410736083984 
model_pd.lambdas: dict_items([('pout', tensor([1.3051])), ('power', tensor([0.7873]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4080])), ('power', tensor([-20.7706]))])
epoch：203	 i:0 	 global-step:4060	 l-p:0.12576541304588318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.0738, 5.9580, 6.2914],
        [5.0738, 5.3600, 5.2951],
        [5.0738, 5.0818, 5.0745],
        [5.0738, 5.0738, 5.0738]], grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.1256221979856491 
model_pd.l_d.mean(): -14.51455307006836 
model_pd.lagr.mean(): -14.388931274414062 
model_pd.lambdas: dict_items([('pout', tensor([1.3065])), ('power', tensor([0.7862]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4074])), ('power', tensor([-20.7694]))])
epoch：204	 i:0 	 global-step:4080	 l-p:0.1256221979856491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[5.0764, 5.7550, 5.9068],
        [5.0764, 5.2265, 5.1548],
        [5.0764, 5.9597, 6.2921],
        [5.0764, 5.3501, 5.2826]], grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.12548048794269562 
model_pd.l_d.mean(): -14.490939140319824 
model_pd.lagr.mean(): -14.365458488464355 
model_pd.lambdas: dict_items([('pout', tensor([1.3079])), ('power', tensor([0.7852]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4067])), ('power', tensor([-20.7683]))])
epoch：205	 i:0 	 global-step:4100	 l-p:0.12548048794269562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.0790, 5.0790, 5.0790],
        [5.0790, 5.0790, 5.0790],
        [5.0790, 5.5229, 5.5180],
        [5.0790, 5.3095, 5.2358]], grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.1253383904695511 
model_pd.l_d.mean(): -14.467327117919922 
model_pd.lagr.mean(): -14.341988563537598 
model_pd.lambdas: dict_items([('pout', tensor([1.3093])), ('power', tensor([0.7842]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4061])), ('power', tensor([-20.7672]))])
epoch：206	 i:0 	 global-step:4120	 l-p:0.1253383904695511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.0816, 5.0816, 5.0816],
        [5.0816, 5.9589, 6.2851],
        [5.0816, 6.2634, 6.9102],
        [5.0816, 5.5398, 5.5425]], grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.12519417703151703 
model_pd.l_d.mean(): -14.443714141845703 
model_pd.lagr.mean(): -14.318519592285156 
model_pd.lambdas: dict_items([('pout', tensor([1.3107])), ('power', tensor([0.7831]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4054])), ('power', tensor([-20.7660]))])
epoch：207	 i:0 	 global-step:4140	 l-p:0.12519417703151703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.0843, 5.0887, 5.0845],
        [5.0843, 5.3148, 5.2410],
        [5.0843, 5.0858, 5.0843],
        [5.0843, 5.8024, 5.9860]], grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.12504646182060242 
model_pd.l_d.mean(): -14.420106887817383 
model_pd.lagr.mean(): -14.295060157775879 
model_pd.lambdas: dict_items([('pout', tensor([1.3121])), ('power', tensor([0.7821]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4048])), ('power', tensor([-20.7648]))])
epoch：208	 i:0 	 global-step:4160	 l-p:0.12504646182060242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.0870, 5.0873, 5.0871],
        [5.0870, 5.2649, 5.1902],
        [5.0870, 5.7191, 5.8344],
        [5.0870, 5.0871, 5.0870]], grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.12489417940378189 
model_pd.l_d.mean(): -14.396498680114746 
model_pd.lagr.mean(): -14.271604537963867 
model_pd.lambdas: dict_items([('pout', tensor([1.3136])), ('power', tensor([0.7810]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4041])), ('power', tensor([-20.7635]))])
epoch：209	 i:0 	 global-step:4180	 l-p:0.12489417940378189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.0899, 5.1065, 5.0921],
        [5.0899, 5.0899, 5.0899],
        [5.0899, 5.0979, 5.0906],
        [5.0899, 6.2727, 6.9197]], grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.12473674863576889 
model_pd.l_d.mean(): -14.372888565063477 
model_pd.lagr.mean(): -14.248151779174805 
model_pd.lambdas: dict_items([('pout', tensor([1.3150])), ('power', tensor([0.7800]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4034])), ('power', tensor([-20.7622]))])
epoch：210	 i:0 	 global-step:4200	 l-p:0.12473674863576889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.0929, 5.5074, 5.4877],
        [5.0929, 5.0944, 5.0930],
        [5.0929, 5.0929, 5.0929],
        [5.0929, 5.0929, 5.0929]], grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.12457405030727386 
model_pd.l_d.mean(): -14.349276542663574 
model_pd.lagr.mean(): -14.224702835083008 
model_pd.lambdas: dict_items([('pout', tensor([1.3164])), ('power', tensor([0.7790]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4026])), ('power', tensor([-20.7608]))])
epoch：211	 i:0 	 global-step:4220	 l-p:0.12457405030727386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.0960, 6.2795, 6.9267],
        [5.0960, 5.1243, 5.1012],
        [5.0960, 5.0960, 5.0960],
        [5.0960, 5.2181, 5.1520]], grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.12440678477287292 
model_pd.l_d.mean(): -14.32567024230957 
model_pd.lagr.mean(): -14.201263427734375 
model_pd.lambdas: dict_items([('pout', tensor([1.3178])), ('power', tensor([0.7779]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4019])), ('power', tensor([-20.7594]))])
epoch：212	 i:0 	 global-step:4240	 l-p:0.12440678477287292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.0991, 5.1529, 5.1138],
        [5.0991, 5.9777, 6.3038],
        [5.0991, 5.1052, 5.0996],
        [5.0991, 5.0991, 5.0991]], grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12423574179410934 
model_pd.l_d.mean(): -14.302059173583984 
model_pd.lagr.mean(): -14.177823066711426 
model_pd.lambdas: dict_items([('pout', tensor([1.3192])), ('power', tensor([0.7769]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4011])), ('power', tensor([-20.7580]))])
epoch：213	 i:0 	 global-step:4260	 l-p:0.12423574179410934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.1023, 5.4764, 5.4390],
        [5.1023, 5.1561, 5.1170],
        [5.1023, 6.2868, 6.9342],
        [5.1023, 5.5609, 5.5632]], grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.12406203895807266 
model_pd.l_d.mean(): -14.278453826904297 
model_pd.lagr.mean(): -14.15439224243164 
model_pd.lambdas: dict_items([('pout', tensor([1.3206])), ('power', tensor([0.7759]))]) 
model_pd.vars: dict_items([('pout', tensor([1.4003])), ('power', tensor([-20.7565]))])
epoch：214	 i:0 	 global-step:4280	 l-p:0.12406203895807266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.1056, 5.7384, 5.8534],
        [5.1056, 5.1056, 5.1055],
        [5.1056, 5.9910, 6.3232],
        [5.1056, 5.1599, 5.1205]], grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.12388702481985092 
model_pd.l_d.mean(): -14.254850387573242 
model_pd.lagr.mean(): -14.130963325500488 
model_pd.lambdas: dict_items([('pout', tensor([1.3220])), ('power', tensor([0.7748]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3995])), ('power', tensor([-20.7550]))])
epoch：215	 i:0 	 global-step:4300	 l-p:0.12388702481985092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[5.1088, 5.1149, 5.1092],
        [5.1088, 5.1094, 5.1088],
        [5.1088, 5.1088, 5.1088],
        [5.1088, 5.4830, 5.4455]], grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.12371192872524261 
model_pd.l_d.mean(): -14.231247901916504 
model_pd.lagr.mean(): -14.107536315917969 
model_pd.lambdas: dict_items([('pout', tensor([1.3234])), ('power', tensor([0.7738]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3987])), ('power', tensor([-20.7535]))])
epoch：216	 i:0 	 global-step:4320	 l-p:0.12371192872524261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.1120, 5.1123, 5.1121],
        [5.1120, 5.9917, 6.3179],
        [5.1120, 5.1440, 5.1183],
        [5.1120, 5.8317, 6.0149]], grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.12353777885437012 
model_pd.l_d.mean(): -14.207658767700195 
model_pd.lagr.mean(): -14.084120750427246 
model_pd.lambdas: dict_items([('pout', tensor([1.3248])), ('power', tensor([0.7727]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3979])), ('power', tensor([-20.7520]))])
epoch：217	 i:0 	 global-step:4340	 l-p:0.12353777885437012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.1153, 5.7486, 5.8635],
        [5.1153, 5.1153, 5.1153],
        [5.1153, 5.1318, 5.1175],
        [5.1153, 5.7957, 5.9468]], grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12336540967226028 
model_pd.l_d.mean(): -14.184069633483887 
model_pd.lagr.mean(): -14.060704231262207 
model_pd.lambdas: dict_items([('pout', tensor([1.3262])), ('power', tensor([0.7717]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3971])), ('power', tensor([-20.7505]))])
epoch：218	 i:0 	 global-step:4360	 l-p:0.12336540967226028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.1185, 5.1187, 5.1185],
        [5.1185, 5.5775, 5.5795],
        [5.1185, 5.1185, 5.1185],
        [5.1185, 5.7990, 5.9502]], grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.12319520860910416 
model_pd.l_d.mean(): -14.160489082336426 
model_pd.lagr.mean(): -14.037293434143066 
model_pd.lambdas: dict_items([('pout', tensor([1.3276])), ('power', tensor([0.7707]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3963])), ('power', tensor([-20.7490]))])
epoch：219	 i:0 	 global-step:4380	 l-p:0.12319520860910416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.1216, 5.5664, 5.5608],
        [5.1216, 5.4961, 5.4584],
        [5.1216, 5.3955, 5.3275],
        [5.1216, 5.1216, 5.1216]], grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.12302745133638382 
model_pd.l_d.mean(): -14.136916160583496 
model_pd.lagr.mean(): -14.013888359069824 
model_pd.lambdas: dict_items([('pout', tensor([1.3289])), ('power', tensor([0.7696]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3955])), ('power', tensor([-20.7475]))])
epoch：220	 i:0 	 global-step:4400	 l-p:0.12302745133638382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.1248, 6.3125, 6.9608],
        [5.1248, 5.3987, 5.3306],
        [5.1248, 5.1794, 5.1398],
        [5.1248, 5.2469, 5.1807]], grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.12286177277565002 
model_pd.l_d.mean(): -14.113350868225098 
model_pd.lagr.mean(): -13.99048900604248 
model_pd.lambdas: dict_items([('pout', tensor([1.3303])), ('power', tensor([0.7686]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3947])), ('power', tensor([-20.7460]))])
epoch：221	 i:0 	 global-step:4420	 l-p:0.12286177277565002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[5.1279, 5.8090, 5.9600],
        [5.1279, 5.3281, 5.2525],
        [5.1279, 6.0090, 6.3352],
        [5.1279, 6.0163, 6.3496]], grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.12269777059555054 
model_pd.l_d.mean(): -14.089792251586914 
model_pd.lagr.mean(): -13.967094421386719 
model_pd.lambdas: dict_items([('pout', tensor([1.3317])), ('power', tensor([0.7676]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3939])), ('power', tensor([-20.7446]))])
epoch：222	 i:0 	 global-step:4440	 l-p:0.12269777059555054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[5.1310, 5.2334, 5.1730],
        [5.1310, 5.1310, 5.1310],
        [5.1310, 5.1389, 5.1317],
        [5.1310, 6.3752, 7.0878]], grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.12253481149673462 
model_pd.l_d.mean(): -14.066238403320312 
model_pd.lagr.mean(): -13.943703651428223 
model_pd.lambdas: dict_items([('pout', tensor([1.3331])), ('power', tensor([0.7665]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3932])), ('power', tensor([-20.7431]))])
epoch：223	 i:0 	 global-step:4460	 l-p:0.12253481149673462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[5.1341, 5.1879, 5.1488],
        [5.1341, 5.1625, 5.1393],
        [5.1341, 5.1884, 5.1491],
        [5.1341, 6.5331, 7.4317]], grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12237221002578735 
model_pd.l_d.mean(): -14.042688369750977 
model_pd.lagr.mean(): -13.920315742492676 
model_pd.lambdas: dict_items([('pout', tensor([1.3345])), ('power', tensor([0.7655]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3924])), ('power', tensor([-20.7417]))])
epoch：224	 i:0 	 global-step:4480	 l-p:0.12237221002578735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[5.1373, 5.1692, 5.1436],
        [5.1373, 5.5120, 5.4740],
        [5.1373, 6.0191, 6.3454],
        [5.1373, 5.4380, 5.3758]], grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.12220931798219681 
model_pd.l_d.mean(): -14.019144058227539 
model_pd.lagr.mean(): -13.896934509277344 
model_pd.lambdas: dict_items([('pout', tensor([1.3359])), ('power', tensor([0.7644]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3916])), ('power', tensor([-20.7402]))])
epoch：225	 i:0 	 global-step:4500	 l-p:0.12220931798219681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.1404, 5.2815, 5.2111],
        [5.1404, 5.2625, 5.1963],
        [5.1404, 5.1723, 5.1467],
        [5.1404, 5.3712, 5.2969]], grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12204562872648239 
model_pd.l_d.mean(): -13.99560260772705 
model_pd.lagr.mean(): -13.873557090759277 
model_pd.lambdas: dict_items([('pout', tensor([1.3373])), ('power', tensor([0.7634]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3908])), ('power', tensor([-20.7387]))])
epoch：226	 i:0 	 global-step:4520	 l-p:0.12204562872648239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.1436, 5.1515, 5.1443],
        [5.1436, 6.0261, 6.3523],
        [5.1436, 5.1602, 5.1458],
        [5.1436, 5.6185, 5.6283]], grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.12188056856393814 
model_pd.l_d.mean(): -13.972064971923828 
model_pd.lagr.mean(): -13.850184440612793 
model_pd.lambdas: dict_items([('pout', tensor([1.3387])), ('power', tensor([0.7624]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3900])), ('power', tensor([-20.7372]))])
epoch：227	 i:0 	 global-step:4540	 l-p:0.12188056856393814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[5.1469, 5.1787, 5.1531],
        [5.1469, 5.1634, 5.1490],
        [5.1469, 6.5483, 7.4478],
        [5.1469, 5.1752, 5.1520]], grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.12171409279108047 
model_pd.l_d.mean(): -13.948531150817871 
model_pd.lagr.mean(): -13.826817512512207 
model_pd.lambdas: dict_items([('pout', tensor([1.3401])), ('power', tensor([0.7613]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3892])), ('power', tensor([-20.7356]))])
epoch：228	 i:0 	 global-step:4560	 l-p:0.12171409279108047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.1501, 5.2044, 5.1650],
        [5.1501, 5.1501, 5.1501],
        [5.1501, 5.2039, 5.1648],
        [5.1501, 6.0396, 6.3720]], grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.12154614180326462 
model_pd.l_d.mean(): -13.924999237060547 
model_pd.lagr.mean(): -13.80345344543457 
model_pd.lambdas: dict_items([('pout', tensor([1.3415])), ('power', tensor([0.7603]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3884])), ('power', tensor([-20.7341]))])
epoch：229	 i:0 	 global-step:4580	 l-p:0.12154614180326462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.1534, 6.0432, 6.3757],
        [5.1534, 5.8360, 5.9867],
        [5.1534, 5.5285, 5.4903],
        [5.1534, 5.1549, 5.1535]], grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.12137706577777863 
model_pd.l_d.mean(): -13.901471138000488 
model_pd.lagr.mean(): -13.780094146728516 
model_pd.lambdas: dict_items([('pout', tensor([1.3429])), ('power', tensor([0.7593]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3876])), ('power', tensor([-20.7325]))])
epoch：230	 i:0 	 global-step:4600	 l-p:0.12137706577777863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.1567, 5.2592, 5.1987],
        [5.1567, 6.0479, 6.3814],
        [5.1567, 5.6321, 5.6417],
        [5.1567, 5.2105, 5.1714]], grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12120705842971802 
model_pd.l_d.mean(): -13.877947807312012 
model_pd.lagr.mean(): -13.75674057006836 
model_pd.lambdas: dict_items([('pout', tensor([1.3442])), ('power', tensor([0.7582]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3868])), ('power', tensor([-20.7309]))])
epoch：231	 i:0 	 global-step:4620	 l-p:0.12120705842971802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.1601, 5.1601, 5.1601],
        [5.1601, 5.1601, 5.1601],
        [5.1601, 5.3910, 5.3166],
        [5.1601, 5.9726, 6.2327]], grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12103720754384995 
model_pd.l_d.mean(): -13.8544282913208 
model_pd.lagr.mean(): -13.733390808105469 
model_pd.lambdas: dict_items([('pout', tensor([1.3456])), ('power', tensor([0.7572]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3860])), ('power', tensor([-20.7293]))])
epoch：232	 i:0 	 global-step:4640	 l-p:0.12103720754384995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.1633, 6.0478, 6.3742],
        [5.1633, 5.3045, 5.2340],
        [5.1633, 5.3414, 5.2663],
        [5.1633, 5.1633, 5.1633]], grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.12087099999189377 
model_pd.l_d.mean(): -13.830911636352539 
model_pd.lagr.mean(): -13.710041046142578 
model_pd.lambdas: dict_items([('pout', tensor([1.3470])), ('power', tensor([0.7562]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3852])), ('power', tensor([-20.7277]))])
epoch：233	 i:0 	 global-step:4660	 l-p:0.12087099999189377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.1665, 5.3168, 5.2447],
        [5.1665, 5.2192, 5.1807],
        [5.1665, 5.2690, 5.2085],
        [5.1665, 5.1668, 5.1665]], grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.12071015685796738 
model_pd.l_d.mean(): -13.80740737915039 
model_pd.lagr.mean(): -13.686697006225586 
model_pd.lambdas: dict_items([('pout', tensor([1.3484])), ('power', tensor([0.7551]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3844])), ('power', tensor([-20.7262]))])
epoch：234	 i:0 	 global-step:4680	 l-p:0.12071015685796738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.1696, 5.4441, 5.3756],
        [5.1696, 5.3199, 5.2478],
        [5.1696, 5.8932, 6.0760],
        [5.1696, 5.1697, 5.1696]], grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.12055511772632599 
model_pd.l_d.mean(): -13.783906936645508 
model_pd.lagr.mean(): -13.663352012634277 
model_pd.lambdas: dict_items([('pout', tensor([1.3498])), ('power', tensor([0.7541]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3836])), ('power', tensor([-20.7247]))])
epoch：235	 i:0 	 global-step:4700	 l-p:0.12055511772632599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.1726, 5.3229, 5.2507],
        [5.1726, 5.1728, 5.1726],
        [5.1726, 5.2009, 5.1777],
        [5.1726, 5.2751, 5.2145]], grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.1204051598906517 
model_pd.l_d.mean(): -13.760418891906738 
model_pd.lagr.mean(): -13.640013694763184 
model_pd.lambdas: dict_items([('pout', tensor([1.3512])), ('power', tensor([0.7530]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3829])), ('power', tensor([-20.7232]))])
epoch：236	 i:0 	 global-step:4720	 l-p:0.1204051598906517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.1755, 5.2977, 5.2313],
        [5.1755, 5.8598, 6.0104],
        [5.1755, 6.5828, 7.4851],
        [5.1755, 5.1920, 5.1776]], grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.12025898694992065 
model_pd.l_d.mean(): -13.736937522888184 
model_pd.lagr.mean(): -13.616678237915039 
model_pd.lambdas: dict_items([('pout', tensor([1.3526])), ('power', tensor([0.7520]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3822])), ('power', tensor([-20.7218]))])
epoch：237	 i:0 	 global-step:4740	 l-p:0.12025898694992065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[5.1783, 6.5863, 7.4889],
        [5.1783, 5.1783, 5.1783],
        [5.1783, 5.2322, 5.1930],
        [5.1783, 5.1844, 5.1788]], grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.12011487782001495 
model_pd.l_d.mean(): -13.713462829589844 
model_pd.lagr.mean(): -13.593347549438477 
model_pd.lambdas: dict_items([('pout', tensor([1.3539])), ('power', tensor([0.7510]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3815])), ('power', tensor([-20.7204]))])
epoch：238	 i:0 	 global-step:4760	 l-p:0.12011487782001495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.1812, 5.8660, 6.0166],
        [5.1812, 5.5981, 5.5771],
        [5.1812, 5.1827, 5.1813],
        [5.1812, 5.2339, 5.1954]], grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.11997102946043015 
model_pd.l_d.mean(): -13.689990043640137 
model_pd.lagr.mean(): -13.570018768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.3553])), ('power', tensor([0.7499]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3807])), ('power', tensor([-20.7190]))])
epoch：239	 i:0 	 global-step:4780	 l-p:0.11997102946043015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.1841, 5.2368, 5.1983],
        [5.1841, 5.1841, 5.1841],
        [5.1841, 5.1841, 5.1841],
        [5.1841, 5.4155, 5.3408]], grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.11982554197311401 
model_pd.l_d.mean(): -13.666525840759277 
model_pd.lagr.mean(): -13.546700477600098 
model_pd.lambdas: dict_items([('pout', tensor([1.3567])), ('power', tensor([0.7489]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3800])), ('power', tensor([-20.7176]))])
epoch：240	 i:0 	 global-step:4800	 l-p:0.11982554197311401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.1871, 6.4411, 7.1573],
        [5.1871, 5.4186, 5.3438],
        [5.1871, 6.0024, 6.2628],
        [5.1871, 5.1871, 5.1871]], grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.11967707425355911 
model_pd.l_d.mean(): -13.643059730529785 
model_pd.lagr.mean(): -13.523382186889648 
model_pd.lambdas: dict_items([('pout', tensor([1.3581])), ('power', tensor([0.7479]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3793])), ('power', tensor([-20.7161]))])
epoch：241	 i:0 	 global-step:4820	 l-p:0.11967707425355911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[5.1902, 6.0058, 6.2663],
        [5.1902, 5.4778, 5.4115],
        [5.1902, 5.8757, 6.0264],
        [5.1902, 5.9156, 6.0985]], grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.11952445656061172 
model_pd.l_d.mean(): -13.619595527648926 
model_pd.lagr.mean(): -13.50007152557373 
model_pd.lambdas: dict_items([('pout', tensor([1.3595])), ('power', tensor([0.7468]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3785])), ('power', tensor([-20.7146]))])
epoch：242	 i:0 	 global-step:4840	 l-p:0.11952445656061172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[5.1934, 5.8315, 5.9457],
        [5.1934, 6.0888, 6.4230],
        [5.1934, 5.2253, 5.1996],
        [5.1934, 5.3719, 5.2965]], grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.119367316365242 
model_pd.l_d.mean(): -13.59613037109375 
model_pd.lagr.mean(): -13.476762771606445 
model_pd.lambdas: dict_items([('pout', tensor([1.3608])), ('power', tensor([0.7458]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3777])), ('power', tensor([-20.7130]))])
epoch：243	 i:0 	 global-step:4860	 l-p:0.119367316365242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[5.1966, 6.0915, 6.4248],
        [5.1966, 5.3752, 5.2998],
        [5.1966, 5.2132, 5.1988],
        [5.1966, 6.0850, 6.4122]], grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.11920566856861115 
model_pd.l_d.mean(): -13.572671890258789 
model_pd.lagr.mean(): -13.453466415405273 
model_pd.lambdas: dict_items([('pout', tensor([1.3622])), ('power', tensor([0.7448]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3769])), ('power', tensor([-20.7114]))])
epoch：244	 i:0 	 global-step:4880	 l-p:0.11920566856861115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.2000, 5.2000, 5.2000],
        [5.2000, 6.4001, 7.0527],
        [5.2000, 5.2166, 5.2022],
        [5.2000, 5.2000, 5.2000]], grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.11904001235961914 
model_pd.l_d.mean(): -13.549211502075195 
model_pd.lagr.mean(): -13.430171966552734 
model_pd.lambdas: dict_items([('pout', tensor([1.3636])), ('power', tensor([0.7437]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3761])), ('power', tensor([-20.7097]))])
epoch：245	 i:0 	 global-step:4900	 l-p:0.11904001235961914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[5.2034, 5.3259, 5.2594],
        [5.2034, 5.2035, 5.2034],
        [5.2034, 6.0992, 6.4327],
        [5.2034, 5.6513, 5.6446]], grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.11887115240097046 
model_pd.l_d.mean(): -13.525752067565918 
model_pd.lagr.mean(): -13.406881332397461 
model_pd.lambdas: dict_items([('pout', tensor([1.3650])), ('power', tensor([0.7427]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3752])), ('power', tensor([-20.7080]))])
epoch：246	 i:0 	 global-step:4920	 l-p:0.11887115240097046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[5.2069, 5.8462, 5.9604],
        [5.2069, 5.4080, 5.3318],
        [5.2069, 6.6215, 7.5273],
        [5.2069, 6.1042, 6.4387]], grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.11870019137859344 
model_pd.l_d.mean(): -13.502298355102539 
model_pd.lagr.mean(): -13.383598327636719 
model_pd.lambdas: dict_items([('pout', tensor([1.3663])), ('power', tensor([0.7417]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3744])), ('power', tensor([-20.7062]))])
epoch：247	 i:0 	 global-step:4940	 l-p:0.11870019137859344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.2104, 5.8500, 5.9642],
        [5.2104, 5.2148, 5.2107],
        [5.2104, 6.7364, 7.7823],
        [5.2104, 5.3132, 5.2524]], grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.11852802336215973 
model_pd.l_d.mean(): -13.47884750366211 
model_pd.lagr.mean(): -13.360319137573242 
model_pd.lambdas: dict_items([('pout', tensor([1.3677])), ('power', tensor([0.7406]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3735])), ('power', tensor([-20.7045]))])
epoch：248	 i:0 	 global-step:4960	 l-p:0.11852802336215973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.2140, 5.2145, 5.2140],
        [5.2140, 5.2200, 5.2144],
        [5.2140, 5.2142, 5.2140],
        [5.2140, 5.3648, 5.2923]], grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.11835574358701706 
model_pd.l_d.mean(): -13.455404281616211 
model_pd.lagr.mean(): -13.337048530578613 
model_pd.lambdas: dict_items([('pout', tensor([1.3691])), ('power', tensor([0.7396]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3726])), ('power', tensor([-20.7027]))])
epoch：249	 i:0 	 global-step:4980	 l-p:0.11835574358701706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.2175, 5.2190, 5.2175],
        [5.2175, 6.0363, 6.2974],
        [5.2175, 5.2175, 5.2175],
        [5.2175, 6.7454, 7.7923]], grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.11818413436412811 
model_pd.l_d.mean(): -13.431968688964844 
model_pd.lagr.mean(): -13.3137845993042 
model_pd.lambdas: dict_items([('pout', tensor([1.3704])), ('power', tensor([0.7385]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3718])), ('power', tensor([-20.7009]))])
epoch：250	 i:0 	 global-step:5000	 l-p:0.11818413436412811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.2210, 5.2214, 5.2210],
        [5.2210, 5.2226, 5.2211],
        [5.2210, 6.4253, 7.0797],
        [5.2210, 5.2210, 5.2210]], grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.11801376193761826 
model_pd.l_d.mean(): -13.408534049987793 
model_pd.lagr.mean(): -13.290520668029785 
model_pd.lambdas: dict_items([('pout', tensor([1.3718])), ('power', tensor([0.7375]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3709])), ('power', tensor([-20.6991]))])
epoch：251	 i:0 	 global-step:5020	 l-p:0.11801376193761826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.2245, 5.4260, 5.3495],
        [5.2245, 5.2774, 5.2387],
        [5.2245, 5.2790, 5.2395],
        [5.2245, 5.9133, 6.0642]], grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.11784477531909943 
model_pd.l_d.mean(): -13.385112762451172 
model_pd.lagr.mean(): -13.267268180847168 
model_pd.lambdas: dict_items([('pout', tensor([1.3732])), ('power', tensor([0.7365]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3700])), ('power', tensor([-20.6973]))])
epoch：252	 i:0 	 global-step:5040	 l-p:0.11784477531909943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[5.2280, 6.6477, 7.5563],
        [5.2280, 5.2280, 5.2280],
        [5.2280, 5.2324, 5.2283],
        [5.2280, 5.4604, 5.3852]], grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.11767710745334625 
model_pd.l_d.mean(): -13.361698150634766 
model_pd.lagr.mean(): -13.24402141571045 
model_pd.lambdas: dict_items([('pout', tensor([1.3746])), ('power', tensor([0.7354]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3692])), ('power', tensor([-20.6956]))])
epoch：253	 i:0 	 global-step:5060	 l-p:0.11767710745334625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.2315, 5.2481, 5.2337],
        [5.2315, 5.3826, 5.3099],
        [5.2315, 5.7432, 5.7704],
        [5.2315, 5.2315, 5.2315]], grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.11751054972410202 
model_pd.l_d.mean(): -13.33829116821289 
model_pd.lagr.mean(): -13.220780372619629 
model_pd.lambdas: dict_items([('pout', tensor([1.3759])), ('power', tensor([0.7344]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3683])), ('power', tensor([-20.6938]))])
epoch：254	 i:0 	 global-step:5080	 l-p:0.11751054972410202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.2350, 6.6563, 7.5658],
        [5.2350, 5.2898, 5.2500],
        [5.2350, 5.6846, 5.6775],
        [5.2350, 5.7146, 5.7234]], grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.11734461784362793 
model_pd.l_d.mean(): -13.314888000488281 
model_pd.lagr.mean(): -13.197543144226074 
model_pd.lambdas: dict_items([('pout', tensor([1.3773])), ('power', tensor([0.7334]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3675])), ('power', tensor([-20.6921]))])
epoch：255	 i:0 	 global-step:5100	 l-p:0.11734461784362793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.2385, 6.1325, 6.4609],
        [5.2385, 5.2385, 5.2385],
        [5.2385, 5.2446, 5.2389],
        [5.2385, 5.2385, 5.2385]], grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.1171787679195404 
model_pd.l_d.mean(): -13.291491508483887 
model_pd.lagr.mean(): -13.174312591552734 
model_pd.lambdas: dict_items([('pout', tensor([1.3787])), ('power', tensor([0.7323]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3666])), ('power', tensor([-20.6903]))])
epoch：256	 i:0 	 global-step:5120	 l-p:0.1171787679195404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[5.2420, 5.5311, 5.4642],
        [5.2420, 6.1364, 6.4650],
        [5.2420, 6.1430, 6.4777],
        [5.2420, 5.2704, 5.2472]], grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.1170123890042305 
model_pd.l_d.mean(): -13.26810359954834 
model_pd.lagr.mean(): -13.151091575622559 
model_pd.lambdas: dict_items([('pout', tensor([1.3800])), ('power', tensor([0.7313]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3657])), ('power', tensor([-20.6885]))])
epoch：257	 i:0 	 global-step:5140	 l-p:0.1170123890042305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.2455, 5.2499, 5.2458],
        [5.2455, 5.2535, 5.2462],
        [5.2455, 6.6694, 7.5802],
        [5.2455, 5.2455, 5.2455]], grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.11684509366750717 
model_pd.l_d.mean(): -13.24471664428711 
model_pd.lagr.mean(): -13.1278715133667 
model_pd.lambdas: dict_items([('pout', tensor([1.3814])), ('power', tensor([0.7303]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3649])), ('power', tensor([-20.6867]))])
epoch：258	 i:0 	 global-step:5160	 l-p:0.11684509366750717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[5.2491, 5.3021, 5.2633],
        [5.2491, 6.1510, 6.4860],
        [5.2491, 5.2491, 5.2491],
        [5.2491, 5.2811, 5.2554]], grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11667640507221222 
model_pd.l_d.mean(): -13.22133731842041 
model_pd.lagr.mean(): -13.104660987854004 
model_pd.lambdas: dict_items([('pout', tensor([1.3828])), ('power', tensor([0.7292]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3640])), ('power', tensor([-20.6849]))])
epoch：259	 i:0 	 global-step:5180	 l-p:0.11667640507221222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[5.2527, 5.2543, 5.2528],
        [5.2527, 5.2527, 5.2527],
        [5.2527, 5.6731, 5.6513],
        [5.2527, 5.3950, 5.3237]], grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.11650607734918594 
model_pd.l_d.mean(): -13.197962760925293 
model_pd.lagr.mean(): -13.081457138061523 
model_pd.lambdas: dict_items([('pout', tensor([1.3841])), ('power', tensor([0.7282]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3631])), ('power', tensor([-20.6830]))])
epoch：260	 i:0 	 global-step:5200	 l-p:0.11650607734918594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.2564, 5.3113, 5.2715],
        [5.2564, 6.1604, 6.4966],
        [5.2564, 5.2564, 5.2564],
        [5.2564, 5.3094, 5.2706]], grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.11633399873971939 
model_pd.l_d.mean(): -13.174592018127441 
model_pd.lagr.mean(): -13.058258056640625 
model_pd.lambdas: dict_items([('pout', tensor([1.3855])), ('power', tensor([0.7272]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3622])), ('power', tensor([-20.6811]))])
epoch：261	 i:0 	 global-step:5220	 l-p:0.11633399873971939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[5.2601, 5.4116, 5.3387],
        [5.2601, 5.4024, 5.3312],
        [5.2601, 5.9928, 6.1764],
        [5.2601, 5.4623, 5.3854]], grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.11616015434265137 
model_pd.l_d.mean(): -13.15122127532959 
model_pd.lagr.mean(): -13.03506088256836 
model_pd.lambdas: dict_items([('pout', tensor([1.3868])), ('power', tensor([0.7261]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3613])), ('power', tensor([-20.6792]))])
epoch：262	 i:0 	 global-step:5240	 l-p:0.11616015434265137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.2639, 5.2654, 5.2639],
        [5.2639, 5.4972, 5.4215],
        [5.2639, 5.2640, 5.2639],
        [5.2639, 5.3181, 5.2786]], grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.11598464101552963 
model_pd.l_d.mean(): -13.127860069274902 
model_pd.lagr.mean(): -13.01187515258789 
model_pd.lambdas: dict_items([('pout', tensor([1.3882])), ('power', tensor([0.7251]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3604])), ('power', tensor([-20.6773]))])
epoch：263	 i:0 	 global-step:5260	 l-p:0.11598464101552963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[5.2677, 6.4812, 7.1396],
        [5.2677, 5.3224, 5.2827],
        [5.2677, 6.1657, 6.4951],
        [5.2677, 5.2677, 5.2677]], grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.11580773442983627 
model_pd.l_d.mean(): -13.104503631591797 
model_pd.lagr.mean(): -12.988696098327637 
model_pd.lambdas: dict_items([('pout', tensor([1.3896])), ('power', tensor([0.7241]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3594])), ('power', tensor([-20.6753]))])
epoch：264	 i:0 	 global-step:5280	 l-p:0.11580773442983627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.2715, 5.5762, 5.5123],
        [5.2715, 5.9649, 6.1161],
        [5.2715, 5.3000, 5.2767],
        [5.2715, 5.3749, 5.3137]], grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11562971025705338 
model_pd.l_d.mean(): -13.081151008605957 
model_pd.lagr.mean(): -12.965520858764648 
model_pd.lambdas: dict_items([('pout', tensor([1.3909])), ('power', tensor([0.7230]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3585])), ('power', tensor([-20.6733]))])
epoch：265	 i:0 	 global-step:5300	 l-p:0.11562971025705338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.2754, 5.3285, 5.2896],
        [5.2754, 6.0097, 6.1936],
        [5.2754, 5.7274, 5.7199],
        [5.2754, 5.3301, 5.2904]], grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.11545092612504959 
model_pd.l_d.mean(): -13.057806015014648 
model_pd.lagr.mean(): -12.942355155944824 
model_pd.lambdas: dict_items([('pout', tensor([1.3923])), ('power', tensor([0.7220]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3575])), ('power', tensor([-20.6713]))])
epoch：266	 i:0 	 global-step:5320	 l-p:0.11545092612504959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.2793, 5.2796, 5.2793],
        [5.2793, 5.7012, 5.6790],
        [5.2793, 5.2793, 5.2793],
        [5.2793, 5.3827, 5.3215]], grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.11527163535356522 
model_pd.l_d.mean(): -13.034466743469238 
model_pd.lagr.mean(): -12.919195175170898 
model_pd.lambdas: dict_items([('pout', tensor([1.3936])), ('power', tensor([0.7210]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3566])), ('power', tensor([-20.6693]))])
epoch：267	 i:0 	 global-step:5340	 l-p:0.11527163535356522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.2832, 5.2832, 5.2832],
        [5.2832, 6.1910, 6.5282],
        [5.2832, 6.8287, 7.8861],
        [5.2832, 5.2835, 5.2832]], grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.11509208381175995 
model_pd.l_d.mean(): -13.011133193969727 
model_pd.lagr.mean(): -12.896040916442871 
model_pd.lambdas: dict_items([('pout', tensor([1.3950])), ('power', tensor([0.7199]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3556])), ('power', tensor([-20.6673]))])
epoch：268	 i:0 	 global-step:5360	 l-p:0.11509208381175995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[5.2871, 6.5620, 7.2880],
        [5.2871, 5.3906, 5.3294],
        [5.2871, 5.9821, 6.1335],
        [5.2871, 5.5778, 5.5103]], grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.11491245031356812 
model_pd.l_d.mean(): -12.987807273864746 
model_pd.lagr.mean(): -12.872895240783691 
model_pd.lambdas: dict_items([('pout', tensor([1.3963])), ('power', tensor([0.7189]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3547])), ('power', tensor([-20.6652]))])
epoch：269	 i:0 	 global-step:5380	 l-p:0.11491245031356812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[5.2911, 5.4713, 5.3949],
        [5.2911, 5.4940, 5.4167],
        [5.2911, 6.8388, 7.8974],
        [5.2911, 5.2911, 5.2911]], grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.11473281681537628 
model_pd.l_d.mean(): -12.964485168457031 
model_pd.lagr.mean(): -12.849752426147461 
model_pd.lambdas: dict_items([('pout', tensor([1.3977])), ('power', tensor([0.7179]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3537])), ('power', tensor([-20.6632]))])
epoch：270	 i:0 	 global-step:5400	 l-p:0.11473281681537628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.2950, 5.3272, 5.3013],
        [5.2950, 5.2950, 5.2950],
        [5.2950, 5.3494, 5.3098],
        [5.2950, 5.2950, 5.2950]], grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.11455319076776505 
model_pd.l_d.mean(): -12.94117259979248 
model_pd.lagr.mean(): -12.826619148254395 
model_pd.lambdas: dict_items([('pout', tensor([1.3991])), ('power', tensor([0.7168]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3528])), ('power', tensor([-20.6611]))])
epoch：271	 i:0 	 global-step:5420	 l-p:0.11455319076776505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[5.2990, 6.0359, 6.2201],
        [5.2990, 6.2014, 6.5320],
        [5.2990, 5.2990, 5.2990],
        [5.2990, 6.5765, 7.3037]], grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.11437347531318665 
model_pd.l_d.mean(): -12.917865753173828 
model_pd.lagr.mean(): -12.803492546081543 
model_pd.lambdas: dict_items([('pout', tensor([1.4004])), ('power', tensor([0.7158]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3518])), ('power', tensor([-20.6591]))])
epoch：272	 i:0 	 global-step:5440	 l-p:0.11437347531318665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[5.3030, 5.8195, 5.8463],
        [5.3030, 5.3030, 5.3030],
        [5.3030, 5.3578, 5.3180],
        [5.3030, 5.4066, 5.3452]], grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.11419357359409332 
model_pd.l_d.mean(): -12.89456558227539 
model_pd.lagr.mean(): -12.78037166595459 
model_pd.lambdas: dict_items([('pout', tensor([1.4018])), ('power', tensor([0.7148]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3508])), ('power', tensor([-20.6570]))])
epoch：273	 i:0 	 global-step:5460	 l-p:0.11419357359409332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.3070, 5.9553, 6.0698],
        [5.3070, 5.3618, 5.3220],
        [5.3070, 5.3075, 5.3070],
        [5.3070, 5.7756, 5.7754]], grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.11401346325874329 
model_pd.l_d.mean(): -12.871272087097168 
model_pd.lagr.mean(): -12.757258415222168 
model_pd.lambdas: dict_items([('pout', tensor([1.4031])), ('power', tensor([0.7137]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3498])), ('power', tensor([-20.6549]))])
epoch：274	 i:0 	 global-step:5480	 l-p:0.11401346325874329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.3110, 5.3112, 5.3110],
        [5.3110, 5.7651, 5.7572],
        [5.3110, 5.4916, 5.4150],
        [5.3110, 6.0084, 6.1600]], grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.11383287608623505 
model_pd.l_d.mean(): -12.847984313964844 
model_pd.lagr.mean(): -12.734151840209961 
model_pd.lambdas: dict_items([('pout', tensor([1.4045])), ('power', tensor([0.7127]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3489])), ('power', tensor([-20.6528]))])
epoch：275	 i:0 	 global-step:5500	 l-p:0.11383287608623505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[5.3150, 5.3153, 5.3150],
        [5.3150, 5.3472, 5.3213],
        [5.3150, 6.5960, 7.3249],
        [5.3150, 5.3211, 5.3155]], grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.11365170776844025 
model_pd.l_d.mean(): -12.82470417022705 
model_pd.lagr.mean(): -12.711052894592285 
model_pd.lambdas: dict_items([('pout', tensor([1.4058])), ('power', tensor([0.7117]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3479])), ('power', tensor([-20.6507]))])
epoch：276	 i:0 	 global-step:5520	 l-p:0.11365170776844025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[5.3191, 5.3191, 5.3191],
        [5.3191, 5.3358, 5.3213],
        [5.3191, 6.2244, 6.5557],
        [5.3191, 5.7432, 5.7205]], grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.11346988379955292 
model_pd.l_d.mean(): -12.801429748535156 
model_pd.lagr.mean(): -12.687959671020508 
model_pd.lambdas: dict_items([('pout', tensor([1.4071])), ('power', tensor([0.7106]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3469])), ('power', tensor([-20.6485]))])
epoch：277	 i:0 	 global-step:5540	 l-p:0.11346988379955292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[5.3232, 5.4471, 5.3796],
        [5.3232, 5.3232, 5.3232],
        [5.3232, 5.7780, 5.7701],
        [5.3232, 6.6060, 7.3358]], grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.1132872998714447 
model_pd.l_d.mean(): -12.778160095214844 
model_pd.lagr.mean(): -12.664873123168945 
model_pd.lambdas: dict_items([('pout', tensor([1.4085])), ('power', tensor([0.7096]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3459])), ('power', tensor([-20.6463]))])
epoch：278	 i:0 	 global-step:5560	 l-p:0.1132872998714447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[5.3273, 5.7100, 5.6696],
        [5.3273, 5.6066, 5.5360],
        [5.3273, 6.2414, 6.5804],
        [5.3273, 5.4513, 5.3837]], grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.11310380697250366 
model_pd.l_d.mean(): -12.754899978637695 
model_pd.lagr.mean(): -12.641796112060547 
model_pd.lambdas: dict_items([('pout', tensor([1.4098])), ('power', tensor([0.7086]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3449])), ('power', tensor([-20.6442]))])
epoch：279	 i:0 	 global-step:5580	 l-p:0.11310380697250366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[5.3314, 6.6161, 7.3468],
        [5.3314, 5.3320, 5.3315],
        [5.3314, 6.5334, 7.1701],
        [5.3314, 6.5581, 7.2226]], grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.11291943490505219 
model_pd.l_d.mean(): -12.73164176940918 
model_pd.lagr.mean(): -12.618721961975098 
model_pd.lambdas: dict_items([('pout', tensor([1.4112])), ('power', tensor([0.7075]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3439])), ('power', tensor([-20.6420]))])
epoch：280	 i:0 	 global-step:5600	 l-p:0.11291943490505219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[5.3356, 5.3890, 5.3499],
        [5.3356, 5.3524, 5.3378],
        [5.3356, 6.2510, 6.5903],
        [5.3356, 5.9868, 6.1014]], grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.1127341240644455 
model_pd.l_d.mean(): -12.708392143249512 
model_pd.lagr.mean(): -12.595658302307129 
model_pd.lambdas: dict_items([('pout', tensor([1.4125])), ('power', tensor([0.7065]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3429])), ('power', tensor([-20.6398]))])
epoch：281	 i:0 	 global-step:5620	 l-p:0.1127341240644455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.3398, 5.3399, 5.3398],
        [5.3398, 5.6324, 5.5642],
        [5.3398, 6.2548, 6.5933],
        [5.3398, 6.1741, 6.4382]], grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.11254791915416718 
model_pd.l_d.mean(): -12.685148239135742 
model_pd.lagr.mean(): -12.572600364685059 
model_pd.lambdas: dict_items([('pout', tensor([1.4139])), ('power', tensor([0.7055]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3419])), ('power', tensor([-20.6375]))])
epoch：282	 i:0 	 global-step:5640	 l-p:0.11254791915416718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.3441, 5.9960, 6.1108],
        [5.3441, 6.2607, 6.6004],
        [5.3441, 5.3521, 5.3448],
        [5.3441, 5.3987, 5.3589]], grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.11236082017421722 
model_pd.l_d.mean(): -12.661909103393555 
model_pd.lagr.mean(): -12.549548149108887 
model_pd.lambdas: dict_items([('pout', tensor([1.4152])), ('power', tensor([0.7044]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3408])), ('power', tensor([-20.6352]))])
epoch：283	 i:0 	 global-step:5660	 l-p:0.11236082017421722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[5.3484, 5.5013, 5.4275],
        [5.3484, 5.6284, 5.5575],
        [5.3484, 6.6368, 7.3694],
        [5.3484, 5.3484, 5.3484]], grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.11217278242111206 
model_pd.l_d.mean(): -12.638679504394531 
model_pd.lagr.mean(): -12.526506423950195 
model_pd.lambdas: dict_items([('pout', tensor([1.4165])), ('power', tensor([0.7034]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3398])), ('power', tensor([-20.6330]))])
epoch：284	 i:0 	 global-step:5680	 l-p:0.11217278242111206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[5.3527, 5.6329, 5.5619],
        [5.3527, 6.0957, 6.2807],
        [5.3527, 5.4081, 5.3679],
        [5.3527, 6.2629, 6.5956]], grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.111984021961689 
model_pd.l_d.mean(): -12.615453720092773 
model_pd.lagr.mean(): -12.503469467163086 
model_pd.lambdas: dict_items([('pout', tensor([1.4179])), ('power', tensor([0.7024]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3388])), ('power', tensor([-20.6307]))])
epoch：285	 i:0 	 global-step:5700	 l-p:0.111984021961689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[5.3570, 6.9231, 7.9931],
        [5.3570, 6.5891, 7.2561],
        [5.3570, 5.8140, 5.8058],
        [5.3570, 5.3650, 5.3577]], grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.11179439723491669 
model_pd.l_d.mean(): -12.592238426208496 
model_pd.lagr.mean(): -12.480443954467773 
model_pd.lambdas: dict_items([('pout', tensor([1.4192])), ('power', tensor([0.7013]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3377])), ('power', tensor([-20.6284]))])
epoch：286	 i:0 	 global-step:5720	 l-p:0.11179439723491669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[5.3614, 5.5145, 5.4406],
        [5.3614, 5.8493, 5.8571],
        [5.3614, 5.3614, 5.3614],
        [5.3614, 5.4169, 5.3766]], grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.11160404235124588 
model_pd.l_d.mean(): -12.569026947021484 
model_pd.lagr.mean(): -12.457423210144043 
model_pd.lambdas: dict_items([('pout', tensor([1.4206])), ('power', tensor([0.7003]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3367])), ('power', tensor([-20.6260]))])
epoch：287	 i:0 	 global-step:5740	 l-p:0.11160404235124588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[5.3658, 5.4903, 5.4224],
        [5.3658, 5.3661, 5.3658],
        [5.3658, 5.3719, 5.3662],
        [5.3658, 6.8200, 7.7480]], grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.11141295731067657 
model_pd.l_d.mean(): -12.545822143554688 
model_pd.lagr.mean(): -12.434409141540527 
model_pd.lambdas: dict_items([('pout', tensor([1.4219])), ('power', tensor([0.6993]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3356])), ('power', tensor([-20.6237]))])
epoch：288	 i:0 	 global-step:5760	 l-p:0.11141295731067657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.3702, 5.8587, 5.8664],
        [5.3702, 5.3717, 5.3702],
        [5.3702, 5.5750, 5.4968],
        [5.3702, 5.8430, 5.8423]], grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11122114956378937 
model_pd.l_d.mean(): -12.522624969482422 
model_pd.lagr.mean(): -12.41140365600586 
model_pd.lambdas: dict_items([('pout', tensor([1.4232])), ('power', tensor([0.6983]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3345])), ('power', tensor([-20.6213]))])
epoch：289	 i:0 	 global-step:5780	 l-p:0.11122114956378937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.3746, 5.3826, 5.3753],
        [5.3746, 6.8312, 7.7605],
        [5.3746, 5.6686, 5.5998],
        [5.3746, 6.0789, 6.2312]], grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.11102860420942307 
model_pd.l_d.mean(): -12.499436378479004 
model_pd.lagr.mean(): -12.388407707214355 
model_pd.lambdas: dict_items([('pout', tensor([1.4246])), ('power', tensor([0.6972]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3335])), ('power', tensor([-20.6189]))])
epoch：290	 i:0 	 global-step:5800	 l-p:0.11102860420942307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.3791, 6.9515, 8.0254],
        [5.3791, 5.5612, 5.4838],
        [5.3791, 5.6156, 5.5385],
        [5.3791, 5.3959, 5.3813]], grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.11083535850048065 
model_pd.l_d.mean(): -12.476250648498535 
model_pd.lagr.mean(): -12.365415573120117 
model_pd.lambdas: dict_items([('pout', tensor([1.4259])), ('power', tensor([0.6962]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3324])), ('power', tensor([-20.6165]))])
epoch：291	 i:0 	 global-step:5820	 l-p:0.11083535850048065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.3836, 5.4124, 5.3888],
        [5.3836, 6.2984, 6.6324],
        [5.3836, 5.8115, 5.7882],
        [5.3836, 5.3851, 5.3836]], grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.11064131557941437 
model_pd.l_d.mean(): -12.453072547912598 
model_pd.lagr.mean(): -12.34243106842041 
model_pd.lambdas: dict_items([('pout', tensor([1.4272])), ('power', tensor([0.6952]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3313])), ('power', tensor([-20.6141]))])
epoch：292	 i:0 	 global-step:5840	 l-p:0.11064131557941437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[5.3881, 5.4429, 5.4030],
        [5.3881, 5.3883, 5.3881],
        [5.3881, 5.4418, 5.4025],
        [5.3881, 6.0939, 6.2464]], grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.1104464903473854 
model_pd.l_d.mean(): -12.429903984069824 
model_pd.lagr.mean(): -12.319457054138184 
model_pd.lambdas: dict_items([('pout', tensor([1.4286])), ('power', tensor([0.6941]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3302])), ('power', tensor([-20.6116]))])
epoch：293	 i:0 	 global-step:5860	 l-p:0.1104464903473854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.3926, 5.7788, 5.7376],
        [5.3926, 5.7021, 5.6365],
        [5.3926, 5.3926, 5.3926],
        [5.3926, 6.1404, 6.3261]], grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.1102508082985878 
model_pd.l_d.mean(): -12.406740188598633 
model_pd.lagr.mean(): -12.296489715576172 
model_pd.lambdas: dict_items([('pout', tensor([1.4299])), ('power', tensor([0.6931]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3291])), ('power', tensor([-20.6091]))])
epoch：294	 i:0 	 global-step:5880	 l-p:0.1102508082985878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[5.3972, 5.4521, 5.4121],
        [5.3972, 5.3988, 5.3973],
        [5.3972, 5.3972, 5.3972],
        [5.3972, 5.5511, 5.4768]], grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.11005420982837677 
model_pd.l_d.mean(): -12.383584976196289 
model_pd.lagr.mean(): -12.273530960083008 
model_pd.lambdas: dict_items([('pout', tensor([1.4312])), ('power', tensor([0.6921]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3280])), ('power', tensor([-20.6067]))])
epoch：295	 i:0 	 global-step:5900	 l-p:0.11005420982837677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.4018, 5.6839, 5.6123],
        [5.4018, 6.3262, 6.6674],
        [5.4018, 5.4575, 5.4171],
        [5.4018, 5.4021, 5.4018]], grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.10985663533210754 
model_pd.l_d.mean(): -12.36043643951416 
model_pd.lagr.mean(): -12.250579833984375 
model_pd.lambdas: dict_items([('pout', tensor([1.4325])), ('power', tensor([0.6910]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3269])), ('power', tensor([-20.6041]))])
epoch：296	 i:0 	 global-step:5920	 l-p:0.10985663533210754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.4065, 6.2498, 6.5160],
        [5.4065, 5.4065, 5.4065],
        [5.4065, 6.6493, 7.3214],
        [5.4065, 6.3315, 6.6729]], grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.10965806245803833 
model_pd.l_d.mean(): -12.33729362487793 
model_pd.lagr.mean(): -12.227635383605957 
model_pd.lambdas: dict_items([('pout', tensor([1.4339])), ('power', tensor([0.6900]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3258])), ('power', tensor([-20.6016]))])
epoch：297	 i:0 	 global-step:5940	 l-p:0.10965806245803833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[5.4111, 5.4112, 5.4111],
        [5.4111, 5.4111, 5.4111],
        [5.4111, 5.5559, 5.4831],
        [5.4111, 5.4111, 5.4111]], grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.1094583049416542 
model_pd.l_d.mean(): -12.314156532287598 
model_pd.lagr.mean(): -12.20469856262207 
model_pd.lambdas: dict_items([('pout', tensor([1.4352])), ('power', tensor([0.6890]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3247])), ('power', tensor([-20.5991]))])
epoch：298	 i:0 	 global-step:5960	 l-p:0.1094583049416542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.4158, 5.4161, 5.4158],
        [5.4158, 5.4159, 5.4158],
        [5.4158, 5.4158, 5.4158],
        [5.4158, 5.4327, 5.4181]], grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.10925742238759995 
model_pd.l_d.mean(): -12.291027069091797 
model_pd.lagr.mean(): -12.181769371032715 
model_pd.lambdas: dict_items([('pout', tensor([1.4365])), ('power', tensor([0.6880]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3236])), ('power', tensor([-20.5965]))])
epoch：299	 i:0 	 global-step:5980	 l-p:0.10925742238759995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.4206, 6.7256, 7.4665],
        [5.4206, 5.6585, 5.5807],
        [5.4206, 5.4375, 5.4228],
        [5.4206, 5.6268, 5.5479]], grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.10905527323484421 
model_pd.l_d.mean(): -12.267906188964844 
model_pd.lagr.mean(): -12.15885066986084 
model_pd.lambdas: dict_items([('pout', tensor([1.4378])), ('power', tensor([0.6869]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3224])), ('power', tensor([-20.5939]))])
epoch：300	 i:0 	 global-step:6000	 l-p:0.10905527323484421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[5.4254, 5.5704, 5.4975],
        [5.4254, 6.3534, 6.6957],
        [5.4254, 5.7214, 5.6520],
        [5.4254, 5.5798, 5.5052]], grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.10885190963745117 
model_pd.l_d.mean(): -12.244790077209473 
model_pd.lagr.mean(): -12.13593864440918 
model_pd.lambdas: dict_items([('pout', tensor([1.4392])), ('power', tensor([0.6859]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3213])), ('power', tensor([-20.5913]))])
epoch：301	 i:0 	 global-step:6020	 l-p:0.10885190963745117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.4302, 5.6135, 5.5355],
        [5.4302, 5.4861, 5.4455],
        [5.4302, 5.4364, 5.4306],
        [5.4302, 5.5557, 5.4872]], grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.10864713042974472 
model_pd.l_d.mean(): -12.221683502197266 
model_pd.lagr.mean(): -12.113036155700684 
model_pd.lambdas: dict_items([('pout', tensor([1.4405])), ('power', tensor([0.6849]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3201])), ('power', tensor([-20.5886]))])
epoch：302	 i:0 	 global-step:6040	 l-p:0.10864713042974472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[5.4350, 6.0963, 6.2118],
        [5.4350, 5.9125, 5.9113],
        [5.4350, 5.4520, 5.4373],
        [5.4350, 5.4902, 5.4500]], grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.10844101011753082 
model_pd.l_d.mean(): -12.19858169555664 
model_pd.lagr.mean(): -12.090140342712402 
model_pd.lambdas: dict_items([('pout', tensor([1.4418])), ('power', tensor([0.6838]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3190])), ('power', tensor([-20.5859]))])
epoch：303	 i:0 	 global-step:6060	 l-p:0.10844101011753082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[5.4399, 5.5946, 5.5199],
        [5.4399, 5.9178, 5.9165],
        [5.4399, 5.4399, 5.4399],
        [5.4399, 5.4399, 5.4399]], grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.10823345184326172 
model_pd.l_d.mean(): -12.175487518310547 
model_pd.lagr.mean(): -12.067254066467285 
model_pd.lambdas: dict_items([('pout', tensor([1.4431])), ('power', tensor([0.6828]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3178])), ('power', tensor([-20.5832]))])
epoch：304	 i:0 	 global-step:6080	 l-p:0.10823345184326172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[5.4449, 5.5006, 5.4601],
        [5.4449, 5.9079, 5.8989],
        [5.4449, 5.4449, 5.4449],
        [5.4449, 7.0365, 8.1224]], grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.10802440345287323 
model_pd.l_d.mean(): -12.152402877807617 
model_pd.lagr.mean(): -12.044378280639648 
model_pd.lambdas: dict_items([('pout', tensor([1.4444])), ('power', tensor([0.6818]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3166])), ('power', tensor([-20.5805]))])
epoch：305	 i:0 	 global-step:6100	 l-p:0.10802440345287323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.4498, 5.5553, 5.4927],
        [5.4498, 5.4499, 5.4498],
        [5.4498, 5.4501, 5.4498],
        [5.4498, 5.6569, 5.5776]], grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.10781379044055939 
model_pd.l_d.mean(): -12.12932014465332 
model_pd.lagr.mean(): -12.021506309509277 
model_pd.lambdas: dict_items([('pout', tensor([1.4457])), ('power', tensor([0.6807]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3155])), ('power', tensor([-20.5777]))])
epoch：306	 i:0 	 global-step:6120	 l-p:0.10781379044055939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[5.4549, 5.6388, 5.5605],
        [5.4549, 5.7521, 5.6823],
        [5.4549, 5.4552, 5.4549],
        [5.4549, 5.5109, 5.4702]], grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.10760161280632019 
model_pd.l_d.mean(): -12.106247901916504 
model_pd.lagr.mean(): -11.99864673614502 
model_pd.lambdas: dict_items([('pout', tensor([1.4471])), ('power', tensor([0.6797]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3143])), ('power', tensor([-20.5749]))])
epoch：307	 i:0 	 global-step:6140	 l-p:0.10760161280632019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.4599, 6.1239, 6.2397],
        [5.4599, 5.4599, 5.4599],
        [5.4599, 5.4599, 5.4599],
        [5.4599, 5.4599, 5.4599]], grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.10738782584667206 
model_pd.l_d.mean(): -12.083181381225586 
model_pd.lagr.mean(): -11.975793838500977 
model_pd.lambdas: dict_items([('pout', tensor([1.4484])), ('power', tensor([0.6787]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3131])), ('power', tensor([-20.5721]))])
epoch：308	 i:0 	 global-step:6160	 l-p:0.10738782584667206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.4650, 6.3926, 6.7304],
        [5.4650, 5.4650, 5.4650],
        [5.4650, 5.4651, 5.4650],
        [5.4650, 5.7497, 5.6771]], grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.10717232525348663 
model_pd.l_d.mean(): -12.0601224899292 
model_pd.lagr.mean(): -11.952950477600098 
model_pd.lambdas: dict_items([('pout', tensor([1.4497])), ('power', tensor([0.6777]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3118])), ('power', tensor([-20.5693]))])
epoch：309	 i:0 	 global-step:6180	 l-p:0.10717232525348663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[5.4702, 5.6255, 5.5504],
        [5.4702, 5.4704, 5.4702],
        [5.4702, 6.7273, 7.4064],
        [5.4702, 6.2275, 6.4149]], grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.10695502907037735 
model_pd.l_d.mean(): -12.037070274353027 
model_pd.lagr.mean(): -11.930115699768066 
model_pd.lambdas: dict_items([('pout', tensor([1.4510])), ('power', tensor([0.6766]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3106])), ('power', tensor([-20.5664]))])
epoch：310	 i:0 	 global-step:6200	 l-p:0.10695502907037735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.4754, 6.7082, 7.3594],
        [5.4754, 5.4816, 5.4758],
        [5.4754, 5.6599, 5.5812],
        [5.4754, 5.6831, 5.6035]], grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.10673593729734421 
model_pd.l_d.mean(): -12.014026641845703 
model_pd.lagr.mean(): -11.9072904586792 
model_pd.lambdas: dict_items([('pout', tensor([1.4523])), ('power', tensor([0.6756]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3094])), ('power', tensor([-20.5635]))])
epoch：311	 i:0 	 global-step:6220	 l-p:0.10673593729734421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[5.4806, 5.5361, 5.4956],
        [5.4806, 5.9775, 5.9845],
        [5.4806, 5.4809, 5.4806],
        [5.4806, 5.4806, 5.4806]], grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.1065148189663887 
model_pd.l_d.mean(): -11.990989685058594 
model_pd.lagr.mean(): -11.884474754333496 
model_pd.lambdas: dict_items([('pout', tensor([1.4536])), ('power', tensor([0.6746]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3081])), ('power', tensor([-20.5606]))])
epoch：312	 i:0 	 global-step:6240	 l-p:0.1065148189663887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[5.4859, 6.9725, 7.9194],
        [5.4859, 5.4865, 5.4859],
        [5.4859, 5.5029, 5.4881],
        [5.4859, 5.4859, 5.4859]], grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.10629170387983322 
model_pd.l_d.mean(): -11.967960357666016 
model_pd.lagr.mean(): -11.861668586730957 
model_pd.lambdas: dict_items([('pout', tensor([1.4549])), ('power', tensor([0.6735]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3069])), ('power', tensor([-20.5576]))])
epoch：313	 i:0 	 global-step:6260	 l-p:0.10629170387983322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[5.4912, 5.5973, 5.5343],
        [5.4912, 5.6762, 5.5973],
        [5.4912, 5.5473, 5.5065],
        [5.4912, 5.4912, 5.4912]], grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.10606643557548523 
model_pd.l_d.mean(): -11.94493579864502 
model_pd.lagr.mean(): -11.838869094848633 
model_pd.lambdas: dict_items([('pout', tensor([1.4562])), ('power', tensor([0.6725]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3056])), ('power', tensor([-20.5546]))])
epoch：314	 i:0 	 global-step:6280	 l-p:0.10606643557548523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[5.4966, 6.3530, 6.6224],
        [5.4966, 5.9320, 5.9076],
        [5.4966, 5.6817, 5.6028],
        [5.4966, 6.8198, 7.5702]], grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.10583890229463577 
model_pd.l_d.mean(): -11.921920776367188 
model_pd.lagr.mean(): -11.816082000732422 
model_pd.lambdas: dict_items([('pout', tensor([1.4575])), ('power', tensor([0.6715]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3044])), ('power', tensor([-20.5516]))])
epoch：315	 i:0 	 global-step:6300	 l-p:0.10583890229463577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.5021, 5.6873, 5.6083],
        [5.5021, 5.7107, 5.6307],
        [5.5021, 6.7666, 7.4493],
        [5.5021, 5.5191, 5.5043]], grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.1056089773774147 
model_pd.l_d.mean(): -11.898909568786621 
model_pd.lagr.mean(): -11.79330062866211 
model_pd.lambdas: dict_items([('pout', tensor([1.4588])), ('power', tensor([0.6705]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3031])), ('power', tensor([-20.5485]))])
epoch：316	 i:0 	 global-step:6320	 l-p:0.1056089773774147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.5076, 5.7941, 5.7209],
        [5.5076, 5.6930, 5.6139],
        [5.5076, 5.5091, 5.5076],
        [5.5076, 5.5077, 5.5076]], grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.10537645220756531 
model_pd.l_d.mean(): -11.875908851623535 
model_pd.lagr.mean(): -11.770532608032227 
model_pd.lambdas: dict_items([('pout', tensor([1.4601])), ('power', tensor([0.6694]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3018])), ('power', tensor([-20.5454]))])
epoch：317	 i:0 	 global-step:6340	 l-p:0.10537645220756531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[5.5131, 5.9065, 5.8638],
        [5.5131, 5.5697, 5.5286],
        [5.5131, 5.5194, 5.5136],
        [5.5131, 6.4486, 6.7888]], grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.10514125972986221 
model_pd.l_d.mean(): -11.852912902832031 
model_pd.lagr.mean(): -11.747771263122559 
model_pd.lambdas: dict_items([('pout', tensor([1.4614])), ('power', tensor([0.6684]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3005])), ('power', tensor([-20.5422]))])
epoch：318	 i:0 	 global-step:6360	 l-p:0.10514125972986221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.5188, 6.0527, 6.0786],
        [5.5188, 5.6459, 5.5764],
        [5.5188, 5.5250, 5.5192],
        [5.5188, 5.5188, 5.5188]], grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.10490312427282333 
model_pd.l_d.mean(): -11.829923629760742 
model_pd.lagr.mean(): -11.725020408630371 
model_pd.lambdas: dict_items([('pout', tensor([1.4627])), ('power', tensor([0.6674]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2991])), ('power', tensor([-20.5390]))])
epoch：319	 i:0 	 global-step:6380	 l-p:0.10490312427282333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[5.5245, 6.4698, 6.8182],
        [5.5245, 5.5260, 5.5245],
        [5.5245, 6.0250, 6.0318],
        [5.5245, 5.6517, 5.5821]], grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.10466188192367554 
model_pd.l_d.mean(): -11.806944847106934 
model_pd.lagr.mean(): -11.702282905578613 
model_pd.lambdas: dict_items([('pout', tensor([1.4640])), ('power', tensor([0.6664]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2978])), ('power', tensor([-20.5358]))])
epoch：320	 i:0 	 global-step:6400	 l-p:0.10466188192367554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[5.5302, 6.4755, 6.8231],
        [5.5302, 5.8461, 5.7786],
        [5.5302, 5.5302, 5.5302],
        [5.5302, 5.5303, 5.5302]], grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.10441742837429047 
model_pd.l_d.mean(): -11.78396987915039 
model_pd.lagr.mean(): -11.67955207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.4653])), ('power', tensor([0.6653]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2964])), ('power', tensor([-20.5325]))])
epoch：321	 i:0 	 global-step:6420	 l-p:0.10441742837429047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[5.5361, 5.5361, 5.5361],
        [5.5361, 6.4754, 6.8169],
        [5.5361, 5.5361, 5.5361],
        [5.5361, 5.6834, 5.6092]], grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.10416946560144424 
model_pd.l_d.mean(): -11.76099967956543 
model_pd.lagr.mean(): -11.656829833984375 
model_pd.lambdas: dict_items([('pout', tensor([1.4666])), ('power', tensor([0.6643]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2951])), ('power', tensor([-20.5291]))])
epoch：322	 i:0 	 global-step:6440	 l-p:0.10416946560144424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[5.5420, 5.5420, 5.5420],
        [5.5420, 5.5502, 5.5427],
        [5.5420, 5.5751, 5.5485],
        [5.5420, 5.8433, 5.7722]], grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.10391780734062195 
model_pd.l_d.mean(): -11.738039016723633 
model_pd.lagr.mean(): -11.63412094116211 
model_pd.lambdas: dict_items([('pout', tensor([1.4679])), ('power', tensor([0.6633]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2937])), ('power', tensor([-20.5258]))])
epoch：323	 i:0 	 global-step:6460	 l-p:0.10391780734062195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[5.5480, 5.9871, 5.9622],
        [5.5480, 5.6040, 5.5632],
        [5.5480, 6.0846, 6.1105],
        [5.5480, 5.5774, 5.5533]], grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.10366221517324448 
model_pd.l_d.mean(): -11.715085983276367 
model_pd.lagr.mean(): -11.61142349243164 
model_pd.lambdas: dict_items([('pout', tensor([1.4692])), ('power', tensor([0.6623]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2923])), ('power', tensor([-20.5223]))])
epoch：324	 i:0 	 global-step:6480	 l-p:0.10366221517324448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[5.5541, 6.8916, 7.6495],
        [5.5541, 5.5557, 5.5541],
        [5.5541, 5.6611, 5.5975],
        [5.5541, 5.5543, 5.5541]], grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.10340243577957153 
model_pd.l_d.mean(): -11.69213581085205 
model_pd.lagr.mean(): -11.588733673095703 
model_pd.lambdas: dict_items([('pout', tensor([1.4705])), ('power', tensor([0.6612]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2908])), ('power', tensor([-20.5188]))])
epoch：325	 i:0 	 global-step:6500	 l-p:0.10340243577957153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[5.5602, 6.0003, 5.9753],
        [5.5602, 6.5107, 6.8600],
        [5.5602, 5.5603, 5.5602],
        [5.5602, 5.5605, 5.5603]], grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.10313814878463745 
model_pd.l_d.mean(): -11.669196128845215 
model_pd.lagr.mean(): -11.566058158874512 
model_pd.lambdas: dict_items([('pout', tensor([1.4718])), ('power', tensor([0.6602]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2894])), ('power', tensor([-20.5153]))])
epoch：326	 i:0 	 global-step:6520	 l-p:0.10313814878463745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[5.5665, 5.5665, 5.5665],
        [5.5665, 6.2939, 6.4496],
        [5.5665, 5.5681, 5.5666],
        [5.5665, 5.7146, 5.6400]], grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.10286906361579895 
model_pd.l_d.mean(): -11.646261215209961 
model_pd.lagr.mean(): -11.543392181396484 
model_pd.lambdas: dict_items([('pout', tensor([1.4731])), ('power', tensor([0.6592]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2879])), ('power', tensor([-20.5116]))])
epoch：327	 i:0 	 global-step:6540	 l-p:0.10286906361579895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[5.5729, 5.5901, 5.5751],
        [5.5729, 5.5731, 5.5729],
        [5.5729, 6.0613, 6.0593],
        [5.5729, 5.6024, 5.5782]], grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.10259483009576797 
model_pd.l_d.mean(): -11.623331069946289 
model_pd.lagr.mean(): -11.520736694335938 
model_pd.lambdas: dict_items([('pout', tensor([1.4744])), ('power', tensor([0.6582]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2864])), ('power', tensor([-20.5080]))])
epoch：328	 i:0 	 global-step:6560	 l-p:0.10259483009576797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[5.5794, 5.8826, 5.8109],
        [5.5794, 5.7668, 5.6868],
        [5.5794, 5.7373, 5.6608],
        [5.5794, 5.5839, 5.5796]], grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.1023150086402893 
model_pd.l_d.mean(): -11.600406646728516 
model_pd.lagr.mean(): -11.498091697692871 
model_pd.lambdas: dict_items([('pout', tensor([1.4757])), ('power', tensor([0.6571]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2849])), ('power', tensor([-20.5042]))])
epoch：329	 i:0 	 global-step:6580	 l-p:0.1023150086402893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[5.5859, 5.7973, 5.7161],
        [5.5859, 6.5420, 6.8938],
        [5.5859, 5.6431, 5.6016],
        [5.5859, 5.8761, 5.8018]], grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.10202915966510773 
model_pd.l_d.mean(): -11.577488899230957 
model_pd.lagr.mean(): -11.475460052490234 
model_pd.lambdas: dict_items([('pout', tensor([1.4769])), ('power', tensor([0.6561]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2834])), ('power', tensor([-20.5004]))])
epoch：330	 i:0 	 global-step:6600	 l-p:0.10202915966510773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[5.5927, 6.2722, 6.3897],
        [5.5927, 5.5930, 5.5927],
        [5.5927, 5.6223, 5.5980],
        [5.5927, 5.8043, 5.7230]], grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.1017366498708725 
model_pd.l_d.mean(): -11.554574966430664 
model_pd.lagr.mean(): -11.452837944030762 
model_pd.lambdas: dict_items([('pout', tensor([1.4782])), ('power', tensor([0.6551]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2818])), ('power', tensor([-20.4965]))])
epoch：331	 i:0 	 global-step:6620	 l-p:0.1017366498708725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[5.5995, 5.5998, 5.5995],
        [5.5995, 5.7073, 5.6432],
        [5.5995, 5.6548, 5.6143],
        [5.5995, 6.5569, 6.9085]], grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.101437047123909 
model_pd.l_d.mean(): -11.53166675567627 
model_pd.lagr.mean(): -11.430230140686035 
model_pd.lambdas: dict_items([('pout', tensor([1.4795])), ('power', tensor([0.6541]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2802])), ('power', tensor([-20.4925]))])
epoch：332	 i:0 	 global-step:6640	 l-p:0.101437047123909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[5.6065, 6.2877, 6.4055],
        [5.6065, 5.8977, 5.8231],
        [5.6065, 5.6066, 5.6065],
        [5.6065, 5.6635, 5.6220]], grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.10112952440977097 
model_pd.l_d.mean(): -11.50876522064209 
model_pd.lagr.mean(): -11.407635688781738 
model_pd.lambdas: dict_items([('pout', tensor([1.4808])), ('power', tensor([0.6530]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2786])), ('power', tensor([-20.4884]))])
epoch：333	 i:0 	 global-step:6660	 l-p:0.10112952440977097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[5.6137, 5.6139, 5.6137],
        [5.6137, 5.6690, 5.6284],
        [5.6137, 6.9665, 7.7327],
        [5.6137, 6.4884, 6.7628]], grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.10081331431865692 
model_pd.l_d.mean(): -11.48586654663086 
model_pd.lagr.mean(): -11.385053634643555 
model_pd.lambdas: dict_items([('pout', tensor([1.4821])), ('power', tensor([0.6520]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2769])), ('power', tensor([-20.4842]))])
epoch：334	 i:0 	 global-step:6680	 l-p:0.10081331431865692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[5.6210, 6.1136, 6.1114],
        [5.6210, 5.9264, 5.8542],
        [5.6210, 5.7292, 5.6648],
        [5.6210, 5.8664, 5.7857]], grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10048753023147583 
model_pd.l_d.mean(): -11.462968826293945 
model_pd.lagr.mean(): -11.362481117248535 
model_pd.lambdas: dict_items([('pout', tensor([1.4833])), ('power', tensor([0.6510]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2752])), ('power', tensor([-20.4799]))])
epoch：335	 i:0 	 global-step:6700	 l-p:0.10048753023147583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.6285, 5.6285, 5.6285],
        [5.6285, 6.1218, 6.1195],
        [5.6285, 5.6459, 5.6308],
        [5.6285, 5.6291, 5.6285]], grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.10015113651752472 
model_pd.l_d.mean(): -11.440074920654297 
model_pd.lagr.mean(): -11.339923858642578 
model_pd.lambdas: dict_items([('pout', tensor([1.4846])), ('power', tensor([0.6500]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2735])), ('power', tensor([-20.4754]))])
epoch：336	 i:0 	 global-step:6720	 l-p:0.10015113651752472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.6362, 6.5933, 6.9405],
        [5.6362, 5.6362, 5.6362],
        [5.6362, 5.6362, 5.6362],
        [5.6362, 5.6368, 5.6362]], grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.09980273246765137 
model_pd.l_d.mean(): -11.41718578338623 
model_pd.lagr.mean(): -11.3173828125 
model_pd.lambdas: dict_items([('pout', tensor([1.4859])), ('power', tensor([0.6489]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2717])), ('power', tensor([-20.4709]))])
epoch：337	 i:0 	 global-step:6740	 l-p:0.09980273246765137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.6441, 6.0466, 6.0025],
        [5.6441, 6.6098, 6.9641],
        [5.6441, 6.0908, 6.0651],
        [5.6441, 5.6441, 5.6441]], grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.09944095462560654 
model_pd.l_d.mean(): -11.394294738769531 
model_pd.lagr.mean(): -11.294854164123535 
model_pd.lambdas: dict_items([('pout', tensor([1.4871])), ('power', tensor([0.6479]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2698])), ('power', tensor([-20.4662]))])
epoch：338	 i:0 	 global-step:6760	 l-p:0.09944095462560654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.6523, 5.6523, 5.6523],
        [5.6523, 5.6529, 5.6524],
        [5.6523, 6.3914, 6.5492],
        [5.6523, 5.6523, 5.6523]], grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.09906402230262756 
model_pd.l_d.mean(): -11.371405601501465 
model_pd.lagr.mean(): -11.27234172821045 
model_pd.lambdas: dict_items([('pout', tensor([1.4884])), ('power', tensor([0.6469]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2679])), ('power', tensor([-20.4613]))])
epoch：339	 i:0 	 global-step:6780	 l-p:0.09906402230262756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[5.6608, 5.6609, 5.6608],
        [5.6608, 7.3201, 8.4500],
        [5.6608, 5.7909, 5.7197],
        [5.6608, 5.6783, 5.6631]], grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.09866977483034134 
model_pd.l_d.mean(): -11.348516464233398 
model_pd.lagr.mean(): -11.249846458435059 
model_pd.lambdas: dict_items([('pout', tensor([1.4897])), ('power', tensor([0.6459]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2660])), ('power', tensor([-20.4562]))])
epoch：340	 i:0 	 global-step:6800	 l-p:0.09866977483034134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[5.6697, 6.9754, 7.6791],
        [5.6697, 6.1510, 6.1406],
        [5.6697, 5.6702, 5.6697],
        [5.6697, 5.6697, 5.6697]], grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.09825560450553894 
model_pd.l_d.mean(): -11.3256196975708 
model_pd.lagr.mean(): -11.227364540100098 
model_pd.lambdas: dict_items([('pout', tensor([1.4909])), ('power', tensor([0.6448]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2639])), ('power', tensor([-20.4510]))])
epoch：341	 i:0 	 global-step:6820	 l-p:0.09825560450553894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[5.6789, 6.3695, 6.4886],
        [5.6789, 5.7089, 5.6843],
        [5.6789, 5.6794, 5.6789],
        [5.6789, 5.6805, 5.6789]], grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.09781823307275772 
model_pd.l_d.mean(): -11.302720069885254 
model_pd.lagr.mean(): -11.204901695251465 
model_pd.lambdas: dict_items([('pout', tensor([1.4922])), ('power', tensor([0.6438]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2618])), ('power', tensor([-20.4455]))])
epoch：342	 i:0 	 global-step:6840	 l-p:0.09781823307275772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[5.6885, 5.9841, 5.9083],
        [5.6885, 5.8796, 5.7979],
        [5.6885, 6.6625, 7.0199],
        [5.6885, 6.9725, 7.6491]], grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.09735376387834549 
model_pd.l_d.mean(): -11.279810905456543 
model_pd.lagr.mean(): -11.182456970214844 
model_pd.lambdas: dict_items([('pout', tensor([1.4935])), ('power', tensor([0.6428]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2596])), ('power', tensor([-20.4397]))])
epoch：343	 i:0 	 global-step:6860	 l-p:0.09735376387834549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[5.6986, 7.2480, 8.2331],
        [5.6986, 7.0119, 7.7196],
        [5.6986, 5.7566, 5.7144],
        [5.6986, 6.1827, 6.1721]], grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.09685685485601425 
model_pd.l_d.mean(): -11.256891250610352 
model_pd.lagr.mean(): -11.1600341796875 
model_pd.lambdas: dict_items([('pout', tensor([1.4947])), ('power', tensor([0.6418]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2573])), ('power', tensor([-20.4336]))])
epoch：344	 i:0 	 global-step:6880	 l-p:0.09685685485601425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[5.7094, 6.4567, 6.6162],
        [5.7094, 5.7094, 5.7094],
        [5.7094, 5.8710, 5.7926],
        [5.7094, 5.7094, 5.7094]], grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.09632089734077454 
model_pd.l_d.mean(): -11.233954429626465 
model_pd.lagr.mean(): -11.137633323669434 
model_pd.lambdas: dict_items([('pout', tensor([1.4960])), ('power', tensor([0.6408]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2548])), ('power', tensor([-20.4271]))])
epoch：345	 i:0 	 global-step:6900	 l-p:0.09632089734077454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[5.7208, 6.5141, 6.7091],
        [5.7208, 5.8524, 5.7804],
        [5.7208, 5.7785, 5.7364],
        [5.7208, 7.2771, 8.2667]], grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.09573701024055481 
model_pd.l_d.mean(): -11.210994720458984 
model_pd.lagr.mean(): -11.115257263183594 
model_pd.lambdas: dict_items([('pout', tensor([1.4972])), ('power', tensor([0.6397]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2522])), ('power', tensor([-20.4201]))])
epoch：346	 i:0 	 global-step:6920	 l-p:0.09573701024055481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[5.7331, 5.9260, 5.8435],
        [5.7331, 6.0609, 5.9904],
        [5.7331, 6.4313, 6.5516],
        [5.7331, 5.7897, 5.7482]], grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.09509311616420746 
model_pd.l_d.mean(): -11.18800163269043 
model_pd.lagr.mean(): -11.09290885925293 
model_pd.lambdas: dict_items([('pout', tensor([1.4985])), ('power', tensor([0.6387]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2494])), ('power', tensor([-20.4126]))])
epoch：347	 i:0 	 global-step:6940	 l-p:0.09509311616420746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[5.7465, 5.7465, 5.7465],
        [5.7465, 6.7248, 7.0795],
        [5.7465, 5.7466, 5.7465],
        [5.7465, 6.2682, 6.2746]], grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.09437229484319687 
model_pd.l_d.mean(): -11.164966583251953 
model_pd.lagr.mean(): -11.07059383392334 
model_pd.lambdas: dict_items([('pout', tensor([1.4997])), ('power', tensor([0.6377]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2463])), ('power', tensor([-20.4045]))])
epoch：348	 i:0 	 global-step:6960	 l-p:0.09437229484319687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[5.7613, 5.8203, 5.7774],
        [5.7613, 5.7629, 5.7613],
        [5.7613, 5.7957, 5.7680],
        [5.7613, 6.4635, 6.5845]], grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.09354989975690842 
model_pd.l_d.mean(): -11.14186954498291 
model_pd.lagr.mean(): -11.048319816589355 
model_pd.lambdas: dict_items([('pout', tensor([1.5010])), ('power', tensor([0.6367]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2429])), ('power', tensor([-20.3954]))])
epoch：349	 i:0 	 global-step:6980	 l-p:0.09354989975690842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[5.7778, 5.7780, 5.7778],
        [5.7778, 5.7782, 5.7778],
        [5.7778, 5.7778, 5.7778],
        [5.7778, 6.1910, 6.1456]], grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.09258855879306793 
model_pd.l_d.mean(): -11.118687629699707 
model_pd.lagr.mean(): -11.02609920501709 
model_pd.lambdas: dict_items([('pout', tensor([1.5022])), ('power', tensor([0.6357]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2392])), ('power', tensor([-20.3852]))])
epoch：350	 i:0 	 global-step:7000	 l-p:0.09258855879306793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[5.7967, 6.3238, 6.3303],
        [5.7967, 6.1128, 6.0379],
        [5.7967, 5.8562, 5.8129],
        [5.7967, 5.7973, 5.7967]], grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.09142807126045227 
model_pd.l_d.mean(): -11.095380783081055 
model_pd.lagr.mean(): -11.003952980041504 
model_pd.lambdas: dict_items([('pout', tensor([1.5034])), ('power', tensor([0.6346]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2349])), ('power', tensor([-20.3735]))])
epoch：351	 i:0 	 global-step:7020	 l-p:0.09142807126045227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[5.8189, 5.8370, 5.8213],
        [5.8189, 5.8537, 5.8257],
        [5.8189, 5.8189, 5.8189],
        [5.8189, 6.3314, 6.3289]], grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.0899643674492836 
model_pd.l_d.mean(): -11.071882247924805 
model_pd.lagr.mean(): -10.981918334960938 
model_pd.lambdas: dict_items([('pout', tensor([1.5047])), ('power', tensor([0.6336]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2299])), ('power', tensor([-20.3597]))])
epoch：352	 i:0 	 global-step:7040	 l-p:0.0899643674492836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[5.8460, 5.8461, 5.8460],
        [5.8460, 5.8460, 5.8460],
        [5.8460, 7.4432, 8.4589],
        [5.8460, 5.8810, 5.8528]], grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.08799818158149719 
model_pd.l_d.mean(): -11.048087120056152 
model_pd.lagr.mean(): -10.960088729858398 
model_pd.lambdas: dict_items([('pout', tensor([1.5059])), ('power', tensor([0.6326]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2238])), ('power', tensor([-20.3427]))])
epoch：353	 i:0 	 global-step:7060	 l-p:0.08799818158149719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[5.8806, 5.8808, 5.8806],
        [5.8806, 5.8989, 5.8830],
        [5.8806, 6.0795, 5.9945],
        [5.8806, 5.8806, 5.8806]], grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.08508604019880295 
model_pd.l_d.mean(): -11.023781776428223 
model_pd.lagr.mean(): -10.938695907592773 
model_pd.lambdas: dict_items([('pout', tensor([1.5071])), ('power', tensor([0.6316]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2160])), ('power', tensor([-20.3209]))])
epoch：354	 i:0 	 global-step:7080	 l-p:0.08508604019880295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[5.9280, 6.7105, 6.8777],
        [5.9280, 6.2697, 6.1964],
        [5.9280, 6.0974, 6.0153],
        [5.9280, 6.2533, 6.1763]], grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.07995638251304626 
model_pd.l_d.mean(): -10.998525619506836 
model_pd.lagr.mean(): -10.918569564819336 
model_pd.lambdas: dict_items([('pout', tensor([1.5083])), ('power', tensor([0.6306]))]) 
model_pd.vars: dict_items([('pout', tensor([1.2054])), ('power', tensor([-20.2907]))])
epoch：355	 i:0 	 global-step:7100	 l-p:0.07995638251304626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[6.0015, 6.2316, 6.1432],
        [6.0015, 6.3319, 6.2538],
        [6.0015, 6.1413, 6.0648],
        [6.0015, 6.4827, 6.4552]], grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.06632848083972931 
model_pd.l_d.mean(): -10.971113204956055 
model_pd.lagr.mean(): -10.90478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.5095])), ('power', tensor([0.6296]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1892])), ('power', tensor([-20.2434]))])
epoch：356	 i:0 	 global-step:7120	 l-p:0.06632848083972931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[6.1374, 6.1378, 6.1374],
        [6.1374, 6.1377, 6.1374],
        [6.1374, 7.2068, 7.6002],
        [6.1374, 6.1425, 6.1377]], grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 1.5813723802566528 
model_pd.l_d.mean(): -10.937542915344238 
model_pd.lagr.mean(): -9.356170654296875 
model_pd.lambdas: dict_items([('pout', tensor([1.5107])), ('power', tensor([0.6285]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1597])), ('power', tensor([-20.1541]))])
epoch：357	 i:0 	 global-step:7140	 l-p:1.5813723802566528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[6.3255, 7.8889, 8.7763],
        [6.3255, 6.6100, 6.5168],
        [6.3255, 6.6971, 6.6179],
        [6.3255, 6.6793, 6.5961]], grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.11819059401750565 
model_pd.l_d.mean(): -10.896238327026367 
model_pd.lagr.mean(): -10.778047561645508 
model_pd.lambdas: dict_items([('pout', tensor([1.5118])), ('power', tensor([0.6275]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1199])), ('power', tensor([-20.0272]))])
epoch：358	 i:0 	 global-step:7160	 l-p:0.11819059401750565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.5079, 6.5772, 6.5268],
        [6.5079, 6.8933, 6.8114],
        [6.5079, 6.5079, 6.5079],
        [6.5079, 6.6382, 6.5609]], grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.10387210547924042 
model_pd.l_d.mean(): -10.852237701416016 
model_pd.lagr.mean(): -10.74836540222168 
model_pd.lambdas: dict_items([('pout', tensor([1.5129])), ('power', tensor([0.6266]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0825])), ('power', tensor([-19.9009]))])
epoch：359	 i:0 	 global-step:7180	 l-p:0.10387210547924042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.6291, 7.5298, 7.7245],
        [6.6291, 8.1780, 8.9979],
        [6.6291, 7.1216, 7.0690],
        [6.6291, 6.6506, 6.6319]], grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.09922664612531662 
model_pd.l_d.mean(): -10.814350128173828 
model_pd.lagr.mean(): -10.715123176574707 
model_pd.lambdas: dict_items([('pout', tensor([1.5139])), ('power', tensor([0.6256]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0582])), ('power', tensor([-19.8153]))])
epoch：360	 i:0 	 global-step:7200	 l-p:0.09922664612531662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[6.6829, 7.6460, 7.8856],
        [6.6829, 8.2793, 9.1437],
        [6.6829, 6.6829, 6.6829],
        [6.6829, 7.8716, 8.3109]], grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.09762177616357803 
model_pd.l_d.mean(): -10.78567123413086 
model_pd.lagr.mean(): -10.68804931640625 
model_pd.lambdas: dict_items([('pout', tensor([1.5150])), ('power', tensor([0.6246]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0476])), ('power', tensor([-19.7769]))])
epoch：361	 i:0 	 global-step:7220	 l-p:0.09762177616357803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.6749, 6.9797, 6.8802],
        [6.6749, 6.8095, 6.7297],
        [6.6749, 6.6756, 6.6749],
        [6.6749, 8.2367, 9.0637]], grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.09784965217113495 
model_pd.l_d.mean(): -10.766216278076172 
model_pd.lagr.mean(): -10.668366432189941 
model_pd.lambdas: dict_items([('pout', tensor([1.5160])), ('power', tensor([0.6236]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0491])), ('power', tensor([-19.7826]))])
epoch：362	 i:0 	 global-step:7240	 l-p:0.09784965217113495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[6.6152, 6.6152, 6.6152],
        [6.6152, 8.2664, 9.2051],
        [6.6152, 6.6856, 6.6344],
        [6.6152, 7.4506, 7.5970]], grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.0996803492307663 
model_pd.l_d.mean(): -10.75417423248291 
model_pd.lagr.mean(): -10.654494285583496 
model_pd.lambdas: dict_items([('pout', tensor([1.5171])), ('power', tensor([0.6226]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0610])), ('power', tensor([-19.8252]))])
epoch：363	 i:0 	 global-step:7260	 l-p:0.0996803492307663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.5163, 7.6600, 8.0773],
        [6.5163, 6.5853, 6.5351],
        [6.5163, 6.5240, 6.5168],
        [6.5163, 8.4843, 9.8285]], grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.10349027812480927 
model_pd.l_d.mean(): -10.746810913085938 
model_pd.lagr.mean(): -10.64332103729248 
model_pd.lambdas: dict_items([('pout', tensor([1.5182])), ('power', tensor([0.6216]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0808])), ('power', tensor([-19.8951]))])
epoch：364	 i:0 	 global-step:7280	 l-p:0.10349027812480927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.3930, 7.5194, 7.9355],
        [6.3930, 6.3930, 6.3930],
        [6.3930, 6.6429, 6.5472],
        [6.3930, 6.3930, 6.3930]], grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.11096597462892532 
model_pd.l_d.mean(): -10.741029739379883 
model_pd.lagr.mean(): -10.630064010620117 
model_pd.lambdas: dict_items([('pout', tensor([1.5193])), ('power', tensor([0.6206]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1059])), ('power', tensor([-19.9809]))])
epoch：365	 i:0 	 global-step:7300	 l-p:0.11096597462892532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[6.2691, 6.2764, 6.2697],
        [6.2691, 6.2786, 6.2699],
        [6.2691, 6.5503, 6.4581],
        [6.2691, 7.3684, 7.7740]], grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.1290591061115265 
model_pd.l_d.mean(): -10.73331356048584 
model_pd.lagr.mean(): -10.604254722595215 
model_pd.lambdas: dict_items([('pout', tensor([1.5204])), ('power', tensor([0.6196]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1317])), ('power', tensor([-20.0657]))])
epoch：366	 i:0 	 global-step:7320	 l-p:0.1290591061115265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.2242, 6.2249, 6.2242],
        [6.2242, 6.2886, 6.2416],
        [6.2242, 6.4385, 6.3471],
        [6.2242, 6.5554, 6.4708]], grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.1466764360666275 
model_pd.l_d.mean(): -10.716317176818848 
model_pd.lagr.mean(): -10.56964111328125 
model_pd.lambdas: dict_items([('pout', tensor([1.5215])), ('power', tensor([0.6186]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1412])), ('power', tensor([-20.0961]))])
epoch：367	 i:0 	 global-step:7340	 l-p:0.1466764360666275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[6.2456, 6.4160, 6.3302],
        [6.2456, 6.8064, 6.8046],
        [6.2456, 6.6111, 6.5331],
        [6.2456, 6.5783, 6.4934]], grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.13658525049686432 
model_pd.l_d.mean(): -10.692768096923828 
model_pd.lagr.mean(): -10.556182861328125 
model_pd.lambdas: dict_items([('pout', tensor([1.5227])), ('power', tensor([0.6176]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1367])), ('power', tensor([-20.0816]))])
epoch：368	 i:0 	 global-step:7360	 l-p:0.13658525049686432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[6.3235, 7.1129, 7.2505],
        [6.3235, 6.6949, 6.6158],
        [6.3235, 7.3352, 7.6540],
        [6.3235, 6.4733, 6.3914]], grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.11849582940340042 
model_pd.l_d.mean(): -10.663461685180664 
model_pd.lagr.mean(): -10.544965744018555 
model_pd.lambdas: dict_items([('pout', tensor([1.5238])), ('power', tensor([0.6166]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1203])), ('power', tensor([-20.0287]))])
epoch：369	 i:0 	 global-step:7380	 l-p:0.11849582940340042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.4079, 6.4285, 6.4106],
        [6.4079, 6.6306, 6.5358],
        [6.4079, 8.3364, 9.6529],
        [6.4079, 6.4749, 6.4261]], grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.10977832973003387 
model_pd.l_d.mean(): -10.632949829101562 
model_pd.lagr.mean(): -10.523171424865723 
model_pd.lambdas: dict_items([('pout', tensor([1.5249])), ('power', tensor([0.6156]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1029])), ('power', tensor([-19.9706]))])
epoch：370	 i:0 	 global-step:7400	 l-p:0.10977832973003387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[6.4699, 6.7237, 6.6266],
        [6.4699, 6.8341, 6.7487],
        [6.4699, 7.5109, 7.8395],
        [6.4699, 6.7628, 6.6670]], grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.10579850524663925 
model_pd.l_d.mean(): -10.60461139678955 
model_pd.lagr.mean(): -10.498812675476074 
model_pd.lambdas: dict_items([('pout', tensor([1.5260])), ('power', tensor([0.6146]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0902])), ('power', tensor([-19.9275]))])
epoch：371	 i:0 	 global-step:7420	 l-p:0.10579850524663925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.5011, 6.5018, 6.5011],
        [6.5011, 6.5011, 6.5011],
        [6.5011, 6.5088, 6.5017],
        [6.5011, 6.5011, 6.5011]], grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.10419467836618423 
model_pd.l_d.mean(): -10.579818725585938 
model_pd.lagr.mean(): -10.475624084472656 
model_pd.lambdas: dict_items([('pout', tensor([1.5271])), ('power', tensor([0.6136]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0839])), ('power', tensor([-19.9057]))])
epoch：372	 i:0 	 global-step:7440	 l-p:0.10419467836618423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.5013, 6.5020, 6.5013],
        [6.5013, 6.8863, 6.8045],
        [6.5013, 6.7282, 6.6316],
        [6.5013, 7.4321, 7.6631]], grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.10418510437011719 
model_pd.l_d.mean(): -10.558808326721191 
model_pd.lagr.mean(): -10.454623222351074 
model_pd.lambdas: dict_items([('pout', tensor([1.5282])), ('power', tensor([0.6126]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0838])), ('power', tensor([-19.9055]))])
epoch：373	 i:0 	 global-step:7460	 l-p:0.10418510437011719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[6.4741, 6.9521, 6.9008],
        [6.4741, 6.4748, 6.4741],
        [6.4741, 6.4949, 6.4769],
        [6.4741, 8.2844, 9.4391]], grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.1055649071931839 
model_pd.l_d.mean(): -10.541037559509277 
model_pd.lagr.mean(): -10.43547248840332 
model_pd.lambdas: dict_items([('pout', tensor([1.5292])), ('power', tensor([0.6116]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0893])), ('power', tensor([-19.9245]))])
epoch：374	 i:0 	 global-step:7480	 l-p:0.1055649071931839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[6.4262, 7.0080, 7.0065],
        [6.4262, 6.4361, 6.4271],
        [6.4262, 7.3436, 7.5711],
        [6.4262, 6.4615, 6.4326]], grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.10846124589443207 
model_pd.l_d.mean(): -10.525433540344238 
model_pd.lagr.mean(): -10.416972160339355 
model_pd.lambdas: dict_items([('pout', tensor([1.5303])), ('power', tensor([0.6106]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0991])), ('power', tensor([-19.9579]))])
epoch：375	 i:0 	 global-step:7500	 l-p:0.10846124589443207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.3674, 6.8354, 6.7850],
        [6.3674, 6.3674, 6.3674],
        [6.3674, 6.6544, 6.5605],
        [6.3674, 8.2811, 9.5873]], grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.1132761538028717 
model_pd.l_d.mean(): -10.510591506958008 
model_pd.lagr.mean(): -10.39731502532959 
model_pd.lambdas: dict_items([('pout', tensor([1.5315])), ('power', tensor([0.6096]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1112])), ('power', tensor([-19.9985]))])
epoch：376	 i:0 	 global-step:7520	 l-p:0.1132761538028717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[6.3128, 6.6836, 6.6046],
        [6.3128, 6.9005, 6.9089],
        [6.3128, 7.4206, 7.8288],
        [6.3128, 6.3202, 6.3134]], grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.1200784221291542 
model_pd.l_d.mean(): -10.49479866027832 
model_pd.lagr.mean(): -10.374720573425293 
model_pd.lambdas: dict_items([('pout', tensor([1.5326])), ('power', tensor([0.6086]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1226])), ('power', tensor([-20.0359]))])
epoch：377	 i:0 	 global-step:7540	 l-p:0.1200784221291542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.2866, 6.7974, 6.7689],
        [6.2866, 6.2868, 6.2866],
        [6.2866, 6.4353, 6.3540],
        [6.2866, 6.6222, 6.5366]], grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.12488352507352829 
model_pd.l_d.mean(): -10.475944519042969 
model_pd.lagr.mean(): -10.35106086730957 
model_pd.lambdas: dict_items([('pout', tensor([1.5337])), ('power', tensor([0.6076]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1280])), ('power', tensor([-20.0538]))])
epoch：378	 i:0 	 global-step:7560	 l-p:0.12488352507352829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[6.3063, 8.1976, 9.4882],
        [6.3063, 6.6765, 6.5976],
        [6.3063, 7.3147, 7.6324],
        [6.3063, 7.8638, 8.7478]], grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.12115301191806793 
model_pd.l_d.mean(): -10.452731132507324 
model_pd.lagr.mean(): -10.331578254699707 
model_pd.lambdas: dict_items([('pout', tensor([1.5348])), ('power', tensor([0.6066]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1239])), ('power', tensor([-20.0404]))])
epoch：379	 i:0 	 global-step:7580	 l-p:0.12115301191806793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.3511, 6.4159, 6.3684],
        [6.3511, 6.9834, 7.0147],
        [6.3511, 7.4592, 7.8630],
        [6.3511, 6.4774, 6.4024]], grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.1149858683347702 
model_pd.l_d.mean(): -10.427093505859375 
model_pd.lagr.mean(): -10.312108039855957 
model_pd.lambdas: dict_items([('pout', tensor([1.5359])), ('power', tensor([0.6056]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1146])), ('power', tensor([-20.0097]))])
epoch：380	 i:0 	 global-step:7600	 l-p:0.1149858683347702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.3967, 6.4042, 6.3972],
        [6.3967, 7.2582, 7.4439],
        [6.3967, 6.3967, 6.3967],
        [6.3967, 6.3967, 6.3967]], grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.11065228283405304 
model_pd.l_d.mean(): -10.401288986206055 
model_pd.lagr.mean(): -10.290637016296387 
model_pd.lambdas: dict_items([('pout', tensor([1.5371])), ('power', tensor([0.6046]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1052])), ('power', tensor([-19.9783]))])
epoch：381	 i:0 	 global-step:7620	 l-p:0.11065228283405304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.4301, 7.0123, 7.0109],
        [6.4301, 6.4355, 6.4304],
        [6.4301, 6.9939, 6.9830],
        [6.4301, 6.7757, 6.6877]], grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.10819721966981888 
model_pd.l_d.mean(): -10.376666069030762 
model_pd.lagr.mean(): -10.268468856811523 
model_pd.lambdas: dict_items([('pout', tensor([1.5382])), ('power', tensor([0.6036]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0983])), ('power', tensor([-19.9552]))])
epoch：382	 i:0 	 global-step:7640	 l-p:0.10819721966981888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[6.4462, 7.5846, 8.0053],
        [6.4462, 6.4538, 6.4468],
        [6.4462, 6.7379, 6.6425],
        [6.4462, 6.8089, 6.7238]], grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.10716353356838226 
model_pd.l_d.mean(): -10.353857040405273 
model_pd.lagr.mean(): -10.24669361114502 
model_pd.lambdas: dict_items([('pout', tensor([1.5392])), ('power', tensor([0.6026]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0950])), ('power', tensor([-19.9439]))])
epoch：383	 i:0 	 global-step:7660	 l-p:0.10716353356838226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[6.4446, 6.5126, 6.4631],
        [6.4446, 6.7912, 6.7030],
        [6.4446, 7.0100, 6.9991],
        [6.4446, 6.4465, 6.4446]], grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.10726673156023026 
model_pd.l_d.mean(): -10.332947731018066 
model_pd.lagr.mean(): -10.22568130493164 
model_pd.lambdas: dict_items([('pout', tensor([1.5403])), ('power', tensor([0.6016]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0954])), ('power', tensor([-19.9451]))])
epoch：384	 i:0 	 global-step:7680	 l-p:0.10726673156023026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[6.4274, 6.4628, 6.4339],
        [6.4274, 6.4278, 6.4275],
        [6.4274, 6.4274, 6.4274],
        [6.4274, 6.6511, 6.5558]], grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.10837382078170776 
model_pd.l_d.mean(): -10.31361198425293 
model_pd.lagr.mean(): -10.205238342285156 
model_pd.lambdas: dict_items([('pout', tensor([1.5414])), ('power', tensor([0.6006]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0989])), ('power', tensor([-19.9570]))])
epoch：385	 i:0 	 global-step:7700	 l-p:0.10837382078170776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.3996, 6.8706, 6.8200],
        [6.3996, 6.6500, 6.5541],
        [6.3996, 6.7431, 6.6557],
        [6.3996, 6.4000, 6.3996]], grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.11041580140590668 
model_pd.l_d.mean(): -10.295259475708008 
model_pd.lagr.mean(): -10.184844017028809 
model_pd.lambdas: dict_items([('pout', tensor([1.5425])), ('power', tensor([0.5996]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1046])), ('power', tensor([-19.9763]))])
epoch：386	 i:0 	 global-step:7720	 l-p:0.11041580140590668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[6.3679, 6.3682, 6.3679],
        [6.3679, 6.3698, 6.3680],
        [6.3679, 6.3679, 6.3679],
        [6.3679, 7.3887, 7.7106]], grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.11322209239006042 
model_pd.l_d.mean(): -10.27711009979248 
model_pd.lagr.mean(): -10.163887977600098 
model_pd.lambdas: dict_items([('pout', tensor([1.5437])), ('power', tensor([0.5986]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1111])), ('power', tensor([-19.9981]))])
epoch：387	 i:0 	 global-step:7740	 l-p:0.11322209239006042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[6.3412, 7.1336, 7.2718],
        [6.3412, 6.3412, 6.3412],
        [6.3412, 6.3418, 6.3412],
        [6.3412, 6.3412, 6.3412]], grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.11614719033241272 
model_pd.l_d.mean(): -10.258317947387695 
model_pd.lagr.mean(): -10.142170906066895 
model_pd.lambdas: dict_items([('pout', tensor([1.5448])), ('power', tensor([0.5976]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1167])), ('power', tensor([-20.0165]))])
epoch：388	 i:0 	 global-step:7760	 l-p:0.11614719033241272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[6.3287, 8.2283, 9.5247],
        [6.3287, 6.3290, 6.3287],
        [6.3287, 7.8931, 8.7811],
        [6.3287, 6.7006, 6.6214]], grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.11774975806474686 
model_pd.l_d.mean(): -10.238137245178223 
model_pd.lagr.mean(): -10.120387077331543 
model_pd.lambdas: dict_items([('pout', tensor([1.5459])), ('power', tensor([0.5966]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1192])), ('power', tensor([-20.0250]))])
epoch：389	 i:0 	 global-step:7780	 l-p:0.11774975806474686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[6.3350, 6.6201, 6.5267],
        [6.3350, 6.9653, 6.9963],
        [6.3350, 6.3350, 6.3350],
        [6.3350, 6.3740, 6.3426]], grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.11691852658987045 
model_pd.l_d.mean(): -10.216288566589355 
model_pd.lagr.mean(): -10.099370002746582 
model_pd.lambdas: dict_items([('pout', tensor([1.5470])), ('power', tensor([0.5956]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1179])), ('power', tensor([-20.0207]))])
epoch：390	 i:0 	 global-step:7800	 l-p:0.11691852658987045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[6.3556, 6.3556, 6.3556],
        [6.3556, 6.4224, 6.3738],
        [6.3556, 6.3630, 6.3561],
        [6.3556, 6.5412, 6.4514]], grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.11449632048606873 
model_pd.l_d.mean(): -10.193198204040527 
model_pd.lagr.mean(): -10.078701972961426 
model_pd.lambdas: dict_items([('pout', tensor([1.5481])), ('power', tensor([0.5946]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1137])), ('power', tensor([-20.0066]))])
epoch：391	 i:0 	 global-step:7820	 l-p:0.11449632048606873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.3812, 7.8596, 8.6411],
        [6.3812, 6.7392, 6.6551],
        [6.3812, 6.3831, 6.3813],
        [6.3812, 6.9396, 6.9287]], grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.1119675561785698 
model_pd.l_d.mean(): -10.169658660888672 
model_pd.lagr.mean(): -10.05769157409668 
model_pd.lambdas: dict_items([('pout', tensor([1.5492])), ('power', tensor([0.5936]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1084])), ('power', tensor([-19.9890]))])
epoch：392	 i:0 	 global-step:7840	 l-p:0.1119675561785698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[6.4042, 7.3176, 7.5441],
        [6.4042, 7.2669, 7.4528],
        [6.4042, 7.5320, 7.9479],
        [6.4042, 6.4140, 6.4050]], grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.11005961149930954 
model_pd.l_d.mean(): -10.146371841430664 
model_pd.lagr.mean(): -10.036312103271484 
model_pd.lambdas: dict_items([('pout', tensor([1.5503])), ('power', tensor([0.5926]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1036])), ('power', tensor([-19.9731]))])
epoch：393	 i:0 	 global-step:7860	 l-p:0.11005961149930954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[6.4195, 6.7643, 6.6765],
        [6.4195, 6.4195, 6.4195],
        [6.4195, 7.0198, 7.0287],
        [6.4195, 7.5424, 7.9518]], grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.10892564803361893 
model_pd.l_d.mean(): -10.123799324035645 
model_pd.lagr.mean(): -10.014873504638672 
model_pd.lambdas: dict_items([('pout', tensor([1.5514])), ('power', tensor([0.5916]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1005])), ('power', tensor([-19.9625]))])
epoch：394	 i:0 	 global-step:7880	 l-p:0.10892564803361893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.4253, 6.4253, 6.4253],
        [6.4253, 6.8987, 6.8478],
        [6.4253, 6.4253, 6.4253],
        [6.4253, 7.3425, 7.5699]], grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.10852482169866562 
model_pd.l_d.mean(): -10.102133750915527 
model_pd.lagr.mean(): -9.993608474731445 
model_pd.lambdas: dict_items([('pout', tensor([1.5525])), ('power', tensor([0.5906]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0993])), ('power', tensor([-19.9585]))])
epoch：395	 i:0 	 global-step:7900	 l-p:0.10852482169866562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.4217, 7.0222, 7.0311],
        [6.4217, 6.5746, 6.4911],
        [6.4217, 6.4894, 6.4401],
        [6.4217, 8.3553, 9.6755]], grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.10877317190170288 
model_pd.l_d.mean(): -10.08133602142334 
model_pd.lagr.mean(): -9.972562789916992 
model_pd.lambdas: dict_items([('pout', tensor([1.5536])), ('power', tensor([0.5896]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1000])), ('power', tensor([-19.9610]))])
epoch：396	 i:0 	 global-step:7920	 l-p:0.10877317190170288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.4107, 6.4107, 6.4107],
        [6.4107, 7.9280, 8.7484],
        [6.4107, 7.4399, 7.7646],
        [6.4107, 6.4107, 6.4107]], grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.10956862568855286 
model_pd.l_d.mean(): -10.061192512512207 
model_pd.lagr.mean(): -9.951623916625977 
model_pd.lambdas: dict_items([('pout', tensor([1.5547])), ('power', tensor([0.5886]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1023])), ('power', tensor([-19.9687]))])
epoch：397	 i:0 	 global-step:7940	 l-p:0.10956862568855286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.3954, 7.5225, 7.9388],
        [6.3954, 8.1789, 9.3160],
        [6.3954, 6.4631, 6.4139],
        [6.3954, 7.8777, 8.6613]], grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.11076056212186813 
model_pd.l_d.mean(): -10.041373252868652 
model_pd.lagr.mean(): -9.930612564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.5558])), ('power', tensor([0.5876]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1054])), ('power', tensor([-19.9792]))])
epoch：398	 i:0 	 global-step:7960	 l-p:0.11076056212186813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[6.3799, 7.4941, 7.9002],
        [6.3799, 6.3853, 6.3802],
        [6.3799, 6.3803, 6.3799],
        [6.3799, 8.1582, 9.2918]], grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11209289729595184 
model_pd.l_d.mean(): -10.021505355834961 
model_pd.lagr.mean(): -9.909412384033203 
model_pd.lambdas: dict_items([('pout', tensor([1.5570])), ('power', tensor([0.5866]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1086])), ('power', tensor([-19.9899]))])
epoch：399	 i:0 	 global-step:7980	 l-p:0.11209289729595184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.3684, 6.3888, 6.3711],
        [6.3684, 6.4358, 6.3868],
        [6.3684, 6.3684, 6.3684],
        [6.3684, 6.4334, 6.3858]], grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.11317695677280426 
model_pd.l_d.mean(): -10.001243591308594 
model_pd.lagr.mean(): -9.888066291809082 
model_pd.lambdas: dict_items([('pout', tensor([1.5581])), ('power', tensor([0.5856]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1110])), ('power', tensor([-19.9978]))])
epoch：400	 i:0 	 global-step:8000	 l-p:0.11317695677280426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[6.3641, 6.3660, 6.3642],
        [6.3641, 6.3845, 6.3668],
        [6.3641, 6.3641, 6.3641],
        [6.3641, 6.5388, 6.4509]], grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.11360438168048859 
model_pd.l_d.mean(): -9.98036003112793 
model_pd.lagr.mean(): -9.866755485534668 
model_pd.lambdas: dict_items([('pout', tensor([1.5592])), ('power', tensor([0.5846]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1119])), ('power', tensor([-20.0008]))])
epoch：401	 i:0 	 global-step:8020	 l-p:0.11360438168048859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.3680, 6.3778, 6.3689],
        [6.3680, 6.6550, 6.5611],
        [6.3680, 6.3684, 6.3680],
        [6.3680, 6.7092, 6.6223]], grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.11321625858545303 
model_pd.l_d.mean(): -9.958812713623047 
model_pd.lagr.mean(): -9.845596313476562 
model_pd.lambdas: dict_items([('pout', tensor([1.5603])), ('power', tensor([0.5836]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1111])), ('power', tensor([-19.9981]))])
epoch：402	 i:0 	 global-step:8040	 l-p:0.11321625858545303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.3784, 7.4012, 7.7236],
        [6.3784, 6.3803, 6.3785],
        [6.3784, 6.3882, 6.3793],
        [6.3784, 6.3784, 6.3784]], grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.112230084836483 
model_pd.l_d.mean(): -9.93675708770752 
model_pd.lagr.mean(): -9.8245267868042 
model_pd.lambdas: dict_items([('pout', tensor([1.5614])), ('power', tensor([0.5826]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1089])), ('power', tensor([-19.9909]))])
epoch：403	 i:0 	 global-step:8060	 l-p:0.112230084836483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[6.3921, 7.3032, 7.5289],
        [6.3921, 6.3921, 6.3921],
        [6.3921, 6.4594, 6.4104],
        [6.3921, 6.7506, 6.6664]], grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.11104097217321396 
model_pd.l_d.mean(): -9.914459228515625 
model_pd.lagr.mean(): -9.803418159484863 
model_pd.lambdas: dict_items([('pout', tensor([1.5625])), ('power', tensor([0.5816]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1061])), ('power', tensor([-19.9816]))])
epoch：404	 i:0 	 global-step:8080	 l-p:0.11104097217321396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.4054, 6.5816, 6.4930],
        [6.4054, 7.5251, 7.9332],
        [6.4054, 6.5929, 6.5023],
        [6.4054, 6.4055, 6.4054]], grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10996972769498825 
model_pd.l_d.mean(): -9.892196655273438 
model_pd.lagr.mean(): -9.7822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.5636])), ('power', tensor([0.5806]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1034])), ('power', tensor([-19.9723]))])
epoch：405	 i:0 	 global-step:8100	 l-p:0.10996972769498825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[6.4159, 6.9779, 6.9670],
        [6.4159, 6.4235, 6.4165],
        [6.4159, 6.4160, 6.4159],
        [6.4159, 7.5474, 7.9654]], grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.10918840765953064 
model_pd.l_d.mean(): -9.870189666748047 
model_pd.lagr.mean(): -9.761001586914062 
model_pd.lambdas: dict_items([('pout', tensor([1.5647])), ('power', tensor([0.5796]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1012])), ('power', tensor([-19.9650]))])
epoch：406	 i:0 	 global-step:8120	 l-p:0.10918840765953064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.4220, 6.4220, 6.4220],
        [6.4220, 6.6101, 6.5192],
        [6.4220, 6.4224, 6.4220],
        [6.4220, 6.9847, 6.9737]], grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.10875608772039413 
model_pd.l_d.mean(): -9.848554611206055 
model_pd.lagr.mean(): -9.739798545837402 
model_pd.lambdas: dict_items([('pout', tensor([1.5658])), ('power', tensor([0.5786]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1000])), ('power', tensor([-19.9608]))])
epoch：407	 i:0 	 global-step:8140	 l-p:0.10875608772039413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]])
 pt:tensor([[6.4234, 7.0239, 7.0327],
        [6.4234, 7.3400, 7.5672],
        [6.4234, 6.9862, 6.9752],
        [6.4234, 7.0646, 7.0963]], grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.10866550356149673 
model_pd.l_d.mean(): -9.82732105255127 
model_pd.lagr.mean(): -9.718655586242676 
model_pd.lambdas: dict_items([('pout', tensor([1.5669])), ('power', tensor([0.5776]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0997])), ('power', tensor([-19.9599]))])
epoch：408	 i:0 	 global-step:8160	 l-p:0.10866550356149673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.4205, 7.3365, 7.5636],
        [6.4205, 7.9404, 8.7622],
        [6.4205, 6.6717, 6.5755],
        [6.4205, 6.4875, 6.4386]], grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.10886925458908081 
model_pd.l_d.mean(): -9.80642318725586 
model_pd.lagr.mean(): -9.697553634643555 
model_pd.lambdas: dict_items([('pout', tensor([1.5680])), ('power', tensor([0.5766]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1003])), ('power', tensor([-19.9619]))])
epoch：409	 i:0 	 global-step:8180	 l-p:0.10886925458908081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[6.4146, 6.6656, 6.5695],
        [6.4146, 6.6024, 6.5116],
        [6.4146, 7.3297, 7.5564],
        [6.4146, 6.7589, 6.6713]], grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.10928735882043839 
model_pd.l_d.mean(): -9.785741806030273 
model_pd.lagr.mean(): -9.676454544067383 
model_pd.lambdas: dict_items([('pout', tensor([1.5691])), ('power', tensor([0.5756]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1015])), ('power', tensor([-19.9660]))])
epoch：410	 i:0 	 global-step:8200	 l-p:0.10928735882043839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.4076, 6.5353, 6.4595],
        [6.4076, 6.4152, 6.4082],
        [6.4076, 7.5372, 7.9543],
        [6.4076, 6.7515, 6.6639]], grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.10980790108442307 
model_pd.l_d.mean(): -9.765131950378418 
model_pd.lagr.mean(): -9.65532398223877 
model_pd.lambdas: dict_items([('pout', tensor([1.5702])), ('power', tensor([0.5746]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1029])), ('power', tensor([-19.9708]))])
epoch：411	 i:0 	 global-step:8220	 l-p:0.10980790108442307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[6.4014, 6.4667, 6.4189],
        [6.4014, 8.1867, 9.3248],
        [6.4014, 7.5295, 7.9461],
        [6.4014, 6.9239, 6.8949]], grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.11029408127069473 
model_pd.l_d.mean(): -9.744443893432617 
model_pd.lagr.mean(): -9.634149551391602 
model_pd.lambdas: dict_items([('pout', tensor([1.5713])), ('power', tensor([0.5736]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1042])), ('power', tensor([-19.9752]))])
epoch：412	 i:0 	 global-step:8240	 l-p:0.11029408127069473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[6.3975, 6.4050, 6.3980],
        [6.3975, 8.3219, 9.6354],
        [6.3975, 6.9756, 6.9740],
        [6.3975, 6.9196, 6.8905]], grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.11060810089111328 
model_pd.l_d.mean(): -9.723563194274902 
model_pd.lagr.mean(): -9.612955093383789 
model_pd.lambdas: dict_items([('pout', tensor([1.5724])), ('power', tensor([0.5726]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1050])), ('power', tensor([-19.9779]))])
epoch：413	 i:0 	 global-step:8260	 l-p:0.11060810089111328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.3969, 7.5240, 7.9402],
        [6.3969, 6.4023, 6.3972],
        [6.3969, 8.1806, 9.3176],
        [6.3969, 6.3969, 6.3969]], grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.1106550395488739 
model_pd.l_d.mean(): -9.70242691040039 
model_pd.lagr.mean(): -9.591772079467773 
model_pd.lambdas: dict_items([('pout', tensor([1.5735])), ('power', tensor([0.5716]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1052])), ('power', tensor([-19.9783]))])
epoch：414	 i:0 	 global-step:8280	 l-p:0.1106550395488739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[6.3998, 7.4266, 7.7502],
        [6.3998, 6.4203, 6.4025],
        [6.3998, 6.5870, 6.4965],
        [6.3998, 6.4676, 6.4184]], grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.11041983962059021 
model_pd.l_d.mean(): -9.681041717529297 
model_pd.lagr.mean(): -9.570621490478516 
model_pd.lambdas: dict_items([('pout', tensor([1.5746])), ('power', tensor([0.5706]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1046])), ('power', tensor([-19.9763]))])
epoch：415	 i:0 	 global-step:8300	 l-p:0.11041983962059021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[6.4056, 6.4731, 6.4240],
        [6.4056, 7.5346, 7.9515],
        [6.4056, 6.7492, 6.6617],
        [6.4056, 7.4335, 7.7575]], grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.10996843129396439 
model_pd.l_d.mean(): -9.659462928771973 
model_pd.lagr.mean(): -9.549494743347168 
model_pd.lambdas: dict_items([('pout', tensor([1.5757])), ('power', tensor([0.5696]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1034])), ('power', tensor([-19.9723]))])
epoch：416	 i:0 	 global-step:8320	 l-p:0.10996843129396439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.4131, 7.4424, 7.7669],
        [6.4131, 6.4135, 6.4131],
        [6.4131, 6.7911, 6.7106],
        [6.4131, 6.4800, 6.4312]], grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.10941123962402344 
model_pd.l_d.mean(): -9.637777328491211 
model_pd.lagr.mean(): -9.528366088867188 
model_pd.lambdas: dict_items([('pout', tensor([1.5768])), ('power', tensor([0.5686]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1018])), ('power', tensor([-19.9671]))])
epoch：417	 i:0 	 global-step:8340	 l-p:0.10941123962402344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[6.4208, 6.9452, 6.9161],
        [6.4208, 7.5517, 7.9686],
        [6.4208, 6.7654, 6.6776],
        [6.4208, 6.5974, 6.5085]], grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.10885750502347946 
model_pd.l_d.mean(): -9.616087913513184 
model_pd.lagr.mean(): -9.507230758666992 
model_pd.lambdas: dict_items([('pout', tensor([1.5779])), ('power', tensor([0.5676]))]) 
model_pd.vars: dict_items([('pout', tensor([1.1003])), ('power', tensor([-19.9618]))])
epoch：418	 i:0 	 global-step:8360	 l-p:0.10885750502347946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[6.4276, 6.6509, 6.5558],
        [6.4276, 6.4629, 6.4340],
        [6.4276, 6.4482, 6.4303],
        [6.4276, 6.9527, 6.9235]], grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.10838853567838669 
model_pd.l_d.mean(): -9.594478607177734 
model_pd.lagr.mean(): -9.486089706420898 
model_pd.lambdas: dict_items([('pout', tensor([1.5790])), ('power', tensor([0.5666]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0989])), ('power', tensor([-19.9571]))])
epoch：419	 i:0 	 global-step:8380	 l-p:0.10838853567838669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[6.4327, 6.4346, 6.4327],
        [6.4327, 6.5008, 6.4513],
        [6.4327, 6.4328, 6.4327],
        [6.4327, 6.4334, 6.4327]], grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.10805008560419083 
model_pd.l_d.mean(): -9.573004722595215 
model_pd.lagr.mean(): -9.464954376220703 
model_pd.lambdas: dict_items([('pout', tensor([1.5801])), ('power', tensor([0.5656]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0978])), ('power', tensor([-19.9536]))])
epoch：420	 i:0 	 global-step:8400	 l-p:0.10805008560419083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[6.4356, 8.3738, 9.6968],
        [6.4356, 6.5035, 6.4541],
        [6.4356, 6.4359, 6.4356],
        [6.4356, 7.0374, 7.0461]], grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.10785675793886185 
model_pd.l_d.mean(): -9.551688194274902 
model_pd.lagr.mean(): -9.443831443786621 
model_pd.lambdas: dict_items([('pout', tensor([1.5812])), ('power', tensor([0.5646]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0972])), ('power', tensor([-19.9515]))])
epoch：421	 i:0 	 global-step:8420	 l-p:0.10785675793886185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[6.4365, 6.4368, 6.4366],
        [6.4365, 7.9299, 8.7194],
        [6.4365, 6.5649, 6.4887],
        [6.4365, 6.4372, 6.4366]], grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.10779904574155807 
model_pd.l_d.mean(): -9.530518531799316 
model_pd.lagr.mean(): -9.422719955444336 
model_pd.lambdas: dict_items([('pout', tensor([1.5823])), ('power', tensor([0.5636]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0970])), ('power', tensor([-19.9509]))])
epoch：422	 i:0 	 global-step:8440	 l-p:0.10779904574155807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[6.4358, 6.4358, 6.4358],
        [6.4358, 6.9997, 6.9886],
        [6.4358, 6.4711, 6.4422],
        [6.4358, 7.5698, 7.9879]], grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10784908384084702 
model_pd.l_d.mean(): -9.509468078613281 
model_pd.lagr.mean(): -9.401618957519531 
model_pd.lambdas: dict_items([('pout', tensor([1.5834])), ('power', tensor([0.5627]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0972])), ('power', tensor([-19.9514]))])
epoch：423	 i:0 	 global-step:8460	 l-p:0.10784908384084702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.4340, 7.0762, 7.1079],
        [6.4340, 7.9266, 8.7157],
        [6.4340, 6.9977, 6.9866],
        [6.4340, 6.4341, 6.4340]], grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.10796576738357544 
model_pd.l_d.mean(): -9.488480567932129 
model_pd.lagr.mean(): -9.380515098571777 
model_pd.lambdas: dict_items([('pout', tensor([1.5845])), ('power', tensor([0.5617]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0976])), ('power', tensor([-19.9527]))])
epoch：424	 i:0 	 global-step:8480	 l-p:0.10796576738357544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[6.4320, 6.4327, 6.4320],
        [6.4320, 6.5001, 6.4506],
        [6.4320, 6.4526, 6.4347],
        [6.4320, 8.3687, 9.6906]], grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.10810089856386185 
model_pd.l_d.mean(): -9.467507362365723 
model_pd.lagr.mean(): -9.359406471252441 
model_pd.lambdas: dict_items([('pout', tensor([1.5856])), ('power', tensor([0.5607]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0980])), ('power', tensor([-19.9541]))])
epoch：425	 i:0 	 global-step:8500	 l-p:0.10810089856386185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[6.4304, 6.6187, 6.5277],
        [6.4304, 6.4304, 6.4304],
        [6.4304, 6.9038, 6.8528],
        [6.4304, 7.5632, 7.9807]], grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.10820575058460236 
model_pd.l_d.mean(): -9.446495056152344 
model_pd.lagr.mean(): -9.338289260864258 
model_pd.lambdas: dict_items([('pout', tensor([1.5867])), ('power', tensor([0.5597]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0983])), ('power', tensor([-19.9552]))])
epoch：426	 i:0 	 global-step:8520	 l-p:0.10820575058460236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228]])
 pt:tensor([[6.4300, 6.9931, 6.9820],
        [6.4300, 6.6533, 6.5581],
        [6.4300, 6.8091, 6.7283],
        [6.4300, 6.5829, 6.4993]], grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.10824015736579895 
model_pd.l_d.mean(): -9.425413131713867 
model_pd.lagr.mean(): -9.31717300415039 
model_pd.lambdas: dict_items([('pout', tensor([1.5878])), ('power', tensor([0.5587]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0984])), ('power', tensor([-19.9555]))])
epoch：427	 i:0 	 global-step:8540	 l-p:0.10824015736579895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.4309, 6.4309, 6.4309],
        [6.4309, 6.4980, 6.4491],
        [6.4309, 6.4312, 6.4309],
        [6.4309, 6.4310, 6.4309]], grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.10818073153495789 
model_pd.l_d.mean(): -9.404239654541016 
model_pd.lagr.mean(): -9.296058654785156 
model_pd.lambdas: dict_items([('pout', tensor([1.5889])), ('power', tensor([0.5577]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0982])), ('power', tensor([-19.9549]))])
epoch：428	 i:0 	 global-step:8560	 l-p:0.10818073153495789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[6.4332, 7.3001, 7.4867],
        [6.4332, 7.0752, 7.1068],
        [6.4332, 8.3702, 9.6923],
        [6.4332, 6.9967, 6.9856]], grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.10802499949932098 
model_pd.l_d.mean(): -9.382980346679688 
model_pd.lagr.mean(): -9.274955749511719 
model_pd.lambdas: dict_items([('pout', tensor([1.5900])), ('power', tensor([0.5567]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0977])), ('power', tensor([-19.9532]))])
epoch：429	 i:0 	 global-step:8580	 l-p:0.10802499949932098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.4369, 6.4372, 6.4369],
        [6.4369, 6.4423, 6.4372],
        [6.4369, 6.4370, 6.4369],
        [6.4369, 6.4575, 6.4396]], grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.10778961330652237 
model_pd.l_d.mean(): -9.361648559570312 
model_pd.lagr.mean(): -9.25385856628418 
model_pd.lambdas: dict_items([('pout', tensor([1.5911])), ('power', tensor([0.5557]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0970])), ('power', tensor([-19.9507]))])
epoch：430	 i:0 	 global-step:8600	 l-p:0.10778961330652237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[6.4414, 6.5072, 6.4590],
        [6.4414, 6.9156, 6.8645],
        [6.4414, 7.5680, 7.9786],
        [6.4414, 7.0241, 7.0225]], grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.10750391334295273 
model_pd.l_d.mean(): -9.34027099609375 
model_pd.lagr.mean(): -9.232767105102539 
model_pd.lambdas: dict_items([('pout', tensor([1.5922])), ('power', tensor([0.5547]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0961])), ('power', tensor([-19.9476]))])
epoch：431	 i:0 	 global-step:8620	 l-p:0.10750391334295273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.4463, 6.4467, 6.4463],
        [6.4463, 6.9209, 6.8698],
        [6.4463, 6.4463, 6.4463],
        [6.4463, 7.9420, 8.7327]], grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.10720083862543106 
model_pd.l_d.mean(): -9.31888484954834 
model_pd.lagr.mean(): -9.211684226989746 
model_pd.lambdas: dict_items([('pout', tensor([1.5933])), ('power', tensor([0.5537]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0951])), ('power', tensor([-19.9442]))])
epoch：432	 i:0 	 global-step:8640	 l-p:0.10720083862543106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.4511, 6.5184, 6.4693],
        [6.4511, 6.4511, 6.4511],
        [6.4511, 7.0163, 7.0052],
        [6.4511, 6.7035, 6.6068]], grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.10691048949956894 
model_pd.l_d.mean(): -9.29751205444336 
model_pd.lagr.mean(): -9.190601348876953 
model_pd.lambdas: dict_items([('pout', tensor([1.5944])), ('power', tensor([0.5527]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0941])), ('power', tensor([-19.9409]))])
epoch：433	 i:0 	 global-step:8660	 l-p:0.10691048949956894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.4554, 7.0396, 7.0380],
        [6.4554, 7.1000, 7.1318],
        [6.4554, 6.4609, 6.4557],
        [6.4554, 7.0211, 7.0100]], grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.10665468871593475 
model_pd.l_d.mean(): -9.276178359985352 
model_pd.lagr.mean(): -9.169523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.5955])), ('power', tensor([0.5517]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0932])), ('power', tensor([-19.9379]))])
epoch：434	 i:0 	 global-step:8680	 l-p:0.10665468871593475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[6.4590, 6.5274, 6.4777],
        [6.4590, 6.4609, 6.4590],
        [6.4590, 6.5250, 6.4766],
        [6.4590, 7.2689, 7.4101]], grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.10644657164812088 
model_pd.l_d.mean(): -9.254898071289062 
model_pd.lagr.mean(): -9.148451805114746 
model_pd.lambdas: dict_items([('pout', tensor([1.5966])), ('power', tensor([0.5507]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0925])), ('power', tensor([-19.9354]))])
epoch：435	 i:0 	 global-step:8700	 l-p:0.10644657164812088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.4617, 6.4617, 6.4617],
        [6.4617, 7.0467, 7.0450],
        [6.4617, 6.5298, 6.4803],
        [6.4617, 6.4622, 6.4617]], grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10628951340913773 
model_pd.l_d.mean(): -9.233675003051758 
model_pd.lagr.mean(): -9.127385139465332 
model_pd.lambdas: dict_items([('pout', tensor([1.5977])), ('power', tensor([0.5497]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0919])), ('power', tensor([-19.9335]))])
epoch：436	 i:0 	 global-step:8720	 l-p:0.10628951340913773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[6.4637, 7.2742, 7.4156],
        [6.4637, 6.6175, 6.5335],
        [6.4637, 6.4692, 6.4640],
        [6.4637, 6.6416, 6.5521]], grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.10617946833372116 
model_pd.l_d.mean(): -9.212505340576172 
model_pd.lagr.mean(): -9.10632610321045 
model_pd.lambdas: dict_items([('pout', tensor([1.5988])), ('power', tensor([0.5487]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0915])), ('power', tensor([-19.9322]))])
epoch：437	 i:0 	 global-step:8740	 l-p:0.10617946833372116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[6.4650, 8.0695, 8.9803],
        [6.4650, 6.6544, 6.5628],
        [6.4650, 6.8282, 6.7429],
        [6.4650, 6.4669, 6.4651]], grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.10610630363225937 
model_pd.l_d.mean(): -9.191377639770508 
model_pd.lagr.mean(): -9.085270881652832 
model_pd.lambdas: dict_items([('pout', tensor([1.5999])), ('power', tensor([0.5477]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0913])), ('power', tensor([-19.9313]))])
epoch：438	 i:0 	 global-step:8760	 l-p:0.10610630363225937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[6.4659, 6.9945, 6.9651],
        [6.4659, 6.4659, 6.4659],
        [6.4659, 6.4714, 6.4663],
        [6.4659, 6.4662, 6.4659]], grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.10605596750974655 
model_pd.l_d.mean(): -9.170273780822754 
model_pd.lagr.mean(): -9.064217567443848 
model_pd.lambdas: dict_items([('pout', tensor([1.6009])), ('power', tensor([0.5467]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0911])), ('power', tensor([-19.9306]))])
epoch：439	 i:0 	 global-step:8780	 l-p:0.10605596750974655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[6.4667, 7.0521, 7.0504],
        [6.4667, 7.9679, 8.7615],
        [6.4667, 6.6915, 6.5957],
        [6.4667, 6.4722, 6.4670]], grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.10601317137479782 
model_pd.l_d.mean(): -9.14918041229248 
model_pd.lagr.mean(): -9.043167114257812 
model_pd.lambdas: dict_items([('pout', tensor([1.6020])), ('power', tensor([0.5457]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0909])), ('power', tensor([-19.9301]))])
epoch：440	 i:0 	 global-step:8800	 l-p:0.10601317137479782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.4676, 6.5031, 6.4741],
        [6.4676, 8.2743, 9.4260],
        [6.4676, 6.5358, 6.4862],
        [6.4676, 6.7208, 6.6239]], grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.10596295446157455 
model_pd.l_d.mean(): -9.128081321716309 
model_pd.lagr.mean(): -9.02211856842041 
model_pd.lambdas: dict_items([('pout', tensor([1.6031])), ('power', tensor([0.5447]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0907])), ('power', tensor([-19.9294]))])
epoch：441	 i:0 	 global-step:8820	 l-p:0.10596295446157455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[6.4689, 6.7611, 6.6655],
        [6.4689, 6.9455, 6.8941],
        [6.4689, 6.6584, 6.5668],
        [6.4689, 6.4765, 6.4695]], grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.10589311271905899 
model_pd.l_d.mean(): -9.106966018676758 
model_pd.lagr.mean(): -9.001072883605957 
model_pd.lambdas: dict_items([('pout', tensor([1.6042])), ('power', tensor([0.5437]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0905])), ('power', tensor([-19.9286]))])
epoch：442	 i:0 	 global-step:8840	 l-p:0.10589311271905899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.4707, 6.4712, 6.4707],
        [6.4707, 6.4708, 6.4707],
        [6.4707, 6.6488, 6.5592],
        [6.4707, 6.5998, 6.5231]], grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.10579588264226913 
model_pd.l_d.mean(): -9.08582878112793 
model_pd.lagr.mean(): -8.980032920837402 
model_pd.lambdas: dict_items([('pout', tensor([1.6053])), ('power', tensor([0.5427]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0901])), ('power', tensor([-19.9273]))])
epoch：443	 i:0 	 global-step:8860	 l-p:0.10579588264226913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[6.4731, 6.4731, 6.4731],
        [6.4731, 6.7655, 6.6698],
        [6.4731, 7.3461, 7.5340],
        [6.4731, 6.6981, 6.6022]], grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.10566787421703339 
model_pd.l_d.mean(): -9.064664840698242 
model_pd.lagr.mean(): -8.958996772766113 
model_pd.lambdas: dict_items([('pout', tensor([1.6064])), ('power', tensor([0.5417]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0896])), ('power', tensor([-19.9257]))])
epoch：444	 i:0 	 global-step:8880	 l-p:0.10566787421703339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[6.4760, 8.0107, 8.8402],
        [6.4760, 6.4760, 6.4760],
        [6.4760, 7.4008, 7.6298],
        [6.4760, 6.6657, 6.5740]], grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.10551128536462784 
model_pd.l_d.mean(): -9.043476104736328 
model_pd.lagr.mean(): -8.93796443939209 
model_pd.lambdas: dict_items([('pout', tensor([1.6075])), ('power', tensor([0.5407]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0890])), ('power', tensor([-19.9236]))])
epoch：445	 i:0 	 global-step:8900	 l-p:0.10551128536462784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.4794, 6.4798, 6.4794],
        [6.4794, 6.4797, 6.4794],
        [6.4794, 6.4799, 6.4794],
        [6.4794, 7.9840, 8.7792]], grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.10533139854669571 
model_pd.l_d.mean(): -9.022273063659668 
model_pd.lagr.mean(): -8.91694164276123 
model_pd.lambdas: dict_items([('pout', tensor([1.6086])), ('power', tensor([0.5397]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0883])), ('power', tensor([-19.9213]))])
epoch：446	 i:0 	 global-step:8920	 l-p:0.10533139854669571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228]])
 pt:tensor([[6.4832, 8.0198, 8.8505],
        [6.4832, 6.9609, 6.9094],
        [6.4832, 7.1308, 7.1628],
        [6.4832, 7.6279, 8.0505]], grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10513633489608765 
model_pd.l_d.mean(): -9.001058578491211 
model_pd.lagr.mean(): -8.895922660827637 
model_pd.lambdas: dict_items([('pout', tensor([1.6097])), ('power', tensor([0.5387]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0876])), ('power', tensor([-19.9187]))])
epoch：447	 i:0 	 global-step:8940	 l-p:0.10513633489608765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[6.4871, 8.3001, 9.4558],
        [6.4871, 7.9938, 8.7901],
        [6.4871, 7.5300, 7.8588],
        [6.4871, 7.0942, 7.1029]], grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.10493475943803787 
model_pd.l_d.mean(): -8.97984504699707 
model_pd.lagr.mean(): -8.874910354614258 
model_pd.lambdas: dict_items([('pout', tensor([1.6107])), ('power', tensor([0.5377]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0868])), ('power', tensor([-19.9159]))])
epoch：448	 i:0 	 global-step:8960	 l-p:0.10493475943803787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.4911, 6.5596, 6.5097],
        [6.4911, 6.7169, 6.6207],
        [6.4911, 6.5267, 6.4976],
        [6.4911, 7.9988, 8.7958]], grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.10473465174436569 
model_pd.l_d.mean(): -8.958638191223145 
model_pd.lagr.mean(): -8.853903770446777 
model_pd.lambdas: dict_items([('pout', tensor([1.6118])), ('power', tensor([0.5367]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0860])), ('power', tensor([-19.9132]))])
epoch：449	 i:0 	 global-step:8980	 l-p:0.10473465174436569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[6.4950, 8.0349, 8.8673],
        [6.4950, 7.0833, 7.0816],
        [6.4950, 6.7886, 6.6924],
        [6.4950, 6.5306, 6.5015]], grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.10454269498586655 
model_pd.l_d.mean(): -8.937444686889648 
model_pd.lagr.mean(): -8.832901954650879 
model_pd.lambdas: dict_items([('pout', tensor([1.6129])), ('power', tensor([0.5357]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0852])), ('power', tensor([-19.9105]))])
epoch：450	 i:0 	 global-step:9000	 l-p:0.10454269498586655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.4987, 7.1481, 7.1801],
        [6.4987, 8.0084, 8.8065],
        [6.4987, 6.4987, 6.4987],
        [6.4987, 7.4272, 7.6572]], grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.10436327755451202 
model_pd.l_d.mean(): -8.91627025604248 
model_pd.lagr.mean(): -8.811906814575195 
model_pd.lambdas: dict_items([('pout', tensor([1.6140])), ('power', tensor([0.5347]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0845])), ('power', tensor([-19.9079]))])
epoch：451	 i:0 	 global-step:9020	 l-p:0.10436327755451202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.5021, 6.5021, 6.5021],
        [6.5021, 6.5021, 6.5021],
        [6.5021, 6.7568, 6.6593],
        [6.5021, 6.5707, 6.5208]], grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.10419835150241852 
model_pd.l_d.mean(): -8.895118713378906 
model_pd.lagr.mean(): -8.79092025756836 
model_pd.lambdas: dict_items([('pout', tensor([1.6151])), ('power', tensor([0.5337]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0838])), ('power', tensor([-19.9055]))])
epoch：452	 i:0 	 global-step:9040	 l-p:0.10419835150241852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.5053, 8.0480, 8.8819],
        [6.5053, 6.6351, 6.5580],
        [6.5053, 6.5053, 6.5053],
        [6.5053, 7.1555, 7.1875]], grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.10404808074235916 
model_pd.l_d.mean(): -8.873983383178711 
model_pd.lagr.mean(): -8.769935607910156 
model_pd.lambdas: dict_items([('pout', tensor([1.6162])), ('power', tensor([0.5328]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0831])), ('power', tensor([-19.9033]))])
epoch：453	 i:0 	 global-step:9060	 l-p:0.10404808074235916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[6.5082, 7.6580, 8.0826],
        [6.5082, 8.0518, 8.8861],
        [6.5082, 6.9881, 6.9363],
        [6.5082, 7.0979, 7.0962]], grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.10391058772802353 
model_pd.l_d.mean(): -8.85287094116211 
model_pd.lagr.mean(): -8.748960494995117 
model_pd.lambdas: dict_items([('pout', tensor([1.6173])), ('power', tensor([0.5318]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0825])), ('power', tensor([-19.9012]))])
epoch：454	 i:0 	 global-step:9080	 l-p:0.10391058772802353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.5110, 6.5111, 6.5110],
        [6.5110, 8.1285, 9.0468],
        [6.5110, 7.3899, 7.5791],
        [6.5110, 6.8610, 6.7718]], grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.1037827804684639 
model_pd.l_d.mean(): -8.831770896911621 
model_pd.lagr.mean(): -8.727988243103027 
model_pd.lambdas: dict_items([('pout', tensor([1.6183])), ('power', tensor([0.5308]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0820])), ('power', tensor([-19.8993]))])
epoch：455	 i:0 	 global-step:9100	 l-p:0.1037827804684639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.5136, 6.5213, 6.5142],
        [6.5136, 6.6437, 6.5665],
        [6.5136, 6.8984, 6.8164],
        [6.5136, 6.5140, 6.5136]], grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.10366085916757584 
model_pd.l_d.mean(): -8.810683250427246 
model_pd.lagr.mean(): -8.707022666931152 
model_pd.lambdas: dict_items([('pout', tensor([1.6194])), ('power', tensor([0.5298]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0814])), ('power', tensor([-19.8975]))])
epoch：456	 i:0 	 global-step:9120	 l-p:0.10366085916757584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[6.5163, 6.9969, 6.9450],
        [6.5163, 7.1068, 7.1051],
        [6.5163, 6.5240, 6.5168],
        [6.5163, 6.6464, 6.5691]], grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.10354077070951462 
model_pd.l_d.mean(): -8.789602279663086 
model_pd.lagr.mean(): -8.68606185913086 
model_pd.lambdas: dict_items([('pout', tensor([1.6205])), ('power', tensor([0.5288]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0809])), ('power', tensor([-19.8956]))])
epoch：457	 i:0 	 global-step:9140	 l-p:0.10354077070951462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[6.5190, 6.5191, 6.5190],
        [6.5190, 7.6710, 8.0964],
        [6.5190, 7.1098, 7.1081],
        [6.5190, 6.5209, 6.5191]], grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.10341881960630417 
model_pd.l_d.mean(): -8.768525123596191 
model_pd.lagr.mean(): -8.665106773376465 
model_pd.lambdas: dict_items([('pout', tensor([1.6216])), ('power', tensor([0.5278]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0804])), ('power', tensor([-19.8937]))])
epoch：458	 i:0 	 global-step:9160	 l-p:0.10341881960630417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[6.5219, 6.5886, 6.5397],
        [6.5219, 7.0556, 7.0259],
        [6.5219, 6.7775, 6.6796],
        [6.5219, 6.5219, 6.5219]], grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.10329191386699677 
model_pd.l_d.mean(): -8.74744987487793 
model_pd.lagr.mean(): -8.644158363342285 
model_pd.lambdas: dict_items([('pout', tensor([1.6227])), ('power', tensor([0.5268]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0798])), ('power', tensor([-19.8917]))])
epoch：459	 i:0 	 global-step:9180	 l-p:0.10329191386699677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.5249, 6.7163, 6.6237],
        [6.5249, 7.6781, 8.1039],
        [6.5249, 6.5256, 6.5249],
        [6.5249, 6.5250, 6.5249]], grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.10315779596567154 
model_pd.l_d.mean(): -8.726370811462402 
model_pd.lagr.mean(): -8.623212814331055 
model_pd.lambdas: dict_items([('pout', tensor([1.6237])), ('power', tensor([0.5258]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0792])), ('power', tensor([-19.8896]))])
epoch：460	 i:0 	 global-step:9200	 l-p:0.10315779596567154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.5282, 6.5282, 6.5282],
        [6.5282, 7.1397, 7.1484],
        [6.5282, 6.5359, 6.5288],
        [6.5282, 6.8793, 6.7898]], grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.1030154600739479 
model_pd.l_d.mean(): -8.705291748046875 
model_pd.lagr.mean(): -8.602275848388672 
model_pd.lambdas: dict_items([('pout', tensor([1.6248])), ('power', tensor([0.5248]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0785])), ('power', tensor([-19.8873]))])
epoch：461	 i:0 	 global-step:9220	 l-p:0.1030154600739479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[6.5317, 6.5318, 6.5317],
        [6.5317, 6.7591, 6.6622],
        [6.5317, 6.5317, 6.5317],
        [6.5317, 7.1238, 7.1221]], grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.10286476463079453 
model_pd.l_d.mean(): -8.684207916259766 
model_pd.lagr.mean(): -8.581342697143555 
model_pd.lambdas: dict_items([('pout', tensor([1.6259])), ('power', tensor([0.5238]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0778])), ('power', tensor([-19.8848]))])
epoch：462	 i:0 	 global-step:9240	 l-p:0.10286476463079453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[6.5355, 6.7917, 6.6935],
        [6.5355, 6.6044, 6.5543],
        [6.5355, 6.9032, 6.8167],
        [6.5355, 7.5873, 7.9189]], grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.10270629823207855 
model_pd.l_d.mean(): -8.663125991821289 
model_pd.lagr.mean(): -8.560420036315918 
model_pd.lambdas: dict_items([('pout', tensor([1.6270])), ('power', tensor([0.5228]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0771])), ('power', tensor([-19.8822]))])
epoch：463	 i:0 	 global-step:9260	 l-p:0.10270629823207855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[6.5394, 7.1935, 7.2257],
        [6.5394, 6.5397, 6.5394],
        [6.5394, 6.6088, 6.5584],
        [6.5394, 7.4227, 7.6128]], grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.1025417149066925 
model_pd.l_d.mean(): -8.642044067382812 
model_pd.lagr.mean(): -8.539502143859863 
model_pd.lambdas: dict_items([('pout', tensor([1.6280])), ('power', tensor([0.5218]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0763])), ('power', tensor([-19.8795]))])
epoch：464	 i:0 	 global-step:9280	 l-p:0.1025417149066925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.5435, 6.6129, 6.5625],
        [6.5435, 6.7713, 6.6742],
        [6.5435, 6.5840, 6.5514],
        [6.5435, 7.1981, 7.2303]], grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.1023726835846901 
model_pd.l_d.mean(): -8.620965957641602 
model_pd.lagr.mean(): -8.518592834472656 
model_pd.lambdas: dict_items([('pout', tensor([1.6291])), ('power', tensor([0.5208]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0755])), ('power', tensor([-19.8766]))])
epoch：465	 i:0 	 global-step:9300	 l-p:0.1023726835846901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.5477, 6.8045, 6.7061],
        [6.5477, 6.6147, 6.5656],
        [6.5477, 6.6162, 6.5663],
        [6.5477, 6.5477, 6.5477]], grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.1022011786699295 
model_pd.l_d.mean(): -8.599892616271973 
model_pd.lagr.mean(): -8.49769115447998 
model_pd.lambdas: dict_items([('pout', tensor([1.6302])), ('power', tensor([0.5198]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0746])), ('power', tensor([-19.8736]))])
epoch：466	 i:0 	 global-step:9320	 l-p:0.1022011786699295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.5520, 6.5520, 6.5520],
        [6.5520, 7.2076, 7.2399],
        [6.5520, 8.3859, 9.5549],
        [6.5520, 6.5524, 6.5520]], grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.1020289957523346 
model_pd.l_d.mean(): -8.578826904296875 
model_pd.lagr.mean(): -8.476798057556152 
model_pd.lambdas: dict_items([('pout', tensor([1.6313])), ('power', tensor([0.5188]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0737])), ('power', tensor([-19.8706]))])
epoch：467	 i:0 	 global-step:9340	 l-p:0.1020289957523346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[6.5563, 7.0933, 7.0634],
        [6.5563, 7.3802, 7.5238],
        [6.5563, 6.5773, 6.5591],
        [6.5563, 8.1130, 8.9545]], grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.10185780376195908 
model_pd.l_d.mean(): -8.557767868041992 
model_pd.lagr.mean(): -8.455909729003906 
model_pd.lambdas: dict_items([('pout', tensor([1.6323])), ('power', tensor([0.5178]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0729])), ('power', tensor([-19.8676]))])
epoch：468	 i:0 	 global-step:9360	 l-p:0.10185780376195908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[6.5606, 6.5609, 6.5606],
        [6.5606, 8.0870, 8.8939],
        [6.5606, 6.5707, 6.5615],
        [6.5606, 7.3851, 7.5289]], grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.10168881714344025 
model_pd.l_d.mean(): -8.536720275878906 
model_pd.lagr.mean(): -8.43503189086914 
model_pd.lambdas: dict_items([('pout', tensor([1.6334])), ('power', tensor([0.5168]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0720])), ('power', tensor([-19.8646]))])
epoch：469	 i:0 	 global-step:9380	 l-p:0.10168881714344025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[6.5649, 6.5653, 6.5650],
        [6.5649, 7.1606, 7.1588],
        [6.5649, 6.7577, 6.6645],
        [6.5649, 6.5750, 6.5658]], grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.10152272880077362 
model_pd.l_d.mean(): -8.515681266784668 
model_pd.lagr.mean(): -8.414158821105957 
model_pd.lambdas: dict_items([('pout', tensor([1.6345])), ('power', tensor([0.5159]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0712])), ('power', tensor([-19.8615]))])
epoch：470	 i:0 	 global-step:9400	 l-p:0.10152272880077362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[6.5692, 6.7981, 6.7006],
        [6.5692, 6.5903, 6.5720],
        [6.5692, 7.1464, 7.1350],
        [6.5692, 6.7621, 6.6688]], grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.10135997086763382 
model_pd.l_d.mean(): -8.494654655456543 
model_pd.lagr.mean(): -8.393294334411621 
model_pd.lambdas: dict_items([('pout', tensor([1.6356])), ('power', tensor([0.5149]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0703])), ('power', tensor([-19.8585]))])
epoch：471	 i:0 	 global-step:9420	 l-p:0.10135997086763382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[6.5735, 7.1122, 7.0822],
        [6.5735, 7.2316, 7.2640],
        [6.5735, 6.8315, 6.7326],
        [6.5735, 6.7548, 6.6636]], grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.10120045393705368 
model_pd.l_d.mean(): -8.473637580871582 
model_pd.lagr.mean(): -8.372437477111816 
model_pd.lambdas: dict_items([('pout', tensor([1.6366])), ('power', tensor([0.5139]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0694])), ('power', tensor([-19.8555]))])
epoch：472	 i:0 	 global-step:9440	 l-p:0.10120045393705368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[6.5777, 6.6466, 6.5964],
        [6.5777, 6.6451, 6.5958],
        [6.5777, 7.6375, 7.9716],
        [6.5777, 6.5777, 6.5777]], grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.10104367882013321 
model_pd.l_d.mean(): -8.45263385772705 
model_pd.lagr.mean(): -8.351590156555176 
model_pd.lambdas: dict_items([('pout', tensor([1.6377])), ('power', tensor([0.5129]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0686])), ('power', tensor([-19.8525]))])
epoch：473	 i:0 	 global-step:9460	 l-p:0.10104367882013321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.5820, 7.0681, 7.0157],
        [6.5820, 6.7136, 6.6354],
        [6.5820, 7.7371, 8.1580],
        [6.5820, 6.5820, 6.5820]], grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.10088919848203659 
model_pd.l_d.mean(): -8.43163776397705 
model_pd.lagr.mean(): -8.330748558044434 
model_pd.lambdas: dict_items([('pout', tensor([1.6388])), ('power', tensor([0.5119]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0678])), ('power', tensor([-19.8495]))])
epoch：474	 i:0 	 global-step:9480	 l-p:0.10088919848203659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.5862, 6.5869, 6.5862],
        [6.5862, 7.1261, 7.0961],
        [6.5862, 6.5862, 6.5862],
        [6.5862, 7.5293, 7.7628]], grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.10073617845773697 
model_pd.l_d.mean(): -8.410651206970215 
model_pd.lagr.mean(): -8.309914588928223 
model_pd.lambdas: dict_items([('pout', tensor([1.6398])), ('power', tensor([0.5109]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0669])), ('power', tensor([-19.8466]))])
epoch：475	 i:0 	 global-step:9500	 l-p:0.10073617845773697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[6.5905, 6.9455, 6.8550],
        [6.5905, 8.2309, 9.1622],
        [6.5905, 6.6006, 6.5913],
        [6.5905, 6.7478, 6.6619]], grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.10058382153511047 
model_pd.l_d.mean(): -8.389676094055176 
model_pd.lagr.mean(): -8.289092063903809 
model_pd.lambdas: dict_items([('pout', tensor([1.6409])), ('power', tensor([0.5099]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0661])), ('power', tensor([-19.8435]))])
epoch：476	 i:0 	 global-step:9520	 l-p:0.10058382153511047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[6.5948, 6.6357, 6.6028],
        [6.5948, 7.5393, 7.7732],
        [6.5948, 7.2554, 7.2880],
        [6.5948, 6.5948, 6.5948]], grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.10043129324913025 
model_pd.l_d.mean(): -8.368707656860352 
model_pd.lagr.mean(): -8.26827621459961 
model_pd.lambdas: dict_items([('pout', tensor([1.6420])), ('power', tensor([0.5089]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0652])), ('power', tensor([-19.8405]))])
epoch：477	 i:0 	 global-step:9540	 l-p:0.10043129324913025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.5992, 6.9712, 6.8838],
        [6.5992, 6.8983, 6.8004],
        [6.5992, 6.7312, 6.6529],
        [6.5992, 6.6690, 6.6183]], grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.1002778634428978 
model_pd.l_d.mean(): -8.347744941711426 
model_pd.lagr.mean(): -8.247467041015625 
model_pd.lambdas: dict_items([('pout', tensor([1.6430])), ('power', tensor([0.5079]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0643])), ('power', tensor([-19.8374]))])
epoch：478	 i:0 	 global-step:9560	 l-p:0.1002778634428978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[6.6038, 8.2480, 9.1815],
        [6.6038, 6.6057, 6.6038],
        [6.6038, 6.7978, 6.7040],
        [6.6038, 6.9596, 6.8689]], grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.10012302547693253 
model_pd.l_d.mean(): -8.32679271697998 
model_pd.lagr.mean(): -8.226669311523438 
model_pd.lambdas: dict_items([('pout', tensor([1.6441])), ('power', tensor([0.5069]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0634])), ('power', tensor([-19.8342]))])
epoch：479	 i:0 	 global-step:9580	 l-p:0.10012302547693253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[6.6084, 6.9997, 6.9163],
        [6.6084, 8.4607, 9.6415],
        [6.6084, 8.2540, 9.1883],
        [6.6084, 6.6786, 6.6276]], grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.09996641427278519 
model_pd.l_d.mean(): -8.305843353271484 
model_pd.lagr.mean(): -8.205877304077148 
model_pd.lambdas: dict_items([('pout', tensor([1.6452])), ('power', tensor([0.5059]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0625])), ('power', tensor([-19.8309]))])
epoch：480	 i:0 	 global-step:9600	 l-p:0.09996641427278519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[6.6131, 7.7831, 8.2144],
        [6.6131, 6.6810, 6.6313],
        [6.6131, 6.6131, 6.6131],
        [6.6131, 6.6131, 6.6131]], grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.09980779886245728 
model_pd.l_d.mean(): -8.284902572631836 
model_pd.lagr.mean(): -8.185094833374023 
model_pd.lambdas: dict_items([('pout', tensor([1.6462])), ('power', tensor([0.5049]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0616])), ('power', tensor([-19.8275]))])
epoch：481	 i:0 	 global-step:9620	 l-p:0.09980779886245728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.6180, 7.2393, 7.2482],
        [6.6180, 6.8489, 6.7505],
        [6.6180, 7.1073, 7.0545],
        [6.6180, 6.6859, 6.6361]], grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.09964710474014282 
model_pd.l_d.mean(): -8.263967514038086 
model_pd.lagr.mean(): -8.16431999206543 
model_pd.lambdas: dict_items([('pout', tensor([1.6473])), ('power', tensor([0.5039]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0606])), ('power', tensor([-19.8241]))])
epoch：482	 i:0 	 global-step:9640	 l-p:0.09964710474014282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[6.6230, 6.6230, 6.6230],
        [6.6230, 7.1127, 7.0599],
        [6.6230, 8.2728, 9.2095],
        [6.6230, 6.6237, 6.6230]], grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.09948459267616272 
model_pd.l_d.mean(): -8.2430419921875 
model_pd.lagr.mean(): -8.14355754852295 
model_pd.lambdas: dict_items([('pout', tensor([1.6483])), ('power', tensor([0.5030]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0596])), ('power', tensor([-19.8206]))])
epoch：483	 i:0 	 global-step:9660	 l-p:0.09948459267616272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[6.6281, 6.9287, 6.8303],
        [6.6281, 6.6288, 6.6281],
        [6.6281, 6.6281, 6.6281],
        [6.6281, 7.2113, 7.1998]], grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.09932033717632294 
model_pd.l_d.mean(): -8.222123146057129 
model_pd.lagr.mean(): -8.122802734375 
model_pd.lambdas: dict_items([('pout', tensor([1.6494])), ('power', tensor([0.5020]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0586])), ('power', tensor([-19.8169]))])
epoch：484	 i:0 	 global-step:9680	 l-p:0.09932033717632294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[6.6333, 7.1240, 7.0711],
        [6.6333, 6.7035, 6.6524],
        [6.6333, 7.0075, 6.9196],
        [6.6333, 7.7989, 8.2237]], grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.09915473312139511 
model_pd.l_d.mean(): -8.201211929321289 
model_pd.lagr.mean(): -8.102057456970215 
model_pd.lambdas: dict_items([('pout', tensor([1.6505])), ('power', tensor([0.5010]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0576])), ('power', tensor([-19.8132]))])
epoch：485	 i:0 	 global-step:9700	 l-p:0.09915473312139511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[6.6386, 6.6752, 6.6453],
        [6.6386, 7.2230, 7.2115],
        [6.6386, 6.7973, 6.7106],
        [6.6386, 6.6386, 6.6386]], grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.09898809343576431 
model_pd.l_d.mean(): -8.180309295654297 
model_pd.lagr.mean(): -8.081320762634277 
model_pd.lambdas: dict_items([('pout', tensor([1.6515])), ('power', tensor([0.5000]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0565])), ('power', tensor([-19.8095]))])
epoch：486	 i:0 	 global-step:9720	 l-p:0.09898809343576431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[6.6440, 6.6443, 6.6440],
        [6.6440, 6.7771, 6.6981],
        [6.6440, 6.6444, 6.6440],
        [6.6440, 7.5968, 7.8328]], grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.09882082045078278 
model_pd.l_d.mean(): -8.159416198730469 
model_pd.lagr.mean(): -8.060595512390137 
model_pd.lambdas: dict_items([('pout', tensor([1.6526])), ('power', tensor([0.4990]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0554])), ('power', tensor([-19.8056]))])
epoch：487	 i:0 	 global-step:9740	 l-p:0.09882082045078278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.6495, 7.8271, 8.2612],
        [6.6495, 6.6496, 6.6495],
        [6.6495, 6.7203, 6.6689],
        [6.6495, 6.9110, 6.8109]], grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.09865324199199677 
model_pd.l_d.mean(): -8.138534545898438 
model_pd.lagr.mean(): -8.039881706237793 
model_pd.lambdas: dict_items([('pout', tensor([1.6536])), ('power', tensor([0.4980]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0544])), ('power', tensor([-19.8017]))])
epoch：488	 i:0 	 global-step:9760	 l-p:0.09865324199199677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[6.6551, 7.1477, 7.0946],
        [6.6551, 7.0308, 6.9425],
        [6.6551, 6.6765, 6.6579],
        [6.6551, 6.6551, 6.6551]], grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.09848567843437195 
model_pd.l_d.mean(): -8.117660522460938 
model_pd.lagr.mean(): -8.019174575805664 
model_pd.lambdas: dict_items([('pout', tensor([1.6547])), ('power', tensor([0.4970]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0533])), ('power', tensor([-19.7978]))])
epoch：489	 i:0 	 global-step:9780	 l-p:0.09848567843437195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.6607, 6.6822, 6.6635],
        [6.6607, 7.2078, 7.1774],
        [6.6607, 6.6975, 6.6674],
        [6.6607, 6.6611, 6.6607]], grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.09831838309764862 
model_pd.l_d.mean(): -8.09679889678955 
model_pd.lagr.mean(): -7.998480319976807 
model_pd.lambdas: dict_items([('pout', tensor([1.6557])), ('power', tensor([0.4960]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0521])), ('power', tensor([-19.7937]))])
epoch：490	 i:0 	 global-step:9800	 l-p:0.09831838309764862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.6664, 6.7371, 6.6857],
        [6.6664, 7.7430, 8.0824],
        [6.6664, 7.2141, 7.1837],
        [6.6664, 6.6668, 6.6664]], grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.0981515571475029 
model_pd.l_d.mean(): -8.075947761535645 
model_pd.lagr.mean(): -7.9777960777282715 
model_pd.lambdas: dict_items([('pout', tensor([1.6568])), ('power', tensor([0.4950]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0510])), ('power', tensor([-19.7897]))])
epoch：491	 i:0 	 global-step:9820	 l-p:0.0981515571475029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[6.6722, 6.6725, 6.6722],
        [6.6722, 6.8318, 6.7446],
        [6.6722, 6.7090, 6.6789],
        [6.6722, 7.2995, 7.3086]], grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.09798529744148254 
model_pd.l_d.mean(): -8.055106163024902 
model_pd.lagr.mean(): -7.957120895385742 
model_pd.lambdas: dict_items([('pout', tensor([1.6578])), ('power', tensor([0.4940]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0499])), ('power', tensor([-19.7856]))])
epoch：492	 i:0 	 global-step:9840	 l-p:0.09798529744148254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[6.6780, 6.6787, 6.6780],
        [6.6780, 7.8628, 8.3005],
        [6.6780, 7.8529, 8.2811],
        [6.6780, 8.2685, 9.1284]], grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09781963378190994 
model_pd.l_d.mean(): -8.03427791595459 
model_pd.lagr.mean(): -7.936458110809326 
model_pd.lambdas: dict_items([('pout', tensor([1.6589])), ('power', tensor([0.4931]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0487])), ('power', tensor([-19.7814]))])
epoch：493	 i:0 	 global-step:9860	 l-p:0.09781963378190994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[6.6838, 7.2922, 7.2905],
        [6.6838, 6.6838, 6.6838],
        [6.6838, 7.8599, 8.2886],
        [6.6838, 6.6846, 6.6838]], grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.09765461087226868 
model_pd.l_d.mean(): -8.013460159301758 
model_pd.lagr.mean(): -7.915805339813232 
model_pd.lambdas: dict_items([('pout', tensor([1.6599])), ('power', tensor([0.4921]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0476])), ('power', tensor([-19.7773]))])
epoch：494	 i:0 	 global-step:9880	 l-p:0.09765461087226868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[6.6897, 6.6897, 6.6897],
        [6.6897, 6.9531, 6.8523],
        [6.6897, 7.0867, 7.0022],
        [6.6897, 6.9937, 6.8942]], grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.09749013185501099 
model_pd.l_d.mean(): -7.992652416229248 
model_pd.lagr.mean(): -7.895162105560303 
model_pd.lambdas: dict_items([('pout', tensor([1.6610])), ('power', tensor([0.4911]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0464])), ('power', tensor([-19.7730]))])
epoch：495	 i:0 	 global-step:9900	 l-p:0.09749013185501099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[6.6957, 6.7671, 6.7152],
        [6.6957, 6.7060, 6.6966],
        [6.6957, 7.3683, 7.4015],
        [6.6957, 6.6958, 6.6957]], grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.09732614457607269 
model_pd.l_d.mean(): -7.971857070922852 
model_pd.lagr.mean(): -7.874530792236328 
model_pd.lambdas: dict_items([('pout', tensor([1.6620])), ('power', tensor([0.4901]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0453])), ('power', tensor([-19.7688]))])
epoch：496	 i:0 	 global-step:9920	 l-p:0.09732614457607269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[6.7017, 7.2529, 7.2223],
        [6.7017, 7.6643, 7.9028],
        [6.7017, 6.7022, 6.7018],
        [6.7017, 6.8873, 6.7939]], grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.09716247767210007 
model_pd.l_d.mean(): -7.951075077056885 
model_pd.lagr.mean(): -7.853912830352783 
model_pd.lambdas: dict_items([('pout', tensor([1.6631])), ('power', tensor([0.4891]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0441])), ('power', tensor([-19.7645]))])
epoch：497	 i:0 	 global-step:9940	 l-p:0.09716247767210007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[6.7079, 7.1061, 7.0214],
        [6.7079, 6.9056, 6.8100],
        [6.7079, 6.7079, 6.7079],
        [6.7079, 7.3392, 7.3483]], grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.09699907153844833 
model_pd.l_d.mean(): -7.930302619934082 
model_pd.lagr.mean(): -7.833303451538086 
model_pd.lambdas: dict_items([('pout', tensor([1.6641])), ('power', tensor([0.4881]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0429])), ('power', tensor([-19.7601]))])
epoch：498	 i:0 	 global-step:9960	 l-p:0.09699907153844833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[6.7140, 6.7140, 6.7140],
        [6.7140, 6.8488, 6.7688],
        [6.7140, 7.0770, 6.9845],
        [6.7140, 7.0193, 6.9194]], grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.09683580696582794 
model_pd.l_d.mean(): -7.909542083740234 
model_pd.lagr.mean(): -7.812706470489502 
model_pd.lambdas: dict_items([('pout', tensor([1.6651])), ('power', tensor([0.4871]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0417])), ('power', tensor([-19.7557]))])
epoch：499	 i:0 	 global-step:9980	 l-p:0.09683580696582794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.7203, 6.7203, 6.7203],
        [6.7203, 7.3530, 7.3622],
        [6.7203, 6.7621, 6.7285],
        [6.7203, 6.7895, 6.7388]], grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.09667249023914337 
model_pd.l_d.mean(): -7.888794422149658 
model_pd.lagr.mean(): -7.792121887207031 
model_pd.lambdas: dict_items([('pout', tensor([1.6662])), ('power', tensor([0.4861]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0405])), ('power', tensor([-19.7512]))])
epoch：500	 i:0 	 global-step:10000	 l-p:0.09667249023914337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[ 6.7266,  6.9621,  6.8618],
        [ 6.7266,  8.7667, 10.1594],
        [ 6.7266,  6.9130,  6.8192],
        [ 6.7266,  7.0326,  6.9324]], grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.09650909155607224 
model_pd.l_d.mean(): -7.868058204650879 
model_pd.lagr.mean(): -7.771549224853516 
model_pd.lambdas: dict_items([('pout', tensor([1.6672])), ('power', tensor([0.4852]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0392])), ('power', tensor([-19.7466]))])
epoch：501	 i:0 	 global-step:10020	 l-p:0.09650909155607224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[6.7330, 6.7434, 6.7339],
        [6.7330, 6.7388, 6.7334],
        [6.7330, 6.7335, 6.7331],
        [6.7330, 6.9985, 6.8968]], grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.09634556621313095 
model_pd.l_d.mean(): -7.84733247756958 
model_pd.lagr.mean(): -7.7509870529174805 
model_pd.lambdas: dict_items([('pout', tensor([1.6683])), ('power', tensor([0.4842]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0380])), ('power', tensor([-19.7420]))])
epoch：502	 i:0 	 global-step:10040	 l-p:0.09634556621313095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.7396, 6.7815, 6.7478],
        [6.7396, 6.7400, 6.7396],
        [6.7396, 7.3540, 7.3523],
        [6.7396, 8.3148, 9.1477]], grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.09618178009986877 
model_pd.l_d.mean(): -7.826618671417236 
model_pd.lagr.mean(): -7.7304368019104 
model_pd.lambdas: dict_items([('pout', tensor([1.6693])), ('power', tensor([0.4832]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0367])), ('power', tensor([-19.7373]))])
epoch：503	 i:0 	 global-step:10060	 l-p:0.09618178009986877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[6.7462, 6.9453, 6.8490],
        [6.7462, 6.7541, 6.7467],
        [6.7462, 6.7462, 6.7462],
        [6.7462, 8.3232, 9.1571]], grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.09601780027151108 
model_pd.l_d.mean(): -7.805915355682373 
model_pd.lagr.mean(): -7.709897518157959 
model_pd.lambdas: dict_items([('pout', tensor([1.6703])), ('power', tensor([0.4822]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0354])), ('power', tensor([-19.7326]))])
epoch：504	 i:0 	 global-step:10080	 l-p:0.09601780027151108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[6.7529, 6.8240, 6.7721],
        [6.7529, 8.4407, 9.3993],
        [6.7529, 6.7949, 6.7611],
        [6.7529, 6.7529, 6.7529]], grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.0958535298705101 
model_pd.l_d.mean(): -7.785225868225098 
model_pd.lagr.mean(): -7.689372539520264 
model_pd.lambdas: dict_items([('pout', tensor([1.6714])), ('power', tensor([0.4812]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0341])), ('power', tensor([-19.7277]))])
epoch：505	 i:0 	 global-step:10100	 l-p:0.0958535298705101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.7596, 6.9592, 6.8627],
        [6.7596, 7.6137, 7.7628],
        [6.7596, 7.9617, 8.4057],
        [6.7596, 6.8308, 6.7789]], grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.0956890657544136 
model_pd.l_d.mean(): -7.764549255371094 
model_pd.lagr.mean(): -7.668859958648682 
model_pd.lambdas: dict_items([('pout', tensor([1.6724])), ('power', tensor([0.4802]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0328])), ('power', tensor([-19.7229]))])
epoch：506	 i:0 	 global-step:10120	 l-p:0.0956890657544136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[6.7665, 6.7665, 6.7665],
        [6.7665, 6.8384, 6.7861],
        [6.7665, 7.4044, 7.4138],
        [6.7665, 6.7673, 6.7665]], grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.09552441537380219 
model_pd.l_d.mean(): -7.743884086608887 
model_pd.lagr.mean(): -7.648359775543213 
model_pd.lambdas: dict_items([('pout', tensor([1.6734])), ('power', tensor([0.4792]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0315])), ('power', tensor([-19.7179]))])
epoch：507	 i:0 	 global-step:10140	 l-p:0.09552441537380219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.7735, 6.7735, 6.7735],
        [6.7735, 6.8110, 6.7803],
        [6.7735, 7.2766, 7.2224],
        [6.7735, 6.7954, 6.7764]], grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09535966068506241 
model_pd.l_d.mean(): -7.723231315612793 
model_pd.lagr.mean(): -7.627871513366699 
model_pd.lambdas: dict_items([('pout', tensor([1.6745])), ('power', tensor([0.4782]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0301])), ('power', tensor([-19.7129]))])
epoch：508	 i:0 	 global-step:10160	 l-p:0.09535966068506241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[6.7805, 6.7826, 6.7806],
        [6.7805, 6.8530, 6.8004],
        [6.7805, 7.0183, 6.9170],
        [6.7805, 6.7809, 6.7806]], grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.09519483894109726 
model_pd.l_d.mean(): -7.702591896057129 
model_pd.lagr.mean(): -7.607397079467773 
model_pd.lambdas: dict_items([('pout', tensor([1.6755])), ('power', tensor([0.4773]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0287])), ('power', tensor([-19.7078]))])
epoch：509	 i:0 	 global-step:10180	 l-p:0.09519483894109726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[6.7877, 7.9956, 8.4419],
        [6.7877, 6.7877, 6.7877],
        [6.7877, 6.7957, 6.7883],
        [6.7877, 6.7882, 6.7877]], grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.09502999484539032 
model_pd.l_d.mean(): -7.681966781616211 
model_pd.lagr.mean(): -7.586936950683594 
model_pd.lambdas: dict_items([('pout', tensor([1.6765])), ('power', tensor([0.4763]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0273])), ('power', tensor([-19.7026]))])
epoch：510	 i:0 	 global-step:10200	 l-p:0.09502999484539032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[6.7949, 7.1046, 7.0033],
        [6.7949, 8.0030, 8.4486],
        [6.7949, 7.1801, 7.0896],
        [6.7949, 6.8373, 6.8032]], grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.09486521780490875 
model_pd.l_d.mean(): -7.66135311126709 
model_pd.lagr.mean(): -7.566487789154053 
model_pd.lambdas: dict_items([('pout', tensor([1.6775])), ('power', tensor([0.4753]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0260])), ('power', tensor([-19.6974]))])
epoch：511	 i:0 	 global-step:10220	 l-p:0.09486521780490875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[6.8022, 7.9048, 8.2527],
        [6.8022, 6.8023, 6.8022],
        [6.8022, 6.9391, 6.8579],
        [6.8022, 7.4442, 7.4536]], grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.09470057487487793 
model_pd.l_d.mean(): -7.640754699707031 
model_pd.lagr.mean(): -7.546053886413574 
model_pd.lambdas: dict_items([('pout', tensor([1.6786])), ('power', tensor([0.4743]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0245])), ('power', tensor([-19.6921]))])
epoch：512	 i:0 	 global-step:10240	 l-p:0.09470057487487793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[6.8096, 6.8104, 6.8096],
        [6.8096, 8.0208, 8.4676],
        [6.8096, 7.1788, 7.0848],
        [6.8096, 6.8177, 6.8102]], grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.09453613311052322 
model_pd.l_d.mean(): -7.620168685913086 
model_pd.lagr.mean(): -7.525632381439209 
model_pd.lambdas: dict_items([('pout', tensor([1.6796])), ('power', tensor([0.4733]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0231])), ('power', tensor([-19.6867]))])
epoch：513	 i:0 	 global-step:10260	 l-p:0.09453613311052322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[6.8171, 6.8897, 6.8369],
        [6.8171, 7.4607, 7.4702],
        [6.8171, 6.8171, 6.8171],
        [6.8171, 7.0187, 6.9213]], grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.09437192976474762 
model_pd.l_d.mean(): -7.59959602355957 
model_pd.lagr.mean(): -7.505224227905273 
model_pd.lambdas: dict_items([('pout', tensor([1.6806])), ('power', tensor([0.4723]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0217])), ('power', tensor([-19.6813]))])
epoch：514	 i:0 	 global-step:10280	 l-p:0.09437192976474762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.8246, 8.0390, 8.4870],
        [6.8246, 7.8082, 8.0521],
        [6.8246, 7.0143, 6.9189],
        [6.8246, 6.8973, 6.8444]], grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.0942080095410347 
model_pd.l_d.mean(): -7.579039573669434 
model_pd.lagr.mean(): -7.4848313331604 
model_pd.lambdas: dict_items([('pout', tensor([1.6816])), ('power', tensor([0.4714]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0202])), ('power', tensor([-19.6758]))])
epoch：515	 i:0 	 global-step:10300	 l-p:0.0942080095410347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.8323, 6.8326, 6.8323],
        [6.8323, 7.2200, 7.1290],
        [6.8323, 7.4776, 7.4871],
        [6.8323, 6.8323, 6.8323]], grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.09404435753822327 
model_pd.l_d.mean(): -7.55849552154541 
model_pd.lagr.mean(): -7.464451313018799 
model_pd.lambdas: dict_items([('pout', tensor([1.6826])), ('power', tensor([0.4704]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0188])), ('power', tensor([-19.6703]))])
epoch：516	 i:0 	 global-step:10320	 l-p:0.09404435753822327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[6.8399, 7.4654, 7.4638],
        [6.8399, 6.8400, 6.8399],
        [6.8399, 7.5299, 7.5642],
        [6.8399, 8.0589, 8.5095]], grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.09388106316328049 
model_pd.l_d.mean(): -7.537968158721924 
model_pd.lagr.mean(): -7.444087028503418 
model_pd.lambdas: dict_items([('pout', tensor([1.6837])), ('power', tensor([0.4694]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0173])), ('power', tensor([-19.6647]))])
epoch：517	 i:0 	 global-step:10340	 l-p:0.09388106316328049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[ 6.8477,  6.8478,  6.8477],
        [ 6.8477,  7.4740,  7.4724],
        [ 6.8477,  8.7789, 10.0106],
        [ 6.8477,  7.0125,  6.9225]], grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.09371816366910934 
model_pd.l_d.mean(): -7.517453193664551 
model_pd.lagr.mean(): -7.42373514175415 
model_pd.lambdas: dict_items([('pout', tensor([1.6847])), ('power', tensor([0.4684]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0158])), ('power', tensor([-19.6590]))])
epoch：518	 i:0 	 global-step:10360	 l-p:0.09371816366910934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[6.8556, 7.0463, 6.9504],
        [6.8556, 7.2278, 7.1330],
        [6.8556, 7.3660, 7.3111],
        [6.8556, 7.2449, 7.1535]], grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.09355562180280685 
model_pd.l_d.mean(): -7.496953964233398 
model_pd.lagr.mean(): -7.403398513793945 
model_pd.lambdas: dict_items([('pout', tensor([1.6857])), ('power', tensor([0.4674]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0143])), ('power', tensor([-19.6533]))])
epoch：519	 i:0 	 global-step:10380	 l-p:0.09355562180280685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[ 6.8635,  8.9523, 10.3788],
        [ 6.8635,  6.8716,  6.8641],
        [ 6.8635,  6.8638,  6.8635],
        [ 6.8635,  7.2362,  7.1414]], grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.09339343011379242 
model_pd.l_d.mean(): -7.476468563079834 
model_pd.lagr.mean(): -7.38307523727417 
model_pd.lambdas: dict_items([('pout', tensor([1.6867])), ('power', tensor([0.4664]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0128])), ('power', tensor([-19.6475]))])
epoch：520	 i:0 	 global-step:10400	 l-p:0.09339343011379242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[ 6.8714,  8.0971,  8.5502],
        [ 6.8714,  8.9632, 10.3916],
        [ 6.8714,  7.9875,  8.3397],
        [ 6.8714,  6.8722,  6.8715]], grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.09323158115148544 
model_pd.l_d.mean(): -7.456000328063965 
model_pd.lagr.mean(): -7.362768650054932 
model_pd.lambdas: dict_items([('pout', tensor([1.6877])), ('power', tensor([0.4655]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0112])), ('power', tensor([-19.6417]))])
epoch：521	 i:0 	 global-step:10420	 l-p:0.09323158115148544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[6.8795, 6.9533, 6.8997],
        [6.8795, 7.4483, 7.4169],
        [6.8795, 6.9177, 6.8865],
        [6.8795, 6.8795, 6.8795]], grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.09307007491588593 
model_pd.l_d.mean(): -7.435545444488525 
model_pd.lagr.mean(): -7.342475414276123 
model_pd.lambdas: dict_items([('pout', tensor([1.6887])), ('power', tensor([0.4645]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0097])), ('power', tensor([-19.6358]))])
epoch：522	 i:0 	 global-step:10440	 l-p:0.09307007491588593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[6.8876, 7.8821, 8.1288],
        [6.8876, 8.0068, 8.3600],
        [6.8876, 7.2989, 7.2114],
        [6.8876, 7.4010, 7.3458]], grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.09290891885757446 
model_pd.l_d.mean(): -7.415105819702148 
model_pd.lagr.mean(): -7.322196960449219 
model_pd.lambdas: dict_items([('pout', tensor([1.6897])), ('power', tensor([0.4635]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0081])), ('power', tensor([-19.6299]))])
epoch：523	 i:0 	 global-step:10460	 l-p:0.09290891885757446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[6.8958, 8.1253, 8.5790],
        [6.8958, 8.5477, 9.4413],
        [6.8958, 6.9018, 6.8962],
        [6.8958, 7.5926, 7.6273]], grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.09274809062480927 
model_pd.l_d.mean(): -7.394682884216309 
model_pd.lagr.mean(): -7.301934719085693 
model_pd.lambdas: dict_items([('pout', tensor([1.6907])), ('power', tensor([0.4625]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0066])), ('power', tensor([-19.6239]))])
epoch：524	 i:0 	 global-step:10480	 l-p:0.09274809062480927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[6.9041, 6.9425, 6.9111],
        [6.9041, 7.7801, 7.9333],
        [6.9041, 7.2199, 7.1166],
        [6.9041, 6.9042, 6.9041]], grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.09258756786584854 
model_pd.l_d.mean(): -7.3742756843566895 
model_pd.lagr.mean(): -7.281688213348389 
model_pd.lambdas: dict_items([('pout', tensor([1.6917])), ('power', tensor([0.4615]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0050])), ('power', tensor([-19.6178]))])
epoch：525	 i:0 	 global-step:10500	 l-p:0.09258756786584854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[ 6.9125,  7.1559,  7.0522],
        [ 6.9125,  8.8652, 10.1109],
        [ 6.9125,  7.1866,  7.0817],
        [ 6.9125,  7.4845,  7.4530]], grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.09242735058069229 
model_pd.l_d.mean(): -7.3538818359375 
model_pd.lagr.mean(): -7.2614545822143555 
model_pd.lambdas: dict_items([('pout', tensor([1.6928])), ('power', tensor([0.4606]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0034])), ('power', tensor([-19.6117]))])
epoch：526	 i:0 	 global-step:10520	 l-p:0.09242735058069229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[6.9209, 7.3148, 7.2224],
        [6.9209, 6.9212, 6.9209],
        [6.9209, 6.9209, 6.9209],
        [6.9209, 6.9949, 6.9411]], grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.09226742386817932 
model_pd.l_d.mean(): -7.333505630493164 
model_pd.lagr.mean(): -7.241238117218018 
model_pd.lambdas: dict_items([('pout', tensor([1.6938])), ('power', tensor([0.4596]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0018])), ('power', tensor([-19.6055]))])
epoch：527	 i:0 	 global-step:10540	 l-p:0.09226742386817932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[ 6.9294,  8.8878, 10.1371],
        [ 6.9294,  7.0035,  6.9496],
        [ 6.9294,  6.9295,  6.9294],
        [ 6.9294,  6.9294,  6.9294]], grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.09210777282714844 
model_pd.l_d.mean(): -7.313146114349365 
model_pd.lagr.mean(): -7.221038341522217 
model_pd.lambdas: dict_items([('pout', tensor([1.6948])), ('power', tensor([0.4586]))]) 
model_pd.vars: dict_items([('pout', tensor([1.0002])), ('power', tensor([-19.5992]))])
epoch：528	 i:0 	 global-step:10560	 l-p:0.09210777282714844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 6.9380,  7.5955,  7.6053],
        [ 6.9380,  7.1441,  7.0445],
        [ 6.9380,  6.9384,  6.9380],
        [ 6.9380,  9.0536, 10.4986]], grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.09194839745759964 
model_pd.l_d.mean(): -7.292799472808838 
model_pd.lagr.mean(): -7.200850963592529 
model_pd.lambdas: dict_items([('pout', tensor([1.6958])), ('power', tensor([0.4576]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9986])), ('power', tensor([-19.5929]))])
epoch：529	 i:0 	 global-step:10580	 l-p:0.09194839745759964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.9467, 6.9472, 6.9467],
        [6.9467, 8.5793, 9.4431],
        [6.9467, 6.9470, 6.9467],
        [6.9467, 7.0187, 6.9660]], grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.09178928285837173 
model_pd.l_d.mean(): -7.2724714279174805 
model_pd.lagr.mean(): -7.180682182312012 
model_pd.lambdas: dict_items([('pout', tensor([1.6968])), ('power', tensor([0.4566]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9969])), ('power', tensor([-19.5865]))])
epoch：530	 i:0 	 global-step:10600	 l-p:0.09178928285837173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.9554, 8.6242, 9.5271],
        [6.9554, 8.5905, 9.4556],
        [6.9554, 8.1885, 8.6385],
        [6.9554, 7.0292, 6.9754]], grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.09163045883178711 
model_pd.l_d.mean(): -7.2521586418151855 
model_pd.lagr.mean(): -7.160528182983398 
model_pd.lambdas: dict_items([('pout', tensor([1.6977])), ('power', tensor([0.4557]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9953])), ('power', tensor([-19.5801]))])
epoch：531	 i:0 	 global-step:10620	 l-p:0.09163045883178711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[6.9642, 7.9720, 8.2222],
        [6.9642, 7.4845, 7.4287],
        [6.9642, 6.9869, 6.9672],
        [6.9642, 7.0381, 6.9843]], grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.09147188067436218 
model_pd.l_d.mean(): -7.2318620681762695 
model_pd.lagr.mean(): -7.140390396118164 
model_pd.lambdas: dict_items([('pout', tensor([1.6987])), ('power', tensor([0.4547]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9936])), ('power', tensor([-19.5736]))])
epoch：532	 i:0 	 global-step:10640	 l-p:0.09147188067436218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[6.9731, 6.9958, 6.9761],
        [6.9731, 7.3532, 7.2565],
        [6.9731, 6.9752, 6.9732],
        [6.9731, 7.0455, 6.9925]], grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.09131360799074173 
model_pd.l_d.mean(): -7.211583137512207 
model_pd.lagr.mean(): -7.120269298553467 
model_pd.lambdas: dict_items([('pout', tensor([1.6997])), ('power', tensor([0.4537]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9919])), ('power', tensor([-19.5670]))])
epoch：533	 i:0 	 global-step:10660	 l-p:0.09131360799074173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[6.9821, 7.3023, 7.1976],
        [6.9821, 7.6446, 7.6546],
        [6.9821, 6.9904, 6.9827],
        [6.9821, 7.0048, 6.9851]], grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.09115561842918396 
model_pd.l_d.mean(): -7.191320896148682 
model_pd.lagr.mean(): -7.100165367126465 
model_pd.lambdas: dict_items([('pout', tensor([1.7007])), ('power', tensor([0.4527]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9902])), ('power', tensor([-19.5604]))])
epoch：534	 i:0 	 global-step:10680	 l-p:0.09115561842918396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[6.9912, 6.9912, 6.9912],
        [6.9912, 7.5711, 7.5392],
        [6.9912, 6.9933, 6.9912],
        [6.9912, 8.7496, 9.7489]], grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.09099790453910828 
model_pd.l_d.mean(): -7.171075344085693 
model_pd.lagr.mean(): -7.080077648162842 
model_pd.lambdas: dict_items([('pout', tensor([1.7017])), ('power', tensor([0.4517]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9885])), ('power', tensor([-19.5537]))])
epoch：535	 i:0 	 global-step:10700	 l-p:0.09099790453910828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[7.0003, 7.0393, 7.0074],
        [7.0003, 8.0143, 8.2661],
        [7.0003, 7.0006, 7.0003],
        [7.0003, 7.0008, 7.0003]], grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.09084051102399826 
model_pd.l_d.mean(): -7.150846481323242 
model_pd.lagr.mean(): -7.060006141662598 
model_pd.lambdas: dict_items([('pout', tensor([1.7027])), ('power', tensor([0.4508]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9868])), ('power', tensor([-19.5470]))])
epoch：536	 i:0 	 global-step:10720	 l-p:0.09084051102399826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[7.0095, 7.0095, 7.0095],
        [7.0095, 7.5339, 7.4777],
        [7.0095, 7.0103, 7.0095],
        [7.0095, 7.0323, 7.0125]], grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.09068340063095093 
model_pd.l_d.mean(): -7.130636215209961 
model_pd.lagr.mean(): -7.039952754974365 
model_pd.lambdas: dict_items([('pout', tensor([1.7037])), ('power', tensor([0.4498]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9851])), ('power', tensor([-19.5401]))])
epoch：537	 i:0 	 global-step:10740	 l-p:0.09068340063095093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[7.0188, 8.0360, 8.2886],
        [7.0188, 7.4194, 7.3255],
        [7.0188, 7.0188, 7.0188],
        [7.0188, 7.2980, 7.1912]], grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.09052664041519165 
model_pd.l_d.mean(): -7.110441207885742 
model_pd.lagr.mean(): -7.019914627075195 
model_pd.lambdas: dict_items([('pout', tensor([1.7047])), ('power', tensor([0.4488]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9834])), ('power', tensor([-19.5333]))])
epoch：538	 i:0 	 global-step:10760	 l-p:0.09052664041519165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[7.0281, 7.6744, 7.6730],
        [7.0281, 7.1039, 7.0488],
        [7.0281, 7.0282, 7.0281],
        [7.0281, 7.0281, 7.0281]], grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.09037017822265625 
model_pd.l_d.mean(): -7.0902628898620605 
model_pd.lagr.mean(): -6.999892711639404 
model_pd.lambdas: dict_items([('pout', tensor([1.7056])), ('power', tensor([0.4478]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9816])), ('power', tensor([-19.5263]))])
epoch：539	 i:0 	 global-step:10780	 l-p:0.09037017822265625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.0375, 7.0818, 7.0462],
        [7.0375, 8.8098, 9.8171],
        [7.0375, 7.3608, 7.2552],
        [7.0375, 7.1134, 7.0583]], grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.0902140811085701 
model_pd.l_d.mean(): -7.070103645324707 
model_pd.lagr.mean(): -6.979889392852783 
model_pd.lambdas: dict_items([('pout', tensor([1.7066])), ('power', tensor([0.4469]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9799])), ('power', tensor([-19.5193]))])
epoch：540	 i:0 	 global-step:10800	 l-p:0.0902140811085701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[7.0470, 7.6954, 7.6941],
        [7.0470, 7.2177, 7.1245],
        [7.0470, 7.0470, 7.0470],
        [7.0470, 7.1227, 7.0676]], grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.09005830436944962 
model_pd.l_d.mean(): -7.04996395111084 
model_pd.lagr.mean(): -6.959905624389648 
model_pd.lambdas: dict_items([('pout', tensor([1.7076])), ('power', tensor([0.4459]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9781])), ('power', tensor([-19.5123]))])
epoch：541	 i:0 	 global-step:10820	 l-p:0.09005830436944962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[7.0566, 7.0571, 7.0566],
        [7.0566, 7.7277, 7.7379],
        [7.0566, 7.7061, 7.7047],
        [7.0566, 7.1317, 7.0770]], grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.08990288525819778 
model_pd.l_d.mean(): -7.029839515686035 
model_pd.lagr.mean(): -6.939936637878418 
model_pd.lambdas: dict_items([('pout', tensor([1.7086])), ('power', tensor([0.4449]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9763])), ('power', tensor([-19.5051]))])
epoch：542	 i:0 	 global-step:10840	 l-p:0.08990288525819778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.0662, 7.2770, 7.1752],
        [7.0662, 7.4905, 7.4005],
        [7.0662, 7.9671, 8.1249],
        [7.0662, 7.0662, 7.0662]], grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.08974780142307281 
model_pd.l_d.mean(): -7.0097336769104 
model_pd.lagr.mean(): -6.919985771179199 
model_pd.lambdas: dict_items([('pout', tensor([1.7096])), ('power', tensor([0.4439]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9745])), ('power', tensor([-19.4980]))])
epoch：543	 i:0 	 global-step:10860	 l-p:0.08974780142307281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[7.0759, 7.1205, 7.0847],
        [7.0759, 7.1155, 7.0831],
        [7.0759, 7.0844, 7.0765],
        [7.0759, 7.7069, 7.6950]], grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.0895930826663971 
model_pd.l_d.mean(): -6.989646911621094 
model_pd.lagr.mean(): -6.900053977966309 
model_pd.lambdas: dict_items([('pout', tensor([1.7105])), ('power', tensor([0.4430]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9727])), ('power', tensor([-19.4907]))])
epoch：544	 i:0 	 global-step:10880	 l-p:0.0895930826663971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[7.0857, 7.2575, 7.1638],
        [7.0857, 7.6752, 7.6430],
        [7.0857, 7.0878, 7.0858],
        [7.0857, 7.7602, 7.7705]], grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.08943870663642883 
model_pd.l_d.mean(): -6.969576835632324 
model_pd.lagr.mean(): -6.880137920379639 
model_pd.lambdas: dict_items([('pout', tensor([1.7115])), ('power', tensor([0.4420]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9709])), ('power', tensor([-19.4834]))])
epoch：545	 i:0 	 global-step:10900	 l-p:0.08943870663642883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[7.0956, 7.1066, 7.0965],
        [7.0956, 7.2946, 7.1945],
        [7.0956, 7.1711, 7.1161],
        [7.0956, 8.0690, 8.2794]], grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.08928468078374863 
model_pd.l_d.mean(): -6.949525833129883 
model_pd.lagr.mean(): -6.860240936279297 
model_pd.lambdas: dict_items([('pout', tensor([1.7125])), ('power', tensor([0.4410]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9691])), ('power', tensor([-19.4761]))])
epoch：546	 i:0 	 global-step:10920	 l-p:0.08928468078374863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[7.1055, 7.1062, 7.1055],
        [7.1055, 7.6970, 7.6646],
        [7.1055, 7.1076, 7.1055],
        [7.1055, 8.3797, 8.8505]], grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.08913101255893707 
model_pd.l_d.mean(): -6.929493427276611 
model_pd.lagr.mean(): -6.840362548828125 
model_pd.lambdas: dict_items([('pout', tensor([1.7134])), ('power', tensor([0.4400]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9672])), ('power', tensor([-19.4686]))])
epoch：547	 i:0 	 global-step:10940	 l-p:0.08913101255893707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 7.1155,  7.1155,  7.1155],
        [ 7.1155,  7.3677,  7.2604],
        [ 7.1155,  7.7508,  7.7388],
        [ 7.1155,  9.2950, 10.7843]], grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.08897768706083298 
model_pd.l_d.mean(): -6.909478187561035 
model_pd.lagr.mean(): -6.820500373840332 
model_pd.lambdas: dict_items([('pout', tensor([1.7144])), ('power', tensor([0.4391]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9654])), ('power', tensor([-19.4612]))])
epoch：548	 i:0 	 global-step:10960	 l-p:0.08897768706083298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[7.1255, 7.2022, 7.1464],
        [7.1255, 7.1255, 7.1255],
        [7.1255, 7.7619, 7.7500],
        [7.1255, 8.3948, 8.8584]], grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.08882468938827515 
model_pd.l_d.mean(): -6.88948392868042 
model_pd.lagr.mean(): -6.8006591796875 
model_pd.lambdas: dict_items([('pout', tensor([1.7154])), ('power', tensor([0.4381]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9635])), ('power', tensor([-19.4536]))])
epoch：549	 i:0 	 global-step:10980	 l-p:0.08882468938827515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[7.1356, 8.1733, 8.4313],
        [7.1356, 8.9373, 9.9616],
        [7.1356, 7.1468, 7.1366],
        [7.1356, 7.7940, 7.7928]], grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.08867206424474716 
model_pd.l_d.mean(): -6.8695068359375 
model_pd.lagr.mean(): -6.780834674835205 
model_pd.lambdas: dict_items([('pout', tensor([1.7163])), ('power', tensor([0.4371]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9617])), ('power', tensor([-19.4460]))])
epoch：550	 i:0 	 global-step:11000	 l-p:0.08867206424474716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[7.1458, 8.4302, 8.9057],
        [7.1458, 7.1520, 7.1462],
        [7.1458, 7.3994, 7.2915],
        [7.1458, 8.1853, 8.4437]], grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.08851975202560425 
model_pd.l_d.mean(): -6.849549770355225 
model_pd.lagr.mean(): -6.761030197143555 
model_pd.lambdas: dict_items([('pout', tensor([1.7173])), ('power', tensor([0.4361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9598])), ('power', tensor([-19.4384]))])
epoch：551	 i:0 	 global-step:11020	 l-p:0.08851975202560425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 7.1561,  7.3019,  7.2154],
        [ 7.1561,  7.1561,  7.1561],
        [ 7.1561,  7.4101,  7.3021],
        [ 7.1561,  9.3503, 10.8499]], grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.088367760181427 
model_pd.l_d.mean(): -6.829611778259277 
model_pd.lagr.mean(): -6.741243839263916 
model_pd.lambdas: dict_items([('pout', tensor([1.7182])), ('power', tensor([0.4352]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9579])), ('power', tensor([-19.4307]))])
epoch：552	 i:0 	 global-step:11040	 l-p:0.088367760181427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[7.1664, 7.5775, 7.4813],
        [7.1664, 7.2067, 7.1738],
        [7.1664, 7.5981, 7.5066],
        [7.1664, 8.8955, 9.8317]], grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.08821610361337662 
model_pd.l_d.mean(): -6.8096923828125 
model_pd.lagr.mean(): -6.721476078033447 
model_pd.lambdas: dict_items([('pout', tensor([1.7192])), ('power', tensor([0.4342]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9560])), ('power', tensor([-19.4229]))])
epoch：553	 i:0 	 global-step:11060	 l-p:0.08821610361337662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[7.1769, 7.1769, 7.1769],
        [7.1769, 7.2519, 7.1969],
        [7.1769, 7.7166, 7.6589],
        [7.1769, 8.9089, 9.8467]], grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.08806474506855011 
model_pd.l_d.mean(): -6.789792060852051 
model_pd.lagr.mean(): -6.701727390289307 
model_pd.lambdas: dict_items([('pout', tensor([1.7202])), ('power', tensor([0.4332]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9541])), ('power', tensor([-19.4151]))])
epoch：554	 i:0 	 global-step:11080	 l-p:0.08806474506855011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[7.1873, 7.7872, 7.7545],
        [7.1873, 7.4427, 7.3341],
        [7.1873, 7.2277, 7.1947],
        [7.1873, 7.1877, 7.1873]], grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.08791372179985046 
model_pd.l_d.mean(): -6.7699127197265625 
model_pd.lagr.mean(): -6.681999206542969 
model_pd.lambdas: dict_items([('pout', tensor([1.7211])), ('power', tensor([0.4323]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9522])), ('power', tensor([-19.4072]))])
epoch：555	 i:0 	 global-step:11100	 l-p:0.08791372179985046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[7.1979, 7.4860, 7.3759],
        [7.1979, 7.4005, 7.2987],
        [7.1979, 8.2464, 8.5073],
        [7.1979, 7.2001, 7.1980]], grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.0877629965543747 
model_pd.l_d.mean(): -6.750051021575928 
model_pd.lagr.mean(): -6.662288188934326 
model_pd.lambdas: dict_items([('pout', tensor([1.7221])), ('power', tensor([0.4313]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9502])), ('power', tensor([-19.3992]))])
epoch：556	 i:0 	 global-step:11120	 l-p:0.0877629965543747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[ 7.2085,  7.2088,  7.2085],
        [ 7.2085,  7.2856,  7.2294],
        [ 7.2085,  9.4217, 10.9344],
        [ 7.2085,  7.2107,  7.2086]], grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.0876125618815422 
model_pd.l_d.mean(): -6.730210304260254 
model_pd.lagr.mean(): -6.642597675323486 
model_pd.lambdas: dict_items([('pout', tensor([1.7230])), ('power', tensor([0.4303]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9483])), ('power', tensor([-19.3912]))])
epoch：557	 i:0 	 global-step:11140	 l-p:0.0876125618815422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[7.2192, 7.5528, 7.4439],
        [7.2192, 7.2279, 7.2198],
        [7.2192, 8.2715, 8.5333],
        [7.2192, 7.8223, 7.7895]], grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.0874624252319336 
model_pd.l_d.mean(): -6.71038818359375 
model_pd.lagr.mean(): -6.622925758361816 
model_pd.lambdas: dict_items([('pout', tensor([1.7240])), ('power', tensor([0.4294]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9464])), ('power', tensor([-19.3831]))])
epoch：558	 i:0 	 global-step:11160	 l-p:0.0874624252319336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[7.2299, 7.2304, 7.2299],
        [7.2299, 7.2299, 7.2299],
        [7.2299, 7.6272, 7.5264],
        [7.2299, 7.6663, 7.5739]], grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.08731256425380707 
model_pd.l_d.mean(): -6.690587043762207 
model_pd.lagr.mean(): -6.603274345397949 
model_pd.lambdas: dict_items([('pout', tensor([1.7249])), ('power', tensor([0.4284]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9444])), ('power', tensor([-19.3750]))])
epoch：559	 i:0 	 global-step:11180	 l-p:0.08731256425380707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[7.2408, 7.4985, 7.3889],
        [7.2408, 7.2408, 7.2408],
        [7.2408, 7.6388, 7.5378],
        [7.2408, 8.5456, 9.0289]], grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.08716300129890442 
model_pd.l_d.mean(): -6.67080545425415 
model_pd.lagr.mean(): -6.583642482757568 
model_pd.lambdas: dict_items([('pout', tensor([1.7258])), ('power', tensor([0.4274]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9424])), ('power', tensor([-19.3668]))])
epoch：560	 i:0 	 global-step:11200	 l-p:0.08716300129890442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[7.2517, 7.8581, 7.8251],
        [7.2517, 8.9698, 9.8798],
        [7.2517, 8.2510, 8.4674],
        [7.2517, 7.5871, 7.4776]], grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.08701372146606445 
model_pd.l_d.mean(): -6.65104341506958 
model_pd.lagr.mean(): -6.564029693603516 
model_pd.lambdas: dict_items([('pout', tensor([1.7268])), ('power', tensor([0.4264]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9405])), ('power', tensor([-19.3585]))])
epoch：561	 i:0 	 global-step:11220	 l-p:0.08701372146606445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 7.2626,  7.9353,  7.9342],
        [ 7.2626,  7.3416,  7.2843],
        [ 7.2626,  7.4807,  7.3754],
        [ 7.2626,  9.4954, 11.0218]], grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.08686468005180359 
model_pd.l_d.mean(): -6.631302356719971 
model_pd.lagr.mean(): -6.544437885284424 
model_pd.lambdas: dict_items([('pout', tensor([1.7277])), ('power', tensor([0.4255]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9385])), ('power', tensor([-19.3502]))])
epoch：562	 i:0 	 global-step:11240	 l-p:0.08686468005180359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[7.2737, 7.2824, 7.2743],
        [7.2737, 7.2737, 7.2737],
        [7.2737, 8.5746, 9.0502],
        [7.2737, 7.2738, 7.2737]], grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.086715929210186 
model_pd.l_d.mean(): -6.611583709716797 
model_pd.lagr.mean(): -6.524868011474609 
model_pd.lambdas: dict_items([('pout', tensor([1.7287])), ('power', tensor([0.4245]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9365])), ('power', tensor([-19.3418]))])
epoch：563	 i:0 	 global-step:11260	 l-p:0.086715929210186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[7.2848, 7.3259, 7.2923],
        [7.2848, 8.4817, 8.8605],
        [7.2848, 9.0122, 9.9273],
        [7.2848, 7.9386, 7.9266]], grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.08656743168830872 
model_pd.l_d.mean(): -6.59188175201416 
model_pd.lagr.mean(): -6.505314350128174 
model_pd.lambdas: dict_items([('pout', tensor([1.7296])), ('power', tensor([0.4235]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9345])), ('power', tensor([-19.3333]))])
epoch：564	 i:0 	 global-step:11280	 l-p:0.08656743168830872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[7.2960, 8.0420, 8.0799],
        [7.2960, 7.2965, 7.2960],
        [7.2960, 7.5562, 7.4455],
        [7.2960, 8.6112, 9.0977]], grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.08641917258501053 
model_pd.l_d.mean(): -6.572203636169434 
model_pd.lagr.mean(): -6.485784530639648 
model_pd.lambdas: dict_items([('pout', tensor([1.7305])), ('power', tensor([0.4226]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9325])), ('power', tensor([-19.3248]))])
epoch：565	 i:0 	 global-step:11300	 l-p:0.08641917258501053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[ 7.3072,  7.5269,  7.4209],
        [ 7.3072,  7.7097,  7.6077],
        [ 7.3072,  9.1603, 10.2145],
        [ 7.3072,  7.9849,  7.9839]], grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.08627115935087204 
model_pd.l_d.mean(): -6.552542686462402 
model_pd.lagr.mean(): -6.46627140045166 
model_pd.lambdas: dict_items([('pout', tensor([1.7315])), ('power', tensor([0.4216]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9304])), ('power', tensor([-19.3163]))])
epoch：566	 i:0 	 global-step:11320	 l-p:0.08627115935087204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228]])
 pt:tensor([[ 7.3185,  9.5716, 11.1121],
        [ 7.3185,  8.0201,  8.0312],
        [ 7.3185,  7.6126,  7.5003],
        [ 7.3185,  8.0674,  8.1054]], grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.08612339943647385 
model_pd.l_d.mean(): -6.532905101776123 
model_pd.lagr.mean(): -6.446781635284424 
model_pd.lambdas: dict_items([('pout', tensor([1.7324])), ('power', tensor([0.4206]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9284])), ('power', tensor([-19.3076]))])
epoch：567	 i:0 	 global-step:11340	 l-p:0.08612339943647385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[7.3299, 8.0328, 8.0440],
        [7.3299, 7.4098, 7.3518],
        [7.3299, 8.0102, 8.0092],
        [7.3299, 7.5505, 7.4440]], grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.08597586303949356 
model_pd.l_d.mean(): -6.5132880210876465 
model_pd.lagr.mean(): -6.42731237411499 
model_pd.lambdas: dict_items([('pout', tensor([1.7333])), ('power', tensor([0.4197]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9264])), ('power', tensor([-19.2989]))])
epoch：568	 i:0 	 global-step:11360	 l-p:0.08597586303949356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[7.3414, 7.7648, 7.6659],
        [7.3414, 7.3436, 7.3415],
        [7.3414, 7.6366, 7.5239],
        [7.3414, 7.3414, 7.3414]], grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.08582855015993118 
model_pd.l_d.mean(): -6.493690490722656 
model_pd.lagr.mean(): -6.407861709594727 
model_pd.lambdas: dict_items([('pout', tensor([1.7342])), ('power', tensor([0.4187]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9243])), ('power', tensor([-19.2902]))])
epoch：569	 i:0 	 global-step:11380	 l-p:0.08582855015993118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[7.3529, 7.3529, 7.3529],
        [7.3529, 8.6820, 9.1746],
        [7.3529, 7.3552, 7.3530],
        [7.3529, 7.3529, 7.3529]], grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.0856814980506897 
model_pd.l_d.mean(): -6.474115371704102 
model_pd.lagr.mean(): -6.388433933258057 
model_pd.lambdas: dict_items([('pout', tensor([1.7352])), ('power', tensor([0.4178]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9222])), ('power', tensor([-19.2813]))])
epoch：570	 i:0 	 global-step:11400	 l-p:0.0856814980506897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[ 7.3645,  9.2349, 10.2991],
        [ 7.3645,  8.6962,  9.1897],
        [ 7.3645,  7.3645,  7.3645],
        [ 7.3645,  7.3888,  7.3677]], grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.08553464710712433 
model_pd.l_d.mean(): -6.454561710357666 
model_pd.lagr.mean(): -6.369027137756348 
model_pd.lambdas: dict_items([('pout', tensor([1.7361])), ('power', tensor([0.4168]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9202])), ('power', tensor([-19.2725]))])
epoch：571	 i:0 	 global-step:11420	 l-p:0.08553464710712433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[7.3762, 7.4563, 7.3981],
        [7.3762, 7.5985, 7.4912],
        [7.3762, 8.5912, 8.9760],
        [7.3762, 7.3762, 7.3762]], grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.08538801968097687 
model_pd.l_d.mean(): -6.435030937194824 
model_pd.lagr.mean(): -6.349642753601074 
model_pd.lambdas: dict_items([('pout', tensor([1.7370])), ('power', tensor([0.4158]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9181])), ('power', tensor([-19.2635]))])
epoch：572	 i:0 	 global-step:11440	 l-p:0.08538801968097687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[7.3879, 7.3879, 7.3879],
        [7.3879, 7.7312, 7.6193],
        [7.3879, 8.7134, 9.1983],
        [7.3879, 7.3881, 7.3879]], grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.08524157851934433 
model_pd.l_d.mean(): -6.415519714355469 
model_pd.lagr.mean(): -6.330277919769287 
model_pd.lambdas: dict_items([('pout', tensor([1.7379])), ('power', tensor([0.4149]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9160])), ('power', tensor([-19.2545]))])
epoch：573	 i:0 	 global-step:11460	 l-p:0.08524157851934433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[7.3998, 7.4778, 7.4207],
        [7.3998, 8.4238, 8.6459],
        [7.3998, 8.0662, 8.0541],
        [7.3998, 7.9601, 7.9006]], grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.08509533852338791 
model_pd.l_d.mean(): -6.396030426025391 
model_pd.lagr.mean(): -6.310935020446777 
model_pd.lambdas: dict_items([('pout', tensor([1.7388])), ('power', tensor([0.4139]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9139])), ('power', tensor([-19.2454]))])
epoch：574	 i:0 	 global-step:11480	 l-p:0.08509533852338791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[7.4116, 7.4536, 7.4193],
        [7.4116, 7.4125, 7.4117],
        [7.4116, 7.4120, 7.4117],
        [7.4116, 7.4120, 7.4117]], grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.08494929224252701 
model_pd.l_d.mean(): -6.376563549041748 
model_pd.lagr.mean(): -6.291614055633545 
model_pd.lambdas: dict_items([('pout', tensor([1.7397])), ('power', tensor([0.4129]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9118])), ('power', tensor([-19.2363]))])
epoch：575	 i:0 	 global-step:11500	 l-p:0.08494929224252701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[7.4236, 7.4239, 7.4236],
        [7.4236, 8.1375, 8.1490],
        [7.4236, 7.8529, 7.7527],
        [7.4236, 8.7680, 9.2665]], grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.08480344712734222 
model_pd.l_d.mean(): -6.357117652893066 
model_pd.lagr.mean(): -6.272314071655273 
model_pd.lambdas: dict_items([('pout', tensor([1.7406])), ('power', tensor([0.4120]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9097])), ('power', tensor([-19.2271]))])
epoch：576	 i:0 	 global-step:11520	 l-p:0.08480344712734222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 7.4356,  7.5141,  7.4567],
        [ 7.4356,  9.5642, 10.9240],
        [ 7.4356,  8.1992,  8.2381],
        [ 7.4356,  9.2421, 10.2211]], grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.08465776592493057 
model_pd.l_d.mean(): -6.337695598602295 
model_pd.lagr.mean(): -6.253037929534912 
model_pd.lambdas: dict_items([('pout', tensor([1.7415])), ('power', tensor([0.4110]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9075])), ('power', tensor([-19.2178]))])
epoch：577	 i:0 	 global-step:11540	 l-p:0.08465776592493057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[7.4477, 7.4568, 7.4484],
        [7.4477, 7.4724, 7.4510],
        [7.4477, 7.6728, 7.5642],
        [7.4477, 8.7861, 9.2759]], grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.08451223373413086 
model_pd.l_d.mean(): -6.318293571472168 
model_pd.lagr.mean(): -6.233781337738037 
model_pd.lambdas: dict_items([('pout', tensor([1.7425])), ('power', tensor([0.4101]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9054])), ('power', tensor([-19.2085]))])
epoch：578	 i:0 	 global-step:11560	 l-p:0.08451223373413086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[7.4599, 8.4941, 8.7185],
        [7.4599, 8.2265, 8.2657],
        [7.4599, 7.4607, 7.4599],
        [7.4599, 7.6135, 7.5224]], grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.08436688780784607 
model_pd.l_d.mean(): -6.298914432525635 
model_pd.lagr.mean(): -6.214547634124756 
model_pd.lambdas: dict_items([('pout', tensor([1.7434])), ('power', tensor([0.4091]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9032])), ('power', tensor([-19.1991]))])
epoch：579	 i:0 	 global-step:11580	 l-p:0.08436688780784607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[7.4722, 7.4812, 7.4728],
        [7.4722, 7.5540, 7.4946],
        [7.4722, 7.4730, 7.4722],
        [7.4722, 8.2402, 8.2795]], grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.08422166854143143 
model_pd.l_d.mean(): -6.279557228088379 
model_pd.lagr.mean(): -6.195335388183594 
model_pd.lambdas: dict_items([('pout', tensor([1.7443])), ('power', tensor([0.4081]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9011])), ('power', tensor([-19.1897]))])
epoch：580	 i:0 	 global-step:11600	 l-p:0.08422166854143143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[7.4845, 7.7109, 7.6017],
        [7.4845, 7.4845, 7.4845],
        [7.4845, 8.8307, 9.3235],
        [7.4845, 8.2541, 8.2935]], grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.08407660573720932 
model_pd.l_d.mean(): -6.260223388671875 
model_pd.lagr.mean(): -6.176146984100342 
model_pd.lambdas: dict_items([('pout', tensor([1.7452])), ('power', tensor([0.4072]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8989])), ('power', tensor([-19.1801]))])
epoch：581	 i:0 	 global-step:11620	 l-p:0.08407660573720932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[7.4969, 8.2193, 8.2311],
        [7.4969, 7.4969, 7.4969],
        [7.4969, 7.4969, 7.4969],
        [7.4969, 8.5982, 8.8729]], grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.08393169194459915 
model_pd.l_d.mean(): -6.24091100692749 
model_pd.lagr.mean(): -6.156979084014893 
model_pd.lambdas: dict_items([('pout', tensor([1.7461])), ('power', tensor([0.4062]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8967])), ('power', tensor([-19.1705]))])
epoch：582	 i:0 	 global-step:11640	 l-p:0.08393169194459915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[7.5093, 7.5094, 7.5093],
        [7.5093, 7.8130, 7.6971],
        [7.5093, 7.8596, 7.7455],
        [7.5093, 7.5093, 7.5093]], grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.08378688991069794 
model_pd.l_d.mean(): -6.221623420715332 
model_pd.lagr.mean(): -6.137836456298828 
model_pd.lambdas: dict_items([('pout', tensor([1.7469])), ('power', tensor([0.4053]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8946])), ('power', tensor([-19.1609]))])
epoch：583	 i:0 	 global-step:11660	 l-p:0.08378688991069794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[ 7.5219,  7.5310,  7.5225],
        [ 7.5219,  9.6796, 11.0584],
        [ 7.5219,  8.2962,  8.3359],
        [ 7.5219,  8.6276,  8.9035]], grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.08364217728376389 
model_pd.l_d.mean(): -6.202357292175293 
model_pd.lagr.mean(): -6.118715286254883 
model_pd.lambdas: dict_items([('pout', tensor([1.7478])), ('power', tensor([0.4043]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8924])), ('power', tensor([-19.1512]))])
epoch：584	 i:0 	 global-step:11680	 l-p:0.08364217728376389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[7.5345, 7.9936, 7.8967],
        [7.5345, 7.6167, 7.5569],
        [7.5345, 8.1700, 8.1359],
        [7.5345, 7.5346, 7.5345]], grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.08349758386611938 
model_pd.l_d.mean(): -6.183114051818848 
model_pd.lagr.mean(): -6.099616527557373 
model_pd.lambdas: dict_items([('pout', tensor([1.7487])), ('power', tensor([0.4033]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8902])), ('power', tensor([-19.1414]))])
epoch：585	 i:0 	 global-step:11700	 l-p:0.08349758386611938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[ 7.5471,  9.3858, 10.3827],
        [ 7.5471,  7.5471,  7.5472],
        [ 7.5471,  7.5472,  7.5471],
        [ 7.5471,  7.6289,  7.5693]], grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.08335307985544205 
model_pd.l_d.mean(): -6.163893699645996 
model_pd.lagr.mean(): -6.080540657043457 
model_pd.lambdas: dict_items([('pout', tensor([1.7496])), ('power', tensor([0.4024]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8880])), ('power', tensor([-19.1315]))])
epoch：586	 i:0 	 global-step:11720	 l-p:0.08335307985544205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[ 7.5599,  7.5604,  7.5599],
        [ 7.5599,  9.9008, 11.5024],
        [ 7.5599,  7.6400,  7.5814],
        [ 7.5599,  7.7753,  7.6672]], grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.08320865780115128 
model_pd.l_d.mean(): -6.144697189331055 
model_pd.lagr.mean(): -6.061488628387451 
model_pd.lambdas: dict_items([('pout', tensor([1.7505])), ('power', tensor([0.4014]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8857])), ('power', tensor([-19.1216]))])
epoch：587	 i:0 	 global-step:11740	 l-p:0.08320865780115128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[ 7.5727,  7.7886,  7.6803],
        [ 7.5727,  9.7477, 11.1378],
        [ 7.5727,  9.4188, 10.4198],
        [ 7.5727,  7.8453,  7.7296]], grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.08306429535150528 
model_pd.l_d.mean(): -6.125523567199707 
model_pd.lagr.mean(): -6.042459487915039 
model_pd.lambdas: dict_items([('pout', tensor([1.7514])), ('power', tensor([0.4005]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8835])), ('power', tensor([-19.1116]))])
epoch：588	 i:0 	 global-step:11760	 l-p:0.08306429535150528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[7.5856, 8.8423, 9.2408],
        [7.5856, 7.6289, 7.5935],
        [7.5856, 7.5856, 7.5856],
        [7.5856, 7.5856, 7.5856]], grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.08291997760534286 
model_pd.l_d.mean(): -6.1063737869262695 
model_pd.lagr.mean(): -6.023453712463379 
model_pd.lambdas: dict_items([('pout', tensor([1.7523])), ('power', tensor([0.3995]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8813])), ('power', tensor([-19.1016]))])
epoch：589	 i:0 	 global-step:11780	 l-p:0.08291997760534286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[7.5986, 7.5987, 7.5986],
        [7.5986, 7.6810, 7.6210],
        [7.5986, 7.6818, 7.6213],
        [7.5986, 8.0405, 7.9375]], grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.08277574181556702 
model_pd.l_d.mean(): -6.087247371673584 
model_pd.lagr.mean(): -6.004471778869629 
model_pd.lambdas: dict_items([('pout', tensor([1.7531])), ('power', tensor([0.3986]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8790])), ('power', tensor([-19.0914]))])
epoch：590	 i:0 	 global-step:11800	 l-p:0.08277574181556702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[7.6117, 7.6117, 7.6117],
        [7.6117, 7.6606, 7.6213],
        [7.6117, 7.8860, 7.7695],
        [7.6117, 7.6184, 7.6121]], grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.08263152837753296 
model_pd.l_d.mean(): -6.068143367767334 
model_pd.lagr.mean(): -5.985511779785156 
model_pd.lambdas: dict_items([('pout', tensor([1.7540])), ('power', tensor([0.3976]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8768])), ('power', tensor([-19.0812]))])
epoch：591	 i:0 	 global-step:11820	 l-p:0.08263152837753296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[7.6248, 8.0491, 7.9418],
        [7.6248, 7.7088, 7.6478],
        [7.6248, 7.8997, 7.7830],
        [7.6248, 7.6254, 7.6248]], grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.0824873149394989 
model_pd.l_d.mean(): -6.049065113067627 
model_pd.lagr.mean(): -5.966578006744385 
model_pd.lambdas: dict_items([('pout', tensor([1.7549])), ('power', tensor([0.3967]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8745])), ('power', tensor([-19.0710]))])
epoch：592	 i:0 	 global-step:11840	 l-p:0.0824873149394989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[7.6380, 7.7962, 7.7025],
        [7.6380, 7.8562, 7.7467],
        [7.6380, 7.6404, 7.6381],
        [7.6380, 7.9135, 7.7966]], grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.08234312385320663 
model_pd.l_d.mean(): -6.030008316040039 
model_pd.lagr.mean(): -5.947665214538574 
model_pd.lambdas: dict_items([('pout', tensor([1.7558])), ('power', tensor([0.3957]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8722])), ('power', tensor([-19.0607]))])
epoch：593	 i:0 	 global-step:11860	 l-p:0.08234312385320663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[7.6513, 8.3455, 8.3335],
        [7.6513, 8.6432, 8.8183],
        [7.6513, 7.6768, 7.6547],
        [7.6513, 7.9621, 7.8436]], grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.08219892531633377 
model_pd.l_d.mean(): -6.0109782218933105 
model_pd.lagr.mean(): -5.928779125213623 
model_pd.lambdas: dict_items([('pout', tensor([1.7566])), ('power', tensor([0.3948]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8699])), ('power', tensor([-19.0502]))])
epoch：594	 i:0 	 global-step:11880	 l-p:0.08219892531633377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[ 7.6647,  9.5374, 10.5532],
        [ 7.6647,  9.0616,  9.5803],
        [ 7.6647,  7.8979,  7.7855],
        [ 7.6647,  7.6652,  7.6647]], grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.08205471187829971 
model_pd.l_d.mean(): -5.991971015930176 
model_pd.lagr.mean(): -5.909916400909424 
model_pd.lambdas: dict_items([('pout', tensor([1.7575])), ('power', tensor([0.3938]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8676])), ('power', tensor([-19.0398]))])
epoch：595	 i:0 	 global-step:11900	 l-p:0.08205471187829971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[7.6781, 8.7489, 8.9819],
        [7.6781, 9.0663, 9.5749],
        [7.6781, 7.6790, 7.6781],
        [7.6781, 8.0382, 7.9210]], grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.08191046863794327 
model_pd.l_d.mean(): -5.972987651824951 
model_pd.lagr.mean(): -5.891077041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.7584])), ('power', tensor([0.3929]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8653])), ('power', tensor([-19.0292]))])
epoch：596	 i:0 	 global-step:11920	 l-p:0.08191046863794327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[7.6916, 8.3434, 8.3087],
        [7.6916, 7.7765, 7.7149],
        [7.6916, 7.8820, 7.7783],
        [7.6916, 7.8512, 7.7567]], grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.08176616579294205 
model_pd.l_d.mean(): -5.954029560089111 
model_pd.lagr.mean(): -5.872263431549072 
model_pd.lambdas: dict_items([('pout', tensor([1.7592])), ('power', tensor([0.3919]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8630])), ('power', tensor([-19.0186]))])
epoch：597	 i:0 	 global-step:11940	 l-p:0.08176616579294205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[7.7052, 7.7549, 7.7150],
        [7.7052, 8.4523, 8.4649],
        [7.7052, 7.7903, 7.7286],
        [7.7052, 7.7058, 7.7053]], grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.08162179589271545 
model_pd.l_d.mean(): -5.935094833374023 
model_pd.lagr.mean(): -5.85347318649292 
model_pd.lambdas: dict_items([('pout', tensor([1.7601])), ('power', tensor([0.3910]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8607])), ('power', tensor([-19.0079]))])
epoch：598	 i:0 	 global-step:11960	 l-p:0.08162179589271545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[7.7189, 7.7687, 7.7287],
        [7.7189, 8.8597, 9.1449],
        [7.7189, 7.7195, 7.7189],
        [7.7189, 8.7966, 9.0311]], grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.08147736638784409 
model_pd.l_d.mean(): -5.91618537902832 
model_pd.lagr.mean(): -5.834708213806152 
model_pd.lambdas: dict_items([('pout', tensor([1.7610])), ('power', tensor([0.3900]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8584])), ('power', tensor([-18.9972]))])
epoch：599	 i:0 	 global-step:11980	 l-p:0.08147736638784409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[7.7327, 9.1326, 9.6458],
        [7.7327, 8.0124, 7.8937],
        [7.7327, 7.8182, 7.7561],
        [7.7327, 8.1842, 8.0791]], grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.08133284747600555 
model_pd.l_d.mean(): -5.89730167388916 
model_pd.lagr.mean(): -5.815968990325928 
model_pd.lambdas: dict_items([('pout', tensor([1.7618])), ('power', tensor([0.3891]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8560])), ('power', tensor([-18.9863]))])
epoch：600	 i:0 	 global-step:12000	 l-p:0.08133284747600555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[7.7465, 9.0352, 9.4444],
        [7.7465, 7.7589, 7.7476],
        [7.7465, 7.7965, 7.7563],
        [7.7465, 7.7534, 7.7469]], grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.08118820190429688 
model_pd.l_d.mean(): -5.878440856933594 
model_pd.lagr.mean(): -5.797252655029297 
model_pd.lambdas: dict_items([('pout', tensor([1.7627])), ('power', tensor([0.3881]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8537])), ('power', tensor([-18.9754]))])
epoch：601	 i:0 	 global-step:12020	 l-p:0.08118820190429688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 7.7605,  8.2139,  8.1084],
        [ 7.7605,  7.7605,  7.7605],
        [ 7.7605,  9.6227, 10.6109],
        [ 7.7605,  7.8050,  7.7686]], grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.08104345202445984 
model_pd.l_d.mean(): -5.85960578918457 
model_pd.lagr.mean(): -5.778562545776367 
model_pd.lambdas: dict_items([('pout', tensor([1.7635])), ('power', tensor([0.3872]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8513])), ('power', tensor([-18.9644]))])
epoch：602	 i:0 	 global-step:12040	 l-p:0.08104345202445984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[7.7745, 7.7753, 7.7745],
        [7.7745, 7.7814, 7.7749],
        [7.7745, 7.8605, 7.7981],
        [7.7745, 8.0915, 7.9707]], grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.08089855313301086 
model_pd.l_d.mean(): -5.840795993804932 
model_pd.lagr.mean(): -5.759897232055664 
model_pd.lambdas: dict_items([('pout', tensor([1.7644])), ('power', tensor([0.3862]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8490])), ('power', tensor([-18.9534]))])
epoch：603	 i:0 	 global-step:12060	 l-p:0.08089855313301086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[7.7886, 7.7886, 7.7886],
        [7.7886, 8.4504, 8.4153],
        [7.7886, 7.7910, 7.7886],
        [7.7886, 7.7886, 7.7886]], grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.08075347542762756 
model_pd.l_d.mean(): -5.822011947631836 
model_pd.lagr.mean(): -5.74125862121582 
model_pd.lambdas: dict_items([('pout', tensor([1.7652])), ('power', tensor([0.3853]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8466])), ('power', tensor([-18.9422]))])
epoch：604	 i:0 	 global-step:12080	 l-p:0.08075347542762756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 7.8027,  7.8123,  7.8034],
        [ 7.8027,  7.8027,  7.8027],
        [ 7.8027,  9.6770, 10.6717],
        [ 7.8027,  9.2179,  9.7369]], grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.08060823380947113 
model_pd.l_d.mean(): -5.803251266479492 
model_pd.lagr.mean(): -5.72264289855957 
model_pd.lambdas: dict_items([('pout', tensor([1.7661])), ('power', tensor([0.3843]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8442])), ('power', tensor([-18.9310]))])
epoch：605	 i:0 	 global-step:12100	 l-p:0.08060823380947113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[7.8170, 8.0416, 7.9290],
        [7.8170, 9.1198, 9.5336],
        [7.8170, 7.8170, 7.8170],
        [7.8170, 8.2545, 8.1441]], grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.08046280592679977 
model_pd.l_d.mean(): -5.784518241882324 
model_pd.lagr.mean(): -5.704055309295654 
model_pd.lambdas: dict_items([('pout', tensor([1.7669])), ('power', tensor([0.3834]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8418])), ('power', tensor([-18.9197]))])
epoch：606	 i:0 	 global-step:12120	 l-p:0.08046280592679977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[ 7.8313,  8.5688,  8.5688],
        [ 7.8313,  9.7524, 10.7951],
        [ 7.8313,  7.8313,  7.8313],
        [ 7.8313,  7.9177,  7.8549]], grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.08031713217496872 
model_pd.l_d.mean(): -5.765809059143066 
model_pd.lagr.mean(): -5.685492038726807 
model_pd.lambdas: dict_items([('pout', tensor([1.7677])), ('power', tensor([0.3824]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8394])), ('power', tensor([-18.9083]))])
epoch：607	 i:0 	 global-step:12140	 l-p:0.08031713217496872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[ 7.8458,  7.8458,  7.8458],
        [ 7.8458,  9.7710, 10.8160],
        [ 7.8458,  8.4478,  8.3846],
        [ 7.8458,  7.8458,  7.8458]], grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.08017124235630035 
model_pd.l_d.mean(): -5.747127056121826 
model_pd.lagr.mean(): -5.666955947875977 
model_pd.lambdas: dict_items([('pout', tensor([1.7686])), ('power', tensor([0.3815]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8370])), ('power', tensor([-18.8969]))])
epoch：608	 i:0 	 global-step:12160	 l-p:0.08017124235630035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[7.8603, 7.9475, 7.8842],
        [7.8603, 8.8850, 9.0665],
        [7.8603, 9.3000, 9.8352],
        [7.8603, 8.4637, 8.4003]], grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.08002505451440811 
model_pd.l_d.mean(): -5.7284698486328125 
model_pd.lagr.mean(): -5.648444652557373 
model_pd.lambdas: dict_items([('pout', tensor([1.7694])), ('power', tensor([0.3805]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8345])), ('power', tensor([-18.8853]))])
epoch：609	 i:0 	 global-step:12180	 l-p:0.08002505451440811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[ 7.8749,  7.9592,  7.8975],
        [ 7.8749, 10.3309, 12.0127],
        [ 7.8749,  7.8753,  7.8749],
        [ 7.8749,  7.9014,  7.8784]], grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.07987861335277557 
model_pd.l_d.mean(): -5.7098388671875 
model_pd.lagr.mean(): -5.629960060119629 
model_pd.lambdas: dict_items([('pout', tensor([1.7702])), ('power', tensor([0.3796]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8321])), ('power', tensor([-18.8737]))])
epoch：610	 i:0 	 global-step:12200	 l-p:0.07987861335277557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[ 7.8896,  9.7886, 10.7968],
        [ 7.8896,  9.3340,  9.8702],
        [ 7.8896,  8.2124,  8.0895],
        [ 7.8896,  7.9741,  7.9123]], grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.07973186671733856 
model_pd.l_d.mean(): -5.6912336349487305 
model_pd.lagr.mean(): -5.611501693725586 
model_pd.lambdas: dict_items([('pout', tensor([1.7711])), ('power', tensor([0.3786]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8296])), ('power', tensor([-18.8620]))])
epoch：611	 i:0 	 global-step:12220	 l-p:0.07973186671733856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[7.9043, 8.3915, 8.2890],
        [7.9043, 7.9044, 7.9044],
        [7.9043, 9.0133, 9.2552],
        [7.9043, 7.9922, 7.9285]], grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.0795847699046135 
model_pd.l_d.mean(): -5.6726555824279785 
model_pd.lagr.mean(): -5.593070983886719 
model_pd.lambdas: dict_items([('pout', tensor([1.7719])), ('power', tensor([0.3777]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8272])), ('power', tensor([-18.8503]))])
epoch：612	 i:0 	 global-step:12240	 l-p:0.0795847699046135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[7.9192, 7.9459, 7.9227],
        [7.9192, 7.9290, 7.9199],
        [7.9192, 8.9532, 9.1365],
        [7.9192, 8.0060, 7.9428]], grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.07943732291460037 
model_pd.l_d.mean(): -5.654102325439453 
model_pd.lagr.mean(): -5.574665069580078 
model_pd.lambdas: dict_items([('pout', tensor([1.7727])), ('power', tensor([0.3768]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8247])), ('power', tensor([-18.8384]))])
epoch：613	 i:0 	 global-step:12260	 l-p:0.07943732291460037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[7.9342, 9.0482, 9.2912],
        [7.9342, 9.1134, 9.4087],
        [7.9342, 7.9342, 7.9342],
        [7.9342, 7.9346, 7.9342]], grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.07928946614265442 
model_pd.l_d.mean(): -5.63557767868042 
model_pd.lagr.mean(): -5.556288242340088 
model_pd.lambdas: dict_items([('pout', tensor([1.7735])), ('power', tensor([0.3758]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8222])), ('power', tensor([-18.8265]))])
epoch：614	 i:0 	 global-step:12280	 l-p:0.07928946614265442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 7.9492,  8.9880,  9.1722],
        [ 7.9492,  7.9620,  7.9503],
        [ 7.9492,  8.1156,  8.0171],
        [ 7.9492,  9.8653, 10.8827]], grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.07914122939109802 
model_pd.l_d.mean(): -5.617077350616455 
model_pd.lagr.mean(): -5.537936210632324 
model_pd.lambdas: dict_items([('pout', tensor([1.7744])), ('power', tensor([0.3749]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8197])), ('power', tensor([-18.8144]))])
epoch：615	 i:0 	 global-step:12300	 l-p:0.07914122939109802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[7.9644, 9.4147, 9.9470],
        [7.9644, 9.4253, 9.9677],
        [7.9644, 8.0518, 7.9882],
        [7.9644, 7.9644, 7.9644]], grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.07899250835180283 
model_pd.l_d.mean(): -5.598606109619141 
model_pd.lagr.mean(): -5.519613742828369 
model_pd.lambdas: dict_items([('pout', tensor([1.7752])), ('power', tensor([0.3739]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8172])), ('power', tensor([-18.8023]))])
epoch：616	 i:0 	 global-step:12320	 l-p:0.07899250835180283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 7.9796,  8.0673,  8.0035],
        [ 7.9796,  8.7592,  8.7729],
        [ 7.9796,  9.9438, 11.0105],
        [ 7.9796,  8.7104,  8.6983]], grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.07884334772825241 
model_pd.l_d.mean(): -5.580160140991211 
model_pd.lagr.mean(): -5.501317024230957 
model_pd.lambdas: dict_items([('pout', tensor([1.7760])), ('power', tensor([0.3730]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8147])), ('power', tensor([-18.7901]))])
epoch：617	 i:0 	 global-step:12340	 l-p:0.07884334772825241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[7.9950, 8.6111, 8.5466],
        [7.9950, 8.0836, 8.0192],
        [7.9950, 9.1193, 9.3647],
        [7.9950, 7.9954, 7.9950]], grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.07869366556406021 
model_pd.l_d.mean(): -5.561741828918457 
model_pd.lagr.mean(): -5.483047962188721 
model_pd.lambdas: dict_items([('pout', tensor([1.7768])), ('power', tensor([0.3721]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8122])), ('power', tensor([-18.7778]))])
epoch：618	 i:0 	 global-step:12360	 l-p:0.07869366556406021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[8.0104, 8.0966, 8.0335],
        [8.0104, 8.0998, 8.0349],
        [8.0104, 9.3519, 9.7787],
        [8.0104, 8.0233, 8.0115]], grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.07854343205690384 
model_pd.l_d.mean(): -5.543349742889404 
model_pd.lagr.mean(): -5.464806079864502 
model_pd.lambdas: dict_items([('pout', tensor([1.7776])), ('power', tensor([0.3711]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8096])), ('power', tensor([-18.7654]))])
epoch：619	 i:0 	 global-step:12380	 l-p:0.07854343205690384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[ 8.0260,  8.0359,  8.0267],
        [ 8.0260,  8.7125,  8.6765],
        [ 8.0260,  9.5020, 10.0512],
        [ 8.0260,  8.0782,  8.0362]], grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.07839261740446091 
model_pd.l_d.mean(): -5.5249857902526855 
model_pd.lagr.mean(): -5.446593284606934 
model_pd.lambdas: dict_items([('pout', tensor([1.7784])), ('power', tensor([0.3702]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8071])), ('power', tensor([-18.7530]))])
epoch：620	 i:0 	 global-step:12400	 l-p:0.07839261740446091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[8.0416, 8.0416, 8.0416],
        [8.0416, 8.0425, 8.0416],
        [8.0416, 8.0417, 8.0416],
        [8.0416, 8.0516, 8.0423]], grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.07824117690324783 
model_pd.l_d.mean(): -5.506649017333984 
model_pd.lagr.mean(): -5.428407669067383 
model_pd.lambdas: dict_items([('pout', tensor([1.7792])), ('power', tensor([0.3693]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8045])), ('power', tensor([-18.7404]))])
epoch：621	 i:0 	 global-step:12420	 l-p:0.07824117690324783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[8.0573, 8.1473, 8.0820],
        [8.0573, 8.0573, 8.0573],
        [8.0573, 9.1922, 9.4401],
        [8.0573, 9.4083, 9.8382]], grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.07808907330036163 
model_pd.l_d.mean(): -5.488338947296143 
model_pd.lagr.mean(): -5.410249710083008 
model_pd.lambdas: dict_items([('pout', tensor([1.7800])), ('power', tensor([0.3683]))]) 
model_pd.vars: dict_items([('pout', tensor([0.8020])), ('power', tensor([-18.7277]))])
epoch：622	 i:0 	 global-step:12440	 l-p:0.07808907330036163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[8.0732, 8.8639, 8.8779],
        [8.0732, 8.4054, 8.2790],
        [8.0732, 8.3070, 8.1898],
        [8.0732, 8.0733, 8.0732]], grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.07793628424406052 
model_pd.l_d.mean(): -5.470058441162109 
model_pd.lagr.mean(): -5.392122268676758 
model_pd.lambdas: dict_items([('pout', tensor([1.7808])), ('power', tensor([0.3674]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7994])), ('power', tensor([-18.7150]))])
epoch：623	 i:0 	 global-step:12460	 l-p:0.07793628424406052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[8.0891, 8.0992, 8.0899],
        [8.0891, 8.1764, 8.1125],
        [8.0891, 8.7823, 8.7461],
        [8.0891, 8.0891, 8.0891]], grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.07778275012969971 
model_pd.l_d.mean(): -5.4518046379089355 
model_pd.lagr.mean(): -5.374022006988525 
model_pd.lambdas: dict_items([('pout', tensor([1.7816])), ('power', tensor([0.3664]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7968])), ('power', tensor([-18.7021]))])
epoch：624	 i:0 	 global-step:12480	 l-p:0.07778275012969971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 8.1052,  8.5836,  8.4727],
        [ 8.1052,  8.6075,  8.5022],
        [ 8.1052,  8.1946,  8.1295],
        [ 8.1052, 10.0657, 11.1075]], grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.07762841135263443 
model_pd.l_d.mean(): -5.4335784912109375 
model_pd.lagr.mean(): -5.355949878692627 
model_pd.lambdas: dict_items([('pout', tensor([1.7824])), ('power', tensor([0.3655]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7942])), ('power', tensor([-18.6892]))])
epoch：625	 i:0 	 global-step:12500	 l-p:0.07762841135263443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[ 8.1213,  9.3340,  9.6383],
        [ 8.1213,  9.1873,  9.3767],
        [ 8.1213,  8.1213,  8.1214],
        [ 8.1213, 10.2212, 11.4191]], grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.0774732232093811 
model_pd.l_d.mean(): -5.415379524230957 
model_pd.lagr.mean(): -5.337906360626221 
model_pd.lambdas: dict_items([('pout', tensor([1.7832])), ('power', tensor([0.3646]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7916])), ('power', tensor([-18.6761]))])
epoch：626	 i:0 	 global-step:12520	 l-p:0.0774732232093811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[ 8.1376,  8.7672,  8.7015],
        [ 8.1376,  8.1376,  8.1376],
        [ 8.1376,  8.2283,  8.1624],
        [ 8.1376, 10.5048, 12.0204]], grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.07731714844703674 
model_pd.l_d.mean(): -5.397210597991943 
model_pd.lagr.mean(): -5.3198933601379395 
model_pd.lambdas: dict_items([('pout', tensor([1.7840])), ('power', tensor([0.3636]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7889])), ('power', tensor([-18.6630]))])
epoch：627	 i:0 	 global-step:12540	 l-p:0.07731714844703674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[ 8.1540,  9.6565, 10.2150],
        [ 8.1540,  8.7851,  8.7193],
        [ 8.1540,  8.1613,  8.1545],
        [ 8.1540,  8.1566,  8.1541]], grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07716011255979538 
model_pd.l_d.mean(): -5.379069805145264 
model_pd.lagr.mean(): -5.301909923553467 
model_pd.lambdas: dict_items([('pout', tensor([1.7848])), ('power', tensor([0.3627]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7863])), ('power', tensor([-18.6497]))])
epoch：628	 i:0 	 global-step:12560	 l-p:0.07716011255979538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[8.1705, 8.4701, 8.3432],
        [8.1705, 8.1705, 8.1705],
        [8.1705, 9.2442, 9.4352],
        [8.1705, 8.4231, 8.3015]], grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.07700205594301224 
model_pd.l_d.mean(): -5.360957145690918 
model_pd.lagr.mean(): -5.283955097198486 
model_pd.lambdas: dict_items([('pout', tensor([1.7856])), ('power', tensor([0.3618]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7836])), ('power', tensor([-18.6363]))])
epoch：629	 i:0 	 global-step:12580	 l-p:0.07700205594301224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 8.1871,  8.6715,  8.5593],
        [ 8.1871,  9.6986, 10.2615],
        [ 8.1871, 10.2118, 11.3121],
        [ 8.1871,  8.2348,  8.1958]], grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.07684293389320374 
model_pd.l_d.mean(): -5.342872619628906 
model_pd.lagr.mean(): -5.2660298347473145 
model_pd.lambdas: dict_items([('pout', tensor([1.7864])), ('power', tensor([0.3608]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7810])), ('power', tensor([-18.6229]))])
epoch：630	 i:0 	 global-step:12600	 l-p:0.07684293389320374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 8.2039,  8.9090,  8.8724],
        [ 8.2039,  8.2045,  8.2039],
        [ 8.2039, 10.1927, 11.2498],
        [ 8.2039,  8.9597,  8.9476]], grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.07668265700340271 
model_pd.l_d.mean(): -5.324816703796387 
model_pd.lagr.mean(): -5.248134136199951 
model_pd.lambdas: dict_items([('pout', tensor([1.7871])), ('power', tensor([0.3599]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7783])), ('power', tensor([-18.6093]))])
epoch：631	 i:0 	 global-step:12620	 l-p:0.07668265700340271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[ 8.2208,  9.7379, 10.3020],
        [ 8.2208,  8.3131,  8.2461],
        [ 8.2208,  8.2208,  8.2208],
        [ 8.2208,  8.4599,  8.3401]], grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.07652116566896439 
model_pd.l_d.mean(): -5.306790351867676 
model_pd.lagr.mean(): -5.230268955230713 
model_pd.lambdas: dict_items([('pout', tensor([1.7879])), ('power', tensor([0.3590]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7756])), ('power', tensor([-18.5956]))])
epoch：632	 i:0 	 global-step:12640	 l-p:0.07652116566896439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[8.2377, 8.5404, 8.4122],
        [8.2377, 8.3298, 8.2629],
        [8.2377, 9.0221, 9.0228],
        [8.2377, 8.2378, 8.2377]], grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.0763583779335022 
model_pd.l_d.mean(): -5.288792610168457 
model_pd.lagr.mean(): -5.2124342918396 
model_pd.lambdas: dict_items([('pout', tensor([1.7887])), ('power', tensor([0.3581]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7729])), ('power', tensor([-18.5818]))])
epoch：633	 i:0 	 global-step:12660	 l-p:0.0763583779335022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[ 8.2549, 10.6621, 12.2038],
        [ 8.2549,  8.4952,  8.3749],
        [ 8.2549,  8.2830,  8.2586],
        [ 8.2549,  8.2549,  8.2549]], grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.07619421929121017 
model_pd.l_d.mean(): -5.270824909210205 
model_pd.lagr.mean(): -5.1946306228637695 
model_pd.lambdas: dict_items([('pout', tensor([1.7895])), ('power', tensor([0.3571]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7701])), ('power', tensor([-18.5678]))])
epoch：634	 i:0 	 global-step:12680	 l-p:0.07619421929121017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[8.2721, 9.0865, 9.1014],
        [8.2721, 8.5763, 8.4475],
        [8.2721, 8.2721, 8.2721],
        [8.2721, 8.2747, 8.2722]], grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.07602856308221817 
model_pd.l_d.mean(): -5.252884387969971 
model_pd.lagr.mean(): -5.17685604095459 
model_pd.lambdas: dict_items([('pout', tensor([1.7902])), ('power', tensor([0.3562]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7674])), ('power', tensor([-18.5538]))])
epoch：635	 i:0 	 global-step:12700	 l-p:0.07602856308221817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[ 8.2895,  8.3795,  8.3137],
        [ 8.2895,  9.8217, 10.3917],
        [ 8.2895,  8.2900,  8.2895],
        [ 8.2895,  8.3823,  8.3149]], grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.07586140185594559 
model_pd.l_d.mean(): -5.234976291656494 
model_pd.lagr.mean(): -5.159114837646484 
model_pd.lambdas: dict_items([('pout', tensor([1.7910])), ('power', tensor([0.3553]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7646])), ('power', tensor([-18.5396]))])
epoch：636	 i:0 	 global-step:12720	 l-p:0.07586140185594559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[8.3070, 9.0744, 9.0623],
        [8.3070, 8.3077, 8.3071],
        [8.3070, 8.4006, 8.3327],
        [8.3070, 8.5166, 8.4026]], grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.0756925418972969 
model_pd.l_d.mean(): -5.21709680557251 
model_pd.lagr.mean(): -5.141404151916504 
model_pd.lambdas: dict_items([('pout', tensor([1.7917])), ('power', tensor([0.3543]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7619])), ('power', tensor([-18.5253]))])
epoch：637	 i:0 	 global-step:12740	 l-p:0.0756925418972969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[ 8.3247,  9.2006,  9.2472],
        [ 8.3247, 10.4866, 11.7207],
        [ 8.3247,  8.7226,  8.5937],
        [ 8.3247,  8.4152,  8.3490]], grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.07552191615104675 
model_pd.l_d.mean(): -5.199246883392334 
model_pd.lagr.mean(): -5.123724937438965 
model_pd.lambdas: dict_items([('pout', tensor([1.7925])), ('power', tensor([0.3534]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7591])), ('power', tensor([-18.5109]))])
epoch：638	 i:0 	 global-step:12760	 l-p:0.07552191615104675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[8.3425, 8.3425, 8.3425],
        [8.3425, 9.5949, 9.9098],
        [8.3425, 8.3431, 8.3425],
        [8.3425, 8.3425, 8.3425]], grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.07534939050674438 
model_pd.l_d.mean(): -5.181426048278809 
model_pd.lagr.mean(): -5.106076717376709 
model_pd.lambdas: dict_items([('pout', tensor([1.7933])), ('power', tensor([0.3525]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7563])), ('power', tensor([-18.4963]))])
epoch：639	 i:0 	 global-step:12780	 l-p:0.07534939050674438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[8.3605, 8.4543, 8.3862],
        [8.3605, 8.3605, 8.3605],
        [8.3605, 9.1855, 9.2007],
        [8.3605, 8.4515, 8.3849]], grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.07517483830451965 
model_pd.l_d.mean(): -5.163636684417725 
model_pd.lagr.mean(): -5.088461875915527 
model_pd.lambdas: dict_items([('pout', tensor([1.7940])), ('power', tensor([0.3516]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7534])), ('power', tensor([-18.4816]))])
epoch：640	 i:0 	 global-step:12800	 l-p:0.07517483830451965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[8.3786, 8.4338, 8.3895],
        [8.3786, 8.3786, 8.3786],
        [8.3786, 8.3787, 8.3786],
        [8.3786, 8.3786, 8.3786]], grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.07499813288450241 
model_pd.l_d.mean(): -5.145876407623291 
model_pd.lagr.mean(): -5.070878505706787 
model_pd.lambdas: dict_items([('pout', tensor([1.7948])), ('power', tensor([0.3507]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7506])), ('power', tensor([-18.4667]))])
epoch：641	 i:0 	 global-step:12820	 l-p:0.07499813288450241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[ 8.3969,  9.8161, 10.2688],
        [ 8.3969,  8.9215,  8.8118],
        [ 8.3969,  8.4075,  8.3977],
        [ 8.3969,  8.3969,  8.3969]], grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.07481911033391953 
model_pd.l_d.mean(): -5.128147125244141 
model_pd.lagr.mean(): -5.053328037261963 
model_pd.lambdas: dict_items([('pout', tensor([1.7955])), ('power', tensor([0.3497]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7477])), ('power', tensor([-18.4517]))])
epoch：642	 i:0 	 global-step:12840	 l-p:0.07481911033391953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[8.4154, 9.1427, 9.1053],
        [8.4154, 8.4154, 8.4154],
        [8.4154, 8.4159, 8.4154],
        [8.4154, 8.6774, 8.5514]], grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.07463760673999786 
model_pd.l_d.mean(): -5.110449314117432 
model_pd.lagr.mean(): -5.035811901092529 
model_pd.lambdas: dict_items([('pout', tensor([1.7963])), ('power', tensor([0.3488]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7448])), ('power', tensor([-18.4366]))])
epoch：643	 i:0 	 global-step:12860	 l-p:0.07463760673999786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[8.4340, 9.2678, 9.2833],
        [8.4340, 8.4340, 8.4340],
        [8.4340, 8.6131, 8.5072],
        [8.4340, 8.4350, 8.4341]], grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.0744534507393837 
model_pd.l_d.mean(): -5.092781066894531 
model_pd.lagr.mean(): -5.018327713012695 
model_pd.lambdas: dict_items([('pout', tensor([1.7970])), ('power', tensor([0.3479]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7419])), ('power', tensor([-18.4213]))])
epoch：644	 i:0 	 global-step:12880	 l-p:0.0744534507393837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[ 8.4529,  9.1841,  9.1465],
        [ 8.4529,  8.4529,  8.4529],
        [ 8.4529, 10.0210, 10.6049],
        [ 8.4529,  8.9565,  8.8401]], grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.07426644116640091 
model_pd.l_d.mean(): -5.075144290924072 
model_pd.lagr.mean(): -5.000877857208252 
model_pd.lambdas: dict_items([('pout', tensor([1.7977])), ('power', tensor([0.3470]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7390])), ('power', tensor([-18.4058]))])
epoch：645	 i:0 	 global-step:12900	 l-p:0.07426644116640091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[ 8.4719,  8.7201,  8.5959],
        [ 8.4719,  8.5278,  8.4829],
        [ 8.4719, 10.9533, 12.5435],
        [ 8.4719,  9.3664,  9.4144]], grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.074076347053051 
model_pd.l_d.mean(): -5.057538032531738 
model_pd.lagr.mean(): -4.983461856842041 
model_pd.lambdas: dict_items([('pout', tensor([1.7985])), ('power', tensor([0.3460]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7360])), ('power', tensor([-18.3902]))])
epoch：646	 i:0 	 global-step:12920	 l-p:0.074076347053051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[8.4911, 8.5050, 8.4923],
        [8.4911, 9.3316, 9.3474],
        [8.4911, 8.5018, 8.4919],
        [8.4911, 8.5872, 8.5175]], grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.07388296723365784 
model_pd.l_d.mean(): -5.039962291717529 
model_pd.lagr.mean(): -4.966079235076904 
model_pd.lambdas: dict_items([('pout', tensor([1.7992])), ('power', tensor([0.3451]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7331])), ('power', tensor([-18.3743]))])
epoch：647	 i:0 	 global-step:12940	 l-p:0.07388296723365784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[8.5105, 8.5244, 8.5117],
        [8.5105, 8.5105, 8.5105],
        [8.5105, 8.5111, 8.5105],
        [8.5105, 8.7601, 8.6352]], grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.07368599623441696 
model_pd.l_d.mean(): -5.02241849899292 
model_pd.lagr.mean(): -4.948732376098633 
model_pd.lambdas: dict_items([('pout', tensor([1.7999])), ('power', tensor([0.3442]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7301])), ('power', tensor([-18.3583]))])
epoch：648	 i:0 	 global-step:12960	 l-p:0.07368599623441696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 8.5301, 10.1153, 10.7057],
        [ 8.5301,  9.4321,  9.4806],
        [ 8.5301,  8.5301,  8.5301],
        [ 8.5301,  9.9761, 10.4377]], grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.07348515093326569 
model_pd.l_d.mean(): -5.004906177520752 
model_pd.lagr.mean(): -4.931420803070068 
model_pd.lambdas: dict_items([('pout', tensor([1.8007])), ('power', tensor([0.3433]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7270])), ('power', tensor([-18.3421]))])
epoch：649	 i:0 	 global-step:12980	 l-p:0.07348515093326569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[ 8.5500, 10.0000, 10.4630],
        [ 8.5500,  9.7686, 10.0361],
        [ 8.5500,  9.0384,  8.9159],
        [ 8.5500,  9.0608,  8.9428]], grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.07328013330698013 
model_pd.l_d.mean(): -4.9874267578125 
model_pd.lagr.mean(): -4.914146423339844 
model_pd.lambdas: dict_items([('pout', tensor([1.8014])), ('power', tensor([0.3424]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7240])), ('power', tensor([-18.3257]))])
epoch：650	 i:0 	 global-step:13000	 l-p:0.07328013330698013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[8.5701, 8.5701, 8.5701],
        [8.5701, 8.5701, 8.5701],
        [8.5701, 8.6674, 8.5968],
        [8.5701, 8.9277, 8.7920]], grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.07307055592536926 
model_pd.l_d.mean(): -4.969978332519531 
model_pd.lagr.mean(): -4.896907806396484 
model_pd.lambdas: dict_items([('pout', tensor([1.8021])), ('power', tensor([0.3415]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7209])), ('power', tensor([-18.3091]))])
epoch：651	 i:0 	 global-step:13020	 l-p:0.07307055592536926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[ 8.5905,  8.5905,  8.5905],
        [ 8.5905, 11.3093, 13.1748],
        [ 8.5905,  8.5911,  8.5905],
        [ 8.5905,  8.6866,  8.6167]], grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.07285600900650024 
model_pd.l_d.mean(): -4.952559947967529 
model_pd.lagr.mean(): -4.879703998565674 
model_pd.lambdas: dict_items([('pout', tensor([1.8028])), ('power', tensor([0.3405]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7178])), ('power', tensor([-18.2922]))])
epoch：652	 i:0 	 global-step:13040	 l-p:0.07285600900650024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[ 8.6111, 10.8605, 12.1457],
        [ 8.6111,  9.5234,  9.5726],
        [ 8.6111,  8.6111,  8.6111],
        [ 8.6111,  9.8401, 10.1101]], grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.07263607531785965 
model_pd.l_d.mean(): -4.935174942016602 
model_pd.lagr.mean(): -4.862538814544678 
model_pd.lambdas: dict_items([('pout', tensor([1.8035])), ('power', tensor([0.3396]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7146])), ('power', tensor([-18.2751]))])
epoch：653	 i:0 	 global-step:13060	 l-p:0.07263607531785965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[ 8.6320,  9.5470,  9.5964],
        [ 8.6320, 10.2396, 10.8387],
        [ 8.6320,  8.6324,  8.6320],
        [ 8.6320,  9.1262,  9.0023]], grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.07241019606590271 
model_pd.l_d.mean(): -4.917822360992432 
model_pd.lagr.mean(): -4.845412254333496 
model_pd.lambdas: dict_items([('pout', tensor([1.8043])), ('power', tensor([0.3387]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7114])), ('power', tensor([-18.2578]))])
epoch：654	 i:0 	 global-step:13080	 l-p:0.07241019606590271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[ 8.6532,  9.1489,  9.0247],
        [ 8.6532, 11.1968, 12.8276],
        [ 8.6532,  8.6560,  8.6533],
        [ 8.6532,  9.0705,  8.9355]], grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.07217779010534286 
model_pd.l_d.mean(): -4.900501728057861 
model_pd.lagr.mean(): -4.828323841094971 
model_pd.lambdas: dict_items([('pout', tensor([1.8050])), ('power', tensor([0.3378]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7082])), ('power', tensor([-18.2402]))])
epoch：655	 i:0 	 global-step:13100	 l-p:0.07217779010534286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[ 8.6748, 10.8419, 12.0216],
        [ 8.6748,  8.7721,  8.7013],
        [ 8.6748,  9.2206,  9.1068],
        [ 8.6748,  9.4293,  9.3909]], grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.07193819433450699 
model_pd.l_d.mean(): -4.883213043212891 
model_pd.lagr.mean(): -4.811275005340576 
model_pd.lambdas: dict_items([('pout', tensor([1.8057])), ('power', tensor([0.3369]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7049])), ('power', tensor([-18.2222]))])
epoch：656	 i:0 	 global-step:13120	 l-p:0.07193819433450699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[ 8.6967,  9.1165,  8.9808],
        [ 8.6967, 10.3186, 10.9232],
        [ 8.6967,  9.2442,  9.1301],
        [ 8.6967,  9.9403, 10.2137]], grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.0716906487941742 
model_pd.l_d.mean(): -4.865957260131836 
model_pd.lagr.mean(): -4.794266700744629 
model_pd.lambdas: dict_items([('pout', tensor([1.8064])), ('power', tensor([0.3360]))]) 
model_pd.vars: dict_items([('pout', tensor([0.7016])), ('power', tensor([-18.2040]))])
epoch：657	 i:0 	 global-step:13140	 l-p:0.0716906487941742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[ 8.7190,  8.7190,  8.7190],
        [ 8.7190, 10.3340, 10.9294],
        [ 8.7190,  8.7218,  8.7191],
        [ 8.7190,  8.9056,  8.7953]], grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07143425196409225 
model_pd.l_d.mean(): -4.848734378814697 
model_pd.lagr.mean(): -4.7773003578186035 
model_pd.lambdas: dict_items([('pout', tensor([1.8071])), ('power', tensor([0.3351]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6983])), ('power', tensor([-18.1854]))])
epoch：658	 i:0 	 global-step:13160	 l-p:0.07143425196409225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[ 8.7417,  8.7561,  8.7429],
        [ 8.7417, 10.0658, 10.3999],
        [ 8.7417,  8.7934,  8.7512],
        [ 8.7417,  8.8414,  8.7691]], grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.07116799056529999 
model_pd.l_d.mean(): -4.83154296875 
model_pd.lagr.mean(): -4.760375022888184 
model_pd.lambdas: dict_items([('pout', tensor([1.8078])), ('power', tensor([0.3342]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6948])), ('power', tensor([-18.1665]))])
epoch：659	 i:0 	 global-step:13180	 l-p:0.07116799056529999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[8.7649, 8.7654, 8.7649],
        [8.7649, 8.8167, 8.7744],
        [8.7649, 9.5288, 9.4901],
        [8.7649, 8.9888, 8.8672]], grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.07089060544967651 
model_pd.l_d.mean(): -4.814384460449219 
model_pd.lagr.mean(): -4.743494033813477 
model_pd.lambdas: dict_items([('pout', tensor([1.8085])), ('power', tensor([0.3333]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6914])), ('power', tensor([-18.1472]))])
epoch：660	 i:0 	 global-step:13200	 l-p:0.07089060544967651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[ 8.7885, 10.4187, 11.0199],
        [ 8.7885,  8.7886,  8.7885],
        [ 8.7885,  9.2137,  9.0763],
        [ 8.7885,  8.7966,  8.7890]], grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.07060068845748901 
model_pd.l_d.mean(): -4.797258377075195 
model_pd.lagr.mean(): -4.726657867431641 
model_pd.lambdas: dict_items([('pout', tensor([1.8091])), ('power', tensor([0.3324]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6878])), ('power', tensor([-18.1274]))])
epoch：661	 i:0 	 global-step:13220	 l-p:0.07060068845748901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[ 8.8127,  8.9134,  8.8404],
        [ 8.8127,  8.8208,  8.8132],
        [ 8.8127, 11.1237, 12.4450],
        [ 8.8127,  8.8127,  8.8127]], grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.0702965036034584 
model_pd.l_d.mean(): -4.7801642417907715 
model_pd.lagr.mean(): -4.70986795425415 
model_pd.lambdas: dict_items([('pout', tensor([1.8098])), ('power', tensor([0.3314]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6842])), ('power', tensor([-18.1072]))])
epoch：662	 i:0 	 global-step:13240	 l-p:0.0702965036034584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[ 8.8375, 10.1051, 10.3841],
        [ 8.8375,  8.8404,  8.8376],
        [ 8.8375,  9.2088,  9.0681],
        [ 8.8375, 10.4904, 11.1071]], grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.06997597217559814 
model_pd.l_d.mean(): -4.7631025314331055 
model_pd.lagr.mean(): -4.693126678466797 
model_pd.lambdas: dict_items([('pout', tensor([1.8105])), ('power', tensor([0.3305]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6805])), ('power', tensor([-18.0864]))])
epoch：663	 i:0 	 global-step:13260	 l-p:0.06997597217559814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[8.8630, 9.3966, 9.2737],
        [8.8630, 9.0901, 8.9667],
        [8.8630, 9.1941, 9.0543],
        [8.8630, 8.9630, 8.8903]], grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.06963654607534409 
model_pd.l_d.mean(): -4.746072292327881 
model_pd.lagr.mean(): -4.676435947418213 
model_pd.lambdas: dict_items([('pout', tensor([1.8112])), ('power', tensor([0.3296]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6767])), ('power', tensor([-18.0651]))])
epoch：664	 i:0 	 global-step:13280	 l-p:0.06963654607534409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[8.8893, 8.8921, 8.8894],
        [8.8893, 8.8899, 8.8893],
        [8.8893, 8.8903, 8.8893],
        [8.8893, 9.2632, 9.1215]], grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.06927511096000671 
model_pd.l_d.mean(): -4.729072570800781 
model_pd.lagr.mean(): -4.659797668457031 
model_pd.lambdas: dict_items([('pout', tensor([1.8119])), ('power', tensor([0.3287]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6728])), ('power', tensor([-18.0430]))])
epoch：665	 i:0 	 global-step:13300	 l-p:0.06927511096000671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[ 8.9164, 10.1973, 10.4795],
        [ 8.9164, 11.5500, 13.2397],
        [ 8.9164,  9.7521,  9.7401],
        [ 8.9164,  9.3491,  9.2094]], grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.06888770312070847 
model_pd.l_d.mean(): -4.712103843688965 
model_pd.lagr.mean(): -4.643216133117676 
model_pd.lambdas: dict_items([('pout', tensor([1.8125])), ('power', tensor([0.3278]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6688])), ('power', tensor([-18.0203]))])
epoch：666	 i:0 	 global-step:13320	 l-p:0.06888770312070847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[ 8.9445, 10.2302, 10.5135],
        [ 8.9445,  8.9445,  8.9445],
        [ 8.9445,  9.5109,  9.3931],
        [ 8.9445,  9.0465,  8.9724]], grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.06846928596496582 
model_pd.l_d.mean(): -4.695164680480957 
model_pd.lagr.mean(): -4.62669563293457 
model_pd.lambdas: dict_items([('pout', tensor([1.8132])), ('power', tensor([0.3269]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6647])), ('power', tensor([-17.9966]))])
epoch：667	 i:0 	 global-step:13340	 l-p:0.06846928596496582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[8.9737, 8.9741, 8.9737],
        [8.9737, 9.8433, 9.8455],
        [8.9737, 9.5154, 9.3907],
        [8.9737, 9.0338, 8.9855]], grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.06801332533359528 
model_pd.l_d.mean(): -4.678253173828125 
model_pd.lagr.mean(): -4.6102399826049805 
model_pd.lambdas: dict_items([('pout', tensor([1.8138])), ('power', tensor([0.3260]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6604])), ('power', tensor([-17.9720]))])
epoch：668	 i:0 	 global-step:13360	 l-p:0.06801332533359528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[ 9.0043,  9.7160,  9.6431],
        [ 9.0043,  9.0048,  9.0043],
        [ 9.0043,  9.4422,  9.3008],
        [ 9.0043, 11.8755, 13.8476]], grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.06751107424497604 
model_pd.l_d.mean(): -4.6613664627075195 
model_pd.lagr.mean(): -4.593855381011963 
model_pd.lambdas: dict_items([('pout', tensor([1.8145])), ('power', tensor([0.3251]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6560])), ('power', tensor([-17.9462]))])
epoch：669	 i:0 	 global-step:13380	 l-p:0.06751107424497604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[ 9.0365,  9.0480,  9.0373],
        [ 9.0365, 11.2639, 12.4511],
        [ 9.0365,  9.0515,  9.0378],
        [ 9.0365,  9.0971,  9.0484]], grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.06695061922073364 
model_pd.l_d.mean(): -4.644503593444824 
model_pd.lagr.mean(): -4.577552795410156 
model_pd.lambdas: dict_items([('pout', tensor([1.8152])), ('power', tensor([0.3242]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6513])), ('power', tensor([-17.9191]))])
epoch：670	 i:0 	 global-step:13400	 l-p:0.06695061922073364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 9.0706,  9.0822,  9.0714],
        [ 9.0706,  9.3578,  9.2200],
        [ 9.0706,  9.1712,  9.0977],
        [ 9.0706, 11.3078, 12.5003]], grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.06631513684988022 
model_pd.l_d.mean(): -4.627658843994141 
model_pd.lagr.mean(): -4.561343669891357 
model_pd.lambdas: dict_items([('pout', tensor([1.8158])), ('power', tensor([0.3234]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6463])), ('power', tensor([-17.8903]))])
epoch：671	 i:0 	 global-step:13420	 l-p:0.06631513684988022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[ 9.1070, 11.4006, 12.6507],
        [ 9.1070,  9.1683,  9.1191],
        [ 9.1070,  9.4492,  9.3048],
        [ 9.1070, 10.6693, 11.1697]], grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.06557992100715637 
model_pd.l_d.mean(): -4.610825538635254 
model_pd.lagr.mean(): -4.54524564743042 
model_pd.lambdas: dict_items([('pout', tensor([1.8164])), ('power', tensor([0.3225]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6411])), ('power', tensor([-17.8594]))])
epoch：672	 i:0 	 global-step:13440	 l-p:0.06557992100715637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[ 9.1465,  9.2519,  9.1755],
        [ 9.1465, 10.0360, 10.0386],
        [ 9.1465,  9.9505,  9.9103],
        [ 9.1465, 11.4515, 12.7080]], grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.06470691412687302 
model_pd.l_d.mean(): -4.593995094299316 
model_pd.lagr.mean(): -4.529288291931152 
model_pd.lambdas: dict_items([('pout', tensor([1.8171])), ('power', tensor([0.3216]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6354])), ('power', tensor([-17.8260]))])
epoch：673	 i:0 	 global-step:13460	 l-p:0.06470691412687302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[9.1898, 9.1898, 9.1898],
        [9.1898, 9.2016, 9.1906],
        [9.1898, 9.1898, 9.1898],
        [9.1898, 9.2517, 9.2020]], grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.06363396346569061 
model_pd.l_d.mean(): -4.577150821685791 
model_pd.lagr.mean(): -4.513516902923584 
model_pd.lambdas: dict_items([('pout', tensor([1.8177])), ('power', tensor([0.3207]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6292])), ('power', tensor([-17.7892]))])
epoch：674	 i:0 	 global-step:13480	 l-p:0.06363396346569061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 9.2383, 10.5740, 10.8690],
        [ 9.2383, 11.5235, 12.7422],
        [ 9.2383,  9.3444,  9.2674],
        [ 9.2383, 10.9668, 11.6056]], grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.06225027143955231 
model_pd.l_d.mean(): -4.560268878936768 
model_pd.lagr.mean(): -4.498018741607666 
model_pd.lambdas: dict_items([('pout', tensor([1.8183])), ('power', tensor([0.3198]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6223])), ('power', tensor([-17.7480]))])
epoch：675	 i:0 	 global-step:13500	 l-p:0.06225027143955231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[ 9.2941,  9.4959,  9.3767],
        [ 9.2941, 10.2007, 10.2036],
        [ 9.2941,  9.2946,  9.2941],
        [ 9.2941, 10.7175, 11.0780]], grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.06033437326550484 
model_pd.l_d.mean(): -4.5433030128479 
model_pd.lagr.mean(): -4.482968807220459 
model_pd.lambdas: dict_items([('pout', tensor([1.8189])), ('power', tensor([0.3189]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6144])), ('power', tensor([-17.7004]))])
epoch：676	 i:0 	 global-step:13520	 l-p:0.06033437326550484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[ 9.3612,  9.5647,  9.4445],
        [ 9.3612, 10.2755, 10.2785],
        [ 9.3612, 12.3637, 14.4274],
        [ 9.3612,  9.4174,  9.3715]], grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.057358771562576294 
model_pd.l_d.mean(): -4.526165008544922 
model_pd.lagr.mean(): -4.468806266784668 
model_pd.lambdas: dict_items([('pout', tensor([1.8195])), ('power', tensor([0.3180]))]) 
model_pd.vars: dict_items([('pout', tensor([0.6049])), ('power', tensor([-17.6431]))])
epoch：677	 i:0 	 global-step:13540	 l-p:0.057358771562576294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[ 9.4474,  9.8051,  9.6543],
        [ 9.4474,  9.9114,  9.7619],
        [ 9.4474, 12.2627, 14.0711],
        [ 9.4474, 11.8403, 13.1457]], grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.051614273339509964 
model_pd.l_d.mean(): -4.508651256561279 
model_pd.lagr.mean(): -4.457036972045898 
model_pd.lambdas: dict_items([('pout', tensor([1.8201])), ('power', tensor([0.3171]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5929])), ('power', tensor([-17.5692]))])
epoch：678	 i:0 	 global-step:13560	 l-p:0.051614273339509964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[ 9.5750, 12.1187, 13.5758],
        [ 9.5750,  9.5750,  9.5750],
        [ 9.5750,  9.8631,  9.7193],
        [ 9.5750, 11.3769, 12.0437]], grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.031615063548088074 
model_pd.l_d.mean(): -4.490135669708252 
model_pd.lagr.mean(): -4.458520412445068 
model_pd.lambdas: dict_items([('pout', tensor([1.8207])), ('power', tensor([0.3163]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5753])), ('power', tensor([-17.4596]))])
epoch：679	 i:0 	 global-step:13580	 l-p:0.031615063548088074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[ 9.7805, 11.6426, 12.3413],
        [ 9.7805,  9.9950,  9.8684],
        [ 9.7805,  9.7805,  9.7805],
        [ 9.7805, 10.7129, 10.7008]], grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.1800176501274109 
model_pd.l_d.mean(): -4.469130516052246 
model_pd.lagr.mean(): -4.2891130447387695 
model_pd.lambdas: dict_items([('pout', tensor([1.8213])), ('power', tensor([0.3154]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5474])), ('power', tensor([-17.2820]))])
epoch：680	 i:0 	 global-step:13600	 l-p:0.1800176501274109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[10.0548, 10.8657, 10.7839],
        [10.0548, 10.0556, 10.0548],
        [10.0548, 11.0178, 11.0057],
        [10.0548, 10.1238, 10.0684]], grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.09058460593223572 
model_pd.l_d.mean(): -4.444665908813477 
model_pd.lagr.mean(): -4.354081153869629 
model_pd.lambdas: dict_items([('pout', tensor([1.8218])), ('power', tensor([0.3146]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5111])), ('power', tensor([-17.0432]))])
epoch：681	 i:0 	 global-step:13620	 l-p:0.09058460593223572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[10.3226, 13.0938, 14.6836],
        [10.3226, 10.3226, 10.3226],
        [10.3226, 13.4363, 15.4396],
        [10.3226, 10.3324, 10.3231]], grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.08138281852006912 
model_pd.l_d.mean(): -4.418789863586426 
model_pd.lagr.mean(): -4.337407112121582 
model_pd.lambdas: dict_items([('pout', tensor([1.8223])), ('power', tensor([0.3137]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766])), ('power', tensor([-16.8082]))])
epoch：682	 i:0 	 global-step:13640	 l-p:0.08138281852006912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[10.5469, 11.5648, 11.5527],
        [10.5469, 10.5477, 10.5469],
        [10.5469, 10.5608, 10.5479],
        [10.5469, 13.3865, 15.0161]], grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.07789462059736252 
model_pd.l_d.mean(): -4.393673896789551 
model_pd.lagr.mean(): -4.315779209136963 
model_pd.lambdas: dict_items([('pout', tensor([1.8227])), ('power', tensor([0.3129]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484])), ('power', tensor([-16.6101]))])
epoch：683	 i:0 	 global-step:13660	 l-p:0.07789462059736252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.7184, 10.7197, 10.7184],
        [10.7184, 10.7326, 10.7194],
        [10.7184, 12.3049, 12.6586],
        [10.7184, 10.7190, 10.7184]], grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.07611138373613358 
model_pd.l_d.mean(): -4.370617389678955 
model_pd.lagr.mean(): -4.294506072998047 
model_pd.lambdas: dict_items([('pout', tensor([1.8231])), ('power', tensor([0.3121]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4273])), ('power', tensor([-16.4579]))])
epoch：684	 i:0 	 global-step:13680	 l-p:0.07611138373613358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[10.8350, 10.8356, 10.8350],
        [10.8350, 11.9549, 11.9803],
        [10.8350, 12.3304, 12.6031],
        [10.8350, 10.8352, 10.8350]], grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.07514064013957977 
model_pd.l_d.mean(): -4.3503642082214355 
model_pd.lagr.mean(): -4.275223731994629 
model_pd.lambdas: dict_items([('pout', tensor([1.8235])), ('power', tensor([0.3112]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4131])), ('power', tensor([-16.3540]))])
epoch：685	 i:0 	 global-step:13700	 l-p:0.07514064013957977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[10.8983, 11.8854, 11.8387],
        [10.8983, 10.9743, 10.9133],
        [10.8983, 10.9658, 10.9107],
        [10.8983, 13.7142, 15.2550]], grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.07467211037874222 
model_pd.l_d.mean(): -4.333176136016846 
model_pd.lagr.mean(): -4.2585039138793945 
model_pd.lambdas: dict_items([('pout', tensor([1.8239])), ('power', tensor([0.3104]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4054])), ('power', tensor([-16.2975]))])
epoch：686	 i:0 	 global-step:13720	 l-p:0.07467211037874222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[10.9122, 14.2278, 16.3631],
        [10.9122, 10.9124, 10.9122],
        [10.9122, 11.9009, 11.8541],
        [10.9122, 10.9136, 10.9123]], grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07457324117422104 
model_pd.l_d.mean(): -4.318926811218262 
model_pd.lagr.mean(): -4.244353771209717 
model_pd.lambdas: dict_items([('pout', tensor([1.8244])), ('power', tensor([0.3096]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4038])), ('power', tensor([-16.2851]))])
epoch：687	 i:0 	 global-step:13740	 l-p:0.07457324117422104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[10.8824, 11.0123, 10.9182],
        [10.8824, 11.0077, 10.9162],
        [10.8824, 12.3856, 12.6599],
        [10.8824, 10.8826, 10.8824]], grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.07478534430265427 
model_pd.l_d.mean(): -4.30720329284668 
model_pd.lagr.mean(): -4.232418060302734 
model_pd.lambdas: dict_items([('pout', tensor([1.8248])), ('power', tensor([0.3088]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4073])), ('power', tensor([-16.3117]))])
epoch：688	 i:0 	 global-step:13760	 l-p:0.07478534430265427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[10.8153, 10.9425, 10.8501],
        [10.8153, 10.8190, 10.8155],
        [10.8153, 12.4189, 12.7767],
        [10.8153, 10.8155, 10.8153]], grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.0752914622426033 
model_pd.l_d.mean(): -4.297414302825928 
model_pd.lagr.mean(): -4.222122669219971 
model_pd.lambdas: dict_items([('pout', tensor([1.8252])), ('power', tensor([0.3080]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4154])), ('power', tensor([-16.3715]))])
epoch：689	 i:0 	 global-step:13780	 l-p:0.0752914622426033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[10.7182, 10.7182, 10.7182],
        [10.7182, 10.8458, 10.7535],
        [10.7182, 11.8248, 11.8498],
        [10.7182, 12.7871, 13.5658]], grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.07610859721899033 
model_pd.l_d.mean(): -4.288877487182617 
model_pd.lagr.mean(): -4.212769031524658 
model_pd.lambdas: dict_items([('pout', tensor([1.8256])), ('power', tensor([0.3072]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4273])), ('power', tensor([-16.4579]))])
epoch：690	 i:0 	 global-step:13800	 l-p:0.07610859721899033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[10.5986, 10.5986, 10.5986],
        [10.5986, 10.5986, 10.5986],
        [10.5986, 11.6910, 11.7155],
        [10.5986, 10.5991, 10.5986]], grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.07729391753673553 
model_pd.l_d.mean(): -4.280909538269043 
model_pd.lagr.mean(): -4.203615665435791 
model_pd.lambdas: dict_items([('pout', tensor([1.8260])), ('power', tensor([0.3063]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4420])), ('power', tensor([-16.5641]))])
epoch：691	 i:0 	 global-step:13820	 l-p:0.07729391753673553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[10.4641, 11.1144, 10.9662],
        [10.4641, 10.4779, 10.4651],
        [10.4641, 10.6968, 10.5596],
        [10.4641, 11.1468, 11.0064]], grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.07896667718887329 
model_pd.l_d.mean(): -4.272884368896484 
model_pd.lagr.mean(): -4.193917751312256 
model_pd.lambdas: dict_items([('pout', tensor([1.8265])), ('power', tensor([0.3055]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4587])), ('power', tensor([-16.6832]))])
epoch：692	 i:0 	 global-step:13840	 l-p:0.07896667718887329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[10.3229, 10.4444, 10.3563],
        [10.3229, 10.3229, 10.3229],
        [10.3229, 10.9352, 10.7835],
        [10.3229, 12.3056, 13.0511]], grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.08135427534580231 
model_pd.l_d.mean(): -4.264266014099121 
model_pd.lagr.mean(): -4.1829118728637695 
model_pd.lambdas: dict_items([('pout', tensor([1.8270])), ('power', tensor([0.3047]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4765])), ('power', tensor([-16.8077]))])
epoch：693	 i:0 	 global-step:13860	 l-p:0.08135427534580231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[10.1844, 10.1844, 10.1844],
        [10.1844, 10.1844, 10.1844],
        [10.1844, 11.0979, 11.0539],
        [10.1844, 10.1978, 10.1854]], grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.08487426489591599 
model_pd.l_d.mean(): -4.254634380340576 
model_pd.lagr.mean(): -4.169760227203369 
model_pd.lambdas: dict_items([('pout', tensor([1.8275])), ('power', tensor([0.3038]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942])), ('power', tensor([-16.9294]))])
epoch：694	 i:0 	 global-step:13880	 l-p:0.08487426489591599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.0610, 10.0615, 10.0610],
        [10.0610, 11.9844, 12.7059],
        [10.0610, 12.7548, 14.2999],
        [10.0610, 10.0969, 10.0657]], grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.09014301002025604 
model_pd.l_d.mean(): -4.243675231933594 
model_pd.lagr.mean(): -4.153532028198242 
model_pd.lambdas: dict_items([('pout', tensor([1.8280])), ('power', tensor([0.3030]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5102])), ('power', tensor([-17.0375]))])
epoch：695	 i:0 	 global-step:13900	 l-p:0.09014301002025604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[ 9.9725, 12.5206, 13.9129],
        [ 9.9725, 10.0856, 10.0030],
        [ 9.9725, 10.9587, 10.9632],
        [ 9.9725, 12.4698, 13.8046]], grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.09685522317886353 
model_pd.l_d.mean(): -4.23112678527832 
model_pd.lagr.mean(): -4.134271621704102 
model_pd.lambdas: dict_items([('pout', tensor([1.8285])), ('power', tensor([0.3021]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5218])), ('power', tensor([-17.1147]))])
epoch：696	 i:0 	 global-step:13920	 l-p:0.09685522317886353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[ 9.9498, 10.2521, 10.1014],
        [ 9.9498, 10.2114, 10.0697],
        [ 9.9498, 10.0662,  9.9818],
        [ 9.9498, 11.8492, 12.5615]], grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.09936495125293732 
model_pd.l_d.mean(): -4.216721534729004 
model_pd.lagr.mean(): -4.117356777191162 
model_pd.lambdas: dict_items([('pout', tensor([1.8290])), ('power', tensor([0.3012]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248])), ('power', tensor([-17.1344]))])
epoch：697	 i:0 	 global-step:13940	 l-p:0.09936495125293732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[10.0001, 10.0172, 10.0016],
        [10.0001, 10.4974, 10.3377],
        [10.0001, 11.3654, 11.6134],
        [10.0001, 10.0003, 10.0001]], grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.09430364519357681 
model_pd.l_d.mean(): -4.200654983520508 
model_pd.lagr.mean(): -4.106351375579834 
model_pd.lambdas: dict_items([('pout', tensor([1.8295])), ('power', tensor([0.3004]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5182])), ('power', tensor([-17.0906]))])
epoch：698	 i:0 	 global-step:13960	 l-p:0.09430364519357681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[10.0878, 12.6699, 14.0812],
        [10.0878, 10.6842, 10.5363],
        [10.0878, 10.0878, 10.0878],
        [10.0878, 13.3598, 15.6126]], grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.08869810402393341 
model_pd.l_d.mean(): -4.183722019195557 
model_pd.lagr.mean(): -4.095024108886719 
model_pd.lambdas: dict_items([('pout', tensor([1.8301])), ('power', tensor([0.2995]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067])), ('power', tensor([-17.0139]))])
epoch：699	 i:0 	 global-step:13980	 l-p:0.08869810402393341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[10.1845, 10.5152, 10.3570],
        [10.1845, 10.2210, 10.1894],
        [10.1845, 10.2548, 10.1984],
        [10.1845, 10.1845, 10.1845]], grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.0848509669303894 
model_pd.l_d.mean(): -4.166516304016113 
model_pd.lagr.mean(): -4.081665515899658 
model_pd.lambdas: dict_items([('pout', tensor([1.8305])), ('power', tensor([0.2987]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942])), ('power', tensor([-16.9291]))])
epoch：700	 i:0 	 global-step:14000	 l-p:0.0848509669303894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[10.2758, 12.2493, 12.9915],
        [10.2758, 10.3973, 10.3093],
        [10.2758, 10.2759, 10.2758],
        [10.2758, 10.2934, 10.2773]], grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.08236034214496613 
model_pd.l_d.mean(): -4.1494221687316895 
model_pd.lagr.mean(): -4.067061901092529 
model_pd.lambdas: dict_items([('pout', tensor([1.8310])), ('power', tensor([0.2979]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825])), ('power', tensor([-16.8490]))])
epoch：701	 i:0 	 global-step:14020	 l-p:0.08236034214496613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[10.3541, 10.8037, 10.6343],
        [10.3541, 10.9692, 10.8169],
        [10.3541, 10.3678, 10.3551],
        [10.3541, 10.3541, 10.3541]], grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.0807337537407875 
model_pd.l_d.mean(): -4.132694244384766 
model_pd.lagr.mean(): -4.051960468292236 
model_pd.lambdas: dict_items([('pout', tensor([1.8315])), ('power', tensor([0.2970]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4725])), ('power', tensor([-16.7800]))])
epoch：702	 i:0 	 global-step:14040	 l-p:0.0807337537407875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[10.4160, 10.9378, 10.7705],
        [10.4160, 10.4160, 10.4160],
        [10.4160, 12.0428, 12.4580],
        [10.4160, 11.0636, 10.9162]], grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.0796724185347557 
model_pd.l_d.mean(): -4.11649751663208 
model_pd.lagr.mean(): -4.036825180053711 
model_pd.lambdas: dict_items([('pout', tensor([1.8320])), ('power', tensor([0.2962]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647])), ('power', tensor([-16.7254]))])
epoch：703	 i:0 	 global-step:14060	 l-p:0.0796724185347557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[10.4602, 10.5844, 10.4945],
        [10.4602, 12.4578, 13.2000],
        [10.4602, 10.5838, 10.4942],
        [10.4602, 10.4702, 10.4608]], grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.0790061354637146 
model_pd.l_d.mean(): -4.100910186767578 
model_pd.lagr.mean(): -4.021903991699219 
model_pd.lambdas: dict_items([('pout', tensor([1.8324])), ('power', tensor([0.2953]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4592])), ('power', tensor([-16.6863]))])
epoch：704	 i:0 	 global-step:14080	 l-p:0.0790061354637146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[10.4868, 12.0367, 12.3825],
        [10.4868, 13.9061, 16.2619],
        [10.4868, 10.4868, 10.4868],
        [10.4868, 10.4881, 10.4868]], grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.0786353200674057 
model_pd.l_d.mean(): -4.085939407348633 
model_pd.lagr.mean(): -4.0073041915893555 
model_pd.lambdas: dict_items([('pout', tensor([1.8329])), ('power', tensor([0.2945]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558])), ('power', tensor([-16.6628]))])
epoch：705	 i:0 	 global-step:14100	 l-p:0.0786353200674057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.4969, 10.6216, 10.5313],
        [10.4969, 10.4977, 10.4969],
        [10.4969, 10.6210, 10.5310],
        [10.4969, 10.4975, 10.4969]], grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.0784994587302208 
model_pd.l_d.mean(): -4.071535587310791 
model_pd.lagr.mean(): -3.9930360317230225 
model_pd.lambdas: dict_items([('pout', tensor([1.8333])), ('power', tensor([0.2937]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546])), ('power', tensor([-16.6539]))])
epoch：706	 i:0 	 global-step:14120	 l-p:0.0784994587302208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[10.4922, 11.1456, 10.9970],
        [10.4922, 11.5392, 11.5448],
        [10.4922, 11.5064, 11.4948],
        [10.4922, 13.9139, 16.2714]], grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.07855997234582901 
model_pd.l_d.mean(): -4.057605266571045 
model_pd.lagr.mean(): -3.9790453910827637 
model_pd.lambdas: dict_items([('pout', tensor([1.8338])), ('power', tensor([0.2928]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4551])), ('power', tensor([-16.6580]))])
epoch：707	 i:0 	 global-step:14140	 l-p:0.07855997234582901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[10.4751, 10.9312, 10.7594],
        [10.4751, 10.4786, 10.4753],
        [10.4751, 12.4910, 13.2485],
        [10.4751, 10.5996, 10.5095]], grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.07879110425710678 
model_pd.l_d.mean(): -4.044031620025635 
model_pd.lagr.mean(): -3.965240478515625 
model_pd.lambdas: dict_items([('pout', tensor([1.8343])), ('power', tensor([0.2920]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4573])), ('power', tensor([-16.6731]))])
epoch：708	 i:0 	 global-step:14160	 l-p:0.07879110425710678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.4482, 11.1309, 10.9907],
        [10.4482, 12.4606, 13.2181],
        [10.4482, 10.4482, 10.4482],
        [10.4482, 10.4488, 10.4482]], grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.07917361706495285 
model_pd.l_d.mean(): -4.03068208694458 
model_pd.lagr.mean(): -3.9515085220336914 
model_pd.lambdas: dict_items([('pout', tensor([1.8347])), ('power', tensor([0.2912]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606])), ('power', tensor([-16.6968]))])
epoch：709	 i:0 	 global-step:14180	 l-p:0.07917361706495285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[10.4143, 12.2425, 12.8323],
        [10.4143, 10.4151, 10.4143],
        [10.4143, 11.8462, 12.1073],
        [10.4143, 11.9525, 12.2957]], grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.07969016581773758 
model_pd.l_d.mean(): -4.017430305480957 
model_pd.lagr.mean(): -3.9377400875091553 
model_pd.lambdas: dict_items([('pout', tensor([1.8352])), ('power', tensor([0.2903]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649])), ('power', tensor([-16.7268]))])
epoch：710	 i:0 	 global-step:14200	 l-p:0.07969016581773758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[10.3763, 10.4977, 10.4095],
        [10.3763, 11.0537, 10.9145],
        [10.3763, 10.6516, 10.5025],
        [10.3763, 13.0442, 14.5035]], grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.08032044768333435 
model_pd.l_d.mean(): -4.0041608810424805 
model_pd.lagr.mean(): -3.9238405227661133 
model_pd.lambdas: dict_items([('pout', tensor([1.8356])), ('power', tensor([0.2895]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697])), ('power', tensor([-16.7603]))])
epoch：711	 i:0 	 global-step:14220	 l-p:0.08032044768333435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[10.3373, 10.3551, 10.3388],
        [10.3373, 10.3374, 10.3373],
        [10.3373, 11.2682, 11.2239],
        [10.3373, 10.3373, 10.3373]], grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.08103526383638382 
model_pd.l_d.mean(): -3.990783214569092 
model_pd.lagr.mean(): -3.909747838973999 
model_pd.lambdas: dict_items([('pout', tensor([1.8361])), ('power', tensor([0.2887]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746])), ('power', tensor([-16.7946]))])
epoch：712	 i:0 	 global-step:14240	 l-p:0.08103526383638382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.3001, 12.2806, 13.0259],
        [10.3001, 11.1367, 11.0532],
        [10.3001, 10.3138, 10.3011],
        [10.3001, 10.3007, 10.3002]], grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.08179032057523727 
model_pd.l_d.mean(): -3.9772281646728516 
model_pd.lagr.mean(): -3.8954379558563232 
model_pd.lambdas: dict_items([('pout', tensor([1.8366])), ('power', tensor([0.2878]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4793])), ('power', tensor([-16.8273]))])
epoch：713	 i:0 	 global-step:14260	 l-p:0.08179032057523727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[10.2677, 11.8693, 12.2781],
        [10.2677, 10.2677, 10.2677],
        [10.2677, 10.9053, 10.7602],
        [10.2677, 10.4959, 10.3614]], grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.08252127468585968 
model_pd.l_d.mean(): -3.963456153869629 
model_pd.lagr.mean(): -3.880934953689575 
model_pd.lambdas: dict_items([('pout', tensor([1.8371])), ('power', tensor([0.2870]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4835])), ('power', tensor([-16.8558]))])
epoch：714	 i:0 	 global-step:14280	 l-p:0.08252127468585968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[10.2425, 10.2601, 10.2440],
        [10.2425, 13.3343, 15.3246],
        [10.2425, 11.7522, 12.0888],
        [10.2425, 10.3055, 10.2540]], grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.08314503729343414 
model_pd.l_d.mean(): -3.9494566917419434 
model_pd.lagr.mean(): -3.866311550140381 
model_pd.lambdas: dict_items([('pout', tensor([1.8376])), ('power', tensor([0.2861]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4867])), ('power', tensor([-16.8780]))])
epoch：715	 i:0 	 global-step:14300	 l-p:0.08314503729343414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[10.2262, 10.3431, 10.2578],
        [10.2262, 10.8926, 10.7556],
        [10.2262, 12.1908, 12.9301],
        [10.2262, 12.0175, 12.5952]], grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.0835733637213707 
model_pd.l_d.mean(): -3.9352355003356934 
model_pd.lagr.mean(): -3.8516621589660645 
model_pd.lambdas: dict_items([('pout', tensor([1.8381])), ('power', tensor([0.2853]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4888])), ('power', tensor([-16.8922]))])
epoch：716	 i:0 	 global-step:14320	 l-p:0.0835733637213707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[10.2202, 10.2202, 10.2202],
        [10.2202, 10.7313, 10.5674],
        [10.2202, 10.2202, 10.2202],
        [10.2202, 10.3412, 10.2536]], grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.0837387666106224 
model_pd.l_d.mean(): -3.920821189880371 
model_pd.lagr.mean(): -3.8370823860168457 
model_pd.lambdas: dict_items([('pout', tensor([1.8385])), ('power', tensor([0.2844]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4895])), ('power', tensor([-16.8975]))])
epoch：717	 i:0 	 global-step:14340	 l-p:0.0837387666106224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[10.2243, 13.5495, 15.8400],
        [10.2243, 13.3104, 15.2971],
        [10.2243, 12.9714, 14.5483],
        [10.2243, 11.1440, 11.1002]], grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.08362065255641937 
model_pd.l_d.mean(): -3.906249523162842 
model_pd.lagr.mean(): -3.822628974914551 
model_pd.lambdas: dict_items([('pout', tensor([1.8390])), ('power', tensor([0.2836]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4890])), ('power', tensor([-16.8938]))])
epoch：718	 i:0 	 global-step:14360	 l-p:0.08362065255641937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.2380, 10.5714, 10.4119],
        [10.2380, 10.3576, 10.2707],
        [10.2380, 10.3587, 10.2712],
        [10.2380, 10.2748, 10.2429]], grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.08325280249118805 
model_pd.l_d.mean(): -3.8915605545043945 
model_pd.lagr.mean(): -3.808307647705078 
model_pd.lambdas: dict_items([('pout', tensor([1.8395])), ('power', tensor([0.2828]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4873])), ('power', tensor([-16.8819]))])
epoch：719	 i:0 	 global-step:14380	 l-p:0.08325280249118805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[10.2595, 11.3848, 11.4495],
        [10.2595, 11.2808, 11.2862],
        [10.2595, 10.2596, 10.2595],
        [10.2595, 10.3227, 10.2711]], grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.08270686864852905 
model_pd.l_d.mean(): -3.8767948150634766 
model_pd.lagr.mean(): -3.7940878868103027 
model_pd.lambdas: dict_items([('pout', tensor([1.8400])), ('power', tensor([0.2819]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4845])), ('power', tensor([-16.8629]))])
epoch：720	 i:0 	 global-step:14400	 l-p:0.08270686864852905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[10.2870, 12.8776, 14.2639],
        [10.2870, 11.3115, 11.3170],
        [10.2870, 11.3452, 11.3691],
        [10.2870, 10.2882, 10.2870]], grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.08206422626972198 
model_pd.l_d.mean(): -3.861987352371216 
model_pd.lagr.mean(): -3.7799232006073 
model_pd.lambdas: dict_items([('pout', tensor([1.8405])), ('power', tensor([0.2811]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810])), ('power', tensor([-16.8387]))])
epoch：721	 i:0 	 global-step:14420	 l-p:0.08206422626972198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[10.3184, 10.3184, 10.3184],
        [10.3184, 10.7172, 10.5497],
        [10.3184, 10.3190, 10.3184],
        [10.3184, 10.3189, 10.3184]], grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.0813940167427063 
model_pd.l_d.mean(): -3.8471713066101074 
model_pd.lagr.mean(): -3.765777349472046 
model_pd.lambdas: dict_items([('pout', tensor([1.8410])), ('power', tensor([0.2802]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4770])), ('power', tensor([-16.8111]))])
epoch：722	 i:0 	 global-step:14440	 l-p:0.0813940167427063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[10.3517, 10.3523, 10.3518],
        [10.3517, 13.0141, 14.4707],
        [10.3517, 10.3552, 10.3519],
        [10.3517, 10.5823, 10.4464]], grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.08074474334716797 
model_pd.l_d.mean(): -3.8323745727539062 
model_pd.lagr.mean(): -3.7516298294067383 
model_pd.lambdas: dict_items([('pout', tensor([1.8414])), ('power', tensor([0.2794]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4727])), ('power', tensor([-16.7817]))])
epoch：723	 i:0 	 global-step:14460	 l-p:0.08074474334716797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[10.3855, 10.8376, 10.6674],
        [10.3855, 10.5046, 10.4176],
        [10.3855, 11.0321, 10.8851],
        [10.3855, 11.3891, 11.3777]], grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.08014525473117828 
model_pd.l_d.mean(): -3.8176209926605225 
model_pd.lagr.mean(): -3.737475633621216 
model_pd.lambdas: dict_items([('pout', tensor([1.8419])), ('power', tensor([0.2786]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685])), ('power', tensor([-16.7520]))])
epoch：724	 i:0 	 global-step:14480	 l-p:0.08014525473117828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[10.4182, 10.7382, 10.5789],
        [10.4182, 10.6506, 10.5136],
        [10.4182, 10.4182, 10.4182],
        [10.4182, 10.8721, 10.7012]], grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.07960978150367737 
model_pd.l_d.mean(): -3.8029284477233887 
model_pd.lagr.mean(): -3.723318576812744 
model_pd.lambdas: dict_items([('pout', tensor([1.8424])), ('power', tensor([0.2777]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4643])), ('power', tensor([-16.7230]))])
epoch：725	 i:0 	 global-step:14500	 l-p:0.07960978150367737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[10.4490, 10.4491, 10.4490],
        [10.4490, 12.2859, 12.8790],
        [10.4490, 10.7270, 10.5765],
        [10.4490, 13.1400, 14.6126]], grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.07914277166128159 
model_pd.l_d.mean(): -3.788311004638672 
model_pd.lagr.mean(): -3.7091681957244873 
model_pd.lambdas: dict_items([('pout', tensor([1.8428])), ('power', tensor([0.2769]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605])), ('power', tensor([-16.6958]))])
epoch：726	 i:0 	 global-step:14520	 l-p:0.07914277166128159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[10.4772, 10.4772, 10.4772],
        [10.4772, 12.3199, 12.9148],
        [10.4772, 10.4772, 10.4772],
        [10.4772, 11.1306, 10.9821]], grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.07874301075935364 
model_pd.l_d.mean(): -3.773777961730957 
model_pd.lagr.mean(): -3.695034980773926 
model_pd.lambdas: dict_items([('pout', tensor([1.8433])), ('power', tensor([0.2760]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569])), ('power', tensor([-16.6709]))])
epoch：727	 i:0 	 global-step:14540	 l-p:0.07874301075935364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[10.5023, 10.5023, 10.5023],
        [10.5023, 10.6231, 10.5349],
        [10.5023, 11.9499, 12.2143],
        [10.5023, 12.3501, 12.9468]], grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.07840617001056671 
model_pd.l_d.mean(): -3.759331703186035 
model_pd.lagr.mean(): -3.6809256076812744 
model_pd.lambdas: dict_items([('pout', tensor([1.8437])), ('power', tensor([0.2752]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538])), ('power', tensor([-16.6487]))])
epoch：728	 i:0 	 global-step:14560	 l-p:0.07840617001056671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.5242, 10.5251, 10.5243],
        [10.5242, 11.3829, 11.2976],
        [10.5242, 13.7139, 15.7685],
        [10.5242, 10.5249, 10.5243]], grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.0781262218952179 
model_pd.l_d.mean(): -3.7449727058410645 
model_pd.lagr.mean(): -3.666846513748169 
model_pd.lambdas: dict_items([('pout', tensor([1.8442])), ('power', tensor([0.2744]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4511])), ('power', tensor([-16.6293]))])
epoch：729	 i:0 	 global-step:14580	 l-p:0.0781262218952179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.5430, 11.6322, 11.6573],
        [10.5430, 12.1953, 12.6178],
        [10.5430, 10.6082, 10.5549],
        [10.5430, 10.5811, 10.5480]], grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.07789653539657593 
model_pd.l_d.mean(): -3.730696678161621 
model_pd.lagr.mean(): -3.6528000831604004 
model_pd.lambdas: dict_items([('pout', tensor([1.8446])), ('power', tensor([0.2736]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487])), ('power', tensor([-16.6127]))])
epoch：730	 i:0 	 global-step:14600	 l-p:0.07789653539657593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.5586, 12.2139, 12.6372],
        [10.5586, 12.5954, 13.3615],
        [10.5586, 11.2181, 11.0684],
        [10.5586, 10.5592, 10.5586]], grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.07771014422178268 
model_pd.l_d.mean(): -3.7164950370788574 
model_pd.lagr.mean(): -3.638784885406494 
model_pd.lambdas: dict_items([('pout', tensor([1.8451])), ('power', tensor([0.2727]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4468])), ('power', tensor([-16.5988]))])
epoch：731	 i:0 	 global-step:14620	 l-p:0.07771014422178268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[10.5716, 12.6136, 13.3830],
        [10.5716, 12.1386, 12.4888],
        [10.5716, 10.5751, 10.5717],
        [10.5716, 10.8535, 10.7010]], grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.07756014913320541 
model_pd.l_d.mean(): -3.7023584842681885 
model_pd.lagr.mean(): -3.62479829788208 
model_pd.lambdas: dict_items([('pout', tensor([1.8455])), ('power', tensor([0.2719]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4452])), ('power', tensor([-16.5873]))])
epoch：732	 i:0 	 global-step:14640	 l-p:0.07756014913320541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[10.5822, 10.5828, 10.5822],
        [10.5822, 11.2435, 11.0933],
        [10.5822, 10.5827, 10.5822],
        [10.5822, 10.5857, 10.5823]], grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.07743982970714569 
model_pd.l_d.mean(): -3.688277244567871 
model_pd.lagr.mean(): -3.610837459564209 
model_pd.lambdas: dict_items([('pout', tensor([1.8460])), ('power', tensor([0.2711]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4438])), ('power', tensor([-16.5779]))])
epoch：733	 i:0 	 global-step:14660	 l-p:0.07743982970714569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.5909, 10.5909, 10.5909],
        [10.5909, 12.0529, 12.3202],
        [10.5909, 12.4570, 13.0599],
        [10.5909, 12.2521, 12.6772]], grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.07734263688325882 
model_pd.l_d.mean(): -3.674240827560425 
model_pd.lagr.mean(): -3.596898078918457 
model_pd.lambdas: dict_items([('pout', tensor([1.8464])), ('power', tensor([0.2702]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4428])), ('power', tensor([-16.5702]))])
epoch：734	 i:0 	 global-step:14680	 l-p:0.07734263688325882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.5982, 12.6440, 13.4137],
        [10.5982, 10.9458, 10.7797],
        [10.5982, 10.5995, 10.5982],
        [10.5982, 12.2608, 12.6862]], grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.07726238667964935 
model_pd.l_d.mean(): -3.6602368354797363 
model_pd.lagr.mean(): -3.582974433898926 
model_pd.lambdas: dict_items([('pout', tensor([1.8469])), ('power', tensor([0.2694]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4419])), ('power', tensor([-16.5637]))])
epoch：735	 i:0 	 global-step:14700	 l-p:0.07726238667964935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[10.6045, 13.8224, 15.8956],
        [10.6045, 10.8420, 10.7021],
        [10.6045, 10.6430, 10.6096],
        [10.6045, 10.6229, 10.6061]], grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.07719322293996811 
model_pd.l_d.mean(): -3.646260976791382 
model_pd.lagr.mean(): -3.5690677165985107 
model_pd.lambdas: dict_items([('pout', tensor([1.8473])), ('power', tensor([0.2686]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411])), ('power', tensor([-16.5581]))])
epoch：736	 i:0 	 global-step:14720	 l-p:0.07719322293996811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[10.6104, 11.0230, 10.8498],
        [10.6104, 10.6109, 10.6104],
        [10.6104, 10.8481, 10.7080],
        [10.6104, 11.5715, 11.5264]], grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.07712969183921814 
model_pd.l_d.mean(): -3.6323020458221436 
model_pd.lagr.mean(): -3.5551724433898926 
model_pd.lambdas: dict_items([('pout', tensor([1.8477])), ('power', tensor([0.2678]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4403])), ('power', tensor([-16.5528]))])
epoch：737	 i:0 	 global-step:14740	 l-p:0.07712969183921814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[10.6163, 10.6347, 10.6178],
        [10.6163, 10.7387, 10.6493],
        [10.6163, 11.6466, 11.6353],
        [10.6163, 10.6168, 10.6163]], grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.07706692814826965 
model_pd.l_d.mean(): -3.618356227874756 
model_pd.lagr.mean(): -3.5412893295288086 
model_pd.lambdas: dict_items([('pout', tensor([1.8482])), ('power', tensor([0.2669]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396])), ('power', tensor([-16.5476]))])
epoch：738	 i:0 	 global-step:14760	 l-p:0.07706692814826965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[10.6225, 10.6225, 10.6225],
        [10.6225, 11.4911, 11.4050],
        [10.6225, 13.3655, 14.8674],
        [10.6225, 12.4955, 13.1007]], grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.07700072228908539 
model_pd.l_d.mean(): -3.6044209003448486 
model_pd.lagr.mean(): -3.5274202823638916 
model_pd.lambdas: dict_items([('pout', tensor([1.8486])), ('power', tensor([0.2661]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4388])), ('power', tensor([-16.5420]))])
epoch：739	 i:0 	 global-step:14780	 l-p:0.07700072228908539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[10.6296, 11.6615, 11.6503],
        [10.6296, 11.1656, 10.9941],
        [10.6296, 10.7522, 10.6627],
        [10.6296, 10.6302, 10.6296]], grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.07692746073007584 
model_pd.l_d.mean(): -3.5904927253723145 
model_pd.lagr.mean(): -3.5135653018951416 
model_pd.lambdas: dict_items([('pout', tensor([1.8491])), ('power', tensor([0.2653]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380])), ('power', tensor([-16.5358]))])
epoch：740	 i:0 	 global-step:14800	 l-p:0.07692746073007584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[10.6376, 12.2166, 12.5698],
        [10.6376, 10.6518, 10.6387],
        [10.6376, 13.3306, 14.7731],
        [10.6376, 13.5131, 15.1655]], grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.07684439420700073 
model_pd.l_d.mean(): -3.5765721797943115 
model_pd.lagr.mean(): -3.499727725982666 
model_pd.lambdas: dict_items([('pout', tensor([1.8495])), ('power', tensor([0.2644]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370])), ('power', tensor([-16.5286]))])
epoch：741	 i:0 	 global-step:14820	 l-p:0.07684439420700073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[10.6470, 11.7144, 11.7208],
        [10.6470, 10.7214, 10.6617],
        [10.6470, 12.7042, 13.4784],
        [10.6470, 10.6506, 10.6471]], grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.07674956321716309 
model_pd.l_d.mean(): -3.5626602172851562 
model_pd.lagr.mean(): -3.485910654067993 
model_pd.lambdas: dict_items([('pout', tensor([1.8499])), ('power', tensor([0.2636]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358])), ('power', tensor([-16.5203]))])
epoch：742	 i:0 	 global-step:14840	 l-p:0.07674956321716309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[10.6578, 10.7835, 10.6922],
        [10.6578, 13.4115, 14.9194],
        [10.6578, 12.7198, 13.4972],
        [10.6578, 11.3251, 11.1737]], grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.07664179056882858 
model_pd.l_d.mean(): -3.5487585067749023 
model_pd.lagr.mean(): -3.472116708755493 
model_pd.lambdas: dict_items([('pout', tensor([1.8504])), ('power', tensor([0.2628]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4345])), ('power', tensor([-16.5107]))])
epoch：743	 i:0 	 global-step:14860	 l-p:0.07664179056882858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[10.6702, 10.6702, 10.6702],
        [10.6702, 10.6805, 10.6708],
        [10.6702, 10.9998, 10.8358],
        [10.6702, 10.9096, 10.7686]], grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.07652072608470917 
model_pd.l_d.mean(): -3.534870147705078 
model_pd.lagr.mean(): -3.4583494663238525 
model_pd.lambdas: dict_items([('pout', tensor([1.8508])), ('power', tensor([0.2620]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4329])), ('power', tensor([-16.4996]))])
epoch：744	 i:0 	 global-step:14880	 l-p:0.07652072608470917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[10.6843, 11.0355, 10.8677],
        [10.6843, 10.9240, 10.7828],
        [10.6843, 10.6985, 10.6853],
        [10.6843, 11.5590, 11.4724]], grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.07638657093048096 
model_pd.l_d.mean(): -3.520996332168579 
model_pd.lagr.mean(): -3.4446096420288086 
model_pd.lambdas: dict_items([('pout', tensor([1.8512])), ('power', tensor([0.2611]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4312])), ('power', tensor([-16.4872]))])
epoch：745	 i:0 	 global-step:14900	 l-p:0.07638657093048096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[10.6999, 11.8828, 11.9518],
        [10.6999, 12.7691, 13.5480],
        [10.6999, 12.2898, 12.6457],
        [10.6999, 13.4662, 14.9812]], grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.07624025642871857 
model_pd.l_d.mean(): -3.507143020629883 
model_pd.lagr.mean(): -3.4309027194976807 
model_pd.lambdas: dict_items([('pout', tensor([1.8517])), ('power', tensor([0.2603]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4293])), ('power', tensor([-16.4732]))])
epoch：746	 i:0 	 global-step:14920	 l-p:0.07624025642871857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[10.7172, 11.7930, 11.7996],
        [10.7172, 10.7172, 10.7172],
        [10.7172, 11.1871, 11.0104],
        [10.7172, 10.8411, 10.7507]], grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.0760829895734787 
model_pd.l_d.mean(): -3.493311643600464 
model_pd.lagr.mean(): -3.4172286987304688 
model_pd.lambdas: dict_items([('pout', tensor([1.8521])), ('power', tensor([0.2595]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272])), ('power', tensor([-16.4579]))])
epoch：747	 i:0 	 global-step:14940	 l-p:0.0760829895734787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[10.7360, 11.2785, 11.1050],
        [10.7360, 11.7803, 11.7692],
        [10.7360, 11.0893, 10.9205],
        [10.7360, 12.8133, 13.5954]], grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.07591639459133148 
model_pd.l_d.mean(): -3.4795069694519043 
model_pd.lagr.mean(): -3.403590679168701 
model_pd.lambdas: dict_items([('pout', tensor([1.8525])), ('power', tensor([0.2587]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4249])), ('power', tensor([-16.4411]))])
epoch：748	 i:0 	 global-step:14960	 l-p:0.07591639459133148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[10.7562, 10.8845, 10.7915],
        [10.7562, 10.7562, 10.7562],
        [10.7562, 13.6687, 15.3428],
        [10.7562, 13.4839, 14.9454]], grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.0757422149181366 
model_pd.l_d.mean(): -3.4657297134399414 
model_pd.lagr.mean(): -3.3899874687194824 
model_pd.lambdas: dict_items([('pout', tensor([1.8529])), ('power', tensor([0.2579]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4224])), ('power', tensor([-16.4231]))])
epoch：749	 i:0 	 global-step:14980	 l-p:0.0757422149181366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[10.7777, 13.5117, 14.9766],
        [10.7777, 11.1113, 10.9454],
        [10.7777, 10.8170, 10.7829],
        [10.7777, 13.5671, 15.0950]], grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.07556219398975372 
model_pd.l_d.mean(): -3.4519851207733154 
model_pd.lagr.mean(): -3.376422882080078 
model_pd.lambdas: dict_items([('pout', tensor([1.8534])), ('power', tensor([0.2570]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4198])), ('power', tensor([-16.4040]))])
epoch：750	 i:0 	 global-step:15000	 l-p:0.07556219398975372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[10.8004, 12.8921, 13.6798],
        [10.8004, 13.5964, 15.1281],
        [10.8004, 12.8944, 13.6844],
        [10.8004, 12.5006, 12.9363]], grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.07537815719842911 
model_pd.l_d.mean(): -3.4382731914520264 
model_pd.lagr.mean(): -3.3628950119018555 
model_pd.lambdas: dict_items([('pout', tensor([1.8538])), ('power', tensor([0.2562]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170])), ('power', tensor([-16.3838]))])
epoch：751	 i:0 	 global-step:15020	 l-p:0.07537815719842911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[10.8240, 13.7575, 15.4440],
        [10.8240, 11.9125, 11.9194],
        [10.8240, 12.4355, 12.7965],
        [10.8240, 10.8249, 10.8241]], grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.07519174367189407 
model_pd.l_d.mean(): -3.424597978591919 
model_pd.lagr.mean(): -3.3494062423706055 
model_pd.lambdas: dict_items([('pout', tensor([1.8542])), ('power', tensor([0.2554]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4142])), ('power', tensor([-16.3627]))])
epoch：752	 i:0 	 global-step:15040	 l-p:0.07519174367189407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[10.8486, 11.3979, 11.2223],
        [10.8486, 11.1397, 10.9822],
        [10.8486, 11.1848, 11.0176],
        [10.8486, 10.8881, 10.8538]], grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.0750044733285904 
model_pd.l_d.mean(): -3.4109597206115723 
model_pd.lagr.mean(): -3.3359551429748535 
model_pd.lambdas: dict_items([('pout', tensor([1.8546])), ('power', tensor([0.2546]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112])), ('power', tensor([-16.3408]))])
epoch：753	 i:0 	 global-step:15060	 l-p:0.0750044733285904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[10.8738, 11.0045, 10.9099],
        [10.8738, 10.8739, 10.8738],
        [10.8738, 11.2325, 11.0613],
        [10.8738, 10.9417, 10.8863]], grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.07481759786605835 
model_pd.l_d.mean(): -3.3973617553710938 
model_pd.lagr.mean(): -3.3225440979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.8550])), ('power', tensor([0.2538]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4081])), ('power', tensor([-16.3183]))])
epoch：754	 i:0 	 global-step:15080	 l-p:0.07481759786605835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[10.8996, 12.9984, 13.7802],
        [10.8996, 10.8996, 10.8996],
        [10.8996, 10.9003, 10.8996],
        [10.8996, 11.7952, 11.7069]], grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.07463229447603226 
model_pd.l_d.mean(): -3.383802890777588 
model_pd.lagr.mean(): -3.3091704845428467 
model_pd.lambdas: dict_items([('pout', tensor([1.8554])), ('power', tensor([0.2529]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050])), ('power', tensor([-16.2952]))])
epoch：755	 i:0 	 global-step:15100	 l-p:0.07463229447603226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[10.9259, 12.0624, 12.0895],
        [10.9259, 10.9259, 10.9259],
        [10.9259, 11.2650, 11.0964],
        [10.9259, 10.9259, 10.9259]], grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.07444945722818375 
model_pd.l_d.mean(): -3.3702869415283203 
model_pd.lagr.mean(): -3.29583740234375 
model_pd.lambdas: dict_items([('pout', tensor([1.8558])), ('power', tensor([0.2521]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4019])), ('power', tensor([-16.2718]))])
epoch：756	 i:0 	 global-step:15120	 l-p:0.07444945722818375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[10.9525, 10.9562, 10.9526],
        [10.9525, 11.0209, 10.9650],
        [10.9525, 10.9631, 10.9531],
        [10.9525, 12.6805, 13.1236]], grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.07426976412534714 
model_pd.l_d.mean(): -3.3568127155303955 
model_pd.lagr.mean(): -3.2825429439544678 
model_pd.lambdas: dict_items([('pout', tensor([1.8562])), ('power', tensor([0.2513]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3987])), ('power', tensor([-16.2480]))])
epoch：757	 i:0 	 global-step:15140	 l-p:0.07426976412534714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[10.9793, 11.9802, 11.9339],
        [10.9793, 11.4629, 11.2812],
        [10.9793, 10.9793, 10.9793],
        [10.9793, 11.1115, 11.0158]], grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.0740937814116478 
model_pd.l_d.mean(): -3.343381404876709 
model_pd.lagr.mean(): -3.269287586212158 
model_pd.lambdas: dict_items([('pout', tensor([1.8566])), ('power', tensor([0.2505]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3955])), ('power', tensor([-16.2240]))])
epoch：758	 i:0 	 global-step:15160	 l-p:0.0740937814116478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[11.0062, 11.0751, 11.0189],
        [11.0062, 11.0063, 11.0062],
        [11.0062, 11.0069, 11.0063],
        [11.0062, 11.9122, 11.8229]], grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.07392185926437378 
model_pd.l_d.mean(): -3.3299946784973145 
model_pd.lagr.mean(): -3.256072759628296 
model_pd.lambdas: dict_items([('pout', tensor([1.8570])), ('power', tensor([0.2497]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3922])), ('power', tensor([-16.1999]))])
epoch：759	 i:0 	 global-step:15180	 l-p:0.07392185926437378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[11.0333, 11.0338, 11.0333],
        [11.0333, 11.0526, 11.0349],
        [11.0333, 12.1829, 12.2104],
        [11.0333, 12.5673, 12.8489]], grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.07375422865152359 
model_pd.l_d.mean(): -3.3166518211364746 
model_pd.lagr.mean(): -3.2428975105285645 
model_pd.lambdas: dict_items([('pout', tensor([1.8574])), ('power', tensor([0.2489]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3890])), ('power', tensor([-16.1757]))])
epoch：760	 i:0 	 global-step:15200	 l-p:0.07375422865152359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[11.0603, 13.2100, 14.0203],
        [11.0603, 11.0610, 11.0603],
        [11.0603, 11.3103, 11.1631],
        [11.0603, 11.4264, 11.2517]], grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.07359100133180618 
model_pd.l_d.mean(): -3.3033535480499268 
model_pd.lagr.mean(): -3.229762554168701 
model_pd.lambdas: dict_items([('pout', tensor([1.8578])), ('power', tensor([0.2481]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3858])), ('power', tensor([-16.1514]))])
epoch：761	 i:0 	 global-step:15220	 l-p:0.07359100133180618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[11.0873, 13.2455, 14.0604],
        [11.0873, 11.0879, 11.0873],
        [11.0873, 11.2204, 11.1240],
        [11.0873, 11.0873, 11.0873]], grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.07343216985464096 
model_pd.l_d.mean(): -3.2900986671447754 
model_pd.lagr.mean(): -3.2166664600372314 
model_pd.lambdas: dict_items([('pout', tensor([1.8582])), ('power', tensor([0.2473]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3826])), ('power', tensor([-16.1272]))])
epoch：762	 i:0 	 global-step:15240	 l-p:0.07343216985464096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[11.1143, 11.2484, 11.1514],
        [11.1143, 13.2760, 14.0910],
        [11.1143, 11.1143, 11.1143],
        [11.1143, 11.1251, 11.1149]], grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.07327768206596375 
model_pd.l_d.mean(): -3.2768890857696533 
model_pd.lagr.mean(): -3.203611373901367 
model_pd.lambdas: dict_items([('pout', tensor([1.8585])), ('power', tensor([0.2465]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3794])), ('power', tensor([-16.1030]))])
epoch：763	 i:0 	 global-step:15260	 l-p:0.07327768206596375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[11.1412, 11.1412, 11.1412],
        [11.1412, 12.2670, 12.2747],
        [11.1412, 11.4417, 11.2792],
        [11.1412, 11.1417, 11.1412]], grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.0731273666024208 
model_pd.l_d.mean(): -3.2637243270874023 
model_pd.lagr.mean(): -3.1905970573425293 
model_pd.lambdas: dict_items([('pout', tensor([1.8589])), ('power', tensor([0.2457]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3763])), ('power', tensor([-16.0789]))])
epoch：764	 i:0 	 global-step:15280	 l-p:0.0731273666024208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[11.1680, 14.2076, 15.9564],
        [11.1680, 11.1694, 11.1680],
        [11.1680, 14.5814, 16.7831],
        [11.1680, 11.5383, 11.3616]], grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.07298102974891663 
model_pd.l_d.mean(): -3.250603199005127 
model_pd.lagr.mean(): -3.177622079849243 
model_pd.lambdas: dict_items([('pout', tensor([1.8593])), ('power', tensor([0.2449]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3731])), ('power', tensor([-16.0548]))])
epoch：765	 i:0 	 global-step:15300	 l-p:0.07298102974891663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[11.1947, 11.1985, 11.1949],
        [11.1947, 11.1954, 11.1948],
        [11.1947, 12.3268, 12.3347],
        [11.1947, 11.8713, 11.7048]], grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.0728384330868721 
model_pd.l_d.mean(): -3.237527370452881 
model_pd.lagr.mean(): -3.1646888256073 
model_pd.lambdas: dict_items([('pout', tensor([1.8597])), ('power', tensor([0.2441]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3700])), ('power', tensor([-16.0308]))])
epoch：766	 i:0 	 global-step:15320	 l-p:0.0728384330868721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[11.2214, 13.4070, 14.2313],
        [11.2214, 14.2775, 16.0360],
        [11.2214, 12.3567, 12.3646],
        [11.2214, 11.2214, 11.2214]], grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.07269932329654694 
model_pd.l_d.mean(): -3.2244954109191895 
model_pd.lagr.mean(): -3.1517961025238037 
model_pd.lambdas: dict_items([('pout', tensor([1.8600])), ('power', tensor([0.2433]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3668])), ('power', tensor([-16.0068]))])
epoch：767	 i:0 	 global-step:15340	 l-p:0.07269932329654694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[11.2481, 14.1183, 15.6579],
        [11.2481, 11.2482, 11.2481],
        [11.2481, 12.9327, 13.3112],
        [11.2481, 11.2481, 11.2481]], grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.07256340980529785 
model_pd.l_d.mean(): -3.2115068435668945 
model_pd.lagr.mean(): -3.1389434337615967 
model_pd.lambdas: dict_items([('pout', tensor([1.8604])), ('power', tensor([0.2425]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3637])), ('power', tensor([-15.9828]))])
epoch：768	 i:0 	 global-step:15360	 l-p:0.07256340980529785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[11.2748, 11.7738, 11.5865],
        [11.2748, 11.6268, 11.4519],
        [11.2748, 13.2813, 13.9316],
        [11.2748, 11.2757, 11.2748]], grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.07243043184280396 
model_pd.l_d.mean(): -3.19856333732605 
model_pd.lagr.mean(): -3.1261329650878906 
model_pd.lambdas: dict_items([('pout', tensor([1.8607])), ('power', tensor([0.2417]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3606])), ('power', tensor([-15.9588]))])
epoch：769	 i:0 	 global-step:15380	 l-p:0.07243043184280396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[11.3015, 11.5579, 11.4070],
        [11.3015, 14.2455, 15.8600],
        [11.3015, 13.5074, 14.3410],
        [11.3015, 11.3815, 11.3173]], grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.07230008393526077 
model_pd.l_d.mean(): -3.1856629848480225 
model_pd.lagr.mean(): -3.1133627891540527 
model_pd.lambdas: dict_items([('pout', tensor([1.8611])), ('power', tensor([0.2409]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3575])), ('power', tensor([-15.9348]))])
epoch：770	 i:0 	 global-step:15400	 l-p:0.07230008393526077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[11.3282, 15.0646, 17.6439],
        [11.3282, 11.3282, 11.3282],
        [11.3282, 11.3481, 11.3299],
        [11.3282, 12.0454, 11.8834]], grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.07217210531234741 
model_pd.l_d.mean(): -3.172807455062866 
model_pd.lagr.mean(): -3.100635290145874 
model_pd.lambdas: dict_items([('pout', tensor([1.8615])), ('power', tensor([0.2401]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3544])), ('power', tensor([-15.9107]))])
epoch：771	 i:0 	 global-step:15420	 l-p:0.07217210531234741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[11.3551, 11.4266, 11.3682],
        [11.3551, 12.2946, 12.2025],
        [11.3551, 11.3551, 11.3551],
        [11.3551, 13.3780, 14.0338]], grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.07204624265432358 
model_pd.l_d.mean(): -3.1599960327148438 
model_pd.lagr.mean(): -3.087949752807617 
model_pd.lambdas: dict_items([('pout', tensor([1.8618])), ('power', tensor([0.2393]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3513])), ('power', tensor([-15.8865]))])
epoch：772	 i:0 	 global-step:15440	 l-p:0.07204624265432358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[11.3820, 13.6059, 14.4465],
        [11.3820, 11.3827, 11.3821],
        [11.3820, 12.3242, 12.2319],
        [11.3820, 11.8307, 11.6428]], grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.07192225754261017 
model_pd.l_d.mean(): -3.1472277641296387 
model_pd.lagr.mean(): -3.075305461883545 
model_pd.lambdas: dict_items([('pout', tensor([1.8622])), ('power', tensor([0.2385]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3482])), ('power', tensor([-15.8621]))])
epoch：773	 i:0 	 global-step:15460	 l-p:0.07192225754261017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[11.4092, 12.1683, 12.0137],
        [11.4092, 11.4204, 11.4099],
        [11.4092, 11.4092, 11.4092],
        [11.4092, 11.4098, 11.4092]], grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.07179990410804749 
model_pd.l_d.mean(): -3.134504795074463 
model_pd.lagr.mean(): -3.0627048015594482 
model_pd.lambdas: dict_items([('pout', tensor([1.8625])), ('power', tensor([0.2377]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3451])), ('power', tensor([-15.8377]))])
epoch：774	 i:0 	 global-step:15480	 l-p:0.07179990410804749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[11.4365, 11.4367, 11.4365],
        [11.4365, 11.8174, 11.6358],
        [11.4365, 13.4761, 14.1375],
        [11.4365, 11.4477, 11.4372]], grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.0716790109872818 
model_pd.l_d.mean(): -3.1218254566192627 
model_pd.lagr.mean(): -3.0501463413238525 
model_pd.lambdas: dict_items([('pout', tensor([1.8628])), ('power', tensor([0.2369]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3419])), ('power', tensor([-15.8130]))])
epoch：775	 i:0 	 global-step:15500	 l-p:0.0716790109872818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[11.4641, 11.9166, 11.7272],
        [11.4641, 11.7750, 11.6070],
        [11.4641, 12.7460, 12.8222],
        [11.4641, 13.5093, 14.1726]], grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.0715593621134758 
model_pd.l_d.mean(): -3.1091909408569336 
model_pd.lagr.mean(): -3.0376315116882324 
model_pd.lambdas: dict_items([('pout', tensor([1.8632])), ('power', tensor([0.2361]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3388])), ('power', tensor([-15.7880]))])
epoch：776	 i:0 	 global-step:15520	 l-p:0.0715593621134758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[11.4920, 15.0173, 17.2924],
        [11.4920, 12.4447, 12.3515],
        [11.4920, 13.2187, 13.6071],
        [11.4920, 11.8520, 11.6732]], grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.07144084572792053 
model_pd.l_d.mean(): -3.0966005325317383 
model_pd.lagr.mean(): -3.0251595973968506 
model_pd.lambdas: dict_items([('pout', tensor([1.8635])), ('power', tensor([0.2353]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3356])), ('power', tensor([-15.7629]))])
epoch：777	 i:0 	 global-step:15540	 l-p:0.07144084572792053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[11.5201, 11.6020, 11.5363],
        [11.5201, 13.3515, 13.8226],
        [11.5201, 12.8092, 12.8860],
        [11.5201, 12.1098, 11.9218]], grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.07132332026958466 
model_pd.l_d.mean(): -3.0840559005737305 
model_pd.lagr.mean(): -3.01273250579834 
model_pd.lambdas: dict_items([('pout', tensor([1.8639])), ('power', tensor([0.2345]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3324])), ('power', tensor([-15.7375]))])
epoch：778	 i:0 	 global-step:15560	 l-p:0.07132332026958466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[11.5485, 11.9338, 11.7501],
        [11.5485, 14.5655, 16.2208],
        [11.5485, 11.6840, 11.5852],
        [11.5485, 14.7055, 16.5232]], grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.07120665162801743 
model_pd.l_d.mean(): -3.071556568145752 
model_pd.lagr.mean(): -3.000349998474121 
model_pd.lambdas: dict_items([('pout', tensor([1.8642])), ('power', tensor([0.2338]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3291])), ('power', tensor([-15.7117]))])
epoch：779	 i:0 	 global-step:15580	 l-p:0.07120665162801743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228]])
 pt:tensor([[11.5773, 12.0921, 11.8991],
        [11.5773, 14.6028, 16.2628],
        [11.5773, 13.6455, 14.3166],
        [11.5773, 12.2810, 12.1082]], grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.07109077274799347 
model_pd.l_d.mean(): -3.059102773666382 
model_pd.lagr.mean(): -2.9880120754241943 
model_pd.lambdas: dict_items([('pout', tensor([1.8645])), ('power', tensor([0.2330]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3259])), ('power', tensor([-15.6857]))])
epoch：780	 i:0 	 global-step:15600	 l-p:0.07109077274799347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[11.6064, 14.6405, 16.3053],
        [11.6064, 12.5702, 12.4760],
        [11.6064, 11.6222, 11.6076],
        [11.6064, 11.8711, 11.7153]], grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.07097563147544861 
model_pd.l_d.mean(): -3.046694755554199 
model_pd.lagr.mean(): -2.9757192134857178 
model_pd.lambdas: dict_items([('pout', tensor([1.8648])), ('power', tensor([0.2322]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3226])), ('power', tensor([-15.6594]))])
epoch：781	 i:0 	 global-step:15620	 l-p:0.07097563147544861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[11.6358, 12.0245, 11.8392],
        [11.6358, 12.0963, 11.9036],
        [11.6358, 12.8586, 12.8889],
        [11.6358, 13.8975, 14.7421]], grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.07086114585399628 
model_pd.l_d.mean(): -3.0343337059020996 
model_pd.lagr.mean(): -2.963472604751587 
model_pd.lambdas: dict_items([('pout', tensor([1.8651])), ('power', tensor([0.2314]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3193])), ('power', tensor([-15.6327]))])
epoch：782	 i:0 	 global-step:15640	 l-p:0.07086114585399628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[11.6657, 11.6658, 11.6657],
        [11.6657, 13.4223, 13.8178],
        [11.6657, 11.8078, 11.7050],
        [11.6657, 11.6657, 11.6657]], grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.07074730843305588 
model_pd.l_d.mean(): -3.0220184326171875 
model_pd.lagr.mean(): -2.9512710571289062 
model_pd.lambdas: dict_items([('pout', tensor([1.8655])), ('power', tensor([0.2306]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3159])), ('power', tensor([-15.6057]))])
epoch：783	 i:0 	 global-step:15660	 l-p:0.07074730843305588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[11.6958, 12.6682, 12.5733],
        [11.6958, 11.6959, 11.6958],
        [11.6958, 12.8496, 12.8390],
        [11.6958, 12.0634, 11.8809]], grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.07063410431146622 
model_pd.l_d.mean(): -3.0097508430480957 
model_pd.lagr.mean(): -2.9391167163848877 
model_pd.lambdas: dict_items([('pout', tensor([1.8658])), ('power', tensor([0.2298]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3125])), ('power', tensor([-15.5783]))])
epoch：784	 i:0 	 global-step:15680	 l-p:0.07063410431146622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[11.7263, 11.8100, 11.7429],
        [11.7263, 12.9209, 12.9301],
        [11.7263, 11.7265, 11.7264],
        [11.7263, 11.8643, 11.7637]], grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.07052149623632431 
model_pd.l_d.mean(): -2.997530460357666 
model_pd.lagr.mean(): -2.927008867263794 
model_pd.lambdas: dict_items([('pout', tensor([1.8661])), ('power', tensor([0.2291]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3091])), ('power', tensor([-15.5506]))])
epoch：785	 i:0 	 global-step:15700	 l-p:0.07052149623632431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[11.7572, 12.1507, 11.9632],
        [11.7572, 11.7572, 11.7572],
        [11.7572, 15.6528, 18.3437],
        [11.7572, 13.6318, 14.1145]], grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.07040952146053314 
model_pd.l_d.mean(): -2.985358238220215 
model_pd.lagr.mean(): -2.9149487018585205 
model_pd.lambdas: dict_items([('pout', tensor([1.8664])), ('power', tensor([0.2283]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3057])), ('power', tensor([-15.5226]))])
epoch：786	 i:0 	 global-step:15720	 l-p:0.07040952146053314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[11.7885, 11.9273, 11.8260],
        [11.7885, 12.5397, 12.3704],
        [11.7885, 11.7885, 11.7885],
        [11.7885, 11.7894, 11.7885]], grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.0702981948852539 
model_pd.l_d.mean(): -2.973234176635742 
model_pd.lagr.mean(): -2.9029359817504883 
model_pd.lambdas: dict_items([('pout', tensor([1.8667])), ('power', tensor([0.2275]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3022])), ('power', tensor([-15.4942]))])
epoch：787	 i:0 	 global-step:15740	 l-p:0.0702981948852539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[11.8201, 11.8208, 11.8201],
        [11.8201, 14.8558, 16.4860],
        [11.8201, 11.8210, 11.8201],
        [11.8201, 14.1389, 15.0151]], grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.070187509059906 
model_pd.l_d.mean(): -2.9611587524414062 
model_pd.lagr.mean(): -2.8909711837768555 
model_pd.lambdas: dict_items([('pout', tensor([1.8670])), ('power', tensor([0.2267]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2987])), ('power', tensor([-15.4655]))])
epoch：788	 i:0 	 global-step:15760	 l-p:0.070187509059906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[11.8520, 13.1839, 13.2638],
        [11.8520, 15.1025, 16.9750],
        [11.8520, 14.1806, 15.0620],
        [11.8520, 11.8535, 11.8520]], grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.07007750868797302 
model_pd.l_d.mean(): -2.9491310119628906 
model_pd.lagr.mean(): -2.8790535926818848 
model_pd.lambdas: dict_items([('pout', tensor([1.8673])), ('power', tensor([0.2260]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2951])), ('power', tensor([-15.4364]))])
epoch：789	 i:0 	 global-step:15780	 l-p:0.07007750868797302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[11.8842, 12.0274, 11.9235],
        [11.8842, 13.6785, 14.0830],
        [11.8842, 12.6802, 12.5186],
        [11.8842, 12.0295, 11.9245]], grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.06996821612119675 
model_pd.l_d.mean(): -2.9371535778045654 
model_pd.lagr.mean(): -2.867185354232788 
model_pd.lambdas: dict_items([('pout', tensor([1.8676])), ('power', tensor([0.2252]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2916])), ('power', tensor([-15.4071]))])
epoch：790	 i:0 	 global-step:15800	 l-p:0.06996821612119675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[11.9168, 11.9178, 11.9168],
        [11.9168, 13.1736, 13.2052],
        [11.9168, 11.9380, 11.9186],
        [11.9168, 11.9183, 11.9168]], grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.06985965371131897 
model_pd.l_d.mean(): -2.925225257873535 
model_pd.lagr.mean(): -2.855365514755249 
model_pd.lambdas: dict_items([('pout', tensor([1.8679])), ('power', tensor([0.2244]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2880])), ('power', tensor([-15.3775]))])
epoch：791	 i:0 	 global-step:15820	 l-p:0.06985965371131897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[11.9497, 12.4839, 12.2838],
        [11.9497, 11.9506, 11.9497],
        [11.9497, 15.6328, 18.0115],
        [11.9497, 13.2104, 13.2422]], grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.06975187361240387 
model_pd.l_d.mean(): -2.913347005844116 
model_pd.lagr.mean(): -2.843595027923584 
model_pd.lambdas: dict_items([('pout', tensor([1.8682])), ('power', tensor([0.2237]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2844])), ('power', tensor([-15.3475]))])
epoch：792	 i:0 	 global-step:15840	 l-p:0.06975187361240387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[11.9828, 11.9828, 11.9828],
        [11.9828, 12.1244, 12.0211],
        [11.9828, 12.0590, 11.9968],
        [11.9828, 13.8984, 14.3922]], grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.06964488327503204 
model_pd.l_d.mean(): -2.901517868041992 
model_pd.lagr.mean(): -2.8318729400634766 
model_pd.lambdas: dict_items([('pout', tensor([1.8684])), ('power', tensor([0.2229]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2807])), ('power', tensor([-15.3173]))])
epoch：793	 i:0 	 global-step:15860	 l-p:0.06964488327503204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[12.0163, 14.3814, 15.2771],
        [12.0163, 12.0163, 12.0163],
        [12.0163, 12.1626, 12.0567],
        [12.0163, 13.2448, 13.2547]], grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.06953871250152588 
model_pd.l_d.mean(): -2.889739513397217 
model_pd.lagr.mean(): -2.8202009201049805 
model_pd.lambdas: dict_items([('pout', tensor([1.8687])), ('power', tensor([0.2221]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2771])), ('power', tensor([-15.2868]))])
epoch：794	 i:0 	 global-step:15880	 l-p:0.06953871250152588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[12.0500, 13.9777, 14.4748],
        [12.0500, 12.0507, 12.0500],
        [12.0500, 13.8727, 14.2840],
        [12.0500, 12.0500, 12.0500]], grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.06943340599536896 
model_pd.l_d.mean(): -2.8780112266540527 
model_pd.lagr.mean(): -2.8085777759552 
model_pd.lambdas: dict_items([('pout', tensor([1.8690])), ('power', tensor([0.2214]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2734])), ('power', tensor([-15.2561]))])
epoch：795	 i:0 	 global-step:15900	 l-p:0.06943340599536896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[12.0839, 14.0178, 14.5165],
        [12.0839, 14.4444, 15.3271],
        [12.0839, 12.4658, 12.2763],
        [12.0839, 13.7867, 14.1017]], grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.06932894885540009 
model_pd.l_d.mean(): -2.8663339614868164 
model_pd.lagr.mean(): -2.7970049381256104 
model_pd.lambdas: dict_items([('pout', tensor([1.8693])), ('power', tensor([0.2206]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2697])), ('power', tensor([-15.2251]))])
epoch：796	 i:0 	 global-step:15920	 l-p:0.06932894885540009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[12.1181, 12.2646, 12.1583],
        [12.1181, 12.2667, 12.1592],
        [12.1181, 14.2963, 15.0045],
        [12.1181, 12.2659, 12.1589]], grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.06922539323568344 
model_pd.l_d.mean(): -2.854707717895508 
model_pd.lagr.mean(): -2.785482406616211 
model_pd.lambdas: dict_items([('pout', tensor([1.8695])), ('power', tensor([0.2199]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2660])), ('power', tensor([-15.1939]))])
epoch：797	 i:0 	 global-step:15940	 l-p:0.06922539323568344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[12.1525, 15.4955, 17.4223],
        [12.1525, 13.9929, 14.4083],
        [12.1525, 12.1978, 12.1585],
        [12.1525, 12.1525, 12.1525]], grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.06912273168563843 
model_pd.l_d.mean(): -2.8431320190429688 
model_pd.lagr.mean(): -2.7740092277526855 
model_pd.lambdas: dict_items([('pout', tensor([1.8698])), ('power', tensor([0.2191]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2623])), ('power', tensor([-15.1625]))])
epoch：798	 i:0 	 global-step:15960	 l-p:0.06912273168563843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[12.1871, 12.2649, 12.2014],
        [12.1871, 14.1398, 14.6436],
        [12.1871, 14.3794, 15.0923],
        [12.1871, 12.9340, 12.7511]], grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.06902099400758743 
model_pd.l_d.mean(): -2.831608295440674 
model_pd.lagr.mean(): -2.762587308883667 
model_pd.lambdas: dict_items([('pout', tensor([1.8700])), ('power', tensor([0.2183]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2586])), ('power', tensor([-15.1308]))])
epoch：799	 i:0 	 global-step:15980	 l-p:0.06902099400758743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[12.2220, 15.9989, 18.4391],
        [12.2220, 12.3713, 12.2632],
        [12.2220, 12.2341, 12.2227],
        [12.2220, 14.1809, 14.6864]], grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.06892017275094986 
model_pd.l_d.mean(): -2.8201351165771484 
model_pd.lagr.mean(): -2.7512149810791016 
model_pd.lambdas: dict_items([('pout', tensor([1.8703])), ('power', tensor([0.2176]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2548])), ('power', tensor([-15.0989]))])
epoch：800	 i:0 	 global-step:16000	 l-p:0.06892017275094986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[12.2570, 12.4068, 12.2984],
        [12.2570, 13.5549, 13.5880],
        [12.2570, 12.5391, 12.3732],
        [12.2570, 12.6452, 12.4526]], grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.0688202753663063 
model_pd.l_d.mean(): -2.808713436126709 
model_pd.lagr.mean(): -2.7398931980133057 
model_pd.lambdas: dict_items([('pout', tensor([1.8705])), ('power', tensor([0.2168]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2511])), ('power', tensor([-15.0669]))])
epoch：801	 i:0 	 global-step:16020	 l-p:0.0688202753663063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[12.2922, 12.4426, 12.3337],
        [12.2922, 12.7066, 12.5093],
        [12.2922, 12.3044, 12.2930],
        [12.2922, 14.5059, 15.2259]], grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.06872130185365677 
model_pd.l_d.mean(): -2.7973439693450928 
model_pd.lagr.mean(): -2.7286226749420166 
model_pd.lambdas: dict_items([('pout', tensor([1.8708])), ('power', tensor([0.2161]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2473])), ('power', tensor([-15.0346]))])
epoch：802	 i:0 	 global-step:16040	 l-p:0.06872130185365677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[12.3276, 12.3292, 12.3276],
        [12.3276, 12.3276, 12.3276],
        [12.3276, 13.1579, 12.9897],
        [12.3276, 13.6341, 13.6675]], grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.06862326711416245 
model_pd.l_d.mean(): -2.7860260009765625 
model_pd.lagr.mean(): -2.717402696609497 
model_pd.lambdas: dict_items([('pout', tensor([1.8710])), ('power', tensor([0.2153]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2435])), ('power', tensor([-15.0022]))])
epoch：803	 i:0 	 global-step:16060	 l-p:0.06862326711416245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[12.3632, 12.3675, 12.3634],
        [12.3632, 12.4095, 12.3693],
        [12.3632, 12.3640, 12.3632],
        [12.3632, 12.3634, 12.3632]], grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.06852613389492035 
model_pd.l_d.mean(): -2.7747600078582764 
model_pd.lagr.mean(): -2.7062339782714844 
model_pd.lambdas: dict_items([('pout', tensor([1.8713])), ('power', tensor([0.2146]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2398])), ('power', tensor([-14.9696]))])
epoch：804	 i:0 	 global-step:16080	 l-p:0.06852613389492035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[12.3990, 12.5516, 12.4413],
        [12.3990, 13.2348, 13.0655],
        [12.3990, 15.8178, 17.7890],
        [12.3990, 12.5508, 12.4409]], grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.06842991709709167 
model_pd.l_d.mean(): -2.7635464668273926 
model_pd.lagr.mean(): -2.6951165199279785 
model_pd.lambdas: dict_items([('pout', tensor([1.8715])), ('power', tensor([0.2138]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2360])), ('power', tensor([-14.9368]))])
epoch：805	 i:0 	 global-step:16100	 l-p:0.06842991709709167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[12.4349, 13.5907, 13.5394],
        [12.4349, 14.6775, 15.4073],
        [12.4349, 12.7218, 12.5531],
        [12.4349, 13.7543, 13.7883]], grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.06833457201719284 
model_pd.l_d.mean(): -2.752385139465332 
model_pd.lagr.mean(): -2.6840505599975586 
model_pd.lambdas: dict_items([('pout', tensor([1.8717])), ('power', tensor([0.2131]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2322])), ('power', tensor([-14.9038]))])
epoch：806	 i:0 	 global-step:16120	 l-p:0.06833457201719284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[12.4710, 13.1175, 12.9120],
        [12.4710, 15.6947, 17.4279],
        [12.4710, 14.4752, 14.9928],
        [12.4710, 12.6193, 12.5112]], grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.06824012100696564 
model_pd.l_d.mean(): -2.7412757873535156 
model_pd.lagr.mean(): -2.6730356216430664 
model_pd.lambdas: dict_items([('pout', tensor([1.8720])), ('power', tensor([0.2123]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2284])), ('power', tensor([-14.8707]))])
epoch：807	 i:0 	 global-step:16140	 l-p:0.06824012100696564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[12.5073, 13.0704, 12.8597],
        [12.5073, 12.6561, 12.5476],
        [12.5073, 14.5180, 15.0374],
        [12.5073, 12.5245, 12.5085]], grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.06814654171466827 
model_pd.l_d.mean(): -2.7302191257476807 
model_pd.lagr.mean(): -2.6620726585388184 
model_pd.lambdas: dict_items([('pout', tensor([1.8722])), ('power', tensor([0.2116]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2246])), ('power', tensor([-14.8374]))])
epoch：808	 i:0 	 global-step:16160	 l-p:0.06814654171466827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[12.5437, 12.5481, 12.5438],
        [12.5437, 13.1945, 12.9877],
        [12.5437, 14.8084, 15.5457],
        [12.5437, 12.9679, 12.7660]], grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.06805379688739777 
model_pd.l_d.mean(): -2.719215154647827 
model_pd.lagr.mean(): -2.6511614322662354 
model_pd.lambdas: dict_items([('pout', tensor([1.8724])), ('power', tensor([0.2109]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2208])), ('power', tensor([-14.8039]))])
epoch：809	 i:0 	 global-step:16180	 l-p:0.06805379688739777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[12.5803, 12.5803, 12.5803],
        [12.5803, 13.6376, 13.5355],
        [12.5803, 15.9012, 17.7265],
        [12.5803, 16.7807, 19.6852]], grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.06796187907457352 
model_pd.l_d.mean(): -2.708263635635376 
model_pd.lagr.mean(): -2.6403017044067383 
model_pd.lambdas: dict_items([('pout', tensor([1.8726])), ('power', tensor([0.2101]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2169])), ('power', tensor([-14.7702]))])
epoch：810	 i:0 	 global-step:16200	 l-p:0.06796187907457352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[12.6171, 13.7922, 13.7402],
        [12.6171, 12.9647, 12.7771],
        [12.6171, 13.6779, 13.5755],
        [12.6171, 14.6477, 15.1725]], grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.06787078082561493 
model_pd.l_d.mean(): -2.6973650455474854 
model_pd.lagr.mean(): -2.6294941902160645 
model_pd.lambdas: dict_items([('pout', tensor([1.8728])), ('power', tensor([0.2094]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2131])), ('power', tensor([-14.7364]))])
epoch：811	 i:0 	 global-step:16220	 l-p:0.06787078082561493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[12.6540, 12.9468, 12.7747],
        [12.6540, 16.1513, 18.1685],
        [12.6540, 13.9167, 13.9064],
        [12.6540, 12.6542, 12.6540]], grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.06778045743703842 
model_pd.l_d.mean(): -2.6865198612213135 
model_pd.lagr.mean(): -2.618739366531372 
model_pd.lambdas: dict_items([('pout', tensor([1.8731])), ('power', tensor([0.2087]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2093])), ('power', tensor([-14.7024]))])
epoch：812	 i:0 	 global-step:16240	 l-p:0.06778045743703842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[12.6911, 15.9784, 17.7463],
        [12.6911, 12.7726, 12.7061],
        [12.6911, 13.0953, 12.8949],
        [12.6911, 12.6919, 12.6911]], grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.0676908940076828 
model_pd.l_d.mean(): -2.675727605819702 
model_pd.lagr.mean(): -2.608036756515503 
model_pd.lambdas: dict_items([('pout', tensor([1.8733])), ('power', tensor([0.2079]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2055])), ('power', tensor([-14.6683]))])
epoch：813	 i:0 	 global-step:16260	 l-p:0.0676908940076828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[12.7284, 12.7284, 12.7284],
        [12.7284, 13.7999, 13.6966],
        [12.7284, 12.7300, 12.7284],
        [12.7284, 13.5491, 13.3649]], grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.06760209053754807 
model_pd.l_d.mean(): -2.6649885177612305 
model_pd.lagr.mean(): -2.597386360168457 
model_pd.lambdas: dict_items([('pout', tensor([1.8735])), ('power', tensor([0.2072]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2016])), ('power', tensor([-14.6339]))])
epoch：814	 i:0 	 global-step:16280	 l-p:0.06760209053754807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[12.7659, 13.0616, 12.8878],
        [12.7659, 16.1414, 17.9973],
        [12.7659, 12.8480, 12.7810],
        [12.7659, 12.7666, 12.7659]], grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.06751400977373123 
model_pd.l_d.mean(): -2.6543021202087402 
model_pd.lagr.mean(): -2.5867881774902344 
model_pd.lambdas: dict_items([('pout', tensor([1.8737])), ('power', tensor([0.2065]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1978])), ('power', tensor([-14.5994]))])
epoch：815	 i:0 	 global-step:16300	 l-p:0.06751400977373123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[12.8035, 13.3820, 13.1657],
        [12.8035, 14.1243, 14.1360],
        [12.8035, 16.3469, 18.3910],
        [12.8035, 13.4699, 13.2582]], grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.06742662191390991 
model_pd.l_d.mean(): -2.643669843673706 
model_pd.lagr.mean(): -2.5762431621551514 
model_pd.lambdas: dict_items([('pout', tensor([1.8739])), ('power', tensor([0.2057]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1939])), ('power', tensor([-14.5647]))])
epoch：816	 i:0 	 global-step:16320	 l-p:0.06742662191390991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[12.8413, 12.8591, 12.8426],
        [12.8413, 14.0403, 13.9875],
        [12.8413, 12.9948, 12.8829],
        [12.8413, 15.3689, 16.3158]], grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.0673399269580841 
model_pd.l_d.mean(): -2.6330907344818115 
model_pd.lagr.mean(): -2.56575083732605 
model_pd.lambdas: dict_items([('pout', tensor([1.8740])), ('power', tensor([0.2050]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1900])), ('power', tensor([-14.5297]))])
epoch：817	 i:0 	 global-step:16340	 l-p:0.0673399269580841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[12.8794, 13.7524, 13.5760],
        [12.8794, 14.2091, 14.2209],
        [12.8794, 12.8794, 12.8794],
        [12.8794, 12.8794, 12.8794]], grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.06725388765335083 
model_pd.l_d.mean(): -2.622565269470215 
model_pd.lagr.mean(): -2.555311441421509 
model_pd.lambdas: dict_items([('pout', tensor([1.8742])), ('power', tensor([0.2043]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1862])), ('power', tensor([-14.4947]))])
epoch：818	 i:0 	 global-step:16360	 l-p:0.06725388765335083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[12.9176, 12.9355, 12.9189],
        [12.9176, 14.8894, 15.3361],
        [12.9176, 12.9176, 12.9176],
        [12.9176, 13.2748, 13.0821]], grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.06716851890087128 
model_pd.l_d.mean(): -2.612093925476074 
model_pd.lagr.mean(): -2.5449254512786865 
model_pd.lambdas: dict_items([('pout', tensor([1.8744])), ('power', tensor([0.2036]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1823])), ('power', tensor([-14.4594]))])
epoch：819	 i:0 	 global-step:16380	 l-p:0.06716851890087128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[12.9560, 14.7986, 15.1410],
        [12.9560, 14.2946, 14.3067],
        [12.9560, 13.2568, 13.0800],
        [12.9560, 13.5424, 13.3232]], grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.06708376109600067 
model_pd.l_d.mean(): -2.601675510406494 
model_pd.lagr.mean(): -2.5345916748046875 
model_pd.lambdas: dict_items([('pout', tensor([1.8746])), ('power', tensor([0.2028]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1784])), ('power', tensor([-14.4239]))])
epoch：820	 i:0 	 global-step:16400	 l-p:0.06708376109600067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[12.9946, 14.8433, 15.1870],
        [12.9946, 13.8765, 13.6984],
        [12.9946, 13.4364, 13.2262],
        [12.9946, 16.3695, 18.1854]], grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.06699962913990021 
model_pd.l_d.mean(): -2.5913119316101074 
model_pd.lagr.mean(): -2.5243122577667236 
model_pd.lambdas: dict_items([('pout', tensor([1.8748])), ('power', tensor([0.2021]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1745])), ('power', tensor([-14.3882]))])
epoch：821	 i:0 	 global-step:16420	 l-p:0.06699962913990021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[13.0334, 13.0464, 13.0342],
        [13.0334, 13.0350, 13.0334],
        [13.0334, 14.2527, 14.1993],
        [13.0334, 13.1175, 13.0488]], grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.0669160857796669 
model_pd.l_d.mean(): -2.581003189086914 
model_pd.lagr.mean(): -2.514087200164795 
model_pd.lambdas: dict_items([('pout', tensor([1.8749])), ('power', tensor([0.2014]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1706])), ('power', tensor([-14.3523]))])
epoch：822	 i:0 	 global-step:16440	 l-p:0.0669160857796669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[13.0724, 13.1567, 13.0879],
        [13.0724, 13.0724, 13.0724],
        [13.0724, 13.1217, 13.0789],
        [13.0724, 13.0769, 13.0725]], grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.06683314591646194 
model_pd.l_d.mean(): -2.5707473754882812 
model_pd.lagr.mean(): -2.5039141178131104 
model_pd.lambdas: dict_items([('pout', tensor([1.8751])), ('power', tensor([0.2007]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1667])), ('power', tensor([-14.3162]))])
epoch：823	 i:0 	 global-step:16460	 l-p:0.06683314591646194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[13.1116, 15.7202, 16.7107],
        [13.1116, 13.1116, 13.1116],
        [13.1116, 13.1132, 13.1116],
        [13.1116, 13.1122, 13.1116]], grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.06675075739622116 
model_pd.l_d.mean(): -2.560546875 
model_pd.lagr.mean(): -2.4937961101531982 
model_pd.lambdas: dict_items([('pout', tensor([1.8753])), ('power', tensor([0.2000]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1627])), ('power', tensor([-14.2800]))])
epoch：824	 i:0 	 global-step:16480	 l-p:0.06675075739622116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[13.1510, 14.5125, 14.5249],
        [13.1510, 13.3136, 13.1959],
        [13.1510, 17.5627, 20.6153],
        [13.1510, 15.0248, 15.3734]], grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.06666895747184753 
model_pd.l_d.mean(): -2.5504002571105957 
model_pd.lagr.mean(): -2.483731269836426 
model_pd.lambdas: dict_items([('pout', tensor([1.8754])), ('power', tensor([0.1993]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1588])), ('power', tensor([-14.2435]))])
epoch：825	 i:0 	 global-step:16500	 l-p:0.06666895747184753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[13.1906, 13.2758, 13.2063],
        [13.1906, 15.2093, 15.6671],
        [13.1906, 15.8138, 16.8084],
        [13.1906, 13.1906, 13.1906]], grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.06658769398927689 
model_pd.l_d.mean(): -2.540308713912964 
model_pd.lagr.mean(): -2.4737210273742676 
model_pd.lambdas: dict_items([('pout', tensor([1.8756])), ('power', tensor([0.1985]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1549])), ('power', tensor([-14.2068]))])
epoch：826	 i:0 	 global-step:16520	 l-p:0.06658769398927689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[13.2304, 13.3950, 13.2760],
        [13.2304, 15.3724, 15.9272],
        [13.2304, 17.3547, 20.0227],
        [13.2304, 13.3160, 13.2461]], grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.06650695949792862 
model_pd.l_d.mean(): -2.5302720069885254 
model_pd.lagr.mean(): -2.4637651443481445 
model_pd.lambdas: dict_items([('pout', tensor([1.8757])), ('power', tensor([0.1978]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1509])), ('power', tensor([-14.1698]))])
epoch：827	 i:0 	 global-step:16540	 l-p:0.06650695949792862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[13.2704, 13.2704, 13.2704],
        [13.2704, 13.2705, 13.2704],
        [13.2704, 13.5797, 13.3980],
        [13.2704, 13.9646, 13.7444]], grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.06642677634954453 
model_pd.l_d.mean(): -2.520289659500122 
model_pd.lagr.mean(): -2.4538629055023193 
model_pd.lambdas: dict_items([('pout', tensor([1.8759])), ('power', tensor([0.1971]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1470])), ('power', tensor([-14.1327]))])
epoch：828	 i:0 	 global-step:16560	 l-p:0.06642677634954453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[13.3106, 13.4755, 13.3562],
        [13.3106, 14.6909, 14.7037],
        [13.3106, 13.4741, 13.3556],
        [13.3106, 13.3117, 13.3107]], grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.06634711474180222 
model_pd.l_d.mean(): -2.510362386703491 
model_pd.lagr.mean(): -2.4440152645111084 
model_pd.lambdas: dict_items([('pout', tensor([1.8760])), ('power', tensor([0.1964]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1430])), ('power', tensor([-14.0954]))])
epoch：829	 i:0 	 global-step:16580	 l-p:0.06634711474180222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[13.3511, 13.3522, 13.3511],
        [13.3511, 17.0629, 19.2058],
        [13.3511, 15.7799, 16.5723],
        [13.3511, 15.5150, 16.0757]], grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.0662679672241211 
model_pd.l_d.mean(): -2.500490427017212 
model_pd.lagr.mean(): -2.434222459793091 
model_pd.lambdas: dict_items([('pout', tensor([1.8762])), ('power', tensor([0.1957]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1390])), ('power', tensor([-14.0578]))])
epoch：830	 i:0 	 global-step:16600	 l-p:0.0662679672241211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[13.3917, 13.5563, 13.4370],
        [13.3917, 13.5587, 13.4380],
        [13.3917, 17.5716, 20.2760],
        [13.3917, 13.3918, 13.3917]], grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.06618933379650116 
model_pd.l_d.mean(): -2.490673780441284 
model_pd.lagr.mean(): -2.4244844913482666 
model_pd.lambdas: dict_items([('pout', tensor([1.8763])), ('power', tensor([0.1950]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1350])), ('power', tensor([-14.0201]))])
epoch：831	 i:0 	 global-step:16620	 l-p:0.06618933379650116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[13.4326, 13.5992, 13.4787],
        [13.4326, 13.4343, 13.4326],
        [13.4326, 13.4326, 13.4326],
        [13.4326, 13.5306, 13.4520]], grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.06611121445894241 
model_pd.l_d.mean(): -2.48091197013855 
model_pd.lagr.mean(): -2.4148006439208984 
model_pd.lambdas: dict_items([('pout', tensor([1.8764])), ('power', tensor([0.1943]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1310])), ('power', tensor([-13.9821]))])
epoch：832	 i:0 	 global-step:16640	 l-p:0.06611121445894241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[13.4737, 13.5248, 13.4805],
        [13.4737, 17.2232, 19.3882],
        [13.4737, 13.6418, 13.5203],
        [13.4737, 18.0049, 21.1412]], grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.06603357940912247 
model_pd.l_d.mean(): -2.4712061882019043 
model_pd.lagr.mean(): -2.40517258644104 
model_pd.lambdas: dict_items([('pout', tensor([1.8766])), ('power', tensor([0.1936]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1270])), ('power', tensor([-13.9439]))])
epoch：833	 i:0 	 global-step:16660	 l-p:0.06603357940912247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[13.5150, 13.5338, 13.5163],
        [13.5150, 13.5663, 13.5218],
        [13.5150, 13.5150, 13.5150],
        [13.5150, 13.5149, 13.5150]], grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.06595644354820251 
model_pd.l_d.mean(): -2.461555242538452 
model_pd.lagr.mean(): -2.395598888397217 
model_pd.lambdas: dict_items([('pout', tensor([1.8767])), ('power', tensor([0.1929]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1230])), ('power', tensor([-13.9055]))])
epoch：834	 i:0 	 global-step:16680	 l-p:0.06595644354820251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[13.5564, 13.6555, 13.5761],
        [13.5564, 17.3315, 19.5114],
        [13.5564, 13.5612, 13.5566],
        [13.5564, 13.5565, 13.5564]], grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.06587979942560196 
model_pd.l_d.mean(): -2.4519598484039307 
model_pd.lagr.mean(): -2.386080026626587 
model_pd.lambdas: dict_items([('pout', tensor([1.8768])), ('power', tensor([0.1922]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1190])), ('power', tensor([-13.8669]))])
epoch：835	 i:0 	 global-step:16700	 l-p:0.06587979942560196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[13.5981, 13.9771, 13.7728],
        [13.5981, 13.6498, 13.6050],
        [13.5981, 13.7621, 13.6426],
        [13.5981, 15.0120, 15.0255]], grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.0658036544919014 
model_pd.l_d.mean(): -2.4424209594726562 
model_pd.lagr.mean(): -2.376617193222046 
model_pd.lambdas: dict_items([('pout', tensor([1.8769])), ('power', tensor([0.1915]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1150])), ('power', tensor([-13.8281]))])
epoch：836	 i:0 	 global-step:16720	 l-p:0.0658036544919014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[13.6400, 13.6409, 13.6400],
        [13.6400, 15.7360, 16.2121],
        [13.6400, 15.2022, 15.2987],
        [13.6400, 14.2620, 14.0298]], grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.06572797894477844 
model_pd.l_d.mean(): -2.4329371452331543 
model_pd.lagr.mean(): -2.3672091960906982 
model_pd.lambdas: dict_items([('pout', tensor([1.8770])), ('power', tensor([0.1909]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1110])), ('power', tensor([-13.7891]))])
epoch：837	 i:0 	 global-step:16740	 l-p:0.06572797894477844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[13.6821, 13.6870, 13.6823],
        [13.6821, 15.0615, 15.0516],
        [13.6821, 13.6821, 13.6821],
        [13.6821, 15.1058, 15.1195]], grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.06565278768539429 
model_pd.l_d.mean(): -2.423509359359741 
model_pd.lagr.mean(): -2.357856512069702 
model_pd.lambdas: dict_items([('pout', tensor([1.8771])), ('power', tensor([0.1902]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1069])), ('power', tensor([-13.7499]))])
epoch：838	 i:0 	 global-step:16760	 l-p:0.06565278768539429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[13.7244, 13.7436, 13.7258],
        [13.7244, 13.7262, 13.7245],
        [13.7244, 15.1086, 15.0987],
        [13.7244, 13.7244, 13.7244]], grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.06557808071374893 
model_pd.l_d.mean(): -2.4141368865966797 
model_pd.lagr.mean(): -2.3485589027404785 
model_pd.lambdas: dict_items([('pout', tensor([1.8772])), ('power', tensor([0.1895]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1029])), ('power', tensor([-13.7104]))])
epoch：839	 i:0 	 global-step:16780	 l-p:0.06557808071374893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[13.7669, 13.9368, 13.8136],
        [13.7669, 14.2107, 13.9909],
        [13.7669, 14.3955, 14.1609],
        [13.7669, 17.3649, 19.3027]], grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.06550385057926178 
model_pd.l_d.mean(): -2.4048213958740234 
model_pd.lagr.mean(): -2.339317560195923 
model_pd.lambdas: dict_items([('pout', tensor([1.8773])), ('power', tensor([0.1888]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0988])), ('power', tensor([-13.6708]))])
epoch：840	 i:0 	 global-step:16800	 l-p:0.06550385057926178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[13.8096, 13.8996, 13.8262],
        [13.8096, 18.1335, 20.9322],
        [13.8096, 16.5704, 17.6184],
        [13.8096, 15.2483, 15.2623]], grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.06543008983135223 
model_pd.l_d.mean(): -2.3955612182617188 
model_pd.lagr.mean(): -2.3301310539245605 
model_pd.lambdas: dict_items([('pout', tensor([1.8774])), ('power', tensor([0.1881]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0948])), ('power', tensor([-13.6309]))])
epoch：841	 i:0 	 global-step:16820	 l-p:0.06543008983135223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[13.8525, 13.8525, 13.8525],
        [13.8525, 18.1911, 20.9996],
        [13.8525, 16.6031, 17.6358],
        [13.8525, 13.8526, 13.8525]], grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.0653567910194397 
model_pd.l_d.mean(): -2.386357307434082 
model_pd.lagr.mean(): -2.321000576019287 
model_pd.lambdas: dict_items([('pout', tensor([1.8775])), ('power', tensor([0.1874]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0907])), ('power', tensor([-13.5909]))])
epoch：842	 i:0 	 global-step:16840	 l-p:0.0653567910194397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[13.8956, 13.8957, 13.8956],
        [13.8956, 13.8956, 13.8956],
        [13.8956, 14.8025, 14.5998],
        [13.8956, 13.9486, 13.9027]], grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.06528396904468536 
model_pd.l_d.mean(): -2.3772101402282715 
model_pd.lagr.mean(): -2.3119261264801025 
model_pd.lambdas: dict_items([('pout', tensor([1.8776])), ('power', tensor([0.1868]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0866])), ('power', tensor([-13.5507]))])
epoch：843	 i:0 	 global-step:16860	 l-p:0.06528396904468536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[13.9389, 14.8940, 14.7018],
        [13.9389, 14.8096, 14.5976],
        [13.9389, 15.3927, 15.4070],
        [13.9389, 14.0412, 13.9592]], grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.06521159410476685 
model_pd.l_d.mean(): -2.368119239807129 
model_pd.lagr.mean(): -2.302907705307007 
model_pd.lambdas: dict_items([('pout', tensor([1.8777])), ('power', tensor([0.1861]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0826])), ('power', tensor([-13.5102]))])
epoch：844	 i:0 	 global-step:16880	 l-p:0.06521159410476685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[13.9824, 14.0736, 13.9992],
        [13.9824, 14.7191, 14.4858],
        [13.9824, 14.0358, 13.9895],
        [13.9824, 13.9874, 13.9826]], grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.06513969600200653 
model_pd.l_d.mean(): -2.359084129333496 
model_pd.lagr.mean(): -2.2939443588256836 
model_pd.lambdas: dict_items([('pout', tensor([1.8778])), ('power', tensor([0.1854]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0785])), ('power', tensor([-13.4695]))])
epoch：845	 i:0 	 global-step:16900	 l-p:0.06513969600200653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[14.0261, 14.1177, 14.0430],
        [14.0261, 16.8381, 17.9078],
        [14.0261, 18.7619, 22.0416],
        [14.0261, 16.1884, 16.6803]], grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.06506825238466263 
model_pd.l_d.mean(): -2.3501060009002686 
model_pd.lagr.mean(): -2.2850377559661865 
model_pd.lambdas: dict_items([('pout', tensor([1.8778])), ('power', tensor([0.1847]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0744])), ('power', tensor([-13.4287]))])
epoch：846	 i:0 	 global-step:16920	 l-p:0.06506825238466263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[14.0700, 16.2399, 16.7335],
        [14.0700, 14.1238, 14.0771],
        [14.0700, 14.2467, 14.1190],
        [14.0700, 15.5391, 15.5537]], grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.06499725580215454 
model_pd.l_d.mean(): -2.341184139251709 
model_pd.lagr.mean(): -2.276186943054199 
model_pd.lambdas: dict_items([('pout', tensor([1.8779])), ('power', tensor([0.1841]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0703])), ('power', tensor([-13.3877]))])
epoch：847	 i:0 	 global-step:16940	 l-p:0.06499725580215454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[14.1141, 18.5427, 21.4103],
        [14.1141, 14.2889, 14.1622],
        [14.1141, 16.9223, 17.9772],
        [14.1141, 14.2063, 14.1311]], grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.06492672115564346 
model_pd.l_d.mean(): -2.3323183059692383 
model_pd.lagr.mean(): -2.2673916816711426 
model_pd.lambdas: dict_items([('pout', tensor([1.8780])), ('power', tensor([0.1834]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0662])), ('power', tensor([-13.3464]))])
epoch：848	 i:0 	 global-step:16960	 l-p:0.06492672115564346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[14.1583, 14.1842, 14.1606],
        [14.1583, 14.9055, 14.6689],
        [14.1583, 14.1586, 14.1583],
        [14.1583, 14.6457, 14.4141]], grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.064856618642807 
model_pd.l_d.mean(): -2.323509931564331 
model_pd.lagr.mean(): -2.258653402328491 
model_pd.lambdas: dict_items([('pout', tensor([1.8780])), ('power', tensor([0.1827]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0621])), ('power', tensor([-13.3050]))])
epoch：849	 i:0 	 global-step:16980	 l-p:0.064856618642807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[14.2028, 14.3804, 14.2519],
        [14.2028, 14.2030, 14.2028],
        [14.2028, 14.2571, 14.2100],
        [14.2028, 16.5213, 17.1236]], grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.06478697061538696 
model_pd.l_d.mean(): -2.3147575855255127 
model_pd.lagr.mean(): -2.2499706745147705 
model_pd.lambdas: dict_items([('pout', tensor([1.8781])), ('power', tensor([0.1821]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0580])), ('power', tensor([-13.2633]))])
epoch：850	 i:0 	 global-step:17000	 l-p:0.06478697061538696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[14.2474, 15.1398, 14.9228],
        [14.2474, 15.4648, 15.3489],
        [14.2474, 17.0851, 18.1513],
        [14.2474, 16.2970, 16.6802]], grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.06471776962280273 
model_pd.l_d.mean(): -2.3060615062713623 
model_pd.lagr.mean(): -2.2413437366485596 
model_pd.lambdas: dict_items([('pout', tensor([1.8781])), ('power', tensor([0.1814]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0539])), ('power', tensor([-13.2215]))])
epoch：851	 i:0 	 global-step:17020	 l-p:0.06471776962280273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[14.2923, 14.8756, 14.6325],
        [14.2923, 17.1398, 18.2098],
        [14.2923, 14.7553, 14.5261],
        [14.2923, 16.3490, 16.7336]], grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.06464899331331253 
model_pd.l_d.mean(): -2.297422409057617 
model_pd.lagr.mean(): -2.2327733039855957 
model_pd.lambdas: dict_items([('pout', tensor([1.8782])), ('power', tensor([0.1808]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0498])), ('power', tensor([-13.1795]))])
epoch：852	 i:0 	 global-step:17040	 l-p:0.06464899331331253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[14.3373, 17.1947, 18.2685],
        [14.3373, 14.3373, 14.3373],
        [14.3373, 14.5115, 14.3845],
        [14.3373, 16.4012, 16.7872]], grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.06458066403865814 
model_pd.l_d.mean(): -2.2888400554656982 
model_pd.lagr.mean(): -2.224259376525879 
model_pd.lambdas: dict_items([('pout', tensor([1.8782])), ('power', tensor([0.1801]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0457])), ('power', tensor([-13.1372]))])
epoch：853	 i:0 	 global-step:17060	 l-p:0.06458066403865814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[14.3825, 14.3834, 14.3825],
        [14.3825, 18.1581, 20.1933],
        [14.3825, 15.3718, 15.1731],
        [14.3825, 19.2502, 22.6224]], grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.06451275944709778 
model_pd.l_d.mean(): -2.2803144454956055 
model_pd.lagr.mean(): -2.21580171585083 
model_pd.lambdas: dict_items([('pout', tensor([1.8783])), ('power', tensor([0.1794]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0415])), ('power', tensor([-13.0948]))])
epoch：854	 i:0 	 global-step:17080	 l-p:0.06451275944709778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[14.4279, 17.3259, 18.4273],
        [14.4279, 14.8333, 14.6149],
        [14.4279, 14.4279, 14.4279],
        [14.4279, 17.3053, 18.3868]], grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.06444529443979263 
model_pd.l_d.mean(): -2.271845817565918 
model_pd.lagr.mean(): -2.2074005603790283 
model_pd.lambdas: dict_items([('pout', tensor([1.8783])), ('power', tensor([0.1788]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0374])), ('power', tensor([-13.0522]))])
epoch：855	 i:0 	 global-step:17100	 l-p:0.06444529443979263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[14.4734, 14.4882, 14.4743],
        [14.4734, 17.3848, 18.4933],
        [14.4734, 18.2754, 20.3249],
        [14.4734, 14.9731, 14.7358]], grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.06437823921442032 
model_pd.l_d.mean(): -2.2634334564208984 
model_pd.lagr.mean(): -2.1990551948547363 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1781]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0333])), ('power', tensor([-13.0094]))])
epoch：856	 i:0 	 global-step:17120	 l-p:0.06437823921442032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[14.5192, 17.4375, 18.5468],
        [14.5192, 16.0410, 16.0566],
        [14.5192, 14.7014, 14.5696],
        [14.5192, 15.4308, 15.2093]], grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.06431161612272263 
model_pd.l_d.mean(): -2.2550785541534424 
model_pd.lagr.mean(): -2.1907670497894287 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1775]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0291])), ('power', tensor([-12.9664]))])
epoch：857	 i:0 	 global-step:17140	 l-p:0.06431161612272263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[14.5651, 14.9749, 14.7542],
        [14.5651, 15.4800, 15.2577],
        [14.5651, 15.9470, 15.8883],
        [14.5651, 14.5703, 14.5653]], grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.06424541026353836 
model_pd.l_d.mean(): -2.2467801570892334 
model_pd.lagr.mean(): -2.182534694671631 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1768]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0250])), ('power', tensor([-12.9233]))])
epoch：858	 i:0 	 global-step:17160	 l-p:0.06424541026353836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[14.6113, 16.1938, 16.2375],
        [14.6113, 16.2985, 16.4040],
        [14.6113, 14.6132, 14.6113],
        [14.6113, 14.6262, 14.6122]], grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.06417961418628693 
model_pd.l_d.mean(): -2.2385387420654297 
model_pd.lagr.mean(): -2.174359083175659 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1762]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0209])), ('power', tensor([-12.8799]))])
epoch：859	 i:0 	 global-step:17180	 l-p:0.06417961418628693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[14.6576, 15.0703, 14.8481],
        [14.6576, 14.6584, 14.6576],
        [14.6576, 14.6588, 14.6576],
        [14.6576, 17.5857, 18.6867]], grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.06411423534154892 
model_pd.l_d.mean(): -2.230354070663452 
model_pd.lagr.mean(): -2.1662397384643555 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1756]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0167])), ('power', tensor([-12.8363]))])
epoch：860	 i:0 	 global-step:17200	 l-p:0.06411423534154892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[14.7041, 17.6635, 18.7887],
        [14.7041, 15.6706, 15.4551],
        [14.7041, 14.7041, 14.7041],
        [14.7041, 15.7184, 15.5148]], grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.06404926627874374 
model_pd.l_d.mean(): -2.22222638130188 
model_pd.lagr.mean(): -2.158177137374878 
model_pd.lambdas: dict_items([('pout', tensor([1.8785])), ('power', tensor([0.1749]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0126])), ('power', tensor([-12.7926]))])
epoch：861	 i:0 	 global-step:17220	 l-p:0.06404926627874374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[14.7508, 19.7549, 23.2227],
        [14.7508, 17.7239, 18.8563],
        [14.7508, 18.6328, 20.7262],
        [14.7508, 17.4641, 18.3521]], grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.06398468464612961 
model_pd.l_d.mean(): -2.2141549587249756 
model_pd.lagr.mean(): -2.15017032623291 
model_pd.lambdas: dict_items([('pout', tensor([1.8785])), ('power', tensor([0.1743]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0084])), ('power', tensor([-12.7487]))])
epoch：862	 i:0 	 global-step:17240	 l-p:0.06398468464612961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[14.7976, 14.9848, 14.8496],
        [14.7976, 14.8950, 14.8156],
        [14.7976, 14.7995, 14.7977],
        [14.7976, 15.4043, 15.1517]], grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.0639205127954483 
model_pd.l_d.mean(): -2.206141233444214 
model_pd.lagr.mean(): -2.1422207355499268 
model_pd.lambdas: dict_items([('pout', tensor([1.8785])), ('power', tensor([0.1736]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0043])), ('power', tensor([-12.7045]))])
epoch：863	 i:0 	 global-step:17260	 l-p:0.0639205127954483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[14.8447, 14.8447, 14.8447],
        [14.8447, 15.0299, 14.8956],
        [14.8447, 15.4535, 15.2000],
        [14.8447, 16.4554, 16.5002]], grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.06385673582553864 
model_pd.l_d.mean(): -2.198184013366699 
model_pd.lagr.mean(): -2.1343271732330322 
model_pd.lambdas: dict_items([('pout', tensor([1.8785])), ('power', tensor([0.1730]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0001])), ('power', tensor([-12.6602]))])
epoch：864	 i:0 	 global-step:17280	 l-p:0.06385673582553864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[14.8919, 14.8972, 14.8921],
        [14.8919, 14.8938, 14.8919],
        [14.8919, 14.8919, 14.8919],
        [14.8919, 15.3771, 15.1371]], grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.0637933611869812 
model_pd.l_d.mean(): -2.1902835369110107 
model_pd.lagr.mean(): -2.1264901161193848 
model_pd.lambdas: dict_items([('pout', tensor([1.8785])), ('power', tensor([0.1724]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0041])), ('power', tensor([-12.6157]))])
epoch：865	 i:0 	 global-step:17300	 l-p:0.0637933611869812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[14.9393, 14.9393, 14.9393],
        [14.9393, 16.2231, 16.1016],
        [14.9393, 15.4572, 15.2114],
        [14.9393, 16.5615, 16.6068]], grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.06373036652803421 
model_pd.l_d.mean(): -2.1824402809143066 
model_pd.lagr.mean(): -2.1187098026275635 
model_pd.lambdas: dict_items([('pout', tensor([1.8785])), ('power', tensor([0.1718]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0082])), ('power', tensor([-12.5710]))])
epoch：866	 i:0 	 global-step:17320	 l-p:0.06373036652803421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[14.9869, 15.4756, 15.2339],
        [14.9869, 14.9872, 14.9869],
        [14.9869, 19.7163, 22.7811],
        [14.9869, 14.9882, 14.9869]], grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.06366775929927826 
model_pd.l_d.mean(): -2.1746537685394287 
model_pd.lagr.mean(): -2.1109859943389893 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1711]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0124])), ('power', tensor([-12.5261]))])
epoch：867	 i:0 	 global-step:17340	 l-p:0.06366775929927826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[15.0347, 15.0359, 15.0347],
        [15.0347, 18.0460, 19.1790],
        [15.0347, 15.0560, 15.0362],
        [15.0347, 15.0348, 15.0347]], grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.06360555440187454 
model_pd.l_d.mean(): -2.166924238204956 
model_pd.lagr.mean(): -2.103318691253662 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1705]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0165])), ('power', tensor([-12.4810]))])
epoch：868	 i:0 	 global-step:17360	 l-p:0.06360555440187454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[15.0826, 15.0846, 15.0827],
        [15.0826, 15.1946, 15.1048],
        [15.0826, 15.0981, 15.0836],
        [15.0826, 19.3274, 21.7827]], grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.06354371458292007 
model_pd.l_d.mean(): -2.1592512130737305 
model_pd.lagr.mean(): -2.095707416534424 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1699]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0207])), ('power', tensor([-12.4358]))])
epoch：869	 i:0 	 global-step:17380	 l-p:0.06354371458292007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[15.1308, 16.5728, 16.5120],
        [15.1308, 15.1327, 15.1308],
        [15.1308, 15.5586, 15.3283],
        [15.1308, 17.6178, 18.2655]], grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.06348226964473724 
model_pd.l_d.mean(): -2.15163516998291 
model_pd.lagr.mean(): -2.0881528854370117 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1693]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0249])), ('power', tensor([-12.3903]))])
epoch：870	 i:0 	 global-step:17400	 l-p:0.06348226964473724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[15.1791, 16.8303, 16.8766],
        [15.1791, 15.1799, 15.1791],
        [15.1791, 15.1800, 15.1791],
        [15.1791, 15.1803, 15.1791]], grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.06342119723558426 
model_pd.l_d.mean(): -2.144075870513916 
model_pd.lagr.mean(): -2.0806546211242676 
model_pd.lambdas: dict_items([('pout', tensor([1.8784])), ('power', tensor([0.1686]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0291])), ('power', tensor([-12.3447]))])
epoch：871	 i:0 	 global-step:17420	 l-p:0.06342119723558426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[15.2275, 15.2275, 15.2275],
        [15.2275, 15.2276, 15.2275],
        [15.2275, 18.3067, 19.4806],
        [15.2275, 16.2328, 16.0090]], grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.06336050480604172 
model_pd.l_d.mean(): -2.136573314666748 
model_pd.lagr.mean(): -2.0732128620147705 
model_pd.lambdas: dict_items([('pout', tensor([1.8783])), ('power', tensor([0.1680]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0332])), ('power', tensor([-12.2989]))])
epoch：872	 i:0 	 global-step:17440	 l-p:0.06336050480604172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[15.2762, 15.6393, 15.4263],
        [15.2762, 15.2765, 15.2762],
        [15.2762, 17.0492, 17.1609],
        [15.2762, 15.2762, 15.2762]], grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.06330017745494843 
model_pd.l_d.mean(): -2.129127264022827 
model_pd.lagr.mean(): -2.0658271312713623 
model_pd.lambdas: dict_items([('pout', tensor([1.8783])), ('power', tensor([0.1674]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0374])), ('power', tensor([-12.2529]))])
epoch：873	 i:0 	 global-step:17460	 l-p:0.06330017745494843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[15.3250, 17.5475, 17.9647],
        [15.3250, 15.3468, 15.3266],
        [15.3250, 17.8474, 18.5046],
        [15.3250, 15.3260, 15.3251]], grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.06324023008346558 
model_pd.l_d.mean(): -2.1217381954193115 
model_pd.lagr.mean(): -2.058497905731201 
model_pd.lambdas: dict_items([('pout', tensor([1.8782])), ('power', tensor([0.1668]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0416])), ('power', tensor([-12.2067]))])
epoch：874	 i:0 	 global-step:17480	 l-p:0.06324023008346558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[15.3741, 15.4757, 15.3928],
        [15.3741, 15.5685, 15.4279],
        [15.3741, 16.3901, 16.1641],
        [15.3741, 16.3462, 16.1105]], grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.06318066269159317 
model_pd.l_d.mean(): -2.114406108856201 
model_pd.lagr.mean(): -2.051225423812866 
model_pd.lambdas: dict_items([('pout', tensor([1.8782])), ('power', tensor([0.1662]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0458])), ('power', tensor([-12.1604]))])
epoch：875	 i:0 	 global-step:17500	 l-p:0.06318066269159317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[15.4232, 15.4288, 15.4234],
        [15.4232, 17.1041, 17.1515],
        [15.4232, 15.4232, 15.4232],
        [15.4232, 15.6125, 15.4746]], grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.06312144547700882 
model_pd.l_d.mean(): -2.107130289077759 
model_pd.lagr.mean(): -2.044008731842041 
model_pd.lambdas: dict_items([('pout', tensor([1.8782])), ('power', tensor([0.1656]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0500])), ('power', tensor([-12.1138]))])
epoch：876	 i:0 	 global-step:17520	 l-p:0.06312144547700882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[15.4726, 15.4726, 15.4726],
        [15.4726, 15.5013, 15.4750],
        [15.4726, 16.4517, 16.2144],
        [15.4726, 16.1106, 15.8451]], grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.06306260824203491 
model_pd.l_d.mean(): -2.0999107360839844 
model_pd.lagr.mean(): -2.0368480682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.8781])), ('power', tensor([0.1650]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0541])), ('power', tensor([-12.0671]))])
epoch：877	 i:0 	 global-step:17540	 l-p:0.06306260824203491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[15.5221, 16.1624, 15.8960],
        [15.5221, 17.3269, 17.4409],
        [15.5221, 15.5234, 15.5221],
        [15.5221, 18.3925, 19.3333]], grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.06300413608551025 
model_pd.l_d.mean(): -2.092747926712036 
model_pd.lagr.mean(): -2.0297436714172363 
model_pd.lambdas: dict_items([('pout', tensor([1.8780])), ('power', tensor([0.1644]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0583])), ('power', tensor([-12.0202]))])
epoch：878	 i:0 	 global-step:17560	 l-p:0.06300413608551025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[15.5718, 16.0820, 15.8298],
        [15.5718, 15.5738, 15.5718],
        [15.5718, 15.5728, 15.5718],
        [15.5718, 16.9165, 16.7898]], grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.06294601410627365 
model_pd.l_d.mean(): -2.085641622543335 
model_pd.lagr.mean(): -2.022695541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.8780])), ('power', tensor([0.1638]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0625])), ('power', tensor([-11.9731]))])
epoch：879	 i:0 	 global-step:17580	 l-p:0.06294601410627365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[15.6217, 15.7382, 15.6448],
        [15.6217, 17.3266, 17.3749],
        [15.6217, 15.6237, 15.6217],
        [15.6217, 19.8383, 22.1645]], grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.0628882572054863 
model_pd.l_d.mean(): -2.0785915851593018 
model_pd.lagr.mean(): -2.0157034397125244 
model_pd.lambdas: dict_items([('pout', tensor([1.8779])), ('power', tensor([0.1632]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0667])), ('power', tensor([-11.9259]))])
epoch：880	 i:0 	 global-step:17600	 l-p:0.0628882572054863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[15.6717, 15.8686, 15.7259],
        [15.6717, 15.6730, 15.6717],
        [15.6717, 15.7325, 15.6798],
        [15.6717, 21.0170, 24.7242]], grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.0628308653831482 
model_pd.l_d.mean(): -2.0715975761413574 
model_pd.lagr.mean(): -2.0087666511535645 
model_pd.lambdas: dict_items([('pout', tensor([1.8778])), ('power', tensor([0.1626]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0709])), ('power', tensor([-11.8785]))])
epoch：881	 i:0 	 global-step:17620	 l-p:0.0628308653831482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[15.7219, 16.7637, 16.5321],
        [15.7219, 19.9681, 22.3108],
        [15.7219, 16.4521, 16.1803],
        [15.7219, 15.9223, 15.7776]], grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.06277383118867874 
model_pd.l_d.mean(): -2.06466007232666 
model_pd.lagr.mean(): -2.0018861293792725 
model_pd.lambdas: dict_items([('pout', tensor([1.8778])), ('power', tensor([0.1620]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0751])), ('power', tensor([-11.8309]))])
epoch：882	 i:0 	 global-step:17640	 l-p:0.06277383118867874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[15.7722, 15.7779, 15.7724],
        [15.7722, 15.7722, 15.7722],
        [15.7722, 20.7724, 24.0148],
        [15.7722, 15.7722, 15.7722]], grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.06271713972091675 
model_pd.l_d.mean(): -2.05777907371521 
model_pd.lagr.mean(): -1.9950618743896484 
model_pd.lambdas: dict_items([('pout', tensor([1.8777])), ('power', tensor([0.1614]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0793])), ('power', tensor([-11.7831]))])
epoch：883	 i:0 	 global-step:17660	 l-p:0.06271713972091675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[15.8227, 16.0218, 15.8776],
        [15.8227, 15.8248, 15.8228],
        [15.8227, 19.0345, 20.2600],
        [15.8227, 17.1916, 17.0628]], grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.0626608207821846 
model_pd.l_d.mean(): -2.0509536266326904 
model_pd.lagr.mean(): -1.9882928133010864 
model_pd.lambdas: dict_items([('pout', tensor([1.8776])), ('power', tensor([0.1608]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0834])), ('power', tensor([-11.7352]))])
epoch：884	 i:0 	 global-step:17680	 l-p:0.0626608207821846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[15.8734, 15.8961, 15.8751],
        [15.8734, 19.0929, 20.3193],
        [15.8734, 18.1839, 18.6185],
        [15.8734, 16.2526, 16.0302]], grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.06260484457015991 
model_pd.l_d.mean(): -2.0441842079162598 
model_pd.lagr.mean(): -1.981579303741455 
model_pd.lambdas: dict_items([('pout', tensor([1.8775])), ('power', tensor([0.1602]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0876])), ('power', tensor([-11.6871]))])
epoch：885	 i:0 	 global-step:17700	 l-p:0.06260484457015991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[15.9243, 16.0433, 15.9479],
        [15.9243, 18.8765, 19.8449],
        [15.9243, 15.9256, 15.9243],
        [15.9243, 16.3775, 16.1336]], grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.06254921853542328 
model_pd.l_d.mean(): -2.037471294403076 
model_pd.lagr.mean(): -1.9749220609664917 
model_pd.lambdas: dict_items([('pout', tensor([1.8774])), ('power', tensor([0.1597]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0918])), ('power', tensor([-11.6388]))])
epoch：886	 i:0 	 global-step:17720	 l-p:0.06254921853542328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[15.9753, 15.9755, 15.9753],
        [15.9753, 15.9753, 15.9753],
        [15.9753, 16.4302, 16.1854],
        [15.9753, 16.5004, 16.2408]], grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.0624939426779747 
model_pd.l_d.mean(): -2.0308139324188232 
model_pd.lagr.mean(): -1.9683200120925903 
model_pd.lambdas: dict_items([('pout', tensor([1.8773])), ('power', tensor([0.1591]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0960])), ('power', tensor([-11.5903]))])
epoch：887	 i:0 	 global-step:17740	 l-p:0.0624939426779747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[16.0264, 21.1143, 24.4142],
        [16.0264, 16.0888, 16.0347],
        [16.0264, 19.2568, 20.4742],
        [16.0264, 18.9995, 19.9748]], grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.06243901327252388 
model_pd.l_d.mean(): -2.024212121963501 
model_pd.lagr.mean(): -1.961773157119751 
model_pd.lambdas: dict_items([('pout', tensor([1.8772])), ('power', tensor([0.1585]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1002])), ('power', tensor([-11.5417]))])
epoch：888	 i:0 	 global-step:17760	 l-p:0.06243901327252388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[16.0777, 16.0777, 16.0777],
        [16.0777, 16.2833, 16.1349],
        [16.0777, 19.3195, 20.5412],
        [16.0777, 16.0777, 16.0777]], grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.062384434044361115 
model_pd.l_d.mean(): -2.0176661014556885 
model_pd.lagr.mean(): -1.9552816152572632 
model_pd.lambdas: dict_items([('pout', tensor([1.8771])), ('power', tensor([0.1579]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1044])), ('power', tensor([-11.4930]))])
epoch：889	 i:0 	 global-step:17780	 l-p:0.062384434044361115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[16.1292, 16.3355, 16.1865],
        [16.1292, 18.4808, 18.9235],
        [16.1292, 16.1459, 16.1302],
        [16.1292, 16.5890, 16.3416]], grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.06233019381761551 
model_pd.l_d.mean(): -2.0111758708953857 
model_pd.lagr.mean(): -1.948845624923706 
model_pd.lambdas: dict_items([('pout', tensor([1.8770])), ('power', tensor([0.1574]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1086])), ('power', tensor([-11.4440]))])
epoch：890	 i:0 	 global-step:17800	 l-p:0.06233019381761551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[16.1808, 16.7135, 16.4503],
        [16.1808, 16.9349, 16.6544],
        [16.1808, 16.2885, 16.2007],
        [16.1808, 20.4763, 22.7960]], grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.06227629631757736 
model_pd.l_d.mean(): -2.0047409534454346 
model_pd.lagr.mean(): -1.9424647092819214 
model_pd.lambdas: dict_items([('pout', tensor([1.8769])), ('power', tensor([0.1568]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1127])), ('power', tensor([-11.3949]))])
epoch：891	 i:0 	 global-step:17820	 l-p:0.06227629631757736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[16.2325, 17.2655, 17.0156],
        [16.2325, 16.2334, 16.2326],
        [16.2325, 17.1038, 16.8290],
        [16.2325, 21.3915, 24.7381]], grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.06222275272011757 
model_pd.l_d.mean(): -1.9983611106872559 
model_pd.lagr.mean(): -1.936138391494751 
model_pd.lambdas: dict_items([('pout', tensor([1.8768])), ('power', tensor([0.1562]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1169])), ('power', tensor([-11.3457]))])
epoch：892	 i:0 	 global-step:17840	 l-p:0.06222275272011757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[16.2844, 19.5954, 20.8575],
        [16.2844, 17.6977, 17.5652],
        [16.2844, 18.9813, 19.6857],
        [16.2844, 19.3101, 20.3032]], grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.06216954067349434 
model_pd.l_d.mean(): -1.992037057876587 
model_pd.lagr.mean(): -1.9298675060272217 
model_pd.lambdas: dict_items([('pout', tensor([1.8767])), ('power', tensor([0.1557]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1211])), ('power', tensor([-11.2963]))])
epoch：893	 i:0 	 global-step:17860	 l-p:0.06216954067349434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[16.3365, 16.3365, 16.3365],
        [16.3365, 16.3379, 16.3365],
        [16.3365, 19.6627, 20.9329],
        [16.3365, 18.0716, 18.0916]], grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.06211667135357857 
model_pd.l_d.mean(): -1.9857673645019531 
model_pd.lagr.mean(): -1.9236507415771484 
model_pd.lambdas: dict_items([('pout', tensor([1.8765])), ('power', tensor([0.1551]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1253])), ('power', tensor([-11.2467]))])
epoch：894	 i:0 	 global-step:17880	 l-p:0.06211667135357857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[16.3887, 16.3946, 16.3889],
        [16.3887, 18.7820, 19.2329],
        [16.3887, 17.2694, 16.9916],
        [16.3887, 16.5958, 16.4458]], grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.06206412613391876 
model_pd.l_d.mean(): -1.97955322265625 
model_pd.lagr.mean(): -1.9174890518188477 
model_pd.lambdas: dict_items([('pout', tensor([1.8764])), ('power', tensor([0.1545]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1294])), ('power', tensor([-11.1970]))])
epoch：895	 i:0 	 global-step:17900	 l-p:0.06206412613391876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[16.4410, 16.4432, 16.4411],
        [16.4410, 16.9834, 16.7154],
        [16.4410, 17.0178, 16.7444],
        [16.4410, 16.5052, 16.4496]], grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.06201193481683731 
model_pd.l_d.mean(): -1.9733939170837402 
model_pd.lagr.mean(): -1.9113819599151611 
model_pd.lambdas: dict_items([('pout', tensor([1.8763])), ('power', tensor([0.1540]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1336])), ('power', tensor([-11.1471]))])
epoch：896	 i:0 	 global-step:17920	 l-p:0.06201193481683731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[16.4935, 16.6036, 16.5138],
        [16.4935, 16.4936, 16.4935],
        [16.4935, 18.3041, 18.3565],
        [16.4935, 16.4935, 16.4935]], grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.06196007877588272 
model_pd.l_d.mean(): -1.9672890901565552 
model_pd.lagr.mean(): -1.9053289890289307 
model_pd.lambdas: dict_items([('pout', tensor([1.8761])), ('power', tensor([0.1534]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1378])), ('power', tensor([-11.0971]))])
epoch：897	 i:0 	 global-step:17940	 l-p:0.06196007877588272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[16.5462, 16.5771, 16.5488],
        [16.5462, 18.4831, 18.6068],
        [16.5462, 17.6490, 17.4044],
        [16.5462, 18.9648, 19.4207]], grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.0619085393846035 
model_pd.l_d.mean(): -1.961239218711853 
model_pd.lagr.mean(): -1.899330735206604 
model_pd.lambdas: dict_items([('pout', tensor([1.8760])), ('power', tensor([0.1529]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1420])), ('power', tensor([-11.0470]))])
epoch：898	 i:0 	 global-step:17960	 l-p:0.0619085393846035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[16.5989, 16.5989, 16.5989],
        [16.5989, 18.3101, 18.3012],
        [16.5989, 17.7057, 17.4602],
        [16.5989, 17.4922, 17.2105]], grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.06185734644532204 
model_pd.l_d.mean(): -1.9552435874938965 
model_pd.lagr.mean(): -1.8933862447738647 
model_pd.lambdas: dict_items([('pout', tensor([1.8758])), ('power', tensor([0.1523]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1461])), ('power', tensor([-10.9966]))])
epoch：899	 i:0 	 global-step:17980	 l-p:0.06185734644532204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[16.6518, 21.1724, 23.6685],
        [16.6518, 17.0519, 16.8174],
        [16.6518, 18.3690, 18.3602],
        [16.6518, 22.3606, 26.3227]], grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.06180647760629654 
model_pd.l_d.mean(): -1.9493021965026855 
model_pd.lagr.mean(): -1.887495756149292 
model_pd.lambdas: dict_items([('pout', tensor([1.8757])), ('power', tensor([0.1518]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1503])), ('power', tensor([-10.9462]))])
epoch：900	 i:0 	 global-step:18000	 l-p:0.06180647760629654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[16.7048, 16.7048, 16.7048],
        [16.7048, 17.1064, 16.8710],
        [16.7048, 16.7063, 16.7049],
        [16.7048, 16.9184, 16.7640]], grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.061755940318107605 
model_pd.l_d.mean(): -1.9434152841567993 
model_pd.lagr.mean(): -1.8816593885421753 
model_pd.lambdas: dict_items([('pout', tensor([1.8755])), ('power', tensor([0.1512]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1545])), ('power', tensor([-10.8956]))])
epoch：901	 i:0 	 global-step:18020	 l-p:0.061755940318107605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[16.7580, 17.5422, 17.2507],
        [16.7580, 21.2205, 23.6317],
        [16.7580, 16.7754, 16.7591],
        [16.7580, 20.1782, 21.4850]], grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.06170571967959404 
model_pd.l_d.mean(): -1.9375821352005005 
model_pd.lagr.mean(): -1.8758764266967773 
model_pd.lambdas: dict_items([('pout', tensor([1.8754])), ('power', tensor([0.1507]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1586])), ('power', tensor([-10.8448]))])
epoch：902	 i:0 	 global-step:18040	 l-p:0.06170571967959404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[16.8113, 17.2930, 17.0339],
        [16.8113, 19.2726, 19.7369],
        [16.8113, 18.6604, 18.7142],
        [16.8113, 16.8428, 16.8140]], grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.061655834317207336 
model_pd.l_d.mean(): -1.9318033456802368 
model_pd.lagr.mean(): -1.870147466659546 
model_pd.lambdas: dict_items([('pout', tensor([1.8752])), ('power', tensor([0.1501]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1628])), ('power', tensor([-10.7940]))])
epoch：903	 i:0 	 global-step:18060	 l-p:0.061655834317207336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[16.8647, 21.4481, 23.9794],
        [16.8647, 22.2419, 25.7316],
        [16.8647, 19.3346, 19.8006],
        [16.8647, 17.4227, 17.1471]], grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.061606284230947495 
model_pd.l_d.mean(): -1.9260778427124023 
model_pd.lagr.mean(): -1.8644715547561646 
model_pd.lambdas: dict_items([('pout', tensor([1.8751])), ('power', tensor([0.1496]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1669])), ('power', tensor([-10.7429]))])
epoch：904	 i:0 	 global-step:18080	 l-p:0.061606284230947495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[16.9183, 17.1360, 16.9788],
        [16.9183, 18.7217, 18.7432],
        [16.9183, 16.9244, 16.9185],
        [16.9183, 18.1045, 17.8680]], grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.061557043343782425 
model_pd.l_d.mean(): -1.9204059839248657 
model_pd.lagr.mean(): -1.8588489294052124 
model_pd.lambdas: dict_items([('pout', tensor([1.8749])), ('power', tensor([0.1491]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1711])), ('power', tensor([-10.6918]))])
epoch：905	 i:0 	 global-step:18100	 l-p:0.061557043343782425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[16.9719, 16.9734, 16.9719],
        [16.9719, 17.5339, 17.2563],
        [16.9719, 17.4587, 17.1970],
        [16.9719, 18.8406, 18.8951]], grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.06150813400745392 
model_pd.l_d.mean(): -1.9147875308990479 
model_pd.lagr.mean(): -1.8532793521881104 
model_pd.lambdas: dict_items([('pout', tensor([1.8747])), ('power', tensor([0.1485]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1752])), ('power', tensor([-10.6405]))])
epoch：906	 i:0 	 global-step:18120	 l-p:0.06150813400745392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[17.0257, 17.0925, 17.0346],
        [17.0257, 20.2024, 21.2464],
        [17.0257, 17.0257, 17.0257],
        [17.0257, 17.2450, 17.0867]], grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.06145954504609108 
model_pd.l_d.mean(): -1.9092222452163696 
model_pd.lagr.mean(): -1.8477627038955688 
model_pd.lambdas: dict_items([('pout', tensor([1.8745])), ('power', tensor([0.1480]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1794])), ('power', tensor([-10.5891]))])
epoch：907	 i:0 	 global-step:18140	 l-p:0.06145954504609108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[17.0796, 17.0796, 17.0796],
        [17.0796, 17.0858, 17.0798],
        [17.0796, 17.0805, 17.0796],
        [17.0796, 17.2920, 17.1374]], grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.06141128018498421 
model_pd.l_d.mean(): -1.9037102460861206 
model_pd.lagr.mean(): -1.842298984527588 
model_pd.lambdas: dict_items([('pout', tensor([1.8743])), ('power', tensor([0.1475]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1835])), ('power', tensor([-10.5375]))])
epoch：908	 i:0 	 global-step:18160	 l-p:0.06141128018498421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[17.1336, 17.3468, 17.1916],
        [17.1336, 17.1347, 17.1337],
        [17.1336, 18.7889, 18.7213],
        [17.1336, 17.1336, 17.1336]], grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.061363335698843 
model_pd.l_d.mean(): -1.8982510566711426 
model_pd.lagr.mean(): -1.8368877172470093 
model_pd.lambdas: dict_items([('pout', tensor([1.8742])), ('power', tensor([0.1470]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1876])), ('power', tensor([-10.4859]))])
epoch：909	 i:0 	 global-step:18180	 l-p:0.061363335698843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[17.1878, 22.6764, 26.2394],
        [17.1878, 17.1878, 17.1878],
        [17.1878, 18.1163, 17.8238],
        [17.1878, 19.0826, 19.1382]], grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.06131570786237717 
model_pd.l_d.mean(): -1.8928440809249878 
model_pd.lagr.mean(): -1.8315284252166748 
model_pd.lambdas: dict_items([('pout', tensor([1.8740])), ('power', tensor([0.1464]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1918])), ('power', tensor([-10.4341]))])
epoch：910	 i:0 	 global-step:18200	 l-p:0.06131570786237717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[17.2420, 17.2420, 17.2420],
        [17.2420, 17.8139, 17.5315],
        [17.2420, 17.2483, 17.2422],
        [17.2420, 18.7476, 18.6072]], grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.0612683929502964 
model_pd.l_d.mean(): -1.8874900341033936 
model_pd.lagr.mean(): -1.8262215852737427 
model_pd.lambdas: dict_items([('pout', tensor([1.8738])), ('power', tensor([0.1459]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1959])), ('power', tensor([-10.3822]))])
epoch：911	 i:0 	 global-step:18220	 l-p:0.0612683929502964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[17.2964, 20.0218, 20.6474],
        [17.2964, 17.2973, 17.2964],
        [17.2964, 23.2444, 27.3742],
        [17.2964, 22.2236, 25.0792]], grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.0612214058637619 
model_pd.l_d.mean(): -1.882188320159912 
model_pd.lagr.mean(): -1.8209669589996338 
model_pd.lambdas: dict_items([('pout', tensor([1.8736])), ('power', tensor([0.1454]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2000])), ('power', tensor([-10.3301]))])
epoch：912	 i:0 	 global-step:18240	 l-p:0.0612214058637619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[17.3508, 17.4674, 17.3723],
        [17.3508, 18.8669, 18.7256],
        [17.3508, 17.9634, 17.6732],
        [17.3508, 17.9268, 17.6424]], grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.06117471680045128 
model_pd.l_d.mean(): -1.8769385814666748 
model_pd.lagr.mean(): -1.8157638311386108 
model_pd.lambdas: dict_items([('pout', tensor([1.8734])), ('power', tensor([0.1449]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2041])), ('power', tensor([-10.2780]))])
epoch：913	 i:0 	 global-step:18260	 l-p:0.06117471680045128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[17.4054, 18.3470, 18.0505],
        [17.4054, 20.9660, 22.3253],
        [17.4054, 17.6223, 17.4644],
        [17.4054, 18.9267, 18.7850]], grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.06112834811210632 
model_pd.l_d.mean(): -1.8717409372329712 
model_pd.lagr.mean(): -1.8106125593185425 
model_pd.lambdas: dict_items([('pout', tensor([1.8732])), ('power', tensor([0.1444]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2082])), ('power', tensor([-10.2257]))])
epoch：914	 i:0 	 global-step:18280	 l-p:0.06112834811210632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[17.4601, 18.2809, 17.9761],
        [17.4601, 21.0368, 22.4047],
        [17.4601, 17.4624, 17.4601],
        [17.4601, 17.5774, 17.4817]], grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.061082299798727036 
model_pd.l_d.mean(): -1.866594910621643 
model_pd.lagr.mean(): -1.8055126667022705 
model_pd.lambdas: dict_items([('pout', tensor([1.8729])), ('power', tensor([0.1439]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2123])), ('power', tensor([-10.1734]))])
epoch：915	 i:0 	 global-step:18300	 l-p:0.061082299798727036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[17.5148, 17.5171, 17.5149],
        [17.5148, 18.1338, 17.8406],
        [17.5148, 17.5838, 17.5240],
        [17.5148, 22.1965, 24.7278]], grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.06103656440973282 
model_pd.l_d.mean(): -1.8615001440048218 
model_pd.lagr.mean(): -1.8004635572433472 
model_pd.lambdas: dict_items([('pout', tensor([1.8727])), ('power', tensor([0.1434]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2164])), ('power', tensor([-10.1209]))])
epoch：916	 i:0 	 global-step:18320	 l-p:0.06103656440973282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[17.5697, 17.5697, 17.5697],
        [17.5697, 17.5712, 17.5697],
        [17.5697, 17.5697, 17.5697],
        [17.5697, 17.5697, 17.5697]], grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.060991134494543076 
model_pd.l_d.mean(): -1.8564568758010864 
model_pd.lagr.mean(): -1.7954657077789307 
model_pd.lambdas: dict_items([('pout', tensor([1.8725])), ('power', tensor([0.1429]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2205])), ('power', tensor([-10.0683]))])
epoch：917	 i:0 	 global-step:18340	 l-p:0.060991134494543076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[17.6246, 17.6311, 17.6249],
        [17.6246, 18.7564, 18.4833],
        [17.6246, 17.6261, 17.6247],
        [17.6246, 18.8075, 18.5458]], grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.0609460212290287 
model_pd.l_d.mean(): -1.851464867591858 
model_pd.lagr.mean(): -1.790518879890442 
model_pd.lambdas: dict_items([('pout', tensor([1.8723])), ('power', tensor([0.1424]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2246])), ('power', tensor([-10.0156]))])
epoch：918	 i:0 	 global-step:18360	 l-p:0.0609460212290287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[17.6797, 20.2807, 20.7726],
        [17.6797, 22.4090, 24.9665],
        [17.6797, 18.4201, 18.1128],
        [17.6797, 17.9004, 17.7397]], grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.0609012134373188 
model_pd.l_d.mean(): -1.846523404121399 
model_pd.lagr.mean(): -1.785622239112854 
model_pd.lambdas: dict_items([('pout', tensor([1.8721])), ('power', tensor([0.1419]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2287])), ('power', tensor([-9.9628]))])
epoch：919	 i:0 	 global-step:18380	 l-p:0.0609012134373188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[17.7348, 17.8047, 17.7441],
        [17.7348, 17.7534, 17.7359],
        [17.7348, 17.8692, 17.7615],
        [17.7348, 17.9644, 17.7987]], grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.060856711119413376 
model_pd.l_d.mean(): -1.841632604598999 
model_pd.lagr.mean(): -1.7807759046554565 
model_pd.lambdas: dict_items([('pout', tensor([1.8718])), ('power', tensor([0.1414]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2328])), ('power', tensor([-9.9099]))])
epoch：920	 i:0 	 global-step:18400	 l-p:0.060856711119413376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[17.7900, 18.3822, 18.0899],
        [17.7900, 19.8878, 20.0232],
        [17.7900, 18.0204, 17.8541],
        [17.7900, 22.8696, 25.8146]], grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.06081252172589302 
model_pd.l_d.mean(): -1.836792230606079 
model_pd.lagr.mean(): -1.77597975730896 
model_pd.lambdas: dict_items([('pout', tensor([1.8716])), ('power', tensor([0.1409]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2368])), ('power', tensor([-9.8569]))])
epoch：921	 i:0 	 global-step:18420	 l-p:0.06081252172589302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[17.8453, 17.8712, 17.8472],
        [17.8453, 17.8453, 17.8453],
        [17.8453, 17.9807, 17.8722],
        [17.8453, 21.5080, 22.9095]], grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.06076863780617714 
model_pd.l_d.mean(): -1.832001805305481 
model_pd.lagr.mean(): -1.7712332010269165 
model_pd.lambdas: dict_items([('pout', tensor([1.8713])), ('power', tensor([0.1404]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2409])), ('power', tensor([-9.8038]))])
epoch：922	 i:0 	 global-step:18440	 l-p:0.06076863780617714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[17.9007, 17.9007, 17.9007],
        [17.9007, 17.9018, 17.9007],
        [17.9007, 19.7603, 19.7520],
        [17.9007, 19.6378, 19.5675]], grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.06072505936026573 
model_pd.l_d.mean(): -1.8272616863250732 
model_pd.lagr.mean(): -1.7665365934371948 
model_pd.lambdas: dict_items([('pout', tensor([1.8711])), ('power', tensor([0.1399]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2450])), ('power', tensor([-9.7507]))])
epoch：923	 i:0 	 global-step:18460	 l-p:0.06072505936026573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[17.9561, 21.6134, 22.9953],
        [17.9561, 17.9561, 17.9561],
        [17.9561, 17.9571, 17.9562],
        [17.9561, 17.9562, 17.9561]], grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.0606817826628685 
model_pd.l_d.mean(): -1.822571039199829 
model_pd.lagr.mean(): -1.7618892192840576 
model_pd.lambdas: dict_items([('pout', tensor([1.8708])), ('power', tensor([0.1394]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2490])), ('power', tensor([-9.6974]))])
epoch：924	 i:0 	 global-step:18480	 l-p:0.0606817826628685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[18.0117, 24.2252, 28.5414],
        [18.0117, 18.0457, 18.0146],
        [18.0117, 18.0128, 18.0117],
        [18.0117, 23.7849, 27.5347]], grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.06063880771398544 
model_pd.l_d.mean(): -1.8179298639297485 
model_pd.lagr.mean(): -1.7572910785675049 
model_pd.lambdas: dict_items([('pout', tensor([1.8706])), ('power', tensor([0.1389]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2530])), ('power', tensor([-9.6441]))])
epoch：925	 i:0 	 global-step:18500	 l-p:0.06063880771398544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[18.0672, 18.0684, 18.0673],
        [18.0672, 23.0057, 25.7359],
        [18.0672, 20.0689, 20.1285],
        [18.0672, 18.5892, 18.3087]], grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.060596153140068054 
model_pd.l_d.mean(): -1.813337802886963 
model_pd.lagr.mean(): -1.7527416944503784 
model_pd.lambdas: dict_items([('pout', tensor([1.8703])), ('power', tensor([0.1385]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2571])), ('power', tensor([-9.5907]))])
epoch：926	 i:0 	 global-step:18520	 l-p:0.060596153140068054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[18.1229, 21.8171, 23.2133],
        [18.1229, 18.1239, 18.1229],
        [18.1229, 21.8476, 23.2732],
        [18.1229, 19.3427, 19.0731]], grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.06055377796292305 
model_pd.l_d.mean(): -1.8087947368621826 
model_pd.lagr.mean(): -1.7482409477233887 
model_pd.lambdas: dict_items([('pout', tensor([1.8701])), ('power', tensor([0.1380]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2611])), ('power', tensor([-9.5372]))])
epoch：927	 i:0 	 global-step:18540	 l-p:0.06055377796292305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[18.1786, 18.2051, 18.1806],
        [18.1786, 19.4026, 19.1321],
        [18.1786, 21.0563, 21.7182],
        [18.1786, 20.1304, 20.1549]], grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.060511719435453415 
model_pd.l_d.mean(): -1.80430006980896 
model_pd.lagr.mean(): -1.7437883615493774 
model_pd.lambdas: dict_items([('pout', tensor([1.8698])), ('power', tensor([0.1375]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2651])), ('power', tensor([-9.4836]))])
epoch：928	 i:0 	 global-step:18560	 l-p:0.060511719435453415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[18.2344, 19.0956, 18.7761],
        [18.2344, 18.4629, 18.2966],
        [18.2344, 18.3065, 18.2440],
        [18.2344, 20.0071, 19.9357]], grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.06046995520591736 
model_pd.l_d.mean(): -1.799854040145874 
model_pd.lagr.mean(): -1.7393840551376343 
model_pd.lambdas: dict_items([('pout', tensor([1.8695])), ('power', tensor([0.1370]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2691])), ('power', tensor([-9.4300]))])
epoch：929	 i:0 	 global-step:18580	 l-p:0.06046995520591736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[18.2902, 23.2946, 26.0617],
        [18.2902, 20.1943, 20.1862],
        [18.2902, 18.2902, 18.2902],
        [18.2902, 18.8193, 18.5350]], grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.060428496450185776 
model_pd.l_d.mean(): -1.7954559326171875 
model_pd.lagr.mean(): -1.7350274324417114 
model_pd.lambdas: dict_items([('pout', tensor([1.8693])), ('power', tensor([0.1366]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2731])), ('power', tensor([-9.3763]))])
epoch：930	 i:0 	 global-step:18600	 l-p:0.060428496450185776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[18.3461, 22.1164, 23.5575],
        [18.3461, 18.3477, 18.3461],
        [18.3461, 20.3817, 20.4426],
        [18.3461, 18.3809, 18.3491]], grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.06038732826709747 
model_pd.l_d.mean(): -1.7911062240600586 
model_pd.lagr.mean(): -1.7307188510894775 
model_pd.lambdas: dict_items([('pout', tensor([1.8690])), ('power', tensor([0.1361]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2771])), ('power', tensor([-9.3226]))])
epoch：931	 i:0 	 global-step:18620	 l-p:0.06038732826709747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[18.4020, 18.4045, 18.4021],
        [18.4020, 20.3801, 20.4052],
        [18.4020, 19.2720, 18.9493],
        [18.4020, 18.6413, 18.4686]], grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.060346461832523346 
model_pd.l_d.mean(): -1.7868036031723022 
model_pd.lagr.mean(): -1.726457118988037 
model_pd.lambdas: dict_items([('pout', tensor([1.8687])), ('power', tensor([0.1356]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2811])), ('power', tensor([-9.2687]))])
epoch：932	 i:0 	 global-step:18640	 l-p:0.060346461832523346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[18.4580, 19.0749, 18.7706],
        [18.4580, 18.5986, 18.4860],
        [18.4580, 19.2347, 18.9125],
        [18.4580, 20.6423, 20.7840]], grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.0603058896958828 
model_pd.l_d.mean(): -1.782548427581787 
model_pd.lagr.mean(): -1.7222425937652588 
model_pd.lambdas: dict_items([('pout', tensor([1.8684])), ('power', tensor([0.1352]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2851])), ('power', tensor([-9.2149]))])
epoch：933	 i:0 	 global-step:18660	 l-p:0.0603058896958828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[18.5140, 23.8171, 26.8934],
        [18.5140, 18.7515, 18.5795],
        [18.5140, 20.1424, 19.9916],
        [18.5140, 19.7629, 19.4871]], grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.060265615582466125 
model_pd.l_d.mean(): -1.778340458869934 
model_pd.lagr.mean(): -1.7180747985839844 
model_pd.lambdas: dict_items([('pout', tensor([1.8681])), ('power', tensor([0.1347]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2890])), ('power', tensor([-9.1609]))])
epoch：934	 i:0 	 global-step:18680	 l-p:0.060265615582466125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[18.5701, 19.0220, 18.7573],
        [18.5701, 18.5711, 18.5701],
        [18.5701, 19.7691, 19.4803],
        [18.5701, 18.8105, 18.6368]], grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.06022563576698303 
model_pd.l_d.mean(): -1.7741789817810059 
model_pd.lagr.mean(): -1.7139533758163452 
model_pd.lambdas: dict_items([('pout', tensor([1.8678])), ('power', tensor([0.1343]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2930])), ('power', tensor([-9.1069]))])
epoch：935	 i:0 	 global-step:18700	 l-p:0.06022563576698303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[18.6262, 18.7524, 18.6495],
        [18.6262, 19.4107, 19.0853],
        [18.6262, 18.6287, 18.6262],
        [18.6262, 18.6272, 18.6262]], grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.060185957700014114 
model_pd.l_d.mean(): -1.770064115524292 
model_pd.lagr.mean(): -1.7098782062530518 
model_pd.lambdas: dict_items([('pout', tensor([1.8675])), ('power', tensor([0.1338]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2969])), ('power', tensor([-9.0529]))])
epoch：936	 i:0 	 global-step:18720	 l-p:0.060185957700014114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[18.6823, 20.0058, 19.7431],
        [18.6823, 23.8025, 26.6345],
        [18.6823, 19.1372, 18.8708],
        [18.6823, 21.6470, 22.3296]], grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.06014655902981758 
model_pd.l_d.mean(): -1.7659955024719238 
model_pd.lagr.mean(): -1.7058489322662354 
model_pd.lambdas: dict_items([('pout', tensor([1.8672])), ('power', tensor([0.1334]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3009])), ('power', tensor([-8.9988]))])
epoch：937	 i:0 	 global-step:18740	 l-p:0.06014655902981758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[18.7385, 18.7385, 18.7385],
        [18.7385, 20.5650, 20.4919],
        [18.7385, 20.6937, 20.6859],
        [18.7385, 19.3658, 19.0563]], grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.06010746583342552 
model_pd.l_d.mean(): -1.7619725465774536 
model_pd.lagr.mean(): -1.7018650770187378 
model_pd.lambdas: dict_items([('pout', tensor([1.8669])), ('power', tensor([0.1329]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3048])), ('power', tensor([-8.9447]))])
epoch：938	 i:0 	 global-step:18760	 l-p:0.06010746583342552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[18.7946, 20.1269, 19.8625],
        [18.7946, 20.0096, 19.7171],
        [18.7946, 18.8145, 18.7958],
        [18.7946, 24.8384, 28.7660]], grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.060068659484386444 
model_pd.l_d.mean(): -1.7579951286315918 
model_pd.lagr.mean(): -1.6979265213012695 
model_pd.lambdas: dict_items([('pout', tensor([1.8666])), ('power', tensor([0.1325]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3087])), ('power', tensor([-8.8906]))])
epoch：939	 i:0 	 global-step:18780	 l-p:0.060068659484386444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[18.8508, 19.6458, 19.3162],
        [18.8508, 18.8707, 18.8520],
        [18.8508, 18.8519, 18.8508],
        [18.8508, 24.2580, 27.3954]], grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.060030147433280945 
model_pd.l_d.mean(): -1.754063367843628 
model_pd.lagr.mean(): -1.6940332651138306 
model_pd.lambdas: dict_items([('pout', tensor([1.8663])), ('power', tensor([0.1320]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3126])), ('power', tensor([-8.8364]))])
epoch：940	 i:0 	 global-step:18800	 l-p:0.060030147433280945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[18.9070, 19.5406, 19.2281],
        [18.9070, 21.7058, 22.2368],
        [18.9070, 19.8035, 19.4711],
        [18.9070, 22.8025, 24.2924]], grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.05999191850423813 
model_pd.l_d.mean(): -1.7501766681671143 
model_pd.lagr.mean(): -1.6901847124099731 
model_pd.lambdas: dict_items([('pout', tensor([1.8660])), ('power', tensor([0.1316]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3165])), ('power', tensor([-8.7822]))])
epoch：941	 i:0 	 global-step:18820	 l-p:0.05999191850423813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[18.9633, 20.6350, 20.4806],
        [18.9633, 20.1903, 19.8950],
        [18.9633, 19.5140, 19.2182],
        [18.9633, 19.2093, 19.0316]], grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.059953976422548294 
model_pd.l_d.mean(): -1.7463343143463135 
model_pd.lagr.mean(): -1.686380386352539 
model_pd.lambdas: dict_items([('pout', tensor([1.8657])), ('power', tensor([0.1311]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3204])), ('power', tensor([-8.7280]))])
epoch：942	 i:0 	 global-step:18840	 l-p:0.059953976422548294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[19.0195, 19.0557, 19.0226],
        [19.0195, 21.0069, 20.9992],
        [19.0195, 19.0195, 19.0195],
        [19.0195, 19.2642, 19.0870]], grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.05991632863879204 
model_pd.l_d.mean(): -1.7425363063812256 
model_pd.lagr.mean(): -1.6826199293136597 
model_pd.lambdas: dict_items([('pout', tensor([1.8654])), ('power', tensor([0.1307]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3243])), ('power', tensor([-8.6737]))])
epoch：943	 i:0 	 global-step:18860	 l-p:0.05991632863879204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[19.0757, 21.9017, 22.4381],
        [19.0757, 20.7584, 20.6030],
        [19.0757, 21.2001, 21.2644],
        [19.0757, 19.3234, 19.1445]], grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.05987896770238876 
model_pd.l_d.mean(): -1.7387824058532715 
model_pd.lagr.mean(): -1.6789034605026245 
model_pd.lambdas: dict_items([('pout', tensor([1.8650])), ('power', tensor([0.1303]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3281])), ('power', tensor([-8.6195]))])
epoch：944	 i:0 	 global-step:18880	 l-p:0.05987896770238876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[19.1320, 19.1684, 19.1351],
        [19.1320, 21.2632, 21.3278],
        [19.1320, 19.3818, 19.2015],
        [19.1320, 19.1332, 19.1320]], grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.05984189361333847 
model_pd.l_d.mean(): -1.7350726127624512 
model_pd.lagr.mean(): -1.6752307415008545 
model_pd.lambdas: dict_items([('pout', tensor([1.8647])), ('power', tensor([0.1299]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3320])), ('power', tensor([-8.5652]))])
epoch：945	 i:0 	 global-step:18900	 l-p:0.05984189361333847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[19.1882, 19.2248, 19.1914],
        [19.1882, 19.4376, 19.2574],
        [19.1882, 20.4312, 20.1322],
        [19.1882, 23.1509, 24.6695]], grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.05980510637164116 
model_pd.l_d.mean(): -1.73140549659729 
model_pd.lagr.mean(): -1.671600341796875 
model_pd.lambdas: dict_items([('pout', tensor([1.8644])), ('power', tensor([0.1294]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3358])), ('power', tensor([-8.5109]))])
epoch：946	 i:0 	 global-step:18920	 l-p:0.05980510637164116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[19.2445, 19.8043, 19.5036],
        [19.2445, 19.4924, 19.3129],
        [19.2445, 19.2457, 19.2445],
        [19.2445, 21.3893, 21.4545]], grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.05976860970258713 
model_pd.l_d.mean(): -1.7277820110321045 
model_pd.lagr.mean(): -1.6680134534835815 
model_pd.lambdas: dict_items([('pout', tensor([1.8640])), ('power', tensor([0.1290]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3397])), ('power', tensor([-8.4566]))])
epoch：947	 i:0 	 global-step:18940	 l-p:0.05976860970258713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[19.3007, 19.7723, 19.4961],
        [19.3007, 19.5441, 19.3670],
        [19.3007, 22.9419, 24.1424],
        [19.3007, 21.1873, 21.1123]], grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.05973238870501518 
model_pd.l_d.mean(): -1.7242012023925781 
model_pd.lagr.mean(): -1.664468765258789 
model_pd.lambdas: dict_items([('pout', tensor([1.8637])), ('power', tensor([0.1286]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3435])), ('power', tensor([-8.4023]))])
epoch：948	 i:0 	 global-step:18960	 l-p:0.05973238870501518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[19.3569, 20.1755, 19.8362],
        [19.3569, 22.2281, 22.7736],
        [19.3569, 19.3569, 19.3569],
        [19.3569, 20.6119, 20.3101]], grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.05969645082950592 
model_pd.l_d.mean(): -1.7206629514694214 
model_pd.lagr.mean(): -1.6609665155410767 
model_pd.lambdas: dict_items([('pout', tensor([1.8633])), ('power', tensor([0.1282]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3473])), ('power', tensor([-8.3480]))])
epoch：949	 i:0 	 global-step:18980	 l-p:0.05969645082950592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[19.4131, 21.7211, 21.8720],
        [19.4131, 21.1283, 20.9703],
        [19.4131, 19.4134, 19.4131],
        [19.4131, 19.5452, 19.4375]], grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.05966080352663994 
model_pd.l_d.mean(): -1.7171666622161865 
model_pd.lagr.mean(): -1.6575058698654175 
model_pd.lambdas: dict_items([('pout', tensor([1.8630])), ('power', tensor([0.1278]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3511])), ('power', tensor([-8.2937]))])
epoch：950	 i:0 	 global-step:19000	 l-p:0.05966080352663994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[19.4692, 20.1652, 19.8360],
        [19.4692, 25.7463, 29.8272],
        [19.4692, 19.4899, 19.4705],
        [19.4692, 19.4978, 19.4713]], grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.059625424444675446 
model_pd.l_d.mean(): -1.713712215423584 
model_pd.lagr.mean(): -1.6540868282318115 
model_pd.lambdas: dict_items([('pout', tensor([1.8626])), ('power', tensor([0.1273]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3549])), ('power', tensor([-8.2395]))])
epoch：951	 i:0 	 global-step:19020	 l-p:0.059625424444675446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[19.5254, 19.7809, 19.5965],
        [19.5254, 19.5254, 19.5254],
        [19.5254, 19.7796, 19.5959],
        [19.5254, 20.8495, 20.5576]], grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.05959033593535423 
model_pd.l_d.mean(): -1.7102992534637451 
model_pd.lagr.mean(): -1.6507089138031006 
model_pd.lambdas: dict_items([('pout', tensor([1.8623])), ('power', tensor([0.1269]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3586])), ('power', tensor([-8.1852]))])
epoch：952	 i:0 	 global-step:19040	 l-p:0.05959033593535423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[19.5815, 19.5815, 19.5815],
        [19.5815, 19.5815, 19.5815],
        [19.5815, 19.5841, 19.5815],
        [19.5815, 22.4890, 23.0416]], grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.05955551937222481 
model_pd.l_d.mean(): -1.7069270610809326 
model_pd.lagr.mean(): -1.647371530532837 
model_pd.lambdas: dict_items([('pout', tensor([1.8619])), ('power', tensor([0.1265]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3624])), ('power', tensor([-8.1310]))])
epoch：953	 i:0 	 global-step:19060	 l-p:0.05955551937222481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[19.6376, 19.7882, 19.6675],
        [19.6376, 20.2101, 19.9027],
        [19.6376, 19.8947, 19.7092],
        [19.6376, 19.6393, 19.6376]], grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.059520989656448364 
model_pd.l_d.mean(): -1.7035961151123047 
model_pd.lagr.mean(): -1.6440751552581787 
model_pd.lambdas: dict_items([('pout', tensor([1.8615])), ('power', tensor([0.1261]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3661])), ('power', tensor([-8.0767]))])
epoch：954	 i:0 	 global-step:19080	 l-p:0.059520989656448364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[19.6936, 20.3985, 20.0651],
        [19.6936, 21.0302, 20.7357],
        [19.6936, 19.6936, 19.6936],
        [19.6936, 19.7312, 19.6968]], grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.05948672443628311 
model_pd.l_d.mean(): -1.7003052234649658 
model_pd.lagr.mean(): -1.640818476676941 
model_pd.lambdas: dict_items([('pout', tensor([1.8612])), ('power', tensor([0.1257]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3699])), ('power', tensor([-8.0225]))])
epoch：955	 i:0 	 global-step:19100	 l-p:0.05948672443628311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[19.7496, 22.1013, 22.2554],
        [19.7496, 20.5865, 20.2397],
        [19.7496, 21.6843, 21.6078],
        [19.7496, 19.7509, 19.7496]], grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.059452738612890244 
model_pd.l_d.mean(): -1.6970546245574951 
model_pd.lagr.mean(): -1.6376018524169922 
model_pd.lambdas: dict_items([('pout', tensor([1.8608])), ('power', tensor([0.1253]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3736])), ('power', tensor([-7.9684]))])
epoch：956	 i:0 	 global-step:19120	 l-p:0.059452738612890244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[19.8056, 22.1645, 22.3191],
        [19.8056, 22.7492, 23.3090],
        [19.8056, 20.0615, 19.8762],
        [19.8056, 19.8434, 19.8088]], grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.05941903963685036 
model_pd.l_d.mean(): -1.6938438415527344 
model_pd.lagr.mean(): -1.6344248056411743 
model_pd.lambdas: dict_items([('pout', tensor([1.8604])), ('power', tensor([0.1249]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3773])), ('power', tensor([-7.9143]))])
epoch：957	 i:0 	 global-step:19140	 l-p:0.05941903963685036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[19.8615, 19.8618, 19.8615],
        [19.8615, 19.8994, 19.8647],
        [19.8615, 26.7623, 31.5612],
        [19.8615, 22.8142, 23.3757]], grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.05938560515642166 
model_pd.l_d.mean(): -1.6906721591949463 
model_pd.lagr.mean(): -1.6312865018844604 
model_pd.lambdas: dict_items([('pout', tensor([1.8600])), ('power', tensor([0.1245]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3810])), ('power', tensor([-7.8602]))])
epoch：958	 i:0 	 global-step:19160	 l-p:0.05938560515642166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[19.9173, 19.9466, 19.9195],
        [19.9173, 20.6310, 20.2935],
        [19.9173, 21.0100, 20.6670],
        [19.9173, 24.0384, 25.6164]], grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.05935244634747505 
model_pd.l_d.mean(): -1.6875395774841309 
model_pd.lagr.mean(): -1.6281871795654297 
model_pd.lambdas: dict_items([('pout', tensor([1.8597])), ('power', tensor([0.1241]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3846])), ('power', tensor([-7.8061]))])
epoch：959	 i:0 	 global-step:19180	 l-p:0.05935244634747505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[19.9731, 19.9731, 19.9731],
        [19.9731, 25.3675, 28.2897],
        [19.9731, 20.0529, 19.9837],
        [19.9731, 19.9943, 19.9744]], grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.05931956321001053 
model_pd.l_d.mean(): -1.684445858001709 
model_pd.lagr.mean(): -1.6251262426376343 
model_pd.lambdas: dict_items([('pout', tensor([1.8593])), ('power', tensor([0.1238]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3883])), ('power', tensor([-7.7521]))])
epoch：960	 i:0 	 global-step:19200	 l-p:0.05931956321001053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[20.0288, 25.4394, 28.3705],
        [20.0288, 20.0289, 20.0288],
        [20.0288, 20.0306, 20.0289],
        [20.0288, 22.4168, 22.5735]], grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.05928695201873779 
model_pd.l_d.mean(): -1.6813902854919434 
model_pd.lagr.mean(): -1.6221033334732056 
model_pd.lambdas: dict_items([('pout', tensor([1.8589])), ('power', tensor([0.1234]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3919])), ('power', tensor([-7.6982]))])
epoch：961	 i:0 	 global-step:19220	 l-p:0.05928695201873779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[20.0845, 20.6714, 20.3563],
        [20.0845, 23.4744, 24.3662],
        [20.0845, 21.4502, 21.1495],
        [20.0845, 21.5175, 21.2339]], grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.05925460904836655 
model_pd.l_d.mean(): -1.6783729791641235 
model_pd.lagr.mean(): -1.619118332862854 
model_pd.lambdas: dict_items([('pout', tensor([1.8585])), ('power', tensor([0.1230]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3956])), ('power', tensor([-7.6443]))])
epoch：962	 i:0 	 global-step:19240	 l-p:0.05925460904836655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[20.1401, 24.3156, 25.9175],
        [20.1401, 25.9458, 29.3173],
        [20.1401, 20.1412, 20.1401],
        [20.1401, 20.7287, 20.4127]], grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.059222541749477386 
model_pd.l_d.mean(): -1.675392985343933 
model_pd.lagr.mean(): -1.6161704063415527 
model_pd.lambdas: dict_items([('pout', tensor([1.8581])), ('power', tensor([0.1226]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3992])), ('power', tensor([-7.5904]))])
epoch：963	 i:0 	 global-step:19260	 l-p:0.059222541749477386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[20.1956, 20.2171, 20.1969],
        [20.1956, 24.3835, 25.9903],
        [20.1956, 20.8770, 20.5412],
        [20.1956, 20.4516, 20.2653]], grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.05919073149561882 
model_pd.l_d.mean(): -1.6724505424499512 
model_pd.lagr.mean(): -1.6132597923278809 
model_pd.lambdas: dict_items([('pout', tensor([1.8577])), ('power', tensor([0.1222]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4028])), ('power', tensor([-7.5367]))])
epoch：964	 i:0 	 global-step:19280	 l-p:0.05919073149561882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[20.2510, 23.4870, 24.2345],
        [20.2510, 24.0866, 25.3528],
        [20.2510, 26.0910, 29.4828],
        [20.2510, 20.3321, 20.2618]], grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.05915920436382294 
model_pd.l_d.mean(): -1.6695446968078613 
model_pd.lagr.mean(): -1.610385537147522 
model_pd.lambdas: dict_items([('pout', tensor([1.8573])), ('power', tensor([0.1219]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4064])), ('power', tensor([-7.4829]))])
epoch：965	 i:0 	 global-step:19300	 l-p:0.05915920436382294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[20.3064, 20.3139, 20.3066],
        [20.3064, 24.5191, 26.1356],
        [20.3064, 20.5695, 20.3790],
        [20.3064, 20.3064, 20.3064]], grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.05912793055176735 
model_pd.l_d.mean(): -1.6666755676269531 
model_pd.lagr.mean(): -1.607547640800476 
model_pd.lambdas: dict_items([('pout', tensor([1.8569])), ('power', tensor([0.1215]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4099])), ('power', tensor([-7.4293]))])
epoch：966	 i:0 	 global-step:19320	 l-p:0.05912793055176735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[20.3616, 20.3620, 20.3616],
        [20.3616, 23.3951, 23.9727],
        [20.3616, 22.5027, 22.4957],
        [20.3616, 21.4811, 21.1299]], grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.05909692868590355 
model_pd.l_d.mean(): -1.6638424396514893 
model_pd.lagr.mean(): -1.6047455072402954 
model_pd.lambdas: dict_items([('pout', tensor([1.8564])), ('power', tensor([0.1211]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4135])), ('power', tensor([-7.3757]))])
epoch：967	 i:0 	 global-step:19340	 l-p:0.05909692868590355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[20.4168, 21.5396, 21.1874],
        [20.4168, 20.5565, 20.4426],
        [20.4168, 20.4196, 20.4168],
        [20.4168, 20.4244, 20.4171]], grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.059066180139780045 
model_pd.l_d.mean(): -1.6610450744628906 
model_pd.lagr.mean(): -1.6019788980484009 
model_pd.lambdas: dict_items([('pout', tensor([1.8560])), ('power', tensor([0.1208]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4170])), ('power', tensor([-7.3223]))])
epoch：968	 i:0 	 global-step:19360	 l-p:0.059066180139780045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[20.4719, 23.5232, 24.1043],
        [20.4719, 24.6870, 26.2843],
        [20.4719, 20.7317, 20.5427],
        [20.4719, 20.4719, 20.4719]], grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.059035710990428925 
model_pd.l_d.mean(): -1.6582834720611572 
model_pd.lagr.mean(): -1.5992478132247925 
model_pd.lambdas: dict_items([('pout', tensor([1.8556])), ('power', tensor([0.1204]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4206])), ('power', tensor([-7.2689]))])
epoch：969	 i:0 	 global-step:19380	 l-p:0.059035710990428925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[20.5268, 21.9944, 21.7043],
        [20.5268, 20.5662, 20.5302],
        [20.5268, 24.7542, 26.3562],
        [20.5268, 20.7931, 20.6004]], grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.0590055026113987 
model_pd.l_d.mean(): -1.6555569171905518 
model_pd.lagr.mean(): -1.5965514183044434 
model_pd.lambdas: dict_items([('pout', tensor([1.8552])), ('power', tensor([0.1200]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4241])), ('power', tensor([-7.2156]))])
epoch：970	 i:0 	 global-step:19400	 l-p:0.0590055026113987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[20.5817, 20.5817, 20.5817],
        [20.5817, 22.8896, 22.9611],
        [20.5817, 26.5241, 29.9760],
        [20.5817, 21.9845, 21.6759]], grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.05897555500268936 
model_pd.l_d.mean(): -1.6528650522232056 
model_pd.lagr.mean(): -1.5938894748687744 
model_pd.lambdas: dict_items([('pout', tensor([1.8548])), ('power', tensor([0.1197]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4276])), ('power', tensor([-7.1624]))])
epoch：971	 i:0 	 global-step:19420	 l-p:0.05897555500268936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[20.6365, 20.7956, 20.6681],
        [20.6365, 21.7726, 21.4163],
        [20.6365, 24.9184, 26.5592],
        [20.6365, 20.6366, 20.6365]], grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.05894586443901062 
model_pd.l_d.mean(): -1.650207281112671 
model_pd.lagr.mean(): -1.591261386871338 
model_pd.lambdas: dict_items([('pout', tensor([1.8543])), ('power', tensor([0.1193]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4310])), ('power', tensor([-7.1092]))])
epoch：972	 i:0 	 global-step:19440	 l-p:0.05894586443901062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[20.6911, 21.2976, 20.9721],
        [20.6911, 20.7217, 20.6933],
        [20.6911, 23.0124, 23.0844],
        [20.6911, 20.8507, 20.7229]], grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.05891643092036247 
model_pd.l_d.mean(): -1.6475837230682373 
model_pd.lagr.mean(): -1.5886672735214233 
model_pd.lambdas: dict_items([('pout', tensor([1.8539])), ('power', tensor([0.1190]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4345])), ('power', tensor([-7.0562]))])
epoch：973	 i:0 	 global-step:19460	 l-p:0.05891643092036247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[20.7456, 20.9057, 20.7775],
        [20.7456, 23.8412, 24.4311],
        [20.7456, 23.2267, 23.3904],
        [20.7456, 20.7456, 20.7456]], grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.058887261897325516 
model_pd.l_d.mean(): -1.644993543624878 
model_pd.lagr.mean(): -1.586106300354004 
model_pd.lambdas: dict_items([('pout', tensor([1.8535])), ('power', tensor([0.1186]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4379])), ('power', tensor([-7.0033]))])
epoch：974	 i:0 	 global-step:19480	 l-p:0.058887261897325516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[20.8000, 20.8001, 20.8000],
        [20.8000, 22.2890, 21.9948],
        [20.8000, 26.5470, 29.7303],
        [20.8000, 22.6497, 22.4803]], grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.05885835736989975 
model_pd.l_d.mean(): -1.6424369812011719 
model_pd.lagr.mean(): -1.5835785865783691 
model_pd.lambdas: dict_items([('pout', tensor([1.8530])), ('power', tensor([0.1183]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4414])), ('power', tensor([-6.9505]))])
epoch：975	 i:0 	 global-step:19500	 l-p:0.05885835736989975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[20.8543, 20.8543, 20.8543],
        [20.8543, 20.8557, 20.8543],
        [20.8543, 20.8544, 20.8543],
        [20.8543, 21.1292, 20.9309]], grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.05882970243692398 
model_pd.l_d.mean(): -1.6399132013320923 
model_pd.lagr.mean(): -1.5810835361480713 
model_pd.lambdas: dict_items([('pout', tensor([1.8526])), ('power', tensor([0.1179]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4448])), ('power', tensor([-6.8978]))])
epoch：976	 i:0 	 global-step:19520	 l-p:0.05882970243692398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[20.9085, 21.0699, 20.9406],
        [20.9085, 20.9086, 20.9085],
        [20.9085, 20.9163, 20.9087],
        [20.9085, 20.9096, 20.9085]], grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.058801308274269104 
model_pd.l_d.mean(): -1.6374216079711914 
model_pd.lagr.mean(): -1.5786203145980835 
model_pd.lambdas: dict_items([('pout', tensor([1.8521])), ('power', tensor([0.1176]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4482])), ('power', tensor([-6.8453]))])
epoch：977	 i:0 	 global-step:19540	 l-p:0.058801308274269104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[20.9625, 22.1183, 21.7559],
        [20.9625, 28.2730, 33.3598],
        [20.9625, 23.2429, 23.2746],
        [20.9625, 21.0467, 20.9737]], grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.058773159980773926 
model_pd.l_d.mean(): -1.634962558746338 
model_pd.lagr.mean(): -1.576189398765564 
model_pd.lambdas: dict_items([('pout', tensor([1.8517])), ('power', tensor([0.1172]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4515])), ('power', tensor([-6.7928]))])
epoch：978	 i:0 	 global-step:19560	 l-p:0.058773159980773926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[21.0163, 22.1754, 21.8121],
        [21.0163, 23.3032, 23.3349],
        [21.0163, 21.0164, 21.0163],
        [21.0163, 27.0933, 30.6244]], grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.05874526500701904 
model_pd.l_d.mean(): -1.6325350999832153 
model_pd.lagr.mean(): -1.5737898349761963 
model_pd.lambdas: dict_items([('pout', tensor([1.8512])), ('power', tensor([0.1169]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4549])), ('power', tensor([-6.7405]))])
epoch：979	 i:0 	 global-step:19580	 l-p:0.05874526500701904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[21.0701, 25.4539, 27.1374],
        [21.0701, 21.0780, 21.0704],
        [21.0701, 21.0701, 21.0701],
        [21.0701, 21.7841, 21.4324]], grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.05871763080358505 
model_pd.l_d.mean(): -1.630138874053955 
model_pd.lagr.mean(): -1.5714212656021118 
model_pd.lambdas: dict_items([('pout', tensor([1.8508])), ('power', tensor([0.1166]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4582])), ('power', tensor([-6.6883]))])
epoch：980	 i:0 	 global-step:19600	 l-p:0.05871763080358505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[21.1237, 23.4978, 23.5719],
        [21.1237, 21.1549, 21.1260],
        [21.1237, 24.2804, 24.8825],
        [21.1237, 22.0248, 21.6518]], grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.05869024246931076 
model_pd.l_d.mean(): -1.6277737617492676 
model_pd.lagr.mean(): -1.569083571434021 
model_pd.lambdas: dict_items([('pout', tensor([1.8503])), ('power', tensor([0.1162]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4616])), ('power', tensor([-6.6363]))])
epoch：981	 i:0 	 global-step:19620	 l-p:0.05869024246931076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[21.1771, 22.5621, 22.2300],
        [21.1771, 21.1775, 21.1771],
        [21.1771, 21.3409, 21.2097],
        [21.1771, 22.6244, 22.3064]], grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.05866311118006706 
model_pd.l_d.mean(): -1.6254395246505737 
model_pd.lagr.mean(): -1.5667763948440552 
model_pd.lambdas: dict_items([('pout', tensor([1.8498])), ('power', tensor([0.1159]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4649])), ('power', tensor([-6.5843]))])
epoch：982	 i:0 	 global-step:19640	 l-p:0.05866311118006706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[21.2304, 21.2333, 21.2304],
        [21.2304, 27.1049, 30.3599],
        [21.2304, 21.5068, 21.3068],
        [21.2304, 21.2304, 21.2304]], grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.058636222034692764 
model_pd.l_d.mean(): -1.6231350898742676 
model_pd.lagr.mean(): -1.5644989013671875 
model_pd.lambdas: dict_items([('pout', tensor([1.8494])), ('power', tensor([0.1156]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4681])), ('power', tensor([-6.5326]))])
epoch：983	 i:0 	 global-step:19660	 l-p:0.058636222034692764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[21.2835, 21.3150, 21.2858],
        [21.2835, 21.2835, 21.2835],
        [21.2835, 22.0513, 21.6885],
        [21.2835, 21.3692, 21.2949]], grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.05860958248376846 
model_pd.l_d.mean(): -1.6208610534667969 
model_pd.lagr.mean(): -1.5622514486312866 
model_pd.lambdas: dict_items([('pout', tensor([1.8489])), ('power', tensor([0.1153]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4714])), ('power', tensor([-6.4809]))])
epoch：984	 i:0 	 global-step:19680	 l-p:0.05860958248376846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[21.3365, 24.7607, 25.5532],
        [21.3365, 22.5149, 22.1456],
        [21.3365, 21.3394, 21.3365],
        [21.3365, 21.3365, 21.3365]], grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.05858319252729416 
model_pd.l_d.mean(): -1.6186163425445557 
model_pd.lagr.mean(): -1.5600332021713257 
model_pd.lambdas: dict_items([('pout', tensor([1.8484])), ('power', tensor([0.1149]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4747])), ('power', tensor([-6.4295]))])
epoch：985	 i:0 	 global-step:19700	 l-p:0.05858319252729416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[21.3893, 27.5817, 31.1808],
        [21.3893, 21.4754, 21.4007],
        [21.3893, 22.3029, 21.9248],
        [21.3893, 21.4122, 21.3906]], grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.05855706334114075 
model_pd.l_d.mean(): -1.6164007186889648 
model_pd.lagr.mean(): -1.5578436851501465 
model_pd.lambdas: dict_items([('pout', tensor([1.8479])), ('power', tensor([0.1146]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4779])), ('power', tensor([-6.3781]))])
epoch：986	 i:0 	 global-step:19720	 l-p:0.05855706334114075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[21.4419, 21.4420, 21.4419],
        [21.4419, 28.9309, 34.1432],
        [21.4419, 21.4832, 21.4454],
        [21.4419, 22.8458, 22.5093]], grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.05853116139769554 
model_pd.l_d.mean(): -1.6142141819000244 
model_pd.lagr.mean(): -1.5556830167770386 
model_pd.lambdas: dict_items([('pout', tensor([1.8475])), ('power', tensor([0.1143]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4811])), ('power', tensor([-6.3270]))])
epoch：987	 i:0 	 global-step:19740	 l-p:0.05853116139769554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[21.4943, 24.0729, 24.2438],
        [21.4943, 22.2705, 21.9037],
        [21.4943, 21.5262, 21.4967],
        [21.4943, 21.4944, 21.4943]], grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.058505505323410034 
model_pd.l_d.mean(): -1.612055778503418 
model_pd.lagr.mean(): -1.5535502433776855 
model_pd.lambdas: dict_items([('pout', tensor([1.8470])), ('power', tensor([0.1140]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4843])), ('power', tensor([-6.2760]))])
epoch：988	 i:0 	 global-step:19760	 l-p:0.058505505323410034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[21.5466, 26.0007, 27.6904],
        [21.5466, 21.5697, 21.5480],
        [21.5466, 22.7377, 22.3645],
        [21.5466, 21.8301, 21.6253]], grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.058480095118284225 
model_pd.l_d.mean(): -1.6099255084991455 
model_pd.lagr.mean(): -1.5514453649520874 
model_pd.lambdas: dict_items([('pout', tensor([1.8465])), ('power', tensor([0.1137]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4875])), ('power', tensor([-6.2252]))])
epoch：989	 i:0 	 global-step:19780	 l-p:0.058480095118284225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[21.5987, 28.6132, 33.1790],
        [21.5987, 22.1327, 21.8203],
        [21.5987, 24.8323, 25.4497],
        [21.5987, 21.7474, 21.6262]], grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.05845492705702782 
model_pd.l_d.mean(): -1.6078228950500488 
model_pd.lagr.mean(): -1.5493680238723755 
model_pd.lambdas: dict_items([('pout', tensor([1.8460])), ('power', tensor([0.1134]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4906])), ('power', tensor([-6.1745]))])
epoch：990	 i:0 	 global-step:19800	 l-p:0.05845492705702782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[21.6506, 24.2495, 24.4219],
        [21.6506, 21.6520, 21.6506],
        [21.6506, 21.8184, 21.6840],
        [21.6506, 28.6831, 33.2607]], grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.05842999368906021 
model_pd.l_d.mean(): -1.6057478189468384 
model_pd.lagr.mean(): -1.5473178625106812 
model_pd.lambdas: dict_items([('pout', tensor([1.8455])), ('power', tensor([0.1131]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4938])), ('power', tensor([-6.1240]))])
epoch：991	 i:0 	 global-step:19820	 l-p:0.05842999368906021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[21.7023, 21.7023, 21.7023],
        [21.7023, 25.1900, 25.9977],
        [21.7023, 22.6306, 22.2465],
        [21.7023, 21.8518, 21.7299]], grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.058405302464962006 
model_pd.l_d.mean(): -1.6036996841430664 
model_pd.lagr.mean(): -1.5452944040298462 
model_pd.lambdas: dict_items([('pout', tensor([1.8450])), ('power', tensor([0.1128]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4969])), ('power', tensor([-6.0737]))])
epoch：992	 i:0 	 global-step:19840	 l-p:0.058405302464962006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[21.7538, 21.7619, 21.7541],
        [21.7538, 21.7538, 21.7538],
        [21.7538, 22.3947, 22.0509],
        [21.7538, 21.7542, 21.7538]], grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.0583808571100235 
model_pd.l_d.mean(): -1.6016781330108643 
model_pd.lagr.mean(): -1.543297290802002 
model_pd.lambdas: dict_items([('pout', tensor([1.8445])), ('power', tensor([0.1125]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5000])), ('power', tensor([-6.0236]))])
epoch：993	 i:0 	 global-step:19860	 l-p:0.0583808571100235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[21.8051, 22.5466, 22.1814],
        [21.8051, 22.0838, 21.8811],
        [21.8051, 21.8063, 21.8051],
        [21.8051, 22.4476, 22.1030]], grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.0583566389977932 
model_pd.l_d.mean(): -1.5996825695037842 
model_pd.lagr.mean(): -1.5413259267807007 
model_pd.lambdas: dict_items([('pout', tensor([1.8440])), ('power', tensor([0.1122]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5031])), ('power', tensor([-5.9736]))])
epoch：994	 i:0 	 global-step:19880	 l-p:0.0583566389977932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[21.8562, 28.1935, 31.8777],
        [21.8562, 21.8576, 21.8562],
        [21.8562, 22.1356, 21.9324],
        [21.8562, 21.8985, 21.8598]], grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.0583326630294323 
model_pd.l_d.mean(): -1.5977132320404053 
model_pd.lagr.mean(): -1.5393805503845215 
model_pd.lambdas: dict_items([('pout', tensor([1.8435])), ('power', tensor([0.1119]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5061])), ('power', tensor([-5.9239]))])
epoch：995	 i:0 	 global-step:19900	 l-p:0.0583326630294323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[21.9071, 21.9956, 21.9189],
        [21.9071, 24.0733, 23.9896],
        [21.9071, 21.9072, 21.9071],
        [21.9071, 22.1933, 21.9862]], grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.0583089143037796 
model_pd.l_d.mean(): -1.5957696437835693 
model_pd.lagr.mean(): -1.5374606847763062 
model_pd.lambdas: dict_items([('pout', tensor([1.8430])), ('power', tensor([0.1116]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5092])), ('power', tensor([-5.8744]))])
epoch：996	 i:0 	 global-step:19920	 l-p:0.0583089143037796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[21.9578, 21.9578, 21.9578],
        [21.9578, 24.4340, 24.5122],
        [21.9578, 25.6910, 26.6759],
        [21.9578, 22.1093, 21.9858]], grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.05828540772199631 
model_pd.l_d.mean(): -1.5938506126403809 
model_pd.lagr.mean(): -1.5355652570724487 
model_pd.lambdas: dict_items([('pout', tensor([1.8425])), ('power', tensor([0.1113]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5122])), ('power', tensor([-5.8250]))])
epoch：997	 i:0 	 global-step:19940	 l-p:0.05828540772199631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[22.0083, 26.2042, 27.5924],
        [22.0083, 22.0083, 22.0083],
        [22.0083, 22.0083, 22.0083],
        [22.0083, 22.0095, 22.0083]], grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.05826213210821152 
model_pd.l_d.mean(): -1.591956615447998 
model_pd.lagr.mean(): -1.5336945056915283 
model_pd.lambdas: dict_items([('pout', tensor([1.8420])), ('power', tensor([0.1110]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5152])), ('power', tensor([-5.7758]))])
epoch：998	 i:0 	 global-step:19960	 l-p:0.05826213210821152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[22.0586, 22.0914, 22.0610],
        [22.0586, 22.6051, 22.2854],
        [22.0586, 26.6591, 28.4246],
        [22.0586, 28.0593, 31.3145]], grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.058239083737134933 
model_pd.l_d.mean(): -1.5900872945785522 
model_pd.lagr.mean(): -1.5318481922149658 
model_pd.lambdas: dict_items([('pout', tensor([1.8414])), ('power', tensor([0.1107]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5182])), ('power', tensor([-5.7269]))])
epoch：999	 i:0 	 global-step:19980	 l-p:0.058239083737134933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[22.1086, 24.6033, 24.6822],
        [22.1086, 26.3252, 27.7203],
        [22.1086, 25.8694, 26.8619],
        [22.1086, 22.1087, 22.1086]], grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.058216266334056854 
model_pd.l_d.mean(): -1.5882418155670166 
model_pd.lagr.mean(): -1.530025601387024 
model_pd.lambdas: dict_items([('pout', tensor([1.8409])), ('power', tensor([0.1104]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5212])), ('power', tensor([-5.6781]))])
epoch：1000	 i:0 	 global-step:20000	 l-p:0.058216266334056854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[22.1584, 23.1082, 22.7154],
        [22.1584, 22.7077, 22.3864],
        [22.1584, 26.3852, 27.7839],
        [22.1584, 22.1914, 22.1609]], grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.05819367989897728 
model_pd.l_d.mean(): -1.5864202976226807 
model_pd.lagr.mean(): -1.528226613998413 
model_pd.lambdas: dict_items([('pout', tensor([1.8404])), ('power', tensor([0.1101]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5241])), ('power', tensor([-5.6296]))])
epoch：1001	 i:0 	 global-step:20020	 l-p:0.05819367989897728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[22.2080, 24.4066, 24.3220],
        [22.2080, 22.2080, 22.2080],
        [22.2080, 22.2979, 22.2200],
        [22.2080, 22.2100, 22.2081]], grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.05817132070660591 
model_pd.l_d.mean(): -1.5846221446990967 
model_pd.lagr.mean(): -1.5264508724212646 
model_pd.lambdas: dict_items([('pout', tensor([1.8399])), ('power', tensor([0.1099]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5270])), ('power', tensor([-5.5813]))])
epoch：1002	 i:0 	 global-step:20040	 l-p:0.05817132070660591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[22.2574, 22.2574, 22.2574],
        [22.2574, 30.0505, 35.4766],
        [22.2574, 22.8094, 22.4865],
        [22.2574, 26.9077, 28.6956]], grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.05814918130636215 
model_pd.l_d.mean(): -1.582846760749817 
model_pd.lagr.mean(): -1.5246975421905518 
model_pd.lambdas: dict_items([('pout', tensor([1.8393])), ('power', tensor([0.1096]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5299])), ('power', tensor([-5.5332]))])
epoch：1003	 i:0 	 global-step:20060	 l-p:0.05814918130636215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[22.3065, 22.3149, 22.3068],
        [22.3065, 26.9628, 28.7500],
        [22.3065, 28.7835, 32.5500],
        [22.3065, 22.3080, 22.3066]], grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.0581272691488266 
model_pd.l_d.mean(): -1.5810942649841309 
model_pd.lagr.mean(): -1.522966980934143 
model_pd.lambdas: dict_items([('pout', tensor([1.8388])), ('power', tensor([0.1093]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5328])), ('power', tensor([-5.4853]))])
epoch：1004	 i:0 	 global-step:20080	 l-p:0.0581272691488266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[22.3554, 22.4459, 22.3675],
        [22.3554, 22.3988, 22.3592],
        [22.3554, 25.9567, 26.7917],
        [22.3554, 26.1616, 27.1664]], grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.05810558423399925 
model_pd.l_d.mean(): -1.57936429977417 
model_pd.lagr.mean(): -1.5212587118148804 
model_pd.lambdas: dict_items([('pout', tensor([1.8383])), ('power', tensor([0.1090]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5357])), ('power', tensor([-5.4377]))])
epoch：1005	 i:0 	 global-step:20100	 l-p:0.05810558423399925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[22.4041, 22.4072, 22.4042],
        [22.4041, 28.5055, 31.8161],
        [22.4041, 28.9114, 32.6957],
        [22.4041, 22.4041, 22.4041]], grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.05808412656188011 
model_pd.l_d.mean(): -1.5776562690734863 
model_pd.lagr.mean(): -1.519572138786316 
model_pd.lambdas: dict_items([('pout', tensor([1.8377])), ('power', tensor([0.1088]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5385])), ('power', tensor([-5.3902]))])
epoch：1006	 i:0 	 global-step:20120	 l-p:0.05808412656188011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[22.4525, 24.8339, 24.8282],
        [22.4525, 22.4526, 22.4525],
        [22.4525, 24.9098, 24.9454],
        [22.4525, 23.6986, 23.3086]], grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.058062877506017685 
model_pd.l_d.mean(): -1.5759698152542114 
model_pd.lagr.mean(): -1.517906904220581 
model_pd.lambdas: dict_items([('pout', tensor([1.8372])), ('power', tensor([0.1085]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5414])), ('power', tensor([-5.3431]))])
epoch：1007	 i:0 	 global-step:20140	 l-p:0.058062877506017685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[22.5007, 22.5007, 22.5007],
        [22.5007, 27.2005, 29.0048],
        [22.5007, 22.5027, 22.5007],
        [22.5007, 25.2106, 25.3913]], grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.05804185941815376 
model_pd.l_d.mean(): -1.5743048191070557 
model_pd.lagr.mean(): -1.5162630081176758 
model_pd.lambdas: dict_items([('pout', tensor([1.8366])), ('power', tensor([0.1082]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5442])), ('power', tensor([-5.2961]))])
epoch：1008	 i:0 	 global-step:20160	 l-p:0.05804185941815376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[22.5486, 25.2647, 25.4460],
        [22.5486, 24.0320, 23.6770],
        [22.5486, 22.5486, 22.5486],
        [22.5486, 26.1835, 27.0266]], grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.058021046221256256 
model_pd.l_d.mean(): -1.5726606845855713 
model_pd.lagr.mean(): -1.5146396160125732 
model_pd.lambdas: dict_items([('pout', tensor([1.8361])), ('power', tensor([0.1080]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5469])), ('power', tensor([-5.2494]))])
epoch：1009	 i:0 	 global-step:20180	 l-p:0.058021046221256256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[22.5963, 23.3674, 22.9878],
        [22.5963, 22.5983, 22.5963],
        [22.5963, 24.8367, 24.7508],
        [22.5963, 23.4163, 23.0291]], grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.058000460267066956 
model_pd.l_d.mean(): -1.5710375308990479 
model_pd.lagr.mean(): -1.513037085533142 
model_pd.lambdas: dict_items([('pout', tensor([1.8355])), ('power', tensor([0.1077]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5497])), ('power', tensor([-5.2029]))])
epoch：1010	 i:0 	 global-step:20200	 l-p:0.058000460267066956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[22.6437, 28.8150, 32.1640],
        [22.6437, 22.6452, 22.6437],
        [22.6437, 25.2040, 25.2856],
        [22.6437, 22.9432, 22.7269]], grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.057980090379714966 
model_pd.l_d.mean(): -1.5694345235824585 
model_pd.lagr.mean(): -1.511454463005066 
model_pd.lambdas: dict_items([('pout', tensor([1.8350])), ('power', tensor([0.1074]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5524])), ('power', tensor([-5.1567]))])
epoch：1011	 i:0 	 global-step:20220	 l-p:0.057980090379714966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[22.6908, 23.3622, 23.0022],
        [22.6908, 22.6994, 22.6911],
        [22.6908, 22.9884, 22.7731],
        [22.6908, 22.8676, 22.7261]], grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.05795993283390999 
model_pd.l_d.mean(): -1.5678515434265137 
model_pd.lagr.mean(): -1.5098916292190552 
model_pd.lambdas: dict_items([('pout', tensor([1.8344])), ('power', tensor([0.1072]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5552])), ('power', tensor([-5.1107]))])
epoch：1012	 i:0 	 global-step:20240	 l-p:0.05795993283390999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[22.7377, 22.7463, 22.7380],
        [22.7377, 27.0837, 28.5228],
        [22.7377, 27.4961, 29.3264],
        [22.7377, 22.7377, 22.7377]], grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.05793999135494232 
model_pd.l_d.mean(): -1.5662882328033447 
model_pd.lagr.mean(): -1.5083482265472412 
model_pd.lambdas: dict_items([('pout', tensor([1.8339])), ('power', tensor([0.1069]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5579])), ('power', tensor([-5.0649]))])
epoch：1013	 i:0 	 global-step:20260	 l-p:0.05793999135494232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[22.7844, 23.7636, 23.3588],
        [22.7844, 22.8184, 22.7869],
        [22.7844, 23.4587, 23.0972],
        [22.7844, 24.2847, 23.9258]], grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.057920265942811966 
model_pd.l_d.mean(): -1.564744472503662 
model_pd.lagr.mean(): -1.506824254989624 
model_pd.lambdas: dict_items([('pout', tensor([1.8333])), ('power', tensor([0.1067]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5605])), ('power', tensor([-5.0194]))])
epoch：1014	 i:0 	 global-step:20280	 l-p:0.057920265942811966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[22.8307, 22.8751, 22.8345],
        [22.8307, 27.6100, 29.4485],
        [22.8307, 25.5838, 25.7678],
        [22.8307, 27.6047, 29.4381]], grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.05790073797106743 
model_pd.l_d.mean(): -1.5632197856903076 
model_pd.lagr.mean(): -1.5053189992904663 
model_pd.lambdas: dict_items([('pout', tensor([1.8328])), ('power', tensor([0.1064]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5632])), ('power', tensor([-4.9742]))])
epoch：1015	 i:0 	 global-step:20300	 l-p:0.05790073797106743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[22.8768, 22.8768, 22.8768],
        [22.8768, 22.9697, 22.8892],
        [22.8768, 22.8768, 22.8768],
        [22.8768, 25.3846, 25.4214]], grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.0578814297914505 
model_pd.l_d.mean(): -1.561713695526123 
model_pd.lagr.mean(): -1.503832221031189 
model_pd.lambdas: dict_items([('pout', tensor([1.8322])), ('power', tensor([0.1062]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5658])), ('power', tensor([-4.9292]))])
epoch：1016	 i:0 	 global-step:20320	 l-p:0.0578814297914505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[22.9227, 22.9313, 22.9230],
        [22.9227, 29.1753, 32.5692],
        [22.9227, 26.3713, 27.0316],
        [22.9227, 27.3067, 28.7587]], grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.05786232650279999 
model_pd.l_d.mean(): -1.5602259635925293 
model_pd.lagr.mean(): -1.502363681793213 
model_pd.lambdas: dict_items([('pout', tensor([1.8316])), ('power', tensor([0.1059]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5685])), ('power', tensor([-4.8845]))])
epoch：1017	 i:0 	 global-step:20340	 l-p:0.05786232650279999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[22.9682, 22.9930, 22.9697],
        [22.9682, 29.3589, 32.9038],
        [22.9682, 22.9682, 22.9682],
        [22.9682, 22.9702, 22.9682]], grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.05784342437982559 
model_pd.l_d.mean(): -1.5587563514709473 
model_pd.lagr.mean(): -1.5009129047393799 
model_pd.lambdas: dict_items([('pout', tensor([1.8311])), ('power', tensor([0.1057]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5711])), ('power', tensor([-4.8401]))])
epoch：1018	 i:0 	 global-step:20360	 l-p:0.05784342437982559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[23.0135, 29.4176, 32.9701],
        [23.0135, 23.3157, 23.0970],
        [23.0135, 24.5985, 24.2513],
        [23.0135, 29.2927, 32.7011]], grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.05782472714781761 
model_pd.l_d.mean(): -1.5573046207427979 
model_pd.lagr.mean(): -1.49947988986969 
model_pd.lambdas: dict_items([('pout', tensor([1.8305])), ('power', tensor([0.1055]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5736])), ('power', tensor([-4.7959]))])
epoch：1019	 i:0 	 global-step:20380	 l-p:0.05782472714781761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[23.0585, 26.5292, 27.1940],
        [23.0585, 27.8891, 29.7477],
        [23.0585, 23.1522, 23.0710],
        [23.0585, 23.7418, 23.3755]], grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.057806242257356644 
model_pd.l_d.mean(): -1.5558702945709229 
model_pd.lagr.mean(): -1.4980640411376953 
model_pd.lambdas: dict_items([('pout', tensor([1.8299])), ('power', tensor([0.1052]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5762])), ('power', tensor([-4.7519]))])
epoch：1020	 i:0 	 global-step:20400	 l-p:0.057806242257356644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[23.1032, 23.8934, 23.5045],
        [23.1032, 23.4094, 23.1883],
        [23.1032, 23.1481, 23.1071],
        [23.1032, 31.2120, 36.8605]], grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.05778796225786209 
model_pd.l_d.mean(): -1.5544531345367432 
model_pd.lagr.mean(): -1.496665120124817 
model_pd.lambdas: dict_items([('pout', tensor([1.8293])), ('power', tensor([0.1050]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5787])), ('power', tensor([-4.7083]))])
epoch：1021	 i:0 	 global-step:20420	 l-p:0.05778796225786209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[23.1476, 23.1476, 23.1476],
        [23.1476, 23.1927, 23.1515],
        [23.1476, 24.1441, 23.7322],
        [23.1476, 23.7240, 23.3870]], grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.05776987224817276 
model_pd.l_d.mean(): -1.5530531406402588 
model_pd.lagr.mean(): -1.4952832460403442 
model_pd.lambdas: dict_items([('pout', tensor([1.8287])), ('power', tensor([0.1048]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5812])), ('power', tensor([-4.6649]))])
epoch：1022	 i:0 	 global-step:20440	 l-p:0.05776987224817276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[23.1918, 28.0129, 29.8448],
        [23.1918, 24.3138, 23.8994],
        [23.1918, 25.6585, 25.6534],
        [23.1918, 23.5009, 23.2780]], grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.05775199458003044 
model_pd.l_d.mean(): -1.5516692399978638 
model_pd.lagr.mean(): -1.4939172267913818 
model_pd.lambdas: dict_items([('pout', tensor([1.8282])), ('power', tensor([0.1045]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5837])), ('power', tensor([-4.6218]))])
epoch：1023	 i:0 	 global-step:20460	 l-p:0.05775199458003044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[23.2356, 23.8145, 23.4760],
        [23.2356, 23.5346, 23.3172],
        [23.2356, 24.3600, 23.9447],
        [23.2356, 29.5797, 33.0239]], grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.057734303176403046 
model_pd.l_d.mean(): -1.5503023862838745 
model_pd.lagr.mean(): -1.4925681352615356 
model_pd.lambdas: dict_items([('pout', tensor([1.8276])), ('power', tensor([0.1043]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5862])), ('power', tensor([-4.5790]))])
epoch：1024	 i:0 	 global-step:20480	 l-p:0.057734303176403046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[23.2792, 23.5881, 23.3651],
        [23.2792, 25.9177, 26.0025],
        [23.2792, 25.5935, 25.5054],
        [23.2792, 23.3246, 23.2831]], grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.05771681293845177 
model_pd.l_d.mean(): -1.5489509105682373 
model_pd.lagr.mean(): -1.4912340641021729 
model_pd.lambdas: dict_items([('pout', tensor([1.8270])), ('power', tensor([0.1041]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5887])), ('power', tensor([-4.5364]))])
epoch：1025	 i:0 	 global-step:20500	 l-p:0.05771681293845177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[23.3225, 24.6216, 24.2154],
        [23.3225, 23.3246, 23.3225],
        [23.3225, 23.4174, 23.3352],
        [23.3225, 25.9663, 26.0513]], grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.057699527591466904 
model_pd.l_d.mean(): -1.5476155281066895 
model_pd.lagr.mean(): -1.4899159669876099 
model_pd.lambdas: dict_items([('pout', tensor([1.8264])), ('power', tensor([0.1038]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5911])), ('power', tensor([-4.4942]))])
epoch：1026	 i:0 	 global-step:20520	 l-p:0.057699527591466904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[23.3655, 25.6891, 25.6008],
        [23.3655, 24.9771, 24.6243],
        [23.3655, 23.3668, 23.3655],
        [23.3655, 30.9938, 35.9638]], grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.057682424783706665 
model_pd.l_d.mean(): -1.5462952852249146 
model_pd.lagr.mean(): -1.4886128902435303 
model_pd.lambdas: dict_items([('pout', tensor([1.8258])), ('power', tensor([0.1036]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5935])), ('power', tensor([-4.4522]))])
epoch：1027	 i:0 	 global-step:20540	 l-p:0.057682424783706665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[23.4082, 23.4103, 23.4082],
        [23.4082, 27.8924, 29.3784],
        [23.4082, 23.4538, 23.4121],
        [23.4082, 23.4082, 23.4082]], grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.057665515691041946 
model_pd.l_d.mean(): -1.5449901819229126 
model_pd.lagr.mean(): -1.4873247146606445 
model_pd.lambdas: dict_items([('pout', tensor([1.8252])), ('power', tensor([0.1034]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5959])), ('power', tensor([-4.4105]))])
epoch：1028	 i:0 	 global-step:20560	 l-p:0.057665515691041946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[23.4506, 23.4506, 23.4506],
        [23.4506, 25.5586, 25.3679],
        [23.4506, 31.6893, 37.4293],
        [23.4506, 23.4521, 23.4506]], grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.05764879658818245 
model_pd.l_d.mean(): -1.5437002182006836 
model_pd.lagr.mean(): -1.4860514402389526 
model_pd.lambdas: dict_items([('pout', tensor([1.8246])), ('power', tensor([0.1032]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5982])), ('power', tensor([-4.3690]))])
epoch：1029	 i:0 	 global-step:20580	 l-p:0.05764879658818245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[23.4927, 26.0740, 26.1125],
        [23.4927, 23.8019, 23.5782],
        [23.4927, 25.0442, 24.6735],
        [23.4927, 25.6048, 25.4138]], grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.05763227492570877 
model_pd.l_d.mean(): -1.5424247980117798 
model_pd.lagr.mean(): -1.4847924709320068 
model_pd.lambdas: dict_items([('pout', tensor([1.8240])), ('power', tensor([0.1030]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6006])), ('power', tensor([-4.3279]))])
epoch：1030	 i:0 	 global-step:20600	 l-p:0.05763227492570877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[23.5345, 31.2216, 36.2304],
        [23.5345, 29.9660, 33.4582],
        [23.5345, 24.5493, 24.1299],
        [23.5345, 25.6507, 25.4594]], grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.057615939527750015 
model_pd.l_d.mean(): -1.5411638021469116 
model_pd.lagr.mean(): -1.4835478067398071 
model_pd.lambdas: dict_items([('pout', tensor([1.8234])), ('power', tensor([0.1028]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6029])), ('power', tensor([-4.2871]))])
epoch：1031	 i:0 	 global-step:20620	 l-p:0.057615939527750015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[23.5760, 31.8617, 37.6347],
        [23.5760, 23.5760, 23.5760],
        [23.5760, 27.6070, 28.6732],
        [23.5760, 25.1335, 24.7614]], grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.05759979784488678 
model_pd.l_d.mean(): -1.539916753768921 
model_pd.lagr.mean(): -1.4823169708251953 
model_pd.lambdas: dict_items([('pout', tensor([1.8228])), ('power', tensor([0.1025]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6052])), ('power', tensor([-4.2465]))])
epoch：1032	 i:0 	 global-step:20640	 l-p:0.05759979784488678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[23.6172, 23.6261, 23.6175],
        [23.6172, 24.4268, 24.0285],
        [23.6172, 25.1777, 24.8049],
        [23.6172, 28.5684, 30.4713]], grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.05758383125066757 
model_pd.l_d.mean(): -1.538683295249939 
model_pd.lagr.mean(): -1.4810994863510132 
model_pd.lambdas: dict_items([('pout', tensor([1.8222])), ('power', tensor([0.1023]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6075])), ('power', tensor([-4.2063]))])
epoch：1033	 i:0 	 global-step:20660	 l-p:0.05758383125066757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[23.6581, 23.6581, 23.6581],
        [23.6581, 27.7043, 28.7746],
        [23.6581, 23.9725, 23.7455],
        [23.6581, 24.3610, 23.9843]], grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.057568054646253586 
model_pd.l_d.mean(): -1.5374635457992554 
model_pd.lagr.mean(): -1.4798954725265503 
model_pd.lambdas: dict_items([('pout', tensor([1.8216])), ('power', tensor([0.1021]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6098])), ('power', tensor([-4.1663]))])
epoch：1034	 i:0 	 global-step:20680	 l-p:0.057568054646253586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[23.6987, 24.0109, 23.7850],
        [23.6987, 25.8310, 25.6384],
        [23.6987, 31.4430, 36.4896],
        [23.6987, 26.3889, 26.4758]], grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.057552456855773926 
model_pd.l_d.mean(): -1.5362571477890015 
model_pd.lagr.mean(): -1.4787046909332275 
model_pd.lambdas: dict_items([('pout', tensor([1.8210])), ('power', tensor([0.1019]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6120])), ('power', tensor([-4.1266]))])
epoch：1035	 i:0 	 global-step:20700	 l-p:0.057552456855773926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[23.7390, 23.7394, 23.7390],
        [23.7390, 25.4594, 25.1214],
        [23.7390, 23.7403, 23.7390],
        [23.7390, 25.0636, 24.6496]], grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.05753704160451889 
model_pd.l_d.mean(): -1.5350635051727295 
model_pd.lagr.mean(): -1.4775264263153076 
model_pd.lambdas: dict_items([('pout', tensor([1.8204])), ('power', tensor([0.1017]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6142])), ('power', tensor([-4.0872]))])
epoch：1036	 i:0 	 global-step:20720	 l-p:0.05753704160451889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[23.7789, 28.3398, 29.8518],
        [23.7789, 27.6287, 28.5236],
        [23.7789, 26.3945, 26.4338],
        [23.7789, 23.7794, 23.7789]], grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.05752181634306908 
model_pd.l_d.mean(): -1.533882737159729 
model_pd.lagr.mean(): -1.4763609170913696 
model_pd.lambdas: dict_items([('pout', tensor([1.8197])), ('power', tensor([0.1015]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6164])), ('power', tensor([-4.0482]))])
epoch：1037	 i:0 	 global-step:20740	 l-p:0.05752181634306908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[23.8186, 24.1354, 23.9067],
        [23.8186, 26.7013, 26.8952],
        [23.8186, 28.7801, 30.6666],
        [23.8186, 23.8651, 23.8226]], grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.05750676244497299 
model_pd.l_d.mean(): -1.5327144861221313 
model_pd.lagr.mean(): -1.4752076864242554 
model_pd.lambdas: dict_items([('pout', tensor([1.8191])), ('power', tensor([0.1013]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6186])), ('power', tensor([-4.0094]))])
epoch：1038	 i:0 	 global-step:20760	 l-p:0.05750676244497299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[23.8580, 23.8580, 23.8580],
        [23.8580, 24.8881, 24.4625],
        [23.8580, 23.8580, 23.8580],
        [23.8580, 24.4539, 24.1055]], grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.057491883635520935 
model_pd.l_d.mean(): -1.5315583944320679 
model_pd.lagr.mean(): -1.4740664958953857 
model_pd.lambdas: dict_items([('pout', tensor([1.8185])), ('power', tensor([0.1011]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6208])), ('power', tensor([-3.9709]))])
epoch：1039	 i:0 	 global-step:20780	 l-p:0.057491883635520935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[23.8970, 23.9945, 23.9100],
        [23.8970, 27.7675, 28.6673],
        [23.8970, 23.8970, 23.8970],
        [23.8970, 24.7691, 24.3576]], grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.057477183640003204 
model_pd.l_d.mean(): -1.5304148197174072 
model_pd.lagr.mean(): -1.4729375839233398 
model_pd.lambdas: dict_items([('pout', tensor([1.8179])), ('power', tensor([0.1009]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6229])), ('power', tensor([-3.9327]))])
epoch：1040	 i:0 	 global-step:20800	 l-p:0.057477183640003204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[23.9357, 23.9359, 23.9358],
        [23.9357, 26.0914, 25.8969],
        [23.9357, 24.1233, 23.9731],
        [23.9357, 23.9379, 23.9358]], grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.0574626624584198 
model_pd.l_d.mean(): -1.529282808303833 
model_pd.lagr.mean(): -1.4718201160430908 
model_pd.lambdas: dict_items([('pout', tensor([1.8172])), ('power', tensor([0.1007]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6250])), ('power', tensor([-3.8949]))])
epoch：1041	 i:0 	 global-step:20820	 l-p:0.0574626624584198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[23.9742, 23.9742, 23.9742],
        [23.9742, 24.1412, 24.0051],
        [23.9742, 26.3639, 26.2736],
        [23.9742, 23.9743, 23.9742]], grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.05744830518960953 
model_pd.l_d.mean(): -1.5281621217727661 
model_pd.lagr.mean(): -1.4707138538360596 
model_pd.lambdas: dict_items([('pout', tensor([1.8166])), ('power', tensor([0.1005]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6271])), ('power', tensor([-3.8573]))])
epoch：1042	 i:0 	 global-step:20840	 l-p:0.05744830518960953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[24.0123, 28.1240, 29.2123],
        [24.0123, 24.0123, 24.0123],
        [24.0123, 27.6387, 28.3348],
        [24.0123, 24.8368, 24.4312]], grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.05743413418531418 
model_pd.l_d.mean(): -1.527052640914917 
model_pd.lagr.mean(): -1.469618558883667 
model_pd.lambdas: dict_items([('pout', tensor([1.8160])), ('power', tensor([0.1003]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6292])), ('power', tensor([-3.8200]))])
epoch：1043	 i:0 	 global-step:20860	 l-p:0.05743413418531418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[24.0501, 26.2170, 26.0215],
        [24.0501, 29.0635, 30.9703],
        [24.0501, 24.0501, 24.0501],
        [24.0501, 24.2386, 24.0877]], grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.057420119643211365 
model_pd.l_d.mean(): -1.5259541273117065 
model_pd.lagr.mean(): -1.468533992767334 
model_pd.lambdas: dict_items([('pout', tensor([1.8154])), ('power', tensor([0.1001]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6313])), ('power', tensor([-3.7830]))])
epoch：1044	 i:0 	 global-step:20880	 l-p:0.057420119643211365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[24.0875, 24.4083, 24.1768],
        [24.0875, 26.2582, 26.0624],
        [24.0875, 24.4100, 24.1775],
        [24.0875, 24.0880, 24.0875]], grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.05740627646446228 
model_pd.l_d.mean(): -1.524867057800293 
model_pd.lagr.mean(): -1.4674607515335083 
model_pd.lambdas: dict_items([('pout', tensor([1.8147])), ('power', tensor([0.1000]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6333])), ('power', tensor([-3.7464]))])
epoch：1045	 i:0 	 global-step:20900	 l-p:0.05740627646446228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[24.1247, 29.1906, 31.1386],
        [24.1247, 24.1268, 24.1247],
        [24.1247, 27.7696, 28.4694],
        [24.1247, 24.4364, 24.2098]], grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.05739261209964752 
model_pd.l_d.mean(): -1.5237903594970703 
model_pd.lagr.mean(): -1.466397762298584 
model_pd.lambdas: dict_items([('pout', tensor([1.8141])), ('power', tensor([0.0998]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6353])), ('power', tensor([-3.7100]))])
epoch：1046	 i:0 	 global-step:20920	 l-p:0.05739261209964752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[24.1616, 24.1617, 24.1616],
        [24.1616, 25.9156, 25.5712],
        [24.1616, 24.3300, 24.1928],
        [24.1616, 24.1707, 24.1619]], grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.0573790967464447 
model_pd.l_d.mean(): -1.522723913192749 
model_pd.lagr.mean(): -1.465344786643982 
model_pd.lambdas: dict_items([('pout', tensor([1.8135])), ('power', tensor([0.0996]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6373])), ('power', tensor([-3.6740]))])
epoch：1047	 i:0 	 global-step:20940	 l-p:0.0573790967464447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[24.1981, 24.2345, 24.2008],
        [24.1981, 26.9502, 27.0397],
        [24.1981, 24.3880, 24.2360],
        [24.1981, 24.1982, 24.1981]], grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.05736576020717621 
model_pd.l_d.mean(): -1.5216679573059082 
model_pd.lagr.mean(): -1.4643021821975708 
model_pd.lambdas: dict_items([('pout', tensor([1.8128])), ('power', tensor([0.0994]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6393])), ('power', tensor([-3.6382]))])
epoch：1048	 i:0 	 global-step:20960	 l-p:0.05736576020717621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[24.2343, 26.9909, 27.0806],
        [24.2343, 25.2824, 24.8495],
        [24.2343, 26.4194, 26.2225],
        [24.2343, 29.2892, 31.2121]], grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.05735258758068085 
model_pd.l_d.mean(): -1.520621657371521 
model_pd.lagr.mean(): -1.4632691144943237 
model_pd.lambdas: dict_items([('pout', tensor([1.8122])), ('power', tensor([0.0992]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6412])), ('power', tensor([-3.6028]))])
epoch：1049	 i:0 	 global-step:20980	 l-p:0.05735258758068085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[24.2702, 24.3177, 24.2743],
        [24.2702, 29.3690, 31.3300],
        [24.2702, 26.9447, 26.9856],
        [24.2702, 28.2062, 29.1219]], grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.05733957141637802 
model_pd.l_d.mean(): -1.5195852518081665 
model_pd.lagr.mean(): -1.4622457027435303 
model_pd.lambdas: dict_items([('pout', tensor([1.8115])), ('power', tensor([0.0991]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6432])), ('power', tensor([-3.5677]))])
epoch：1050	 i:0 	 global-step:21000	 l-p:0.05733957141637802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[24.3058, 26.9019, 26.8979],
        [24.3058, 25.0301, 24.6420],
        [24.3058, 25.6652, 25.2406],
        [24.3058, 27.9804, 28.6862]], grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.05732670798897743 
model_pd.l_d.mean(): -1.5185587406158447 
model_pd.lagr.mean(): -1.46123206615448 
model_pd.lambdas: dict_items([('pout', tensor([1.8109])), ('power', tensor([0.0989]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6451])), ('power', tensor([-3.5329]))])
epoch：1051	 i:0 	 global-step:21020	 l-p:0.05732670798897743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[24.3411, 24.3411, 24.3411],
        [24.3411, 29.4559, 31.4233],
        [24.3411, 24.3427, 24.3411],
        [24.3411, 24.3411, 24.3411]], grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.05731401965022087 
model_pd.l_d.mean(): -1.5175410509109497 
model_pd.lagr.mean(): -1.4602270126342773 
model_pd.lambdas: dict_items([('pout', tensor([1.8102])), ('power', tensor([0.0987]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6470])), ('power', tensor([-3.4983]))])
epoch：1052	 i:0 	 global-step:21040	 l-p:0.05731401965022087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[24.3760, 28.5552, 29.6620],
        [24.3760, 24.3782, 24.3760],
        [24.3760, 24.3760, 24.3760],
        [24.3760, 26.5751, 26.3771]], grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.05730148404836655 
model_pd.l_d.mean(): -1.5165328979492188 
model_pd.lagr.mean(): -1.4592313766479492 
model_pd.lambdas: dict_items([('pout', tensor([1.8096])), ('power', tensor([0.0985]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6489])), ('power', tensor([-3.4641]))])
epoch：1053	 i:0 	 global-step:21060	 l-p:0.05730148404836655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[24.4106, 26.6132, 26.4149],
        [24.4106, 24.4107, 24.4106],
        [24.4106, 24.6024, 24.4489],
        [24.4106, 24.4106, 24.4107]], grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.057289090007543564 
model_pd.l_d.mean(): -1.515533447265625 
model_pd.lagr.mean(): -1.4582443237304688 
model_pd.lambdas: dict_items([('pout', tensor([1.8089])), ('power', tensor([0.0984]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6507])), ('power', tensor([-3.4302]))])
epoch：1054	 i:0 	 global-step:21080	 l-p:0.057289090007543564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[24.4450, 25.6339, 25.1953],
        [24.4450, 24.4471, 24.4450],
        [24.4450, 24.4450, 24.4450],
        [24.4450, 28.4117, 29.3349]], grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.05727686733007431 
model_pd.l_d.mean(): -1.5145429372787476 
model_pd.lagr.mean(): -1.457266092300415 
model_pd.lambdas: dict_items([('pout', tensor([1.8083])), ('power', tensor([0.0982]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6526])), ('power', tensor([-3.3967]))])
epoch：1055	 i:0 	 global-step:21100	 l-p:0.05727686733007431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[24.4790, 24.4790, 24.4790],
        [24.4790, 24.5158, 24.4817],
        [24.4790, 24.4794, 24.4790],
        [24.4790, 31.3207, 35.1196]], grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.0572647899389267 
model_pd.l_d.mean(): -1.5135611295700073 
model_pd.lagr.mean(): -1.4562963247299194 
model_pd.lambdas: dict_items([('pout', tensor([1.8076])), ('power', tensor([0.0980]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6544])), ('power', tensor([-3.3634]))])
epoch：1056	 i:0 	 global-step:21120	 l-p:0.0572647899389267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[24.5126, 27.2164, 27.2580],
        [24.5126, 28.7173, 29.8311],
        [24.5126, 24.5126, 24.5126],
        [24.5126, 33.1496, 39.1704]], grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.05725287273526192 
model_pd.l_d.mean(): -1.512587547302246 
model_pd.lagr.mean(): -1.4553346633911133 
model_pd.lambdas: dict_items([('pout', tensor([1.8070])), ('power', tensor([0.0979]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6562])), ('power', tensor([-3.3304]))])
epoch：1057	 i:0 	 global-step:21140	 l-p:0.05725287273526192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[24.5460, 25.6090, 25.1701],
        [24.5460, 32.5863, 37.8283],
        [24.5460, 25.3909, 24.9754],
        [24.5460, 24.5830, 24.5487]], grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.057241104543209076 
model_pd.l_d.mean(): -1.5116221904754639 
model_pd.lagr.mean(): -1.4543811082839966 
model_pd.lambdas: dict_items([('pout', tensor([1.8063])), ('power', tensor([0.0977]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6580])), ('power', tensor([-3.2977]))])
epoch：1058	 i:0 	 global-step:21160	 l-p:0.057241104543209076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[24.5791, 26.2828, 25.9106],
        [24.5791, 24.6796, 24.5925],
        [24.5791, 32.6310, 37.8806],
        [24.5791, 25.7752, 25.3340]], grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.05722948536276817 
model_pd.l_d.mean(): -1.510664701461792 
model_pd.lagr.mean(): -1.4534351825714111 
model_pd.lambdas: dict_items([('pout', tensor([1.8057])), ('power', tensor([0.0975]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6597])), ('power', tensor([-3.2654]))])
epoch：1059	 i:0 	 global-step:21180	 l-p:0.05722948536276817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[24.6118, 26.8343, 26.6343],
        [24.6118, 28.3367, 29.0528],
        [24.6118, 24.6118, 24.6118],
        [24.6118, 24.7838, 24.6436]], grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.05721801146864891 
model_pd.l_d.mean(): -1.5097154378890991 
model_pd.lagr.mean(): -1.4524974822998047 
model_pd.lambdas: dict_items([('pout', tensor([1.8050])), ('power', tensor([0.0974]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6615])), ('power', tensor([-3.2333]))])
epoch：1060	 i:0 	 global-step:21200	 l-p:0.05721801146864891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[24.6442, 26.3529, 25.9797],
        [24.6442, 27.4518, 27.5437],
        [24.6442, 24.6443, 24.6442],
        [24.6442, 29.8280, 31.8225]], grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.05720668286085129 
model_pd.l_d.mean(): -1.508773684501648 
model_pd.lagr.mean(): -1.4515670537948608 
model_pd.lambdas: dict_items([('pout', tensor([1.8043])), ('power', tensor([0.0972]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6632])), ('power', tensor([-3.2016]))])
epoch：1061	 i:0 	 global-step:21220	 l-p:0.05720668286085129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[24.6763, 24.6777, 24.6763],
        [24.6763, 24.6763, 24.6763],
        [24.6763, 24.6763, 24.6763],
        [24.6763, 24.8704, 24.7150]], grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.057195499539375305 
model_pd.l_d.mean(): -1.5078392028808594 
model_pd.lagr.mean(): -1.4506436586380005 
model_pd.lambdas: dict_items([('pout', tensor([1.8037])), ('power', tensor([0.0970]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6649])), ('power', tensor([-3.1701]))])
epoch：1062	 i:0 	 global-step:21240	 l-p:0.057195499539375305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[24.7081, 29.4619, 31.0398],
        [24.7081, 27.4354, 27.4777],
        [24.7081, 31.6187, 35.4564],
        [24.7081, 25.0351, 24.7986]], grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.05718446150422096 
model_pd.l_d.mean(): -1.5069122314453125 
model_pd.lagr.mean(): -1.4497277736663818 
model_pd.lambdas: dict_items([('pout', tensor([1.8030])), ('power', tensor([0.0969]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6666])), ('power', tensor([-3.1390]))])
epoch：1063	 i:0 	 global-step:21260	 l-p:0.05718446150422096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[24.7396, 24.7769, 24.7423],
        [24.7396, 27.5591, 27.6515],
        [24.7396, 25.9444, 25.5000],
        [24.7396, 31.9751, 36.1891]], grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.05717356503009796 
model_pd.l_d.mean(): -1.5059921741485596 
model_pd.lagr.mean(): -1.448818564414978 
model_pd.lambdas: dict_items([('pout', tensor([1.8023])), ('power', tensor([0.0967]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6682])), ('power', tensor([-3.1082]))])
epoch：1064	 i:0 	 global-step:21280	 l-p:0.05717356503009796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[24.7707, 24.8193, 24.7749],
        [24.7707, 24.7977, 24.7723],
        [24.7707, 26.5735, 26.2201],
        [24.7707, 25.0987, 24.8615]], grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.0571628175675869 
model_pd.l_d.mean(): -1.5050796270370483 
model_pd.lagr.mean(): -1.447916865348816 
model_pd.lambdas: dict_items([('pout', tensor([1.8017])), ('power', tensor([0.0966]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6699])), ('power', tensor([-3.0777]))])
epoch：1065	 i:0 	 global-step:21300	 l-p:0.0571628175675869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[24.8016, 24.8390, 24.8043],
        [24.8016, 24.8286, 24.8032],
        [24.8016, 24.8016, 24.8016],
        [24.8016, 25.7104, 25.2818]], grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.05715220421552658 
model_pd.l_d.mean(): -1.5041736364364624 
model_pd.lagr.mean(): -1.447021484375 
model_pd.lambdas: dict_items([('pout', tensor([1.8010])), ('power', tensor([0.0964]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6715])), ('power', tensor([-3.0475]))])
epoch：1066	 i:0 	 global-step:21320	 l-p:0.05715220421552658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[24.8321, 24.8696, 24.8349],
        [24.8321, 26.6398, 26.2855],
        [24.8321, 24.8321, 24.8321],
        [24.8321, 24.9339, 24.8457]], grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.057141732424497604 
model_pd.l_d.mean(): -1.5032743215560913 
model_pd.lagr.mean(): -1.4461325407028198 
model_pd.lambdas: dict_items([('pout', tensor([1.8003])), ('power', tensor([0.0963]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6731])), ('power', tensor([-3.0176]))])
epoch：1067	 i:0 	 global-step:21340	 l-p:0.057141732424497604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[24.8623, 29.6484, 31.2373],
        [24.8623, 24.8623, 24.8623],
        [24.8623, 24.8623, 24.8623],
        [24.8623, 33.0137, 38.3291]], grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.05713139474391937 
model_pd.l_d.mean(): -1.502381682395935 
model_pd.lagr.mean(): -1.4452502727508545 
model_pd.lambdas: dict_items([('pout', tensor([1.7996])), ('power', tensor([0.0961]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6747])), ('power', tensor([-2.9880]))])
epoch：1068	 i:0 	 global-step:21360	 l-p:0.05713139474391937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]])
 pt:tensor([[24.8922, 27.5572, 27.5537],
        [24.8922, 27.7308, 27.8241],
        [24.8922, 29.1678, 30.3011],
        [24.8922, 26.6200, 26.2428]], grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.05712120234966278 
model_pd.l_d.mean(): -1.501495122909546 
model_pd.lagr.mean(): -1.4443739652633667 
model_pd.lambdas: dict_items([('pout', tensor([1.7990])), ('power', tensor([0.0960]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6763])), ('power', tensor([-2.9587]))])
epoch：1069	 i:0 	 global-step:21380	 l-p:0.05712120234966278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[24.9218, 32.2147, 36.4627],
        [24.9218, 24.9218, 24.9218],
        [24.9218, 27.5902, 27.5869],
        [24.9218, 29.2029, 30.3378]], grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.05711112916469574 
model_pd.l_d.mean(): -1.5006153583526611 
model_pd.lagr.mean(): -1.4435042142868042 
model_pd.lambdas: dict_items([('pout', tensor([1.7983])), ('power', tensor([0.0958]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6779])), ('power', tensor([-2.9297]))])
epoch：1070	 i:0 	 global-step:21400	 l-p:0.05711112916469574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[24.9511, 33.1337, 38.4698],
        [24.9511, 24.9511, 24.9511],
        [24.9511, 24.9511, 24.9511],
        [24.9511, 33.7533, 39.8910]], grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.05710120499134064 
model_pd.l_d.mean(): -1.4997413158416748 
model_pd.lagr.mean(): -1.4426400661468506 
model_pd.lambdas: dict_items([('pout', tensor([1.7976])), ('power', tensor([0.0957]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6794])), ('power', tensor([-2.9010]))])
epoch：1071	 i:0 	 global-step:21420	 l-p:0.05710120499134064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[24.9801, 25.0178, 24.9829],
        [24.9801, 25.7268, 25.3268],
        [24.9801, 26.7998, 26.4432],
        [24.9801, 27.6554, 27.6521]], grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.057091400027275085 
model_pd.l_d.mean(): -1.498873233795166 
model_pd.lagr.mean(): -1.4417818784713745 
model_pd.lambdas: dict_items([('pout', tensor([1.7969])), ('power', tensor([0.0955]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6809])), ('power', tensor([-2.8726]))])
epoch：1072	 i:0 	 global-step:21440	 l-p:0.057091400027275085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[25.0088, 27.5118, 27.4185],
        [25.0088, 30.2815, 32.3146],
        [25.0088, 27.2708, 27.0678],
        [25.0088, 25.1113, 25.0224]], grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.05708172917366028 
model_pd.l_d.mean(): -1.4980106353759766 
model_pd.lagr.mean(): -1.4409289360046387 
model_pd.lambdas: dict_items([('pout', tensor([1.7962])), ('power', tensor([0.0954]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6824])), ('power', tensor([-2.8445]))])
epoch：1073	 i:0 	 global-step:21460	 l-p:0.05708172917366028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[25.0371, 32.3663, 36.6359],
        [25.0371, 32.0469, 35.9407],
        [25.0371, 29.1087, 30.0576],
        [25.0371, 25.0406, 25.0372]], grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.057072192430496216 
model_pd.l_d.mean(): -1.4971543550491333 
model_pd.lagr.mean(): -1.4400821924209595 
model_pd.lambdas: dict_items([('pout', tensor([1.7956])), ('power', tensor([0.0953]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6839])), ('power', tensor([-2.8167]))])
epoch：1074	 i:0 	 global-step:21480	 l-p:0.057072192430496216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[25.0652, 25.0925, 25.0668],
        [25.0652, 33.9105, 40.0787],
        [25.0652, 25.1144, 25.0694],
        [25.0652, 25.0747, 25.0655]], grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.0570627823472023 
model_pd.l_d.mean(): -1.4963034391403198 
model_pd.lagr.mean(): -1.4392406940460205 
model_pd.lambdas: dict_items([('pout', tensor([1.7949])), ('power', tensor([0.0951]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6854])), ('power', tensor([-2.7892]))])
epoch：1075	 i:0 	 global-step:21500	 l-p:0.0570627823472023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[25.0929, 30.3792, 32.4143],
        [25.0929, 26.0137, 25.5796],
        [25.0929, 25.0952, 25.0930],
        [25.0929, 25.0929, 25.0929]], grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.057053498923778534 
model_pd.l_d.mean(): -1.4954577684402466 
model_pd.lagr.mean(): -1.4384043216705322 
model_pd.lambdas: dict_items([('pout', tensor([1.7942])), ('power', tensor([0.0950]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6868])), ('power', tensor([-2.7620]))])
epoch：1076	 i:0 	 global-step:21520	 l-p:0.057053498923778534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[25.1204, 25.1208, 25.1204],
        [25.1204, 30.4129, 32.4505],
        [25.1204, 25.1226, 25.1204],
        [25.1204, 26.7907, 26.3928]], grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.057044342160224915 
model_pd.l_d.mean(): -1.494617223739624 
model_pd.lagr.mean(): -1.4375728368759155 
model_pd.lambdas: dict_items([('pout', tensor([1.7935])), ('power', tensor([0.0948]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6882])), ('power', tensor([-2.7351]))])
epoch：1077	 i:0 	 global-step:21540	 l-p:0.057044342160224915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[25.1475, 25.4741, 25.2367],
        [25.1475, 32.5116, 36.8018],
        [25.1475, 25.7795, 25.4102],
        [25.1475, 25.1510, 25.1476]], grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.057035308331251144 
model_pd.l_d.mean(): -1.4937821626663208 
model_pd.lagr.mean(): -1.4367468357086182 
model_pd.lambdas: dict_items([('pout', tensor([1.7928])), ('power', tensor([0.0947]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6897])), ('power', tensor([-2.7085]))])
epoch：1078	 i:0 	 global-step:21560	 l-p:0.057035308331251144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228]])
 pt:tensor([[25.1744, 26.2678, 25.8166],
        [25.1744, 26.0435, 25.6163],
        [25.1744, 30.4793, 32.5218],
        [25.1744, 30.4420, 32.4482]], grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.057026397436857224 
model_pd.l_d.mean(): -1.4929521083831787 
model_pd.lagr.mean(): -1.4359257221221924 
model_pd.lambdas: dict_items([('pout', tensor([1.7921])), ('power', tensor([0.0946]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6911])), ('power', tensor([-2.6822]))])
epoch：1079	 i:0 	 global-step:21580	 l-p:0.057026397436857224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[25.2009, 26.1261, 25.6899],
        [25.2009, 25.2009, 25.2009],
        [25.2009, 27.7253, 27.6315],
        [25.2009, 30.0581, 31.6715]], grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.05701761320233345 
model_pd.l_d.mean(): -1.4921268224716187 
model_pd.lagr.mean(): -1.435109257698059 
model_pd.lambdas: dict_items([('pout', tensor([1.7914])), ('power', tensor([0.0944]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6924])), ('power', tensor([-2.6562]))])
epoch：1080	 i:0 	 global-step:21600	 l-p:0.05701761320233345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[25.2271, 25.4040, 25.2599],
        [25.2271, 32.2944, 36.2209],
        [25.2271, 26.9808, 26.5983],
        [25.2271, 34.1339, 40.3456]], grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.05700894817709923 
model_pd.l_d.mean(): -1.4913060665130615 
model_pd.lagr.mean(): -1.4342970848083496 
model_pd.lambdas: dict_items([('pout', tensor([1.7907])), ('power', tensor([0.0943]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6938])), ('power', tensor([-2.6305]))])
epoch：1081	 i:0 	 global-step:21620	 l-p:0.05700894817709923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[25.2531, 25.2807, 25.2547],
        [25.2531, 25.5913, 25.3472],
        [25.2531, 28.1372, 28.2326],
        [25.2531, 30.5761, 32.6259]], grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.057000402361154556 
model_pd.l_d.mean(): -1.4904905557632446 
model_pd.lagr.mean(): -1.4334901571273804 
model_pd.lambdas: dict_items([('pout', tensor([1.7900])), ('power', tensor([0.0942]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6951])), ('power', tensor([-2.6050]))])
epoch：1082	 i:0 	 global-step:21640	 l-p:0.057000402361154556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[25.2787, 25.2884, 25.2790],
        [25.2787, 25.2787, 25.2787],
        [25.2787, 25.2788, 25.2787],
        [25.2787, 25.3169, 25.2815]], grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.056991979479789734 
model_pd.l_d.mean(): -1.4896793365478516 
model_pd.lagr.mean(): -1.4326874017715454 
model_pd.lambdas: dict_items([('pout', tensor([1.7893])), ('power', tensor([0.0940]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6965])), ('power', tensor([-2.5799]))])
epoch：1083	 i:0 	 global-step:21660	 l-p:0.056991979479789734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[25.3040, 25.3040, 25.3041],
        [25.3040, 25.6431, 25.3984],
        [25.3040, 25.3076, 25.3041],
        [25.3040, 25.6448, 25.3992]], grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.05698366463184357 
model_pd.l_d.mean(): -1.4888726472854614 
model_pd.lagr.mean(): -1.4318889379501343 
model_pd.lambdas: dict_items([('pout', tensor([1.7886])), ('power', tensor([0.0939]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6978])), ('power', tensor([-2.5551]))])
epoch：1084	 i:0 	 global-step:21680	 l-p:0.05698366463184357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[25.3291, 32.2892, 36.0737],
        [25.3291, 29.4529, 30.4146],
        [25.3291, 26.4301, 25.9758],
        [25.3291, 26.5661, 26.1102]], grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.05697547271847725 
model_pd.l_d.mean(): -1.4880701303482056 
model_pd.lagr.mean(): -1.4310946464538574 
model_pd.lambdas: dict_items([('pout', tensor([1.7879])), ('power', tensor([0.0938]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6991])), ('power', tensor([-2.5305]))])
epoch：1085	 i:0 	 global-step:21700	 l-p:0.05697547271847725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[25.3538, 27.2040, 26.8418],
        [25.3538, 26.7787, 26.3343],
        [25.3538, 25.6835, 25.4438],
        [25.3538, 25.3538, 25.3538]], grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.056967396289110184 
model_pd.l_d.mean(): -1.4872719049453735 
model_pd.lagr.mean(): -1.4303045272827148 
model_pd.lambdas: dict_items([('pout', tensor([1.7872])), ('power', tensor([0.0937]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7004])), ('power', tensor([-2.5062]))])
epoch：1086	 i:0 	 global-step:21720	 l-p:0.056967396289110184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[25.3783, 25.3788, 25.3783],
        [25.3783, 26.3109, 25.8713],
        [25.3783, 32.3531, 36.1458],
        [25.3783, 25.3783, 25.3783]], grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.05695943534374237 
model_pd.l_d.mean(): -1.4864778518676758 
model_pd.lagr.mean(): -1.429518461227417 
model_pd.lambdas: dict_items([('pout', tensor([1.7865])), ('power', tensor([0.0935]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7016])), ('power', tensor([-2.4822]))])
epoch：1087	 i:0 	 global-step:21740	 l-p:0.05695943534374237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[25.4025, 32.5231, 36.4799],
        [25.4025, 25.4025, 25.4025],
        [25.4025, 25.4060, 25.4025],
        [25.4025, 32.3845, 36.1812]], grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.05695158615708351 
model_pd.l_d.mean(): -1.4856877326965332 
model_pd.lagr.mean(): -1.4287360906600952 
model_pd.lambdas: dict_items([('pout', tensor([1.7858])), ('power', tensor([0.0934]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7029])), ('power', tensor([-2.4585]))])
epoch：1088	 i:0 	 global-step:21760	 l-p:0.05695158615708351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.4263, 25.4264, 25.4263],
        [25.4263, 26.8558, 26.4100],
        [25.4263, 28.5231, 28.7339],
        [25.4263, 25.4265, 25.4264]], grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.05694384500384331 
model_pd.l_d.mean(): -1.4849015474319458 
model_pd.lagr.mean(): -1.4279576539993286 
model_pd.lambdas: dict_items([('pout', tensor([1.7851])), ('power', tensor([0.0933]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7041])), ('power', tensor([-2.4351]))])
epoch：1089	 i:0 	 global-step:21780	 l-p:0.05694384500384331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[25.4499, 32.4462, 36.2508],
        [25.4499, 27.7566, 27.5501],
        [25.4499, 28.0022, 27.9078],
        [25.4499, 34.4415, 40.7135]], grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.056936219334602356 
model_pd.l_d.mean(): -1.4841194152832031 
model_pd.lagr.mean(): -1.4271831512451172 
model_pd.lambdas: dict_items([('pout', tensor([1.7844])), ('power', tensor([0.0932]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7053])), ('power', tensor([-2.4120]))])
epoch：1090	 i:0 	 global-step:21800	 l-p:0.056936219334602356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[25.4732, 25.4734, 25.4732],
        [25.4732, 25.4737, 25.4732],
        [25.4732, 25.4747, 25.4733],
        [25.4732, 26.9057, 26.4590]], grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.05692869797348976 
model_pd.l_d.mean(): -1.4833403825759888 
model_pd.lagr.mean(): -1.4264116287231445 
model_pd.lambdas: dict_items([('pout', tensor([1.7837])), ('power', tensor([0.0931]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7065])), ('power', tensor([-2.3891]))])
epoch：1091	 i:0 	 global-step:21820	 l-p:0.05692869797348976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[25.4962, 25.4967, 25.4963],
        [25.4962, 25.8400, 25.5923],
        [25.4962, 26.7426, 26.2833],
        [25.4962, 30.8815, 32.9595]], grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.05692128837108612 
model_pd.l_d.mean(): -1.4825656414031982 
model_pd.lagr.mean(): -1.4256443977355957 
model_pd.lambdas: dict_items([('pout', tensor([1.7830])), ('power', tensor([0.0929]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7077])), ('power', tensor([-2.3666]))])
epoch：1092	 i:0 	 global-step:21840	 l-p:0.05692128837108612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[25.5190, 25.5287, 25.5193],
        [25.5190, 25.8613, 25.6143],
        [25.5190, 33.0011, 37.3617],
        [25.5190, 34.5369, 40.8277]], grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.056913986802101135 
model_pd.l_d.mean(): -1.481793999671936 
model_pd.lagr.mean(): -1.424880027770996 
model_pd.lambdas: dict_items([('pout', tensor([1.7823])), ('power', tensor([0.0928]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7089])), ('power', tensor([-2.3443]))])
epoch：1093	 i:0 	 global-step:21860	 l-p:0.056913986802101135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[25.5414, 25.8739, 25.6322],
        [25.5414, 28.4624, 28.5595],
        [25.5414, 26.4809, 26.0381],
        [25.5414, 28.6539, 28.8660]], grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.05690678581595421 
model_pd.l_d.mean(): -1.4810258150100708 
model_pd.lagr.mean(): -1.424118995666504 
model_pd.lambdas: dict_items([('pout', tensor([1.7816])), ('power', tensor([0.0927]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7100])), ('power', tensor([-2.3223]))])
epoch：1094	 i:0 	 global-step:21880	 l-p:0.05690678581595421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.5636, 30.9586, 33.0371],
        [25.5636, 25.5734, 25.5639],
        [25.5636, 25.5653, 25.5636],
        [25.5636, 25.5637, 25.5636]], grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.056899696588516235 
model_pd.l_d.mean(): -1.4802613258361816 
model_pd.lagr.mean(): -1.4233616590499878 
model_pd.lambdas: dict_items([('pout', tensor([1.7809])), ('power', tensor([0.0926]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7111])), ('power', tensor([-2.3005]))])
epoch：1095	 i:0 	 global-step:21900	 l-p:0.056899696588516235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228]])
 pt:tensor([[25.5855, 30.9916, 33.0778],
        [25.5855, 26.6992, 26.2398],
        [25.5855, 33.0889, 37.4621],
        [25.5855, 26.2301, 25.8535]], grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.05689270794391632 
model_pd.l_d.mean(): -1.4794996976852417 
model_pd.lagr.mean(): -1.4226069450378418 
model_pd.lambdas: dict_items([('pout', tensor([1.7802])), ('power', tensor([0.0925]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7123])), ('power', tensor([-2.2790]))])
epoch：1096	 i:0 	 global-step:21920	 l-p:0.05689270794391632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[25.6071, 25.6071, 25.6071],
        [25.6071, 29.4982, 30.2484],
        [25.6071, 28.3574, 28.3550],
        [25.6071, 26.5494, 26.1053]], grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.056885819882154465 
model_pd.l_d.mean(): -1.4787415266036987 
model_pd.lagr.mean(): -1.4218556880950928 
model_pd.lambdas: dict_items([('pout', tensor([1.7795])), ('power', tensor([0.0924]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7134])), ('power', tensor([-2.2578]))])
epoch：1097	 i:0 	 global-step:21940	 l-p:0.056885819882154465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[25.6284, 25.6565, 25.6301],
        [25.6284, 27.0709, 26.6212],
        [25.6284, 25.7340, 25.6425],
        [25.6284, 25.8086, 25.6618]], grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.05687903240323067 
model_pd.l_d.mean(): -1.477986454963684 
model_pd.lagr.mean(): -1.4211074113845825 
model_pd.lambdas: dict_items([('pout', tensor([1.7788])), ('power', tensor([0.0923]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7145])), ('power', tensor([-2.2369]))])
epoch：1098	 i:0 	 global-step:21960	 l-p:0.05687903240323067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.6495, 33.1735, 37.5590],
        [25.6495, 31.0266, 33.0759],
        [25.6495, 26.5374, 26.1011],
        [25.6495, 25.6496, 25.6495]], grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.05687233805656433 
model_pd.l_d.mean(): -1.4772343635559082 
model_pd.lagr.mean(): -1.4203619956970215 
model_pd.lambdas: dict_items([('pout', tensor([1.7780])), ('power', tensor([0.0921]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7155])), ('power', tensor([-2.2162]))])
epoch：1099	 i:0 	 global-step:21980	 l-p:0.05687233805656433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[25.6703, 28.2475, 28.1526],
        [25.6703, 26.0150, 25.7663],
        [25.6703, 25.6739, 25.6704],
        [25.6703, 25.6704, 25.6703]], grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.05686574801802635 
model_pd.l_d.mean(): -1.476485252380371 
model_pd.lagr.mean(): -1.4196195602416992 
model_pd.lambdas: dict_items([('pout', tensor([1.7773])), ('power', tensor([0.0920]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7166])), ('power', tensor([-2.1958]))])
epoch：1100	 i:0 	 global-step:22000	 l-p:0.05686574801802635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[25.6908, 25.6908, 25.6908],
        [25.6908, 31.1156, 33.2060],
        [25.6908, 25.6944, 25.6909],
        [25.6908, 25.7190, 25.6925]], grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.05685925856232643 
model_pd.l_d.mean(): -1.4757386445999146 
model_pd.lagr.mean(): -1.4188793897628784 
model_pd.lambdas: dict_items([('pout', tensor([1.7766])), ('power', tensor([0.0919]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7177])), ('power', tensor([-2.1757]))])
epoch：1101	 i:0 	 global-step:22020	 l-p:0.05685925856232643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[25.7111, 28.8469, 29.0610],
        [25.7111, 31.1406, 33.2329],
        [25.7111, 26.0534, 25.8059],
        [25.7111, 26.4828, 26.0697]], grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.05685286596417427 
model_pd.l_d.mean(): -1.4749950170516968 
model_pd.lagr.mean(): -1.4181421995162964 
model_pd.lambdas: dict_items([('pout', tensor([1.7759])), ('power', tensor([0.0918]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7187])), ('power', tensor([-2.1558]))])
epoch：1102	 i:0 	 global-step:22040	 l-p:0.05685286596417427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[25.7311, 28.3153, 28.2202],
        [25.7311, 26.6223, 26.1845],
        [25.7311, 28.0667, 27.8581],
        [25.7311, 25.7347, 25.7312]], grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.05684655159711838 
model_pd.l_d.mean(): -1.4742541313171387 
model_pd.lagr.mean(): -1.4174076318740845 
model_pd.lambdas: dict_items([('pout', tensor([1.7752])), ('power', tensor([0.0917]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7197])), ('power', tensor([-2.1361]))])
epoch：1103	 i:0 	 global-step:22060	 l-p:0.05684655159711838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[25.7509, 25.7545, 25.7510],
        [25.7509, 26.8730, 26.4102],
        [25.7509, 34.2175, 39.7426],
        [25.7509, 27.5459, 27.1549]], grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.056840334087610245 
model_pd.l_d.mean(): -1.4735158681869507 
model_pd.lagr.mean(): -1.4166755676269531 
model_pd.lambdas: dict_items([('pout', tensor([1.7744])), ('power', tensor([0.0916]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7207])), ('power', tensor([-2.1167]))])
epoch：1104	 i:0 	 global-step:22080	 l-p:0.056840334087610245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[25.7704, 28.7210, 28.8196],
        [25.7704, 26.5442, 26.1300],
        [25.7704, 25.7704, 25.7704],
        [25.7704, 29.6893, 30.4454]], grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.05683420971035957 
model_pd.l_d.mean(): -1.4727801084518433 
model_pd.lagr.mean(): -1.4159458875656128 
model_pd.lambdas: dict_items([('pout', tensor([1.7737])), ('power', tensor([0.0915]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7217])), ('power', tensor([-2.0976]))])
epoch：1105	 i:0 	 global-step:22100	 l-p:0.05683420971035957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[25.7897, 29.9971, 30.9797],
        [25.7897, 32.8884, 36.7503],
        [25.7897, 33.0292, 37.0538],
        [25.7897, 26.1363, 25.8862]], grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.05682817101478577 
model_pd.l_d.mean(): -1.4720467329025269 
model_pd.lagr.mean(): -1.4152185916900635 
model_pd.lambdas: dict_items([('pout', tensor([1.7730])), ('power', tensor([0.0914]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7227])), ('power', tensor([-2.0787]))])
epoch：1106	 i:0 	 global-step:22120	 l-p:0.05682817101478577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[25.8087, 30.7956, 32.4541],
        [25.8087, 25.8088, 25.8087],
        [25.8087, 26.1556, 25.9053],
        [25.8087, 25.8105, 25.8088]], grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.056822214275598526 
model_pd.l_d.mean(): -1.4713157415390015 
model_pd.lagr.mean(): -1.4144935607910156 
model_pd.lambdas: dict_items([('pout', tensor([1.7723])), ('power', tensor([0.0913]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7237])), ('power', tensor([-2.0600]))])
epoch：1107	 i:0 	 global-step:22140	 l-p:0.056822214275598526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[25.8276, 28.1731, 27.9638],
        [25.8276, 31.2904, 33.3996],
        [25.8276, 26.9535, 26.4892],
        [25.8276, 28.6047, 28.6028]], grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.05681633949279785 
model_pd.l_d.mean(): -1.470587134361267 
model_pd.lagr.mean(): -1.4137707948684692 
model_pd.lambdas: dict_items([('pout', tensor([1.7715])), ('power', tensor([0.0912]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7246])), ('power', tensor([-2.0415]))])
epoch：1108	 i:0 	 global-step:22160	 l-p:0.05681633949279785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[25.8461, 25.9527, 25.8604],
        [25.8461, 27.7372, 27.3677],
        [25.8461, 25.8462, 25.8462],
        [25.8461, 33.4334, 37.8569]], grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.056810542941093445 
model_pd.l_d.mean(): -1.4698610305786133 
model_pd.lagr.mean(): -1.4130505323410034 
model_pd.lambdas: dict_items([('pout', tensor([1.7708])), ('power', tensor([0.0911]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7256])), ('power', tensor([-2.0233]))])
epoch：1109	 i:0 	 global-step:22180	 l-p:0.056810542941093445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[25.8645, 28.2139, 28.0044],
        [25.8645, 27.6687, 27.2759],
        [25.8645, 27.1316, 26.6650],
        [25.8645, 26.2141, 25.9622]], grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.056804828345775604 
model_pd.l_d.mean(): -1.4691367149353027 
model_pd.lagr.mean(): -1.4123319387435913 
model_pd.lambdas: dict_items([('pout', tensor([1.7701])), ('power', tensor([0.0910]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7265])), ('power', tensor([-2.0052]))])
epoch：1110	 i:0 	 global-step:22200	 l-p:0.056804828345775604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[25.8827, 33.4817, 37.9123],
        [25.8827, 25.8829, 25.8827],
        [25.8827, 26.7800, 26.3392],
        [25.8827, 26.2204, 25.9749]], grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.05679917708039284 
model_pd.l_d.mean(): -1.468414545059204 
model_pd.lagr.mean(): -1.4116153717041016 
model_pd.lambdas: dict_items([('pout', tensor([1.7694])), ('power', tensor([0.0909]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7274])), ('power', tensor([-1.9874]))])
epoch：1111	 i:0 	 global-step:22220	 l-p:0.05679917708039284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[25.9007, 25.9024, 25.9007],
        [25.9007, 31.3363, 33.4090],
        [25.9007, 26.2490, 25.9977],
        [25.9007, 27.7964, 27.4260]], grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.05679360032081604 
model_pd.l_d.mean(): -1.4676944017410278 
model_pd.lagr.mean(): -1.4109008312225342 
model_pd.lambdas: dict_items([('pout', tensor([1.7686])), ('power', tensor([0.0908]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7283])), ('power', tensor([-1.9697]))])
epoch：1112	 i:0 	 global-step:22240	 l-p:0.05679360032081604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[25.9185, 26.2640, 26.0142],
        [25.9185, 28.8883, 28.9879],
        [25.9185, 25.9190, 25.9185],
        [25.9185, 35.0900, 41.4905]], grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.056788090616464615 
model_pd.l_d.mean(): -1.466976284980774 
model_pd.lagr.mean(): -1.4101881980895996 
model_pd.lambdas: dict_items([('pout', tensor([1.7679])), ('power', tensor([0.0907]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7292])), ('power', tensor([-1.9523]))])
epoch：1113	 i:0 	 global-step:22260	 l-p:0.056788090616464615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[25.9361, 26.2819, 26.0319],
        [25.9361, 26.2868, 26.0341],
        [25.9361, 25.9361, 25.9361],
        [25.9361, 25.9755, 25.9390]], grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.056782640516757965 
model_pd.l_d.mean(): -1.4662604331970215 
model_pd.lagr.mean(): -1.4094778299331665 
model_pd.lambdas: dict_items([('pout', tensor([1.7672])), ('power', tensor([0.0906]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7301])), ('power', tensor([-1.9350]))])
epoch：1114	 i:0 	 global-step:22280	 l-p:0.056782640516757965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[25.9536, 25.9537, 25.9536],
        [25.9536, 26.0607, 25.9679],
        [25.9536, 26.6090, 26.2262],
        [25.9536, 25.9536, 25.9536]], grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.05677725747227669 
model_pd.l_d.mean(): -1.4655461311340332 
model_pd.lagr.mean(): -1.408768892288208 
model_pd.lambdas: dict_items([('pout', tensor([1.7665])), ('power', tensor([0.0905]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7310])), ('power', tensor([-1.9178]))])
epoch：1115	 i:0 	 global-step:22300	 l-p:0.05677725747227669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[25.9708, 28.5825, 28.4869],
        [25.9708, 25.9709, 25.9708],
        [25.9708, 26.0780, 25.9852],
        [25.9708, 29.1425, 29.3598]], grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.05677192658185959 
model_pd.l_d.mean(): -1.4648336172103882 
model_pd.lagr.mean(): -1.4080617427825928 
model_pd.lambdas: dict_items([('pout', tensor([1.7657])), ('power', tensor([0.0904]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7319])), ('power', tensor([-1.9009]))])
epoch：1116	 i:0 	 global-step:22320	 l-p:0.05677192658185959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[25.9880, 28.8736, 28.9207],
        [25.9880, 25.9916, 25.9881],
        [25.9880, 26.8895, 26.4467],
        [25.9880, 27.8910, 27.5193]], grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.05676665157079697 
model_pd.l_d.mean(): -1.4641231298446655 
model_pd.lagr.mean(): -1.4073565006256104 
model_pd.lambdas: dict_items([('pout', tensor([1.7650])), ('power', tensor([0.0903]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7327])), ('power', tensor([-1.8840]))])
epoch：1117	 i:0 	 global-step:22340	 l-p:0.05676665157079697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.0050, 28.8038, 28.8024],
        [26.0050, 28.3689, 28.1584],
        [26.0050, 26.3519, 26.1011],
        [26.0050, 26.3445, 26.0977]], grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.05676143243908882 
model_pd.l_d.mean(): -1.463414192199707 
model_pd.lagr.mean(): -1.406652808189392 
model_pd.lambdas: dict_items([('pout', tensor([1.7643])), ('power', tensor([0.0902]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7336])), ('power', tensor([-1.8673]))])
epoch：1118	 i:0 	 global-step:22360	 l-p:0.05676143243908882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.0219, 35.2332, 41.6621],
        [26.0219, 26.0224, 26.0219],
        [26.0219, 26.3740, 26.1203],
        [26.0219, 28.3876, 28.1768]], grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.056756261736154556 
model_pd.l_d.mean(): -1.4627070426940918 
model_pd.lagr.mean(): -1.4059507846832275 
model_pd.lambdas: dict_items([('pout', tensor([1.7635])), ('power', tensor([0.0901]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7344])), ('power', tensor([-1.8508]))])
epoch：1119	 i:0 	 global-step:22380	 l-p:0.056756261736154556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.0386, 26.0901, 26.0430],
        [26.0386, 26.3787, 26.1315],
        [26.0386, 26.6966, 26.3123],
        [26.0386, 29.2196, 29.4377]], grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.056751132011413574 
model_pd.l_d.mean(): -1.4620012044906616 
model_pd.lagr.mean(): -1.405250072479248 
model_pd.lambdas: dict_items([('pout', tensor([1.7628])), ('power', tensor([0.0900]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7353])), ('power', tensor([-1.8343]))])
epoch：1120	 i:0 	 global-step:22400	 l-p:0.056751132011413574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.0552, 30.3113, 31.3061],
        [26.0552, 29.2385, 29.4568],
        [26.0552, 27.7965, 27.3827],
        [26.0552, 26.0652, 26.0556]], grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.05674605444073677 
model_pd.l_d.mean(): -1.4612977504730225 
model_pd.lagr.mean(): -1.4045517444610596 
model_pd.lambdas: dict_items([('pout', tensor([1.7620])), ('power', tensor([0.0899]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7361])), ('power', tensor([-1.8180]))])
epoch：1121	 i:0 	 global-step:22420	 l-p:0.05674605444073677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[26.0717, 26.4246, 26.1704],
        [26.0717, 30.3308, 31.3263],
        [26.0717, 29.0614, 29.1621],
        [26.0717, 26.0735, 26.0718]], grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.05674101412296295 
model_pd.l_d.mean(): -1.4605952501296997 
model_pd.lagr.mean(): -1.403854250907898 
model_pd.lambdas: dict_items([('pout', tensor([1.7613])), ('power', tensor([0.0899]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7369])), ('power', tensor([-1.8018]))])
epoch：1122	 i:0 	 global-step:22440	 l-p:0.05674101412296295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.0881, 26.2721, 26.1222],
        [26.0881, 27.9995, 27.6264],
        [26.0881, 26.0918, 26.0882],
        [26.0881, 30.3501, 31.3465]], grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.056736014783382416 
model_pd.l_d.mean(): -1.4598941802978516 
model_pd.lagr.mean(): -1.403158187866211 
model_pd.lambdas: dict_items([('pout', tensor([1.7606])), ('power', tensor([0.0898]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7378])), ('power', tensor([-1.7857]))])
epoch：1123	 i:0 	 global-step:22460	 l-p:0.056736014783382416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228]])
 pt:tensor([[26.1044, 27.2443, 26.7745],
        [26.1044, 27.9277, 27.5311],
        [26.1044, 31.5874, 33.6790],
        [26.1044, 31.6262, 33.7556]], grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.05673106014728546 
model_pd.l_d.mean(): -1.4591948986053467 
model_pd.lagr.mean(): -1.4024637937545776 
model_pd.lambdas: dict_items([('pout', tensor([1.7598])), ('power', tensor([0.0897]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7386])), ('power', tensor([-1.7697]))])
epoch：1124	 i:0 	 global-step:22480	 l-p:0.05673106014728546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.1206, 26.4742, 26.2194],
        [26.1206, 31.6073, 33.7004],
        [26.1206, 30.0993, 30.8680],
        [26.1206, 27.0848, 26.6306]], grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.05672615393996239 
model_pd.l_d.mean(): -1.4584968090057373 
model_pd.lagr.mean(): -1.4017707109451294 
model_pd.lambdas: dict_items([('pout', tensor([1.7591])), ('power', tensor([0.0896]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7394])), ('power', tensor([-1.7539]))])
epoch：1125	 i:0 	 global-step:22500	 l-p:0.05672615393996239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[26.1366, 28.7671, 28.6711],
        [26.1366, 27.1015, 26.6470],
        [26.1366, 31.1939, 32.8772],
        [26.1366, 27.4191, 26.9470]], grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.0567212849855423 
model_pd.l_d.mean(): -1.4578006267547607 
model_pd.lagr.mean(): -1.4010792970657349 
model_pd.lambdas: dict_items([('pout', tensor([1.7584])), ('power', tensor([0.0895]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7402])), ('power', tensor([-1.7381]))])
epoch：1126	 i:0 	 global-step:22520	 l-p:0.0567212849855423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.1525, 27.0606, 26.6146],
        [26.1525, 33.5037, 37.5923],
        [26.1525, 33.3608, 37.2841],
        [26.1525, 26.1540, 26.1525]], grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.05671647936105728 
model_pd.l_d.mean(): -1.4571055173873901 
model_pd.lagr.mean(): -1.4003890752792358 
model_pd.lambdas: dict_items([('pout', tensor([1.7576])), ('power', tensor([0.0894]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7410])), ('power', tensor([-1.7225]))])
epoch：1127	 i:0 	 global-step:22540	 l-p:0.05671647936105728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[26.1682, 26.3760, 26.2097],
        [26.1682, 26.5208, 26.2664],
        [26.1682, 26.1682, 26.1682],
        [26.1682, 34.7844, 40.4096]], grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.05671171098947525 
model_pd.l_d.mean(): -1.4564117193222046 
model_pd.lagr.mean(): -1.3997000455856323 
model_pd.lambdas: dict_items([('pout', tensor([1.7569])), ('power', tensor([0.0893]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7418])), ('power', tensor([-1.7071]))])
epoch：1128	 i:0 	 global-step:22560	 l-p:0.05671171098947525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.1838, 30.1733, 30.9443],
        [26.1838, 27.0932, 26.6467],
        [26.1838, 26.3686, 26.2181],
        [26.1838, 26.1839, 26.1838]], grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.056707002222537994 
model_pd.l_d.mean(): -1.4557191133499146 
model_pd.lagr.mean(): -1.3990120887756348 
model_pd.lambdas: dict_items([('pout', tensor([1.7561])), ('power', tensor([0.0892]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7426])), ('power', tensor([-1.6917]))])
epoch：1129	 i:0 	 global-step:22580	 l-p:0.056707002222537994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.1994, 30.7243, 31.9277],
        [26.1994, 28.8370, 28.7409],
        [26.1994, 27.9517, 27.5354],
        [26.1994, 26.5493, 26.2963]], grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.056702304631471634 
model_pd.l_d.mean(): -1.4550279378890991 
model_pd.lagr.mean(): -1.3983256816864014 
model_pd.lambdas: dict_items([('pout', tensor([1.7554])), ('power', tensor([0.0892]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7433])), ('power', tensor([-1.6765]))])
epoch：1130	 i:0 	 global-step:22600	 l-p:0.056702304631471634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.2149, 30.5000, 31.5021],
        [26.2149, 28.8543, 28.7582],
        [26.2149, 30.2097, 30.9818],
        [26.2149, 33.5852, 37.6847]], grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.056697625666856766 
model_pd.l_d.mean(): -1.4543378353118896 
model_pd.lagr.mean(): -1.3976402282714844 
model_pd.lambdas: dict_items([('pout', tensor([1.7546])), ('power', tensor([0.0891]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7441])), ('power', tensor([-1.6612]))])
epoch：1131	 i:0 	 global-step:22620	 l-p:0.056697625666856766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.2304, 34.8687, 40.5087],
        [26.2304, 26.2327, 26.2304],
        [26.2304, 33.6054, 37.7076],
        [26.2304, 26.2305, 26.2304]], grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.05669297277927399 
model_pd.l_d.mean(): -1.4536491632461548 
model_pd.lagr.mean(): -1.396956205368042 
model_pd.lambdas: dict_items([('pout', tensor([1.7539])), ('power', tensor([0.0890]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7449])), ('power', tensor([-1.6460]))])
epoch：1132	 i:0 	 global-step:22640	 l-p:0.05669297277927399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.2458, 26.5964, 26.3429],
        [26.2458, 28.0016, 27.5845],
        [26.2458, 26.2746, 26.2475],
        [26.2458, 33.6255, 37.7304]], grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.05668835714459419 
model_pd.l_d.mean(): -1.4529615640640259 
model_pd.lagr.mean(): -1.3962732553482056 
model_pd.lambdas: dict_items([('pout', tensor([1.7532])), ('power', tensor([0.0889]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7457])), ('power', tensor([-1.6310]))])
epoch：1133	 i:0 	 global-step:22660	 l-p:0.05668835714459419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.2610, 31.8254, 33.9754],
        [26.2610, 33.5019, 37.4435],
        [26.2610, 28.9056, 28.8094],
        [26.2610, 33.6454, 37.7530]], grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.05668378993868828 
model_pd.l_d.mean(): -1.4522751569747925 
model_pd.lagr.mean(): -1.395591378211975 
model_pd.lambdas: dict_items([('pout', tensor([1.7524])), ('power', tensor([0.0888]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7464])), ('power', tensor([-1.6160]))])
epoch：1134	 i:0 	 global-step:22680	 l-p:0.05668378993868828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.2760, 34.9306, 40.5815],
        [26.2760, 26.2861, 26.2764],
        [26.2760, 26.3161, 26.2790],
        [26.2760, 26.2760, 26.2760]], grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.05667927861213684 
model_pd.l_d.mean(): -1.4515899419784546 
model_pd.lagr.mean(): -1.3949106931686401 
model_pd.lambdas: dict_items([('pout', tensor([1.7517])), ('power', tensor([0.0888]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7472])), ('power', tensor([-1.6012]))])
epoch：1135	 i:0 	 global-step:22700	 l-p:0.05667927861213684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.2908, 27.2623, 26.8048],
        [26.2908, 27.5820, 27.1068],
        [26.2908, 26.2932, 26.2909],
        [26.2908, 26.3009, 26.2912]], grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.05667485296726227 
model_pd.l_d.mean(): -1.4509057998657227 
model_pd.lagr.mean(): -1.3942309617996216 
model_pd.lambdas: dict_items([('pout', tensor([1.7509])), ('power', tensor([0.0887]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7479])), ('power', tensor([-1.5867]))])
epoch：1136	 i:0 	 global-step:22720	 l-p:0.05667485296726227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.3053, 28.9550, 28.8586],
        [26.3053, 26.3068, 26.3053],
        [26.3053, 33.7033, 37.8186],
        [26.3053, 27.7916, 27.3289]], grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.056670527905225754 
model_pd.l_d.mean(): -1.4502224922180176 
model_pd.lagr.mean(): -1.3935519456863403 
model_pd.lambdas: dict_items([('pout', tensor([1.7502])), ('power', tensor([0.0886]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7486])), ('power', tensor([-1.5725]))])
epoch：1137	 i:0 	 global-step:22740	 l-p:0.056670527905225754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.3194, 26.3194, 26.3194],
        [26.3194, 28.0808, 27.6625],
        [26.3194, 27.1124, 26.6880],
        [26.3194, 26.3194, 26.3194]], grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.05666631832718849 
model_pd.l_d.mean(): -1.449540376663208 
model_pd.lagr.mean(): -1.392874002456665 
model_pd.lambdas: dict_items([('pout', tensor([1.7494])), ('power', tensor([0.0885]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7493])), ('power', tensor([-1.5586]))])
epoch：1138	 i:0 	 global-step:22760	 l-p:0.05666631832718849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.3331, 28.0955, 27.6770],
        [26.3331, 28.1746, 27.7742],
        [26.3331, 26.3336, 26.3331],
        [26.3331, 26.5423, 26.3748]], grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.05666225403547287 
model_pd.l_d.mean(): -1.4488592147827148 
model_pd.lagr.mean(): -1.3921970129013062 
model_pd.lambdas: dict_items([('pout', tensor([1.7487])), ('power', tensor([0.0884]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7500])), ('power', tensor([-1.5452]))])
epoch：1139	 i:0 	 global-step:22780	 l-p:0.05666225403547287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.3462, 31.9305, 34.0886],
        [26.3462, 29.1865, 29.1857],
        [26.3462, 26.3486, 26.3462],
        [26.3462, 26.3477, 26.3462]], grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.05665833503007889 
model_pd.l_d.mean(): -1.4481792449951172 
model_pd.lagr.mean(): -1.3915208578109741 
model_pd.lambdas: dict_items([('pout', tensor([1.7479])), ('power', tensor([0.0884]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7507])), ('power', tensor([-1.5323]))])
epoch：1140	 i:0 	 global-step:22800	 l-p:0.05665833503007889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[26.3588, 28.2024, 27.8017],
        [26.3588, 33.7733, 37.8980],
        [26.3588, 28.2928, 27.9156],
        [26.3588, 27.5115, 27.0366]], grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.05665458366274834 
model_pd.l_d.mean(): -1.4475003480911255 
model_pd.lagr.mean(): -1.390845775604248 
model_pd.lambdas: dict_items([('pout', tensor([1.7472])), ('power', tensor([0.0883]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7513])), ('power', tensor([-1.5199]))])
epoch：1141	 i:0 	 global-step:22820	 l-p:0.05665458366274834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.3710, 27.1658, 26.7405],
        [26.3710, 26.3711, 26.3710],
        [26.3710, 28.2156, 27.8146],
        [26.3710, 27.3459, 26.8868]], grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.05665097385644913 
model_pd.l_d.mean(): -1.446822166442871 
model_pd.lagr.mean(): -1.3901711702346802 
model_pd.lambdas: dict_items([('pout', tensor([1.7464])), ('power', tensor([0.0882]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7519])), ('power', tensor([-1.5080]))])
epoch：1142	 i:0 	 global-step:22840	 l-p:0.05665097385644913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.3827, 35.0755, 40.7518],
        [26.3827, 27.6790, 27.2020],
        [26.3827, 26.3827, 26.3827],
        [26.3827, 29.6110, 29.8331]], grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.05664750561118126 
model_pd.l_d.mean(): -1.4461450576782227 
model_pd.lagr.mean(): -1.3894975185394287 
model_pd.lambdas: dict_items([('pout', tensor([1.7457])), ('power', tensor([0.0881]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7525])), ('power', tensor([-1.4965]))])
epoch：1143	 i:0 	 global-step:22860	 l-p:0.05664750561118126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.3939, 27.3698, 26.9103],
        [26.3939, 27.1895, 26.7638],
        [26.3939, 29.6238, 29.8461],
        [26.3939, 26.5804, 26.4285]], grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.056644171476364136 
model_pd.l_d.mean(): -1.445468783378601 
model_pd.lagr.mean(): -1.3888245820999146 
model_pd.lambdas: dict_items([('pout', tensor([1.7449])), ('power', tensor([0.0881]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7530])), ('power', tensor([-1.4855]))])
epoch：1144	 i:0 	 global-step:22880	 l-p:0.056644171476364136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.4048, 29.4375, 29.5405],
        [26.4048, 26.4066, 26.4048],
        [26.4048, 26.7611, 26.5040],
        [26.4048, 26.4048, 26.4048]], grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.056640952825546265 
model_pd.l_d.mean(): -1.444793462753296 
model_pd.lagr.mean(): -1.3881524801254272 
model_pd.lambdas: dict_items([('pout', tensor([1.7441])), ('power', tensor([0.0880]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7536])), ('power', tensor([-1.4748]))])
epoch：1145	 i:0 	 global-step:22900	 l-p:0.056640952825546265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.4153, 26.7737, 26.5155],
        [26.4153, 35.1200, 40.8043],
        [26.4153, 26.4169, 26.4154],
        [26.4153, 27.0844, 26.6938]], grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.056637827306985855 
model_pd.l_d.mean(): -1.4441189765930176 
model_pd.lagr.mean(): -1.3874810934066772 
model_pd.lambdas: dict_items([('pout', tensor([1.7434])), ('power', tensor([0.0879]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7541])), ('power', tensor([-1.4644]))])
epoch：1146	 i:0 	 global-step:22920	 l-p:0.056637827306985855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.4257, 34.1991, 38.7340],
        [26.4257, 30.9942, 32.2101],
        [26.4257, 26.7717, 26.5203],
        [26.4257, 26.4259, 26.4257]], grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.05663476884365082 
model_pd.l_d.mean(): -1.4434449672698975 
model_pd.lagr.mean(): -1.3868101835250854 
model_pd.lambdas: dict_items([('pout', tensor([1.7426])), ('power', tensor([0.0878]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7546])), ('power', tensor([-1.4542]))])
epoch：1147	 i:0 	 global-step:22940	 l-p:0.05663476884365082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.4359, 35.1479, 40.8372],
        [26.4359, 32.0353, 34.1960],
        [26.4359, 27.1056, 26.7146],
        [26.4359, 26.4359, 26.4359]], grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.05663175508379936 
model_pd.l_d.mean(): -1.4427719116210938 
model_pd.lagr.mean(): -1.3861401081085205 
model_pd.lambdas: dict_items([('pout', tensor([1.7419])), ('power', tensor([0.0878]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7551])), ('power', tensor([-1.4442]))])
epoch：1148	 i:0 	 global-step:22960	 l-p:0.05663175508379936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.4459, 32.0084, 34.1317],
        [26.4459, 26.8048, 26.5462],
        [26.4459, 35.1616, 40.8534],
        [26.4459, 26.4984, 26.4504]], grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.056628789752721786 
model_pd.l_d.mean(): -1.4420992136001587 
model_pd.lagr.mean(): -1.3854703903198242 
model_pd.lambdas: dict_items([('pout', tensor([1.7411])), ('power', tensor([0.0877]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7556])), ('power', tensor([-1.4343]))])
epoch：1149	 i:0 	 global-step:22980	 l-p:0.056628789752721786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.4559, 28.3074, 27.9051],
        [26.4559, 29.6945, 29.9176],
        [26.4559, 27.3763, 26.9244],
        [26.4559, 26.4559, 26.4559]], grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.05662587285041809 
model_pd.l_d.mean(): -1.4414273500442505 
model_pd.lagr.mean(): -1.3848015069961548 
model_pd.lambdas: dict_items([('pout', tensor([1.7404])), ('power', tensor([0.0876]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7561])), ('power', tensor([-1.4246]))])
epoch：1150	 i:0 	 global-step:23000	 l-p:0.05662587285041809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.4657, 30.5035, 31.2847],
        [26.4657, 33.9135, 38.0573],
        [26.4657, 26.4681, 26.4658],
        [26.4657, 26.4659, 26.4657]], grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.056622959673404694 
model_pd.l_d.mean(): -1.4407559633255005 
model_pd.lagr.mean(): -1.384132981300354 
model_pd.lambdas: dict_items([('pout', tensor([1.7396])), ('power', tensor([0.0876]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7566])), ('power', tensor([-1.4149]))])
epoch：1151	 i:0 	 global-step:23020	 l-p:0.056622959673404694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.4755, 27.9730, 27.5070],
        [26.4755, 28.4195, 28.0405],
        [26.4755, 26.4792, 26.4755],
        [26.4755, 32.0843, 34.2489]], grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.05662008747458458 
model_pd.l_d.mean(): -1.440085530281067 
model_pd.lagr.mean(): -1.3834654092788696 
model_pd.lambdas: dict_items([('pout', tensor([1.7389])), ('power', tensor([0.0875]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7571])), ('power', tensor([-1.4053]))])
epoch：1152	 i:0 	 global-step:23040	 l-p:0.05662008747458458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.4851, 26.6959, 26.5272],
        [26.4851, 26.5949, 26.4998],
        [26.4851, 26.8428, 26.5848],
        [26.4851, 26.4853, 26.4852]], grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.05661724507808685 
model_pd.l_d.mean(): -1.4394155740737915 
model_pd.lagr.mean(): -1.3827983140945435 
model_pd.lambdas: dict_items([('pout', tensor([1.7381])), ('power', tensor([0.0874]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7576])), ('power', tensor([-1.3958]))])
epoch：1153	 i:0 	 global-step:23060	 l-p:0.05661724507808685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.4947, 27.6545, 27.1767],
        [26.4947, 35.2282, 40.9318],
        [26.4947, 26.5050, 26.4951],
        [26.4947, 32.1144, 34.2868]], grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.056614428758621216 
model_pd.l_d.mean(): -1.4387458562850952 
model_pd.lagr.mean(): -1.3821314573287964 
model_pd.lambdas: dict_items([('pout', tensor([1.7373])), ('power', tensor([0.0873]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7580])), ('power', tensor([-1.3864]))])
epoch：1154	 i:0 	 global-step:23080	 l-p:0.056614428758621216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.5042, 26.5334, 26.5060],
        [26.5042, 28.2801, 27.8586],
        [26.5042, 26.8515, 26.5991],
        [26.5042, 27.3038, 26.8760]], grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.05661163851618767 
model_pd.l_d.mean(): -1.4380772113800049 
model_pd.lagr.mean(): -1.3814655542373657 
model_pd.lambdas: dict_items([('pout', tensor([1.7366])), ('power', tensor([0.0873]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7585])), ('power', tensor([-1.3771]))])
epoch：1155	 i:0 	 global-step:23100	 l-p:0.05661163851618767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.5136, 26.5662, 26.5181],
        [26.5136, 32.0922, 34.2219],
        [26.5136, 26.5173, 26.5137],
        [26.5136, 29.3747, 29.3744]], grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.056608896702528 
model_pd.l_d.mean(): -1.4374090433120728 
model_pd.lagr.mean(): -1.3808001279830933 
model_pd.lambdas: dict_items([('pout', tensor([1.7358])), ('power', tensor([0.0872]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7590])), ('power', tensor([-1.3679]))])
epoch：1156	 i:0 	 global-step:23120	 l-p:0.056608896702528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.5229, 26.5247, 26.5229],
        [26.5229, 35.9270, 42.4935],
        [26.5229, 26.8779, 26.6212],
        [26.5229, 26.8704, 26.6178]], grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.056606173515319824 
model_pd.l_d.mean(): -1.4367412328720093 
model_pd.lagr.mean(): -1.3801350593566895 
model_pd.lambdas: dict_items([('pout', tensor([1.7351])), ('power', tensor([0.0871]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7594])), ('power', tensor([-1.3588]))])
epoch：1157	 i:0 	 global-step:23140	 l-p:0.056606173515319824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.5320, 26.6420, 26.5467],
        [26.5320, 26.5725, 26.5350],
        [26.5320, 26.7432, 26.5742],
        [26.5320, 29.3954, 29.3952]], grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.056603506207466125 
model_pd.l_d.mean(): -1.4360742568969727 
model_pd.lagr.mean(): -1.379470705986023 
model_pd.lambdas: dict_items([('pout', tensor([1.7343])), ('power', tensor([0.0871]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7599])), ('power', tensor([-1.3498]))])
epoch：1158	 i:0 	 global-step:23160	 l-p:0.056603506207466125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.5410, 26.5415, 26.5410],
        [26.5410, 26.5410, 26.5410],
        [26.5410, 29.4965, 29.5461],
        [26.5410, 26.5425, 26.5410]], grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.05660087615251541 
model_pd.l_d.mean(): -1.4354076385498047 
model_pd.lagr.mean(): -1.378806710243225 
model_pd.lambdas: dict_items([('pout', tensor([1.7335])), ('power', tensor([0.0870]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7603])), ('power', tensor([-1.3410]))])
epoch：1159	 i:0 	 global-step:23180	 l-p:0.05660087615251541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.5498, 26.5498, 26.5498],
        [26.5498, 30.8966, 31.9144],
        [26.5498, 26.7611, 26.5920],
        [26.5498, 32.1765, 34.3483]], grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.056598301976919174 
model_pd.l_d.mean(): -1.4347416162490845 
model_pd.lagr.mean(): -1.378143310546875 
model_pd.lambdas: dict_items([('pout', tensor([1.7328])), ('power', tensor([0.0869]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7608])), ('power', tensor([-1.3323]))])
epoch：1160	 i:0 	 global-step:23200	 l-p:0.056598301976919174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.5585, 26.5687, 26.5588],
        [26.5585, 29.5163, 29.5660],
        [26.5585, 26.5585, 26.5585],
        [26.5585, 30.9070, 31.9252]], grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.05659576505422592 
model_pd.l_d.mean(): -1.4340758323669434 
model_pd.lagr.mean(): -1.3774800300598145 
model_pd.lambdas: dict_items([('pout', tensor([1.7320])), ('power', tensor([0.0869]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7612])), ('power', tensor([-1.3238]))])
epoch：1161	 i:0 	 global-step:23220	 l-p:0.05659576505422592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.5670, 26.5688, 26.5670],
        [26.5670, 26.9260, 26.6670],
        [26.5670, 26.9228, 26.6656],
        [26.5670, 26.7550, 26.6019]], grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.05659327656030655 
model_pd.l_d.mean(): -1.4334107637405396 
model_pd.lagr.mean(): -1.3768174648284912 
model_pd.lambdas: dict_items([('pout', tensor([1.7313])), ('power', tensor([0.0868]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7616])), ('power', tensor([-1.3154]))])
epoch：1162	 i:0 	 global-step:23240	 l-p:0.05659327656030655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.5754, 26.5754, 26.5754],
        [26.5754, 26.9238, 26.6706],
        [26.5754, 29.5354, 29.5852],
        [26.5754, 26.6856, 26.5901]], grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.05659082904458046 
model_pd.l_d.mean(): -1.4327460527420044 
model_pd.lagr.mean(): -1.3761552572250366 
model_pd.lambdas: dict_items([('pout', tensor([1.7305])), ('power', tensor([0.0867]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7620])), ('power', tensor([-1.3071]))])
epoch：1163	 i:0 	 global-step:23260	 l-p:0.05659082904458046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.5837, 29.0080, 28.7933],
        [26.5837, 26.9397, 26.6823],
        [26.5837, 26.5852, 26.5837],
        [26.5837, 30.9369, 31.9564]], grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.05658841133117676 
model_pd.l_d.mean(): -1.4320822954177856 
model_pd.lagr.mean(): -1.3754938840866089 
model_pd.lambdas: dict_items([('pout', tensor([1.7297])), ('power', tensor([0.0867]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7624])), ('power', tensor([-1.2990]))])
epoch：1164	 i:0 	 global-step:23280	 l-p:0.05658841133117676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.5918, 27.2663, 26.8726],
        [26.5918, 31.7479, 33.4660],
        [26.5918, 26.6324, 26.5948],
        [26.5918, 26.5919, 26.5918]], grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.05658602714538574 
model_pd.l_d.mean(): -1.4314184188842773 
model_pd.lagr.mean(): -1.3748323917388916 
model_pd.lambdas: dict_items([('pout', tensor([1.7290])), ('power', tensor([0.0866]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7628])), ('power', tensor([-1.2910]))])
epoch：1165	 i:0 	 global-step:23300	 l-p:0.05658602714538574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228]])
 pt:tensor([[26.5999, 32.2449, 34.4277],
        [26.5999, 27.2747, 26.8808],
        [26.5999, 28.4634, 28.0588],
        [26.5999, 31.2029, 32.4288]], grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.05658367648720741 
model_pd.l_d.mean(): -1.4307551383972168 
model_pd.lagr.mean(): -1.374171495437622 
model_pd.lambdas: dict_items([('pout', tensor([1.7282])), ('power', tensor([0.0866]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7632])), ('power', tensor([-1.2830]))])
epoch：1166	 i:0 	 global-step:23320	 l-p:0.05658367648720741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.6079, 27.2829, 26.8889],
        [26.6079, 26.6103, 26.6080],
        [26.6079, 31.2126, 32.4389],
        [26.6079, 26.6095, 26.6080]], grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.056581348180770874 
model_pd.l_d.mean(): -1.4300923347473145 
model_pd.lagr.mean(): -1.3735109567642212 
model_pd.lambdas: dict_items([('pout', tensor([1.7274])), ('power', tensor([0.0865]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7636])), ('power', tensor([-1.2751]))])
epoch：1167	 i:0 	 global-step:23340	 l-p:0.056581348180770874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[26.6159, 26.6196, 26.6160],
        [26.6159, 27.4196, 26.9896],
        [26.6159, 26.6160, 26.6159],
        [26.6159, 27.6017, 27.1377]], grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.05657903105020523 
model_pd.l_d.mean(): -1.4294296503067017 
model_pd.lagr.mean(): -1.3728506565093994 
model_pd.lambdas: dict_items([('pout', tensor([1.7267])), ('power', tensor([0.0864]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7640])), ('power', tensor([-1.2673]))])
epoch：1168	 i:0 	 global-step:23360	 l-p:0.05657903105020523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[26.6238, 32.2684, 34.4475],
        [26.6238, 27.7904, 27.3099],
        [26.6238, 26.6243, 26.6238],
        [26.6238, 26.8359, 26.6661]], grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.05657673999667168 
model_pd.l_d.mean(): -1.4287678003311157 
model_pd.lagr.mean(): -1.372191071510315 
model_pd.lambdas: dict_items([('pout', tensor([1.7259])), ('power', tensor([0.0864]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7644])), ('power', tensor([-1.2596]))])
epoch：1169	 i:0 	 global-step:23380	 l-p:0.05657673999667168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6316, 26.8438, 26.6740],
        [26.6316, 29.5991, 29.6492],
        [26.6316, 27.9427, 27.4605],
        [26.6316, 28.1398, 27.6707]], grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.056574463844299316 
model_pd.l_d.mean(): -1.4281061887741089 
model_pd.lagr.mean(): -1.3715317249298096 
model_pd.lambdas: dict_items([('pout', tensor([1.7252])), ('power', tensor([0.0863]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7648])), ('power', tensor([-1.2519]))])
epoch：1170	 i:0 	 global-step:23400	 l-p:0.056574463844299316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.6394, 28.1482, 27.6789],
        [26.6394, 32.2940, 34.4808],
        [26.6394, 26.9889, 26.7350],
        [26.6394, 30.7079, 31.4958]], grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.05657219514250755 
model_pd.l_d.mean(): -1.4274446964263916 
model_pd.lagr.mean(): -1.3708724975585938 
model_pd.lambdas: dict_items([('pout', tensor([1.7244])), ('power', tensor([0.0862]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7652])), ('power', tensor([-1.2442]))])
epoch：1171	 i:0 	 global-step:23420	 l-p:0.05657219514250755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[26.6472, 29.9132, 30.1389],
        [26.6472, 31.0126, 32.0352],
        [26.6472, 32.2579, 34.4006],
        [26.6472, 34.1521, 38.3290]], grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.05656994879245758 
model_pd.l_d.mean(): -1.4267834424972534 
model_pd.lagr.mean(): -1.370213508605957 
model_pd.lambdas: dict_items([('pout', tensor([1.7236])), ('power', tensor([0.0862]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7656])), ('power', tensor([-1.2366]))])
epoch：1172	 i:0 	 global-step:23440	 l-p:0.05656994879245758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.6549, 26.7655, 26.6697],
        [26.6549, 29.3456, 29.2487],
        [26.6549, 31.8251, 33.5482],
        [26.6549, 27.9673, 27.4847]], grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.0565677210688591 
model_pd.l_d.mean(): -1.4261232614517212 
model_pd.lagr.mean(): -1.3695555925369263 
model_pd.lambdas: dict_items([('pout', tensor([1.7229])), ('power', tensor([0.0861]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7660])), ('power', tensor([-1.2290]))])
epoch：1173	 i:0 	 global-step:23460	 l-p:0.0565677210688591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.6625, 27.9754, 27.4926],
        [26.6625, 27.5917, 27.1356],
        [26.6625, 27.0231, 26.7630],
        [26.6625, 28.1728, 27.7031]], grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.056565508246421814 
model_pd.l_d.mean(): -1.4254628419876099 
model_pd.lagr.mean(): -1.3688973188400269 
model_pd.lambdas: dict_items([('pout', tensor([1.7221])), ('power', tensor([0.0861]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7663])), ('power', tensor([-1.2215]))])
epoch：1174	 i:0 	 global-step:23480	 l-p:0.056565508246421814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.6701, 31.0398, 32.0636],
        [26.6701, 32.2863, 34.4313],
        [26.6701, 26.8826, 26.7125],
        [26.6701, 27.0200, 26.7657]], grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.056563325226306915 
model_pd.l_d.mean(): -1.4248028993606567 
model_pd.lagr.mean(): -1.3682395219802856 
model_pd.lambdas: dict_items([('pout', tensor([1.7213])), ('power', tensor([0.0860]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7667])), ('power', tensor([-1.2141]))])
epoch：1175	 i:0 	 global-step:23500	 l-p:0.056563325226306915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.6775, 26.6813, 26.6776],
        [26.6775, 29.6511, 29.7015],
        [26.6775, 34.1921, 38.3746],
        [26.6775, 27.0352, 26.7767]], grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.05656116455793381 
model_pd.l_d.mean(): -1.4241430759429932 
model_pd.lagr.mean(): -1.3675819635391235 
model_pd.lambdas: dict_items([('pout', tensor([1.7206])), ('power', tensor([0.0859]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7671])), ('power', tensor([-1.2067]))])
epoch：1176	 i:0 	 global-step:23520	 l-p:0.05656116455793381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.6849, 26.7957, 26.6997],
        [26.6849, 31.3050, 32.5359],
        [26.6849, 27.4912, 27.0599],
        [26.6849, 31.8619, 33.5874]], grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.05655902996659279 
model_pd.l_d.mean(): -1.423483967781067 
model_pd.lagr.mean(): -1.3669248819351196 
model_pd.lambdas: dict_items([('pout', tensor([1.7198])), ('power', tensor([0.0859]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7674])), ('power', tensor([-1.1995]))])
epoch：1177	 i:0 	 global-step:23540	 l-p:0.05655902996659279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.6922, 26.6946, 26.6923],
        [26.6922, 29.7637, 29.8689],
        [26.6922, 26.7217, 26.6940],
        [26.6922, 28.5635, 28.1574]], grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.05655692517757416 
model_pd.l_d.mean(): -1.422824740409851 
model_pd.lagr.mean(): -1.3662678003311157 
model_pd.lambdas: dict_items([('pout', tensor([1.7190])), ('power', tensor([0.0858]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7678])), ('power', tensor([-1.1923]))])
epoch：1178	 i:0 	 global-step:23560	 l-p:0.05655692517757416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]])
 pt:tensor([[26.6994, 26.7031, 26.6995],
        [26.6994, 32.3227, 34.4706],
        [26.6994, 26.7402, 26.7024],
        [26.6994, 29.1364, 28.9209]], grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.05655486136674881 
model_pd.l_d.mean(): -1.422165870666504 
model_pd.lagr.mean(): -1.365610957145691 
model_pd.lambdas: dict_items([('pout', tensor([1.7183])), ('power', tensor([0.0858]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7681])), ('power', tensor([-1.1852]))])
epoch：1179	 i:0 	 global-step:23580	 l-p:0.05655486136674881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[26.7065, 32.3774, 34.5710],
        [26.7065, 29.9811, 30.2076],
        [26.7065, 26.7102, 26.7065],
        [26.7065, 31.8883, 33.6155]], grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.05655281990766525 
model_pd.l_d.mean(): -1.421507477760315 
model_pd.lagr.mean(): -1.3649547100067139 
model_pd.lambdas: dict_items([('pout', tensor([1.7175])), ('power', tensor([0.0857]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7685])), ('power', tensor([-1.1783]))])
epoch：1180	 i:0 	 global-step:23600	 l-p:0.05655281990766525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.7134, 26.7134, 26.7134],
        [26.7134, 26.7134, 26.7134],
        [26.7134, 28.2273, 27.7565],
        [26.7134, 26.7149, 26.7134]], grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.05655081197619438 
model_pd.l_d.mean(): -1.420849323272705 
model_pd.lagr.mean(): -1.3642984628677368 
model_pd.lambdas: dict_items([('pout', tensor([1.7167])), ('power', tensor([0.0856]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7688])), ('power', tensor([-1.1715]))])
epoch：1181	 i:0 	 global-step:23620	 l-p:0.05655081197619438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.7202, 29.1596, 28.9439],
        [26.7202, 28.5136, 28.0884],
        [26.7202, 27.0710, 26.8161],
        [26.7202, 26.9096, 26.7554]], grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.056548845022916794 
model_pd.l_d.mean(): -1.4201914072036743 
model_pd.lagr.mean(): -1.3636425733566284 
model_pd.lambdas: dict_items([('pout', tensor([1.7159])), ('power', tensor([0.0856]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7692])), ('power', tensor([-1.1648]))])
epoch：1182	 i:0 	 global-step:23640	 l-p:0.056548845022916794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.7269, 26.7373, 26.7273],
        [26.7269, 32.3967, 34.5863],
        [26.7269, 28.0437, 27.5596],
        [26.7269, 27.6589, 27.2016]], grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.05654691159725189 
model_pd.l_d.mean(): -1.4195338487625122 
model_pd.lagr.mean(): -1.3629869222640991 
model_pd.lambdas: dict_items([('pout', tensor([1.7152])), ('power', tensor([0.0855]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7695])), ('power', tensor([-1.1582]))])
epoch：1183	 i:0 	 global-step:23660	 l-p:0.05654691159725189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.7335, 26.7353, 26.7335],
        [26.7335, 26.7438, 26.7339],
        [26.7335, 26.7631, 26.7353],
        [26.7335, 29.7146, 29.7653]], grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.05654502287507057 
model_pd.l_d.mean(): -1.4188766479492188 
model_pd.lagr.mean(): -1.3623316287994385 
model_pd.lambdas: dict_items([('pout', tensor([1.7144])), ('power', tensor([0.0855]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7698])), ('power', tensor([-1.1517]))])
epoch：1184	 i:0 	 global-step:23680	 l-p:0.05654502287507057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[26.7400, 27.0911, 26.8360],
        [26.7400, 28.6154, 28.2085],
        [26.7400, 29.7219, 29.7727],
        [26.7400, 26.7415, 26.7400]], grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.05654316395521164 
model_pd.l_d.mean(): -1.4182193279266357 
model_pd.lagr.mean(): -1.3616762161254883 
model_pd.lambdas: dict_items([('pout', tensor([1.7136])), ('power', tensor([0.0854]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7701])), ('power', tensor([-1.1453]))])
epoch：1185	 i:0 	 global-step:23700	 l-p:0.05654316395521164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.7463, 27.1103, 26.8481],
        [26.7463, 32.3811, 34.5338],
        [26.7463, 28.6223, 28.2153],
        [26.7463, 27.4257, 27.0292]], grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.05654134601354599 
model_pd.l_d.mean(): -1.417562484741211 
model_pd.lagr.mean(): -1.3610211610794067 
model_pd.lambdas: dict_items([('pout', tensor([1.7129])), ('power', tensor([0.0853]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7705])), ('power', tensor([-1.1391]))])
epoch：1186	 i:0 	 global-step:23720	 l-p:0.05654134601354599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.7525, 29.6444, 29.6449],
        [26.7525, 26.7525, 26.7525],
        [26.7525, 36.2472, 42.8791],
        [26.7525, 29.7361, 29.7871]], grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.05653955787420273 
model_pd.l_d.mean(): -1.4169061183929443 
model_pd.lagr.mean(): -1.3603665828704834 
model_pd.lambdas: dict_items([('pout', tensor([1.7121])), ('power', tensor([0.0853]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7708])), ('power', tensor([-1.1330]))])
epoch：1187	 i:0 	 global-step:23740	 l-p:0.05653955787420273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.7586, 26.7586, 26.7586],
        [26.7586, 26.9484, 26.7938],
        [26.7586, 30.8486, 31.6414],
        [26.7586, 32.4363, 34.6291]], grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.05653780326247215 
model_pd.l_d.mean(): -1.4162498712539673 
model_pd.lagr.mean(): -1.3597121238708496 
model_pd.lambdas: dict_items([('pout', tensor([1.7113])), ('power', tensor([0.0852]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7711])), ('power', tensor([-1.1270]))])
epoch：1188	 i:0 	 global-step:23760	 l-p:0.05653780326247215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.7646, 27.4446, 27.0478],
        [26.7646, 30.0479, 30.2753],
        [26.7646, 27.7573, 27.2902],
        [26.7646, 26.7646, 26.7646]], grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.056536078453063965 
model_pd.l_d.mean(): -1.4155936241149902 
model_pd.lagr.mean(): -1.3590575456619263 
model_pd.lambdas: dict_items([('pout', tensor([1.7106])), ('power', tensor([0.0852]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7714])), ('power', tensor([-1.1211]))])
epoch：1189	 i:0 	 global-step:23780	 l-p:0.056536078453063965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.7705, 26.7707, 26.7705],
        [26.7705, 28.2884, 27.8165],
        [26.7705, 26.8001, 26.7723],
        [26.7705, 31.4081, 32.6442]], grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.05653439089655876 
model_pd.l_d.mean(): -1.4149376153945923 
model_pd.lagr.mean(): -1.358403205871582 
model_pd.lambdas: dict_items([('pout', tensor([1.7098])), ('power', tensor([0.0851]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7716])), ('power', tensor([-1.1153]))])
epoch：1190	 i:0 	 global-step:23800	 l-p:0.05653439089655876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.7763, 26.7801, 26.7764],
        [26.7763, 26.7787, 26.7764],
        [26.7763, 36.2806, 42.9194],
        [26.7763, 31.1667, 32.1959]], grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.05653272569179535 
model_pd.l_d.mean(): -1.4142818450927734 
model_pd.lagr.mean(): -1.357749104499817 
model_pd.lambdas: dict_items([('pout', tensor([1.7090])), ('power', tensor([0.0851]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7719])), ('power', tensor([-1.1096]))])
epoch：1191	 i:0 	 global-step:23820	 l-p:0.05653272569179535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[26.7820, 26.7838, 26.7820],
        [26.7820, 26.7836, 26.7820],
        [26.7820, 27.1447, 26.8831],
        [26.7820, 27.7164, 27.2579]], grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.05653109401464462 
model_pd.l_d.mean(): -1.4136261940002441 
model_pd.lagr.mean(): -1.3570951223373413 
model_pd.lambdas: dict_items([('pout', tensor([1.7082])), ('power', tensor([0.0850]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7722])), ('power', tensor([-1.1040]))])
epoch：1192	 i:0 	 global-step:23840	 l-p:0.05653109401464462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[26.7876, 26.7894, 26.7876],
        [26.7876, 26.7900, 26.7876],
        [26.7876, 27.1472, 26.8873],
        [26.7876, 31.4288, 32.6659]], grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.05652949959039688 
model_pd.l_d.mean(): -1.412971019744873 
model_pd.lagr.mean(): -1.3564414978027344 
model_pd.lambdas: dict_items([('pout', tensor([1.7075])), ('power', tensor([0.0850]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7725])), ('power', tensor([-1.0985]))])
epoch：1193	 i:0 	 global-step:23860	 l-p:0.05652949959039688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[26.7931, 29.6904, 29.6911],
        [26.7931, 26.7931, 26.7931],
        [26.7931, 32.4794, 34.6759],
        [26.7931, 30.0806, 30.3085]], grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.05652792006731033 
model_pd.l_d.mean(): -1.4123157262802124 
model_pd.lagr.mean(): -1.355787754058838 
model_pd.lambdas: dict_items([('pout', tensor([1.7067])), ('power', tensor([0.0849]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7728])), ('power', tensor([-1.0931]))])
epoch：1194	 i:0 	 global-step:23880	 l-p:0.05652792006731033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[26.7985, 32.4862, 34.6832],
        [26.7985, 26.7985, 26.7985],
        [26.7985, 26.7985, 26.7985],
        [26.7985, 31.1933, 32.2237]], grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.056526366621255875 
model_pd.l_d.mean(): -1.4116610288619995 
model_pd.lagr.mean(): -1.3551346063613892 
model_pd.lambdas: dict_items([('pout', tensor([1.7059])), ('power', tensor([0.0848]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7730])), ('power', tensor([-1.0878]))])
epoch：1195	 i:0 	 global-step:23900	 l-p:0.056526366621255875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[26.8038, 27.1561, 26.9002],
        [26.8038, 30.9022, 31.6969],
        [26.8038, 32.4530, 34.6116],
        [26.8038, 26.8039, 26.8038]], grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.056524842977523804 
model_pd.l_d.mean(): -1.4110060930252075 
model_pd.lagr.mean(): -1.3544812202453613 
model_pd.lambdas: dict_items([('pout', tensor([1.7051])), ('power', tensor([0.0848]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7733])), ('power', tensor([-1.0825]))])
epoch：1196	 i:0 	 global-step:23920	 l-p:0.056524842977523804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.8091, 27.0231, 26.8518],
        [26.8091, 34.7095, 39.3215],
        [26.8091, 27.8039, 27.3358],
        [26.8091, 29.8970, 30.0033]], grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.05652334541082382 
model_pd.l_d.mean(): -1.410351276397705 
model_pd.lagr.mean(): -1.353827953338623 
model_pd.lambdas: dict_items([('pout', tensor([1.7044])), ('power', tensor([0.0847]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7735])), ('power', tensor([-1.0774]))])
epoch：1197	 i:0 	 global-step:23940	 l-p:0.05652334541082382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.8142, 34.7164, 39.3295],
        [26.8142, 27.7501, 27.2909],
        [26.8142, 35.6658, 41.4495],
        [26.8142, 27.1794, 26.9164]], grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.05652187764644623 
model_pd.l_d.mean(): -1.4096968173980713 
model_pd.lagr.mean(): -1.3531749248504639 
model_pd.lambdas: dict_items([('pout', tensor([1.7036])), ('power', tensor([0.0847]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7738])), ('power', tensor([-1.0723]))])
epoch：1198	 i:0 	 global-step:23960	 l-p:0.05652187764644623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.8193, 26.8193, 26.8193],
        [26.8193, 32.4724, 34.6326],
        [26.8193, 32.5185, 34.7238],
        [26.8193, 27.1718, 26.9157]], grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.056520428508520126 
model_pd.l_d.mean(): -1.4090427160263062 
model_pd.lagr.mean(): -1.3525222539901733 
model_pd.lambdas: dict_items([('pout', tensor([1.7028])), ('power', tensor([0.0846]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7740])), ('power', tensor([-1.0673]))])
epoch：1199	 i:0 	 global-step:23980	 l-p:0.056520428508520126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.8243, 28.1472, 27.6610],
        [26.8243, 27.1877, 26.9256],
        [26.8243, 34.2396, 38.2799],
        [26.8243, 26.8540, 26.8261]], grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.05651901289820671 
model_pd.l_d.mean(): -1.4083887338638306 
model_pd.lagr.mean(): -1.3518697023391724 
model_pd.lambdas: dict_items([('pout', tensor([1.7021])), ('power', tensor([0.0846]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7743])), ('power', tensor([-1.0624]))])
epoch：1200	 i:0 	 global-step:24000	 l-p:0.05651901289820671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.8292, 27.0434, 26.8720],
        [26.8292, 32.4847, 34.6460],
        [26.8292, 28.1524, 27.6661],
        [26.8292, 30.9323, 31.7281]], grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.05651761218905449 
model_pd.l_d.mean(): -1.4077346324920654 
model_pd.lagr.mean(): -1.3512170314788818 
model_pd.lambdas: dict_items([('pout', tensor([1.7013])), ('power', tensor([0.0845]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7745])), ('power', tensor([-1.0576]))])
epoch：1201	 i:0 	 global-step:24020	 l-p:0.05651761218905449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.8340, 27.1944, 26.9339],
        [26.8340, 26.8340, 26.8340],
        [26.8340, 28.3565, 27.8833],
        [26.8340, 26.8340, 26.8340]], grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.05651623755693436 
model_pd.l_d.mean(): -1.4070807695388794 
model_pd.lagr.mean(): -1.3505644798278809 
model_pd.lambdas: dict_items([('pout', tensor([1.7005])), ('power', tensor([0.0845]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7748])), ('power', tensor([-1.0529]))])
epoch：1202	 i:0 	 global-step:24040	 l-p:0.05651623755693436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.8387, 27.6510, 27.2166],
        [26.8387, 32.4967, 34.6590],
        [26.8387, 26.8684, 26.8405],
        [26.8387, 27.2043, 26.9410]], grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.05651489272713661 
model_pd.l_d.mean(): -1.4064271450042725 
model_pd.lagr.mean(): -1.3499122858047485 
model_pd.lambdas: dict_items([('pout', tensor([1.6997])), ('power', tensor([0.0844]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7750])), ('power', tensor([-1.0482]))])
epoch：1203	 i:0 	 global-step:24060	 l-p:0.05651489272713661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.8434, 30.9492, 31.7456],
        [26.8434, 27.8399, 27.3710],
        [26.8434, 26.8471, 26.8434],
        [26.8434, 27.1963, 26.9399]], grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.05651356279850006 
model_pd.l_d.mean(): -1.4057735204696655 
model_pd.lagr.mean(): -1.3492599725723267 
model_pd.lambdas: dict_items([('pout', tensor([1.6990])), ('power', tensor([0.0844]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7752])), ('power', tensor([-1.0436]))])
epoch：1204	 i:0 	 global-step:24080	 l-p:0.05651356279850006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]])
 pt:tensor([[26.8479, 31.2526, 32.2858],
        [26.8479, 27.6606, 27.2260],
        [26.8479, 32.5083, 34.6716],
        [26.8479, 27.7854, 27.3255]], grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.0565122589468956 
model_pd.l_d.mean(): -1.4051201343536377 
model_pd.lagr.mean(): -1.3486078977584839 
model_pd.lambdas: dict_items([('pout', tensor([1.6982])), ('power', tensor([0.0843]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7754])), ('power', tensor([-1.0392]))])
epoch：1205	 i:0 	 global-step:24100	 l-p:0.0565122589468956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.8524, 27.2183, 26.9548],
        [26.8524, 26.8562, 26.8525],
        [26.8524, 27.0669, 26.8953],
        [26.8524, 34.7678, 39.3890]], grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.05651098117232323 
model_pd.l_d.mean(): -1.4044668674468994 
model_pd.lagr.mean(): -1.3479559421539307 
model_pd.lambdas: dict_items([('pout', tensor([1.6974])), ('power', tensor([0.0843]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7757])), ('power', tensor([-1.0347]))])
epoch：1206	 i:0 	 global-step:24120	 l-p:0.05651098117232323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[26.8568, 29.8550, 29.9067],
        [26.8568, 26.8980, 26.8599],
        [26.8568, 27.5400, 27.1413],
        [26.8568, 32.5195, 34.6837]], grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.05650972202420235 
model_pd.l_d.mean(): -1.4038136005401611 
model_pd.lagr.mean(): -1.347303867340088 
model_pd.lambdas: dict_items([('pout', tensor([1.6966])), ('power', tensor([0.0842]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7759])), ('power', tensor([-1.0304]))])
epoch：1207	 i:0 	 global-step:24140	 l-p:0.05650972202420235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.8612, 27.7993, 27.3390],
        [26.8612, 27.2272, 26.9636],
        [26.8612, 26.8617, 26.8612],
        [26.8612, 28.3858, 27.9120]], grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.056508488953113556 
model_pd.l_d.mean(): -1.403160572052002 
model_pd.lagr.mean(): -1.3466520309448242 
model_pd.lambdas: dict_items([('pout', tensor([1.6959])), ('power', tensor([0.0842]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7761])), ('power', tensor([-1.0261]))])
epoch：1208	 i:0 	 global-step:24160	 l-p:0.056508488953113556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.8654, 27.2316, 26.9678],
        [26.8654, 34.4413, 38.6596],
        [26.8654, 27.2264, 26.9655],
        [26.8654, 30.9755, 31.7729]], grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.056507278233766556 
model_pd.l_d.mean(): -1.4025076627731323 
model_pd.lagr.mean(): -1.3460004329681396 
model_pd.lambdas: dict_items([('pout', tensor([1.6951])), ('power', tensor([0.0841]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7763])), ('power', tensor([-1.0219]))])
epoch：1209	 i:0 	 global-step:24180	 l-p:0.056507278233766556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.8696, 26.8994, 26.8714],
        [26.8696, 26.9108, 26.8726],
        [26.8696, 26.9232, 26.8742],
        [26.8696, 27.5532, 27.1543]], grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.05650608241558075 
model_pd.l_d.mean(): -1.4018547534942627 
model_pd.lagr.mean(): -1.3453487157821655 
model_pd.lambdas: dict_items([('pout', tensor([1.6943])), ('power', tensor([0.0841]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7765])), ('power', tensor([-1.0178]))])
epoch：1210	 i:0 	 global-step:24200	 l-p:0.05650608241558075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.8737, 26.8737, 26.8737],
        [26.8737, 30.9854, 31.7832],
        [26.8737, 27.2381, 26.9753],
        [26.8737, 29.5919, 29.4950]], grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.05650491267442703 
model_pd.l_d.mean(): -1.4012019634246826 
model_pd.lagr.mean(): -1.3446969985961914 
model_pd.lambdas: dict_items([('pout', tensor([1.6935])), ('power', tensor([0.0840]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7767])), ('power', tensor([-1.0138]))])
epoch：1211	 i:0 	 global-step:24220	 l-p:0.05650491267442703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[26.8778, 27.8761, 27.4064],
        [26.8778, 26.8882, 26.8781],
        [26.8778, 26.8779, 26.8778],
        [26.8778, 30.9902, 31.7882]], grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.05650375783443451 
model_pd.l_d.mean(): -1.4005495309829712 
model_pd.lagr.mean(): -1.3440457582473755 
model_pd.lambdas: dict_items([('pout', tensor([1.6927])), ('power', tensor([0.0840]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7769])), ('power', tensor([-1.0098]))])
epoch：1212	 i:0 	 global-step:24240	 l-p:0.05650375783443451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.8818, 26.8823, 26.8818],
        [26.8818, 29.7913, 29.7925],
        [26.8818, 26.9354, 26.8864],
        [26.8818, 28.6891, 28.2610]], grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.056502629071474075 
model_pd.l_d.mean(): -1.3998970985412598 
model_pd.lagr.mean(): -1.3433945178985596 
model_pd.lambdas: dict_items([('pout', tensor([1.6920])), ('power', tensor([0.0839]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7771])), ('power', tensor([-1.0058]))])
epoch：1213	 i:0 	 global-step:24260	 l-p:0.056502629071474075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.8857, 26.8858, 26.8857],
        [26.8857, 27.2503, 26.9873],
        [26.8857, 31.5478, 32.7914],
        [26.8857, 26.8857, 26.8857]], grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.05650150775909424 
model_pd.l_d.mean(): -1.3992446660995483 
model_pd.lagr.mean(): -1.342743158340454 
model_pd.lambdas: dict_items([('pout', tensor([1.6912])), ('power', tensor([0.0839]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7773])), ('power', tensor([-1.0020]))])
epoch：1214	 i:0 	 global-step:24280	 l-p:0.05650150775909424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[26.8895, 32.1141, 33.8572],
        [26.8895, 31.0043, 31.8029],
        [26.8895, 26.8896, 26.8895],
        [26.8895, 34.8179, 39.4473]], grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.05650041252374649 
model_pd.l_d.mean(): -1.3985925912857056 
model_pd.lagr.mean(): -1.3420921564102173 
model_pd.lambdas: dict_items([('pout', tensor([1.6904])), ('power', tensor([0.0838]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7775])), ('power', tensor([-0.9982]))])
epoch：1215	 i:0 	 global-step:24300	 l-p:0.05650041252374649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.8933, 34.3315, 38.3851],
        [26.8933, 32.6118, 34.8254],
        [26.8933, 27.0844, 26.9288],
        [26.8933, 27.2549, 26.9936]], grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.056499332189559937 
model_pd.l_d.mean(): -1.3979401588439941 
model_pd.lagr.mean(): -1.3414407968521118 
model_pd.lambdas: dict_items([('pout', tensor([1.6896])), ('power', tensor([0.0838]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7777])), ('power', tensor([-0.9945]))])
epoch：1216	 i:0 	 global-step:24320	 l-p:0.056499332189559937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[26.8971, 29.3570, 29.1403],
        [26.8971, 34.3365, 38.3908],
        [26.8971, 29.8088, 29.8101],
        [26.8971, 27.2510, 26.9939]], grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.05649827420711517 
model_pd.l_d.mean(): -1.3972880840301514 
model_pd.lagr.mean(): -1.340789794921875 
model_pd.lambdas: dict_items([('pout', tensor([1.6889])), ('power', tensor([0.0837]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7779])), ('power', tensor([-0.9908]))])
epoch：1217	 i:0 	 global-step:24340	 l-p:0.05649827420711517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[26.9008, 32.1281, 33.8722],
        [26.9008, 26.9023, 26.9008],
        [26.9008, 26.9008, 26.9008],
        [26.9008, 36.4561, 43.1324]], grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.056497227400541306 
model_pd.l_d.mean(): -1.3966357707977295 
model_pd.lagr.mean(): -1.340138554573059 
model_pd.lambdas: dict_items([('pout', tensor([1.6881])), ('power', tensor([0.0837]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7780])), ('power', tensor([-0.9871]))])
epoch：1218	 i:0 	 global-step:24360	 l-p:0.056497227400541306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[26.9044, 26.9148, 26.9048],
        [26.9044, 27.9041, 27.4338],
        [26.9044, 26.9044, 26.9044],
        [26.9044, 32.6258, 34.8406]], grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.05649619922041893 
model_pd.l_d.mean(): -1.3959840536117554 
model_pd.lagr.mean(): -1.339487910270691 
model_pd.lambdas: dict_items([('pout', tensor([1.6873])), ('power', tensor([0.0836]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7782])), ('power', tensor([-0.9836]))])
epoch：1219	 i:0 	 global-step:24380	 l-p:0.05649619922041893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[26.9080, 26.9080, 26.9080],
        [26.9080, 26.9080, 26.9080],
        [26.9080, 28.0907, 27.6040],
        [26.9080, 28.4362, 27.9615]], grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.05649518594145775 
model_pd.l_d.mean(): -1.3953322172164917 
model_pd.lagr.mean(): -1.3388370275497437 
model_pd.lambdas: dict_items([('pout', tensor([1.6865])), ('power', tensor([0.0836]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7784])), ('power', tensor([-0.9801]))])
epoch：1220	 i:0 	 global-step:24400	 l-p:0.05649518594145775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.9115, 26.9153, 26.9115],
        [26.9115, 36.4713, 43.1510],
        [26.9115, 27.2785, 27.0142],
        [26.9115, 28.2403, 27.7521]], grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.05649418756365776 
model_pd.l_d.mean(): -1.3946802616119385 
model_pd.lagr.mean(): -1.3381860256195068 
model_pd.lambdas: dict_items([('pout', tensor([1.6857])), ('power', tensor([0.0835]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7786])), ('power', tensor([-0.9766]))])
epoch：1221	 i:0 	 global-step:24420	 l-p:0.05649418756365776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[26.9149, 31.5836, 32.8292],
        [26.9149, 31.0348, 31.8346],
        [26.9149, 28.8065, 28.3966],
        [26.9149, 26.9150, 26.9149]], grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.05649320036172867 
model_pd.l_d.mean(): -1.3940287828445435 
model_pd.lagr.mean(): -1.3375356197357178 
model_pd.lambdas: dict_items([('pout', tensor([1.6850])), ('power', tensor([0.0835]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7787])), ('power', tensor([-0.9732]))])
epoch：1222	 i:0 	 global-step:24440	 l-p:0.05649320036172867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[26.9184, 26.9185, 26.9184],
        [26.9184, 27.8593, 27.3978],
        [26.9184, 30.2256, 30.4557],
        [26.9184, 26.9184, 26.9184]], grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.05649223178625107 
model_pd.l_d.mean(): -1.3933768272399902 
model_pd.lagr.mean(): -1.336884617805481 
model_pd.lambdas: dict_items([('pout', tensor([1.6842])), ('power', tensor([0.0834]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7789])), ('power', tensor([-0.9698]))])
epoch：1223	 i:0 	 global-step:24460	 l-p:0.05649223178625107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[26.9217, 26.9217, 26.9217],
        [26.9217, 29.3847, 29.1679],
        [26.9217, 27.2762, 27.0187],
        [26.9217, 27.7376, 27.3014]], grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.05649128556251526 
model_pd.l_d.mean(): -1.3927257061004639 
model_pd.lagr.mean(): -1.336234450340271 
model_pd.lambdas: dict_items([('pout', tensor([1.6834])), ('power', tensor([0.0834]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7791])), ('power', tensor([-0.9665]))])
epoch：1224	 i:0 	 global-step:24480	 l-p:0.05649128556251526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.9250, 27.8663, 27.4046],
        [26.9250, 26.9788, 26.9297],
        [26.9250, 36.4907, 43.1746],
        [26.9250, 28.9102, 28.5245]], grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.05649033933877945 
model_pd.l_d.mean(): -1.3920738697052002 
model_pd.lagr.mean(): -1.3355835676193237 
model_pd.lambdas: dict_items([('pout', tensor([1.6826])), ('power', tensor([0.0833]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7792])), ('power', tensor([-0.9632]))])
epoch：1225	 i:0 	 global-step:24500	 l-p:0.05649033933877945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[26.9283, 28.8211, 28.4111],
        [26.9283, 26.9301, 26.9283],
        [26.9283, 28.9138, 28.5281],
        [26.9283, 27.2957, 27.0311]], grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.05648941919207573 
model_pd.l_d.mean(): -1.3914225101470947 
model_pd.lagr.mean(): -1.3349330425262451 
model_pd.lambdas: dict_items([('pout', tensor([1.6818])), ('power', tensor([0.0833]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7794])), ('power', tensor([-0.9600]))])
epoch：1226	 i:0 	 global-step:24520	 l-p:0.05648941919207573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[26.9315, 34.5300, 38.7620],
        [26.9315, 34.8750, 39.5138],
        [26.9315, 27.2938, 27.0320],
        [26.9315, 32.6540, 34.8659]], grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.0564885139465332 
model_pd.l_d.mean(): -1.3907710313796997 
model_pd.lagr.mean(): -1.3342825174331665 
model_pd.lambdas: dict_items([('pout', tensor([1.6811])), ('power', tensor([0.0832]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7796])), ('power', tensor([-0.9569]))])
epoch：1227	 i:0 	 global-step:24540	 l-p:0.0564885139465332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[26.9347, 27.7511, 27.3146],
        [26.9347, 26.9352, 26.9347],
        [26.9347, 26.9347, 26.9347],
        [26.9347, 29.9444, 29.9969]], grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.056487612426280975 
model_pd.l_d.mean(): -1.390120029449463 
model_pd.lagr.mean(): -1.333632469177246 
model_pd.lambdas: dict_items([('pout', tensor([1.6803])), ('power', tensor([0.0832]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7797])), ('power', tensor([-0.9538]))])
epoch：1228	 i:0 	 global-step:24560	 l-p:0.056487612426280975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[26.9378, 26.9383, 26.9378],
        [26.9378, 26.9915, 26.9424],
        [26.9378, 34.8836, 39.5238],
        [26.9378, 30.0451, 30.1530]], grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.05648672953248024 
model_pd.l_d.mean(): -1.3894686698913574 
model_pd.lagr.mean(): -1.3329819440841675 
model_pd.lambdas: dict_items([('pout', tensor([1.6795])), ('power', tensor([0.0831]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7799])), ('power', tensor([-0.9507]))])
epoch：1229	 i:0 	 global-step:24580	 l-p:0.05648672953248024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.9408, 36.5133, 43.2023],
        [26.9408, 29.8590, 29.8607],
        [26.9408, 26.9408, 26.9409],
        [26.9408, 28.9277, 28.5417]], grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.056485868990421295 
model_pd.l_d.mean(): -1.3888176679611206 
model_pd.lagr.mean(): -1.3323317766189575 
model_pd.lambdas: dict_items([('pout', tensor([1.6787])), ('power', tensor([0.0831]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7800])), ('power', tensor([-0.9477]))])
epoch：1230	 i:0 	 global-step:24600	 l-p:0.056485868990421295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.9439, 27.0561, 26.9589],
        [26.9439, 34.3993, 38.4630],
        [26.9439, 28.1289, 27.6413],
        [26.9439, 26.9738, 26.9457]], grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.05648501217365265 
model_pd.l_d.mean(): -1.3881665468215942 
model_pd.lagr.mean(): -1.331681489944458 
model_pd.lambdas: dict_items([('pout', tensor([1.6779])), ('power', tensor([0.0830]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7802])), ('power', tensor([-0.9447]))])
epoch：1231	 i:0 	 global-step:24620	 l-p:0.05648501217365265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[26.9468, 26.9506, 26.9469],
        [26.9468, 27.3095, 27.0474],
        [26.9468, 26.9573, 26.9472],
        [26.9468, 29.8659, 29.8677]], grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.0564841702580452 
model_pd.l_d.mean(): -1.3875154256820679 
model_pd.lagr.mean(): -1.3310312032699585 
model_pd.lambdas: dict_items([('pout', tensor([1.6772])), ('power', tensor([0.0830]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7803])), ('power', tensor([-0.9418]))])
epoch：1232	 i:0 	 global-step:24640	 l-p:0.0564841702580452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.9498, 32.6836, 34.9038],
        [26.9498, 26.9797, 26.9516],
        [26.9498, 35.8545, 41.6750],
        [26.9498, 27.3124, 27.0503]], grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.05648334324359894 
model_pd.l_d.mean(): -1.3868643045425415 
model_pd.lagr.mean(): -1.330380916595459 
model_pd.lambdas: dict_items([('pout', tensor([1.6764])), ('power', tensor([0.0829]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7805])), ('power', tensor([-0.9389]))])
epoch：1233	 i:0 	 global-step:24660	 l-p:0.05648334324359894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[26.9526, 26.9526, 26.9526],
        [26.9526, 28.8480, 28.4375],
        [26.9526, 30.2657, 30.4965],
        [26.9526, 27.0650, 26.9676]], grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.05648253113031387 
model_pd.l_d.mean(): -1.3862133026123047 
model_pd.lagr.mean(): -1.329730749130249 
model_pd.lambdas: dict_items([('pout', tensor([1.6756])), ('power', tensor([0.0829]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7806])), ('power', tensor([-0.9361]))])
epoch：1234	 i:0 	 global-step:24680	 l-p:0.05648253113031387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.9555, 28.7698, 28.3404],
        [26.9555, 27.7728, 27.3359],
        [26.9555, 31.0836, 31.8855],
        [26.9555, 26.9554, 26.9555]], grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.05648173391819 
model_pd.l_d.mean(): -1.3855624198913574 
model_pd.lagr.mean(): -1.3290807008743286 
model_pd.lambdas: dict_items([('pout', tensor([1.6748])), ('power', tensor([0.0828]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7807])), ('power', tensor([-0.9333]))])
epoch：1235	 i:0 	 global-step:24700	 l-p:0.05648173391819
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.9582, 27.3211, 27.0589],
        [26.9582, 34.9116, 39.5566],
        [26.9582, 29.8791, 29.8809],
        [26.9582, 27.1500, 26.9938]], grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.056480951607227325 
model_pd.l_d.mean(): -1.3849116563796997 
model_pd.lagr.mean(): -1.3284306526184082 
model_pd.lambdas: dict_items([('pout', tensor([1.6740])), ('power', tensor([0.0828]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7809])), ('power', tensor([-0.9305]))])
epoch：1236	 i:0 	 global-step:24720	 l-p:0.056480951607227325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[26.9610, 27.9638, 27.4922],
        [26.9610, 28.7759, 28.3464],
        [26.9610, 27.7786, 27.3415],
        [26.9610, 30.0720, 30.1803]], grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.05648016929626465 
model_pd.l_d.mean(): -1.384260654449463 
model_pd.lagr.mean(): -1.3277804851531982 
model_pd.lambdas: dict_items([('pout', tensor([1.6733])), ('power', tensor([0.0828]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7810])), ('power', tensor([-0.9278]))])
epoch：1237	 i:0 	 global-step:24740	 l-p:0.05648016929626465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[26.9637, 30.0752, 30.1835],
        [26.9637, 30.2786, 30.5097],
        [26.9637, 31.6436, 32.8928],
        [26.9637, 26.9675, 26.9637]], grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.056479405611753464 
model_pd.l_d.mean(): -1.3836102485656738 
model_pd.lagr.mean(): -1.3271307945251465 
model_pd.lambdas: dict_items([('pout', tensor([1.6725])), ('power', tensor([0.0827]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7811])), ('power', tensor([-0.9252]))])
epoch：1238	 i:0 	 global-step:24760	 l-p:0.056479405611753464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[26.9663, 26.9679, 26.9663],
        [26.9663, 27.3345, 27.0693],
        [26.9663, 34.9227, 39.5697],
        [26.9663, 28.2992, 27.8096]], grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.05647866427898407 
model_pd.l_d.mean(): -1.3829596042633057 
model_pd.lagr.mean(): -1.3264809846878052 
model_pd.lambdas: dict_items([('pout', tensor([1.6717])), ('power', tensor([0.0827]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7813])), ('power', tensor([-0.9226]))])
epoch：1239	 i:0 	 global-step:24780	 l-p:0.05647866427898407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[26.9689, 28.5023, 28.0262],
        [26.9689, 27.1848, 27.0121],
        [26.9689, 29.7004, 29.6037],
        [26.9689, 28.9588, 28.5724]], grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.056477922946214676 
model_pd.l_d.mean(): -1.3823089599609375 
model_pd.lagr.mean(): -1.3258310556411743 
model_pd.lambdas: dict_items([('pout', tensor([1.6709])), ('power', tensor([0.0826]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7814])), ('power', tensor([-0.9200]))])
epoch：1240	 i:0 	 global-step:24800	 l-p:0.056477922946214676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.9715, 26.9715, 26.9715],
        [26.9715, 31.1031, 31.9058],
        [26.9715, 28.8688, 28.4580],
        [26.9715, 27.1634, 27.0071]], grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.05647720769047737 
model_pd.l_d.mean(): -1.3816581964492798 
model_pd.lagr.mean(): -1.325181007385254 
model_pd.lambdas: dict_items([('pout', tensor([1.6701])), ('power', tensor([0.0826]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7815])), ('power', tensor([-0.9175]))])
epoch：1241	 i:0 	 global-step:24820	 l-p:0.05647720769047737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.9740, 27.3295, 27.0712],
        [26.9740, 29.8973, 29.8993],
        [26.9740, 27.9776, 27.5056],
        [26.9740, 27.6616, 27.2604]], grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.05647648870944977 
model_pd.l_d.mean(): -1.3810076713562012 
model_pd.lagr.mean(): -1.3245311975479126 
model_pd.lambdas: dict_items([('pout', tensor([1.6694])), ('power', tensor([0.0825]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7816])), ('power', tensor([-0.9150]))])
epoch：1242	 i:0 	 global-step:24840	 l-p:0.05647648870944977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[26.9765, 28.8743, 28.4634],
        [26.9765, 28.9672, 28.5807],
        [26.9765, 28.3101, 27.8203],
        [26.9765, 29.9929, 30.0459]], grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.056475795805454254 
model_pd.l_d.mean(): -1.3803571462631226 
model_pd.lagr.mean(): -1.3238813877105713 
model_pd.lambdas: dict_items([('pout', tensor([1.6686])), ('power', tensor([0.0825]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7818])), ('power', tensor([-0.9126]))])
epoch：1243	 i:0 	 global-step:24860	 l-p:0.056475795805454254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[26.9789, 27.1949, 27.0221],
        [26.9789, 26.9789, 26.9789],
        [26.9789, 35.8957, 41.7247],
        [26.9789, 28.1663, 27.6779]], grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.05647511035203934 
model_pd.l_d.mean(): -1.379706859588623 
model_pd.lagr.mean(): -1.3232316970825195 
model_pd.lambdas: dict_items([('pout', tensor([1.6678])), ('power', tensor([0.0824]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7819])), ('power', tensor([-0.9102]))])
epoch：1244	 i:0 	 global-step:24880	 l-p:0.05647511035203934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[26.9813, 26.9814, 26.9813],
        [26.9813, 29.4522, 29.2351],
        [26.9813, 27.3447, 27.0821],
        [26.9813, 27.0112, 26.9831]], grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.05647442862391472 
model_pd.l_d.mean(): -1.3790559768676758 
model_pd.lagr.mean(): -1.3225815296173096 
model_pd.lambdas: dict_items([('pout', tensor([1.6670])), ('power', tensor([0.0824]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7820])), ('power', tensor([-0.9078]))])
epoch：1245	 i:0 	 global-step:24900	 l-p:0.05647442862391472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.9836, 26.9855, 26.9837],
        [26.9836, 26.9836, 26.9837],
        [26.9836, 27.3523, 27.0868],
        [26.9836, 26.9842, 26.9836]], grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.05647376552224159 
model_pd.l_d.mean(): -1.3784056901931763 
model_pd.lagr.mean(): -1.3219319581985474 
model_pd.lambdas: dict_items([('pout', tensor([1.6662])), ('power', tensor([0.0823]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7821])), ('power', tensor([-0.9055]))])
epoch：1246	 i:0 	 global-step:24920	 l-p:0.05647376552224159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[26.9859, 27.3546, 27.0891],
        [26.9859, 34.4564, 38.5291],
        [26.9859, 26.9884, 26.9860],
        [26.9859, 26.9859, 26.9860]], grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.05647311359643936 
model_pd.l_d.mean(): -1.3777551651000977 
model_pd.lagr.mean(): -1.3212820291519165 
model_pd.lambdas: dict_items([('pout', tensor([1.6654])), ('power', tensor([0.0823]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7822])), ('power', tensor([-0.9032]))])
epoch：1247	 i:0 	 global-step:24940	 l-p:0.05647311359643936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[26.9882, 30.1039, 30.2126],
        [26.9882, 27.0182, 26.9900],
        [26.9882, 28.1763, 27.6876],
        [26.9882, 27.0422, 26.9929]], grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.056472472846508026 
model_pd.l_d.mean(): -1.3771048784255981 
model_pd.lagr.mean(): -1.3206324577331543 
model_pd.lambdas: dict_items([('pout', tensor([1.6647])), ('power', tensor([0.0823]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7823])), ('power', tensor([-0.9010]))])
epoch：1248	 i:0 	 global-step:24960	 l-p:0.056472472846508026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[26.9905, 28.3252, 27.8351],
        [26.9905, 34.4625, 38.5363],
        [26.9905, 32.6895, 34.8694],
        [26.9905, 27.1826, 27.0261]], grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.05647183582186699 
model_pd.l_d.mean(): -1.3764548301696777 
model_pd.lagr.mean(): -1.3199830055236816 
model_pd.lambdas: dict_items([('pout', tensor([1.6639])), ('power', tensor([0.0822]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7825])), ('power', tensor([-0.8988]))])
epoch：1249	 i:0 	 global-step:24980	 l-p:0.05647183582186699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[26.9927, 27.3596, 27.0950],
        [26.9927, 26.9945, 26.9927],
        [26.9927, 30.0118, 30.0650],
        [26.9927, 27.6810, 27.2795]], grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.056471217423677444 
model_pd.l_d.mean(): -1.3758043050765991 
model_pd.lagr.mean(): -1.3193330764770508 
model_pd.lambdas: dict_items([('pout', tensor([1.6631])), ('power', tensor([0.0822]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7826])), ('power', tensor([-0.8966]))])
epoch：1250	 i:0 	 global-step:25000	 l-p:0.056471217423677444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[26.9948, 27.0488, 26.9995],
        [26.9948, 27.0248, 26.9966],
        [26.9948, 32.6951, 34.8757],
        [26.9948, 26.9953, 26.9948]], grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.056470613926649094 
model_pd.l_d.mean(): -1.3751541376113892 
model_pd.lagr.mean(): -1.3186835050582886 
model_pd.lambdas: dict_items([('pout', tensor([1.6623])), ('power', tensor([0.0821]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7827])), ('power', tensor([-0.8945]))])
epoch：1251	 i:0 	 global-step:25020	 l-p:0.056470613926649094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[26.9969, 26.9970, 26.9969],
        [26.9969, 30.1141, 30.2230],
        [26.9969, 26.9969, 26.9969],
        [26.9969, 27.0074, 26.9973]], grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.05647001415491104 
model_pd.l_d.mean(): -1.3745037317276 
model_pd.lagr.mean(): -1.3180336952209473 
model_pd.lambdas: dict_items([('pout', tensor([1.6615])), ('power', tensor([0.0821]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7828])), ('power', tensor([-0.8924]))])
epoch：1252	 i:0 	 global-step:25040	 l-p:0.05647001415491104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[26.9990, 31.6876, 32.9397],
        [26.9990, 28.8994, 28.4881],
        [26.9990, 26.9990, 26.9990],
        [26.9990, 26.9990, 26.9990]], grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.056469421833753586 
model_pd.l_d.mean(): -1.3738536834716797 
model_pd.lagr.mean(): -1.3173842430114746 
model_pd.lambdas: dict_items([('pout', tensor([1.6608])), ('power', tensor([0.0820]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7829])), ('power', tensor([-0.8903]))])
epoch：1253	 i:0 	 global-step:25060	 l-p:0.056469421833753586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[27.0011, 27.0116, 27.0014],
        [27.0011, 27.3701, 27.1043],
        [27.0011, 27.0426, 27.0041],
        [27.0011, 29.9288, 29.9312]], grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.056468844413757324 
model_pd.l_d.mean(): -1.3732037544250488 
model_pd.lagr.mean(): -1.3167349100112915 
model_pd.lambdas: dict_items([('pout', tensor([1.6600])), ('power', tensor([0.0820]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7830])), ('power', tensor([-0.8883]))])
epoch：1254	 i:0 	 global-step:25080	 l-p:0.056468844413757324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[27.0031, 30.0240, 30.0774],
        [27.0031, 32.7059, 34.8875],
        [27.0031, 28.0085, 27.5357],
        [27.0031, 27.3593, 27.1005]], grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.05646827816963196 
model_pd.l_d.mean(): -1.3725535869598389 
model_pd.lagr.mean(): -1.3160853385925293 
model_pd.lambdas: dict_items([('pout', tensor([1.6592])), ('power', tensor([0.0819]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7831])), ('power', tensor([-0.8863]))])
epoch：1255	 i:0 	 global-step:25100	 l-p:0.05646827816963196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[27.0051, 27.3690, 27.1060],
        [27.0051, 27.0466, 27.0081],
        [27.0051, 32.7549, 34.9824],
        [27.0051, 31.4442, 32.4874]], grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.05646771937608719 
model_pd.l_d.mean(): -1.3719035387039185 
model_pd.lagr.mean(): -1.315435767173767 
model_pd.lambdas: dict_items([('pout', tensor([1.6584])), ('power', tensor([0.0819]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7832])), ('power', tensor([-0.8844]))])
epoch：1256	 i:0 	 global-step:25120	 l-p:0.05646771937608719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[27.0070, 28.3431, 27.8526],
        [27.0070, 27.3633, 27.1045],
        [27.0070, 27.2234, 27.0503],
        [27.0070, 28.1965, 27.7073]], grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.056467171758413315 
model_pd.l_d.mean(): -1.371253252029419 
model_pd.lagr.mean(): -1.3147860765457153 
model_pd.lambdas: dict_items([('pout', tensor([1.6576])), ('power', tensor([0.0819]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7833])), ('power', tensor([-0.8825]))])
epoch：1257	 i:0 	 global-step:25140	 l-p:0.056467171758413315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228]])
 pt:tensor([[27.0089, 30.3322, 30.5645],
        [27.0089, 29.0035, 28.6165],
        [27.0089, 27.6980, 27.2961],
        [27.0089, 34.4879, 38.5660]], grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.05646663159132004 
model_pd.l_d.mean(): -1.370603322982788 
model_pd.lagr.mean(): -1.3141367435455322 
model_pd.lambdas: dict_items([('pout', tensor([1.6568])), ('power', tensor([0.0818]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7834])), ('power', tensor([-0.8806]))])
epoch：1258	 i:0 	 global-step:25160	 l-p:0.05646663159132004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[27.0108, 34.4905, 38.5690],
        [27.0108, 34.9845, 39.6426],
        [27.0108, 27.0108, 27.0108],
        [27.0108, 28.9125, 28.5010]], grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.056466102600097656 
model_pd.l_d.mean(): -1.3699530363082886 
model_pd.lagr.mean(): -1.313486933708191 
model_pd.lambdas: dict_items([('pout', tensor([1.6561])), ('power', tensor([0.0818]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7835])), ('power', tensor([-0.8787]))])
epoch：1259	 i:0 	 global-step:25180	 l-p:0.056466102600097656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[27.0127, 29.9424, 29.9449],
        [27.0127, 27.9588, 27.4949],
        [27.0127, 28.5501, 28.0729],
        [27.0127, 35.9440, 41.7835]], grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.05646557733416557 
model_pd.l_d.mean(): -1.3693032264709473 
model_pd.lagr.mean(): -1.3128376007080078 
model_pd.lambdas: dict_items([('pout', tensor([1.6553])), ('power', tensor([0.0817]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7836])), ('power', tensor([-0.8769]))])
epoch：1260	 i:0 	 global-step:25200	 l-p:0.05646557733416557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.0145, 27.3787, 27.1155],
        [27.0145, 27.0145, 27.0145],
        [27.0145, 31.7071, 32.9606],
        [27.0145, 27.0169, 27.0145]], grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.056465063244104385 
model_pd.l_d.mean(): -1.368653416633606 
model_pd.lagr.mean(): -1.3121883869171143 
model_pd.lambdas: dict_items([('pout', tensor([1.6545])), ('power', tensor([0.0817]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7836])), ('power', tensor([-0.8751]))])
epoch：1261	 i:0 	 global-step:25220	 l-p:0.056465063244104385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[27.0163, 32.7231, 34.9066],
        [27.0163, 30.3410, 30.5735],
        [27.0163, 31.4582, 32.5023],
        [27.0163, 35.9492, 41.7899]], grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.05646456405520439 
model_pd.l_d.mean(): -1.368003010749817 
model_pd.lagr.mean(): -1.3115384578704834 
model_pd.lambdas: dict_items([('pout', tensor([1.6537])), ('power', tensor([0.0816]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7837])), ('power', tensor([-0.8733]))])
epoch：1262	 i:0 	 global-step:25240	 l-p:0.05646456405520439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[27.0180, 27.8386, 27.4000],
        [27.0180, 27.1308, 27.0331],
        [27.0180, 30.0417, 30.0953],
        [27.0180, 28.9206, 28.5090]], grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.0564640611410141 
model_pd.l_d.mean(): -1.3673534393310547 
model_pd.lagr.mean(): -1.3108893632888794 
model_pd.lambdas: dict_items([('pout', tensor([1.6529])), ('power', tensor([0.0816]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7838])), ('power', tensor([-0.8716]))])
epoch：1263	 i:0 	 global-step:25260	 l-p:0.0564640611410141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]])
 pt:tensor([[27.0197, 30.3452, 30.5778],
        [27.0197, 28.8410, 28.4104],
        [27.0197, 30.1411, 30.2505],
        [27.0197, 34.9971, 39.6576]], grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.0564635805785656 
model_pd.l_d.mean(): -1.3667033910751343 
model_pd.lagr.mean(): -1.3102397918701172 
model_pd.lambdas: dict_items([('pout', tensor([1.6521])), ('power', tensor([0.0815]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7839])), ('power', tensor([-0.8699]))])
epoch：1264	 i:0 	 global-step:25280	 l-p:0.0564635805785656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[27.0214, 32.7701, 34.9938],
        [27.0214, 28.5597, 28.0824],
        [27.0214, 36.6302, 43.3468],
        [27.0214, 27.2381, 27.0648]], grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.056463103741407394 
model_pd.l_d.mean(): -1.3660534620285034 
model_pd.lagr.mean(): -1.3095903396606445 
model_pd.lambdas: dict_items([('pout', tensor([1.6513])), ('power', tensor([0.0815]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7840])), ('power', tensor([-0.8682]))])
epoch：1265	 i:0 	 global-step:25300	 l-p:0.056463103741407394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.0231, 27.0532, 27.0249],
        [27.0231, 27.3798, 27.1207],
        [27.0231, 27.0647, 27.0262],
        [27.0231, 30.0477, 30.1014]], grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.05646262690424919 
model_pd.l_d.mean(): -1.3654035329818726 
model_pd.lagr.mean(): -1.3089408874511719 
model_pd.lambdas: dict_items([('pout', tensor([1.6506])), ('power', tensor([0.0815]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7841])), ('power', tensor([-0.8666]))])
epoch：1266	 i:0 	 global-step:25320	 l-p:0.05646262690424919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[27.0248, 36.6351, 43.3529],
        [27.0248, 27.0248, 27.0248],
        [27.0248, 27.2415, 27.0681],
        [27.0248, 28.2156, 27.7259]], grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.056462157517671585 
model_pd.l_d.mean(): -1.3647534847259521 
model_pd.lagr.mean(): -1.3082913160324097 
model_pd.lambdas: dict_items([('pout', tensor([1.6498])), ('power', tensor([0.0814]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7842])), ('power', tensor([-0.8649]))])
epoch：1267	 i:0 	 global-step:25340	 l-p:0.056462157517671585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.0264, 27.0282, 27.0264],
        [27.0264, 29.5039, 29.2867],
        [27.0264, 31.7222, 32.9768],
        [27.0264, 27.0679, 27.0294]], grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.05646171048283577 
model_pd.l_d.mean(): -1.3641040325164795 
model_pd.lagr.mean(): -1.3076423406600952 
model_pd.lambdas: dict_items([('pout', tensor([1.6490])), ('power', tensor([0.0814]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7842])), ('power', tensor([-0.8633]))])
epoch：1268	 i:0 	 global-step:25360	 l-p:0.05646171048283577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[27.0279, 27.0318, 27.0280],
        [27.0279, 30.1509, 30.2605],
        [27.0279, 27.0280, 27.0279],
        [27.0279, 27.1409, 27.0430]], grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.056461263447999954 
model_pd.l_d.mean(): -1.3634541034698486 
model_pd.lagr.mean(): -1.3069928884506226 
model_pd.lambdas: dict_items([('pout', tensor([1.6482])), ('power', tensor([0.0813]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7843])), ('power', tensor([-0.8618]))])
epoch：1269	 i:0 	 global-step:25380	 l-p:0.056461263447999954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[27.0295, 27.3974, 27.1321],
        [27.0295, 27.0295, 27.0295],
        [27.0295, 27.9768, 27.5124],
        [27.0295, 27.3941, 27.1306]], grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.05646082013845444 
model_pd.l_d.mean(): -1.3628042936325073 
model_pd.lagr.mean(): -1.30634343624115 
model_pd.lambdas: dict_items([('pout', tensor([1.6474])), ('power', tensor([0.0813]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7844])), ('power', tensor([-0.8602]))])
epoch：1270	 i:0 	 global-step:25400	 l-p:0.05646082013845444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[27.0310, 27.7212, 27.3187],
        [27.0310, 28.5703, 28.0927],
        [27.0310, 29.7725, 29.6762],
        [27.0310, 29.9642, 29.9669]], grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.05646039545536041 
model_pd.l_d.mean(): -1.3621546030044556 
model_pd.lagr.mean(): -1.3056942224502563 
model_pd.lambdas: dict_items([('pout', tensor([1.6466])), ('power', tensor([0.0812]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7845])), ('power', tensor([-0.8587]))])
epoch：1271	 i:0 	 global-step:25420	 l-p:0.05646039545536041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[27.0325, 32.2952, 34.0533],
        [27.0325, 27.2253, 27.0683],
        [27.0325, 27.4024, 27.1361],
        [27.0325, 27.0344, 27.0326]], grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.05645997077226639 
model_pd.l_d.mean(): -1.3615049123764038 
model_pd.lagr.mean(): -1.3050448894500732 
model_pd.lambdas: dict_items([('pout', tensor([1.6459])), ('power', tensor([0.0812]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7845])), ('power', tensor([-0.8573]))])
epoch：1272	 i:0 	 global-step:25440	 l-p:0.05645997077226639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[27.0340, 27.2509, 27.0774],
        [27.0340, 27.9816, 27.5171],
        [27.0340, 27.0340, 27.0340],
        [27.0340, 28.3726, 27.8813]], grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.05645955726504326 
model_pd.l_d.mean(): -1.3608548641204834 
model_pd.lagr.mean(): -1.304395318031311 
model_pd.lambdas: dict_items([('pout', tensor([1.6451])), ('power', tensor([0.0812]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7846])), ('power', tensor([-0.8558]))])
epoch：1273	 i:0 	 global-step:25460	 l-p:0.05645955726504326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[27.0355, 27.0655, 27.0373],
        [27.0355, 27.0355, 27.0355],
        [27.0355, 27.0370, 27.0355],
        [27.0355, 27.0354, 27.0355]], grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.05645914375782013 
model_pd.l_d.mean(): -1.3602054119110107 
model_pd.lagr.mean(): -1.303746223449707 
model_pd.lambdas: dict_items([('pout', tensor([1.6443])), ('power', tensor([0.0811]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7847])), ('power', tensor([-0.8544]))])
epoch：1274	 i:0 	 global-step:25480	 l-p:0.05645914375782013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.0369, 32.3009, 34.0596],
        [27.0369, 27.4069, 27.1404],
        [27.0369, 35.0215, 39.6869],
        [27.0369, 32.7504, 34.9370]], grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.056458745151758194 
model_pd.l_d.mean(): -1.3595556020736694 
model_pd.lagr.mean(): -1.303096890449524 
model_pd.lambdas: dict_items([('pout', tensor([1.6435])), ('power', tensor([0.0811]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7848])), ('power', tensor([-0.8530]))])
epoch：1275	 i:0 	 global-step:25500	 l-p:0.056458745151758194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[27.0383, 27.8601, 27.4209],
        [27.0383, 28.2303, 27.7402],
        [27.0383, 27.0488, 27.0387],
        [27.0383, 36.6553, 43.3784]], grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.056458353996276855 
model_pd.l_d.mean(): -1.3589056730270386 
model_pd.lagr.mean(): -1.3024473190307617 
model_pd.lambdas: dict_items([('pout', tensor([1.6427])), ('power', tensor([0.0810]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7848])), ('power', tensor([-0.8516]))])
epoch：1276	 i:0 	 global-step:25520	 l-p:0.056458353996276855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228]])
 pt:tensor([[27.0397, 31.7392, 32.9952],
        [27.0397, 35.9835, 41.8321],
        [27.0397, 28.8634, 28.4324],
        [27.0397, 29.5194, 29.3022]], grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.05645796284079552 
model_pd.l_d.mean(): -1.3582559823989868 
model_pd.lagr.mean(): -1.3017979860305786 
model_pd.lambdas: dict_items([('pout', tensor([1.6419])), ('power', tensor([0.0810]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7849])), ('power', tensor([-0.8502]))])
epoch：1277	 i:0 	 global-step:25540	 l-p:0.05645796284079552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.0410, 28.0490, 27.5752],
        [27.0410, 28.9466, 28.5345],
        [27.0410, 27.3982, 27.1388],
        [27.0410, 27.4092, 27.1437]], grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.05645757541060448 
model_pd.l_d.mean(): -1.3576061725616455 
model_pd.lagr.mean(): -1.3011486530303955 
model_pd.lambdas: dict_items([('pout', tensor([1.6412])), ('power', tensor([0.0809]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7850])), ('power', tensor([-0.8489]))])
epoch：1278	 i:0 	 global-step:25560	 l-p:0.05645757541060448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.0423, 27.0423, 27.0423],
        [27.0423, 27.7331, 27.3302],
        [27.0423, 28.5829, 28.1050],
        [27.0423, 29.7859, 29.6897]], grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.05645720660686493 
model_pd.l_d.mean(): -1.3569568395614624 
model_pd.lagr.mean(): -1.300499677658081 
model_pd.lambdas: dict_items([('pout', tensor([1.6404])), ('power', tensor([0.0809]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7850])), ('power', tensor([-0.8476]))])
epoch：1279	 i:0 	 global-step:25580	 l-p:0.05645720660686493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.0436, 28.3832, 27.8916],
        [27.0436, 32.8060, 35.0394],
        [27.0436, 29.5241, 29.3069],
        [27.0436, 27.0437, 27.0436]], grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.05645683407783508 
model_pd.l_d.mean(): -1.3563071489334106 
model_pd.lagr.mean(): -1.299850344657898 
model_pd.lambdas: dict_items([('pout', tensor([1.6396])), ('power', tensor([0.0809]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7851])), ('power', tensor([-0.8463]))])
epoch：1280	 i:0 	 global-step:25600	 l-p:0.05645683407783508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[27.0449, 32.3115, 34.0714],
        [27.0449, 30.1714, 30.2814],
        [27.0449, 31.1939, 32.0015],
        [27.0449, 27.4022, 27.1427]], grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.05645647644996643 
model_pd.l_d.mean(): -1.3556575775146484 
model_pd.lagr.mean(): -1.2992011308670044 
model_pd.lambdas: dict_items([('pout', tensor([1.6388])), ('power', tensor([0.0808]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7852])), ('power', tensor([-0.8450]))])
epoch：1281	 i:0 	 global-step:25620	 l-p:0.05645647644996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[27.0462, 29.0458, 28.6582],
        [27.0462, 27.0567, 27.0466],
        [27.0462, 28.9525, 28.5404],
        [27.0462, 32.8095, 35.0433]], grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.05645611882209778 
model_pd.l_d.mean(): -1.3550078868865967 
model_pd.lagr.mean(): -1.2985517978668213 
model_pd.lambdas: dict_items([('pout', tensor([1.6380])), ('power', tensor([0.0808]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7852])), ('power', tensor([-0.8438]))])
epoch：1282	 i:0 	 global-step:25640	 l-p:0.05645611882209778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.0474, 34.6901, 38.9496],
        [27.0474, 32.8049, 35.0327],
        [27.0474, 27.0580, 27.0478],
        [27.0474, 27.0891, 27.0505]], grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.056455764919519424 
model_pd.l_d.mean(): -1.3543583154678345 
model_pd.lagr.mean(): -1.2979025840759277 
model_pd.lambdas: dict_items([('pout', tensor([1.6372])), ('power', tensor([0.0807]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7853])), ('power', tensor([-0.8425]))])
epoch：1283	 i:0 	 global-step:25660	 l-p:0.056455764919519424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.0487, 35.9968, 41.8487],
        [27.0487, 27.0903, 27.0517],
        [27.0487, 27.9973, 27.5323],
        [27.0487, 30.0784, 30.1327]], grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.056455422192811966 
model_pd.l_d.mean(): -1.3537085056304932 
model_pd.lagr.mean(): -1.297253131866455 
model_pd.lambdas: dict_items([('pout', tensor([1.6364])), ('power', tensor([0.0807]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7853])), ('power', tensor([-0.8413]))])
epoch：1284	 i:0 	 global-step:25680	 l-p:0.056455422192811966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[27.0499, 29.0500, 28.6623],
        [27.0499, 34.6936, 38.9538],
        [27.0499, 30.0798, 30.1342],
        [27.0499, 27.0500, 27.0499]], grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.056455083191394806 
model_pd.l_d.mean(): -1.353058934211731 
model_pd.lagr.mean(): -1.296603798866272 
model_pd.lambdas: dict_items([('pout', tensor([1.6357])), ('power', tensor([0.0806]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7854])), ('power', tensor([-0.8401]))])
epoch：1285	 i:0 	 global-step:25700	 l-p:0.056455083191394806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.0510, 27.1052, 27.0557],
        [27.0510, 27.0616, 27.0514],
        [27.0510, 29.9882, 29.9913],
        [27.0510, 27.4215, 27.1547]], grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.05645475536584854 
model_pd.l_d.mean(): -1.3524096012115479 
model_pd.lagr.mean(): -1.2959548234939575 
model_pd.lambdas: dict_items([('pout', tensor([1.6349])), ('power', tensor([0.0806]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7855])), ('power', tensor([-0.8390]))])
epoch：1286	 i:0 	 global-step:25720	 l-p:0.05645475536584854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[27.0522, 27.0540, 27.0522],
        [27.0522, 28.0611, 27.5869],
        [27.0522, 29.7977, 29.7016],
        [27.0522, 27.0522, 27.0522]], grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.05645442381501198 
model_pd.l_d.mean(): -1.351759910583496 
model_pd.lagr.mean(): -1.2953054904937744 
model_pd.lambdas: dict_items([('pout', tensor([1.6341])), ('power', tensor([0.0806]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7855])), ('power', tensor([-0.8378]))])
epoch：1287	 i:0 	 global-step:25740	 l-p:0.05645442381501198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[27.0533, 27.2705, 27.0968],
        [27.0533, 28.0024, 27.5372],
        [27.0533, 35.0453, 39.7157],
        [27.0533, 28.2468, 27.7562]], grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.05645410716533661 
model_pd.l_d.mean(): -1.3511102199554443 
model_pd.lagr.mean(): -1.2946561574935913 
model_pd.lambdas: dict_items([('pout', tensor([1.6333])), ('power', tensor([0.0805]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7856])), ('power', tensor([-0.8367]))])
epoch：1288	 i:0 	 global-step:25760	 l-p:0.05645410716533661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[27.0544, 32.3243, 34.0855],
        [27.0544, 34.7003, 38.9618],
        [27.0544, 32.8207, 35.0560],
        [27.0544, 27.0550, 27.0544]], grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.05645378679037094 
model_pd.l_d.mean(): -1.3504605293273926 
model_pd.lagr.mean(): -1.2940067052841187 
model_pd.lambdas: dict_items([('pout', tensor([1.6325])), ('power', tensor([0.0805]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7856])), ('power', tensor([-0.8356]))])
epoch：1289	 i:0 	 global-step:25780	 l-p:0.05645378679037094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[27.0555, 27.0556, 27.0556],
        [27.0555, 27.4132, 27.1534],
        [27.0555, 27.1687, 27.0707],
        [27.0555, 35.0486, 39.7197]], grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.05645347759127617 
model_pd.l_d.mean(): -1.34981107711792 
model_pd.lagr.mean(): -1.2933576107025146 
model_pd.lambdas: dict_items([('pout', tensor([1.6317])), ('power', tensor([0.0804]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7857])), ('power', tensor([-0.8345]))])
epoch：1290	 i:0 	 global-step:25800	 l-p:0.05645347759127617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.0566, 31.5098, 32.5575],
        [27.0566, 35.0502, 39.7217],
        [27.0566, 31.7614, 33.0193],
        [27.0566, 27.4272, 27.1604]], grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.056453172117471695 
model_pd.l_d.mean(): -1.3491615056991577 
model_pd.lagr.mean(): -1.2927082777023315 
model_pd.lambdas: dict_items([('pout', tensor([1.6309])), ('power', tensor([0.0804]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7857])), ('power', tensor([-0.8334]))])
epoch：1291	 i:0 	 global-step:25820	 l-p:0.056453172117471695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[27.0577, 27.0577, 27.0577],
        [27.0577, 32.3287, 34.0904],
        [27.0577, 27.0577, 27.0577],
        [27.0577, 28.9657, 28.5533]], grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.05645287036895752 
model_pd.l_d.mean(): -1.3485121726989746 
model_pd.lagr.mean(): -1.292059302330017 
model_pd.lambdas: dict_items([('pout', tensor([1.6302])), ('power', tensor([0.0804]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7858])), ('power', tensor([-0.8324]))])
epoch：1292	 i:0 	 global-step:25840	 l-p:0.05645287036895752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[27.0587, 29.5420, 29.3248],
        [27.0587, 27.0593, 27.0587],
        [27.0587, 28.0682, 27.5938],
        [27.0587, 27.0606, 27.0588]], grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.05645257607102394 
model_pd.l_d.mean(): -1.3478624820709229 
model_pd.lagr.mean(): -1.2914098501205444 
model_pd.lambdas: dict_items([('pout', tensor([1.6294])), ('power', tensor([0.0803]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7859])), ('power', tensor([-0.8313]))])
epoch：1293	 i:0 	 global-step:25860	 l-p:0.05645257607102394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.0598, 34.7081, 38.9712],
        [27.0598, 30.0919, 30.1466],
        [27.0598, 31.7656, 33.0238],
        [27.0598, 27.0613, 27.0598]], grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.05645228549838066 
model_pd.l_d.mean(): -1.347212791442871 
model_pd.lagr.mean(): -1.2907605171203613 
model_pd.lambdas: dict_items([('pout', tensor([1.6286])), ('power', tensor([0.0803]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7859])), ('power', tensor([-0.8303]))])
epoch：1294	 i:0 	 global-step:25880	 l-p:0.05645228549838066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]])
 pt:tensor([[27.0608, 32.8232, 35.0534],
        [27.0608, 28.0104, 27.5450],
        [27.0608, 28.8875, 28.4560],
        [27.0608, 36.0152, 41.8717]], grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.05645199865102768 
model_pd.l_d.mean(): -1.3465633392333984 
model_pd.lagr.mean(): -1.2901113033294678 
model_pd.lambdas: dict_items([('pout', tensor([1.6278])), ('power', tensor([0.0802]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7860])), ('power', tensor([-0.8293]))])
epoch：1295	 i:0 	 global-step:25900	 l-p:0.05645199865102768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[27.0618, 30.3966, 30.6308],
        [27.0618, 27.2550, 27.0977],
        [27.0618, 31.7682, 33.0268],
        [27.0618, 32.8245, 35.0550]], grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.056451715528964996 
model_pd.l_d.mean(): -1.3459140062332153 
model_pd.lagr.mean(): -1.2894623279571533 
model_pd.lambdas: dict_items([('pout', tensor([1.6270])), ('power', tensor([0.0802]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7860])), ('power', tensor([-0.8283]))])
epoch：1296	 i:0 	 global-step:25920	 l-p:0.056451715528964996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[27.0627, 27.7547, 27.3512],
        [27.0627, 27.2802, 27.1062],
        [27.0627, 32.7856, 34.9767],
        [27.0627, 28.4044, 27.9121]], grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.05645144358277321 
model_pd.l_d.mean(): -1.3452644348144531 
model_pd.lagr.mean(): -1.2888129949569702 
model_pd.lambdas: dict_items([('pout', tensor([1.6262])), ('power', tensor([0.0802]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7861])), ('power', tensor([-0.8274]))])
epoch：1297	 i:0 	 global-step:25940	 l-p:0.05645144358277321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[27.0637, 28.0136, 27.5481],
        [27.0637, 31.7708, 33.0296],
        [27.0637, 28.2582, 27.7673],
        [27.0637, 30.1945, 30.3051]], grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.05645117163658142 
model_pd.l_d.mean(): -1.3446149826049805 
model_pd.lagr.mean(): -1.2881637811660767 
model_pd.lambdas: dict_items([('pout', tensor([1.6254])), ('power', tensor([0.0801]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7861])), ('power', tensor([-0.8264]))])
epoch：1298	 i:0 	 global-step:25960	 l-p:0.05645117163658142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.0646, 27.0647, 27.0646],
        [27.0646, 28.2593, 27.7683],
        [27.0646, 28.0146, 27.5491],
        [27.0646, 27.0662, 27.0647]], grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.05645090714097023 
model_pd.l_d.mean(): -1.3439654111862183 
model_pd.lagr.mean(): -1.2875144481658936 
model_pd.lambdas: dict_items([('pout', tensor([1.6247])), ('power', tensor([0.0801]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7861])), ('power', tensor([-0.8255]))])
epoch：1299	 i:0 	 global-step:25980	 l-p:0.05645090714097023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[27.0656, 27.0672, 27.0656],
        [27.0656, 27.0656, 27.0656],
        [27.0656, 27.0674, 27.0656],
        [27.0656, 31.5216, 32.5703]], grad_fn=<SliceBackward0>)

training epoch:1300, step:0 
model_pd.l_p.mean(): 0.05645064264535904 
model_pd.l_d.mean(): -1.3433160781860352 
model_pd.lagr.mean(): -1.286865472793579 
model_pd.lambdas: dict_items([('pout', tensor([1.6239])), ('power', tensor([0.0800]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7862])), ('power', tensor([-0.8246]))])
epoch：1300	 i:0 	 global-step:26000	 l-p:0.05645064264535904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1301
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228]])
 pt:tensor([[27.0665, 28.6101, 28.1315],
        [27.0665, 29.5513, 29.3342],
        [27.0665, 30.4026, 30.6370],
        [27.0665, 28.2613, 27.7702]], grad_fn=<SliceBackward0>)

training epoch:1301, step:0 
model_pd.l_p.mean(): 0.056450389325618744 
model_pd.l_d.mean(): -1.342666506767273 
model_pd.lagr.mean(): -1.286216139793396 
model_pd.lambdas: dict_items([('pout', tensor([1.6231])), ('power', tensor([0.0800]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7862])), ('power', tensor([-0.8237]))])
epoch：1301	 i:0 	 global-step:26020	 l-p:0.056450389325618744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[27.0674, 27.1217, 27.0721],
        [27.0674, 31.5240, 32.5729],
        [27.0674, 27.4254, 27.1654],
        [27.0674, 27.0692, 27.0674]], grad_fn=<SliceBackward0>)

training epoch:1302, step:0 
model_pd.l_p.mean(): 0.05645013973116875 
model_pd.l_d.mean(): -1.3420171737670898 
model_pd.lagr.mean(): -1.285567045211792 
model_pd.lambdas: dict_items([('pout', tensor([1.6223])), ('power', tensor([0.0799]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7863])), ('power', tensor([-0.8228]))])
epoch：1302	 i:0 	 global-step:26040	 l-p:0.05645013973116875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[27.0683, 32.7933, 34.9856],
        [27.0683, 30.4048, 30.6393],
        [27.0683, 29.8172, 29.7214],
        [27.0683, 27.0788, 27.0686]], grad_fn=<SliceBackward0>)

training epoch:1303, step:0 
model_pd.l_p.mean(): 0.05644988641142845 
model_pd.l_d.mean(): -1.3413677215576172 
model_pd.lagr.mean(): -1.2849178314208984 
model_pd.lambdas: dict_items([('pout', tensor([1.6215])), ('power', tensor([0.0799]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7863])), ('power', tensor([-0.8219]))])
epoch：1303	 i:0 	 global-step:26060	 l-p:0.05644988641142845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[27.0691, 27.0716, 27.0692],
        [27.0691, 27.2625, 27.1050],
        [27.0691, 28.8972, 28.4655],
        [27.0691, 27.0691, 27.0691]], grad_fn=<SliceBackward0>)

training epoch:1304, step:0 
model_pd.l_p.mean(): 0.056449636816978455 
model_pd.l_d.mean(): -1.3407179117202759 
model_pd.lagr.mean(): -1.2842682600021362 
model_pd.lambdas: dict_items([('pout', tensor([1.6207])), ('power', tensor([0.0799]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7864])), ('power', tensor([-0.8210]))])
epoch：1304	 i:0 	 global-step:26080	 l-p:0.056449636816978455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[27.0700, 36.7041, 43.4410],
        [27.0700, 27.1001, 27.0718],
        [27.0700, 29.5555, 29.3384],
        [27.0700, 31.2264, 32.0362]], grad_fn=<SliceBackward0>)

training epoch:1305, step:0 
model_pd.l_p.mean(): 0.05644940212368965 
model_pd.l_d.mean(): -1.3400686979293823 
model_pd.lagr.mean(): -1.2836192846298218 
model_pd.lambdas: dict_items([('pout', tensor([1.6199])), ('power', tensor([0.0798]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7864])), ('power', tensor([-0.8202]))])
epoch：1305	 i:0 	 global-step:26100	 l-p:0.05644940212368965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.0708, 27.4290, 27.1689],
        [27.0708, 27.1010, 27.0726],
        [27.0708, 34.5763, 38.6711],
        [27.0708, 29.0745, 28.6864]], grad_fn=<SliceBackward0>)

training epoch:1306, step:0 
model_pd.l_p.mean(): 0.05644916370511055 
model_pd.l_d.mean(): -1.3394191265106201 
model_pd.lagr.mean(): -1.2829699516296387 
model_pd.lambdas: dict_items([('pout', tensor([1.6191])), ('power', tensor([0.0798]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7865])), ('power', tensor([-0.8194]))])
epoch：1306	 i:0 	 global-step:26120	 l-p:0.05644916370511055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.0716, 27.4428, 27.1756],
        [27.0716, 28.0223, 27.5564],
        [27.0716, 27.4409, 27.1747],
        [27.0716, 27.0717, 27.0716]], grad_fn=<SliceBackward0>)

training epoch:1307, step:0 
model_pd.l_p.mean(): 0.056448932737112045 
model_pd.l_d.mean(): -1.3387696743011475 
model_pd.lagr.mean(): -1.2823207378387451 
model_pd.lambdas: dict_items([('pout', tensor([1.6184])), ('power', tensor([0.0797]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7865])), ('power', tensor([-0.8185]))])
epoch：1307	 i:0 	 global-step:26140	 l-p:0.056448932737112045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[27.0724, 28.0232, 27.5573],
        [27.0724, 29.5586, 29.3415],
        [27.0724, 27.2901, 27.1160],
        [27.0724, 32.8459, 35.0847]], grad_fn=<SliceBackward0>)

training epoch:1308, step:0 
model_pd.l_p.mean(): 0.05644870176911354 
model_pd.l_d.mean(): -1.3381202220916748 
model_pd.lagr.mean(): -1.2816715240478516 
model_pd.lambdas: dict_items([('pout', tensor([1.6176])), ('power', tensor([0.0797]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7865])), ('power', tensor([-0.8177]))])
epoch：1308	 i:0 	 global-step:26160	 l-p:0.05644870176911354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]])
 pt:tensor([[27.0732, 27.0732, 27.0732],
        [27.0732, 32.3501, 34.1144],
        [27.0732, 27.0732, 27.0733],
        [27.0732, 27.1034, 27.0751]], grad_fn=<SliceBackward0>)

training epoch:1309, step:0 
model_pd.l_p.mean(): 0.05644848197698593 
model_pd.l_d.mean(): -1.3374706506729126 
model_pd.lagr.mean(): -1.2810221910476685 
model_pd.lambdas: dict_items([('pout', tensor([1.6168])), ('power', tensor([0.0797]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7866])), ('power', tensor([-0.8170]))])
epoch：1309	 i:0 	 global-step:26180	 l-p:0.05644848197698593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.0740, 27.4433, 27.1771],
        [27.0740, 28.6187, 28.1399],
        [27.0740, 31.5330, 32.5827],
        [27.0740, 27.4453, 27.1780]], grad_fn=<SliceBackward0>)

training epoch:1310, step:0 
model_pd.l_p.mean(): 0.056448258459568024 
model_pd.l_d.mean(): -1.3368213176727295 
model_pd.lagr.mean(): -1.2803730964660645 
model_pd.lambdas: dict_items([('pout', tensor([1.6160])), ('power', tensor([0.0796]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7866])), ('power', tensor([-0.8162]))])
epoch：1310	 i:0 	 global-step:26200	 l-p:0.056448258459568024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1311
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[27.0748, 31.5340, 32.5839],
        [27.0748, 30.1107, 30.1659],
        [27.0748, 27.8992, 27.4588],
        [27.0748, 27.1291, 27.0795]], grad_fn=<SliceBackward0>)

training epoch:1311, step:0 
model_pd.l_p.mean(): 0.056448038667440414 
model_pd.l_d.mean(): -1.3361717462539673 
model_pd.lagr.mean(): -1.2797237634658813 
model_pd.lambdas: dict_items([('pout', tensor([1.6152])), ('power', tensor([0.0796]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7867])), ('power', tensor([-0.8154]))])
epoch：1311	 i:0 	 global-step:26220	 l-p:0.056448038667440414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.0755, 28.9866, 28.5739],
        [27.0755, 31.7869, 33.0474],
        [27.0755, 27.0756, 27.0755],
        [27.0755, 29.8263, 29.7306]], grad_fn=<SliceBackward0>)

training epoch:1312, step:0 
model_pd.l_p.mean(): 0.0564478263258934 
model_pd.l_d.mean(): -1.3355222940444946 
model_pd.lagr.mean(): -1.2790744304656982 
model_pd.lambdas: dict_items([('pout', tensor([1.6144])), ('power', tensor([0.0795]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7867])), ('power', tensor([-0.8147]))])
epoch：1312	 i:0 	 global-step:26240	 l-p:0.0564478263258934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[27.0763, 27.0763, 27.0763],
        [27.0763, 27.7692, 27.3652],
        [27.0763, 28.0875, 27.6124],
        [27.0763, 27.2698, 27.1122]], grad_fn=<SliceBackward0>)

training epoch:1313, step:0 
model_pd.l_p.mean(): 0.05644761025905609 
model_pd.l_d.mean(): -1.334872841835022 
model_pd.lagr.mean(): -1.2784252166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.6136])), ('power', tensor([0.0795]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7867])), ('power', tensor([-0.8139]))])
epoch：1313	 i:0 	 global-step:26260	 l-p:0.05644761025905609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[27.0770, 27.0786, 27.0770],
        [27.0770, 28.0282, 27.5621],
        [27.0770, 27.0770, 27.0770],
        [27.0770, 32.8524, 35.0923]], grad_fn=<SliceBackward0>)

training epoch:1314, step:0 
model_pd.l_p.mean(): 0.056447405368089676 
model_pd.l_d.mean(): -1.3342235088348389 
model_pd.lagr.mean(): -1.2777761220932007 
model_pd.lambdas: dict_items([('pout', tensor([1.6129])), ('power', tensor([0.0795]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7868])), ('power', tensor([-0.8132]))])
epoch：1314	 i:0 	 global-step:26280	 l-p:0.056447405368089676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.0777, 29.0827, 28.6946],
        [27.0777, 27.0802, 27.0778],
        [27.0777, 27.1912, 27.0929],
        [27.0777, 27.0793, 27.0777]], grad_fn=<SliceBackward0>)

training epoch:1315, step:0 
model_pd.l_p.mean(): 0.056447211652994156 
model_pd.l_d.mean(): -1.3335740566253662 
model_pd.lagr.mean(): -1.2771267890930176 
model_pd.lambdas: dict_items([('pout', tensor([1.6121])), ('power', tensor([0.0794]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7868])), ('power', tensor([-0.8125]))])
epoch：1315	 i:0 	 global-step:26300	 l-p:0.056447211652994156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.0784, 27.2720, 27.1144],
        [27.0784, 27.0786, 27.0784],
        [27.0784, 29.0835, 28.6954],
        [27.0784, 28.6238, 28.1448]], grad_fn=<SliceBackward0>)

training epoch:1316, step:0 
model_pd.l_p.mean(): 0.05644701048731804 
model_pd.l_d.mean(): -1.332924723625183 
model_pd.lagr.mean(): -1.2764776945114136 
model_pd.lambdas: dict_items([('pout', tensor([1.6113])), ('power', tensor([0.0794]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7869])), ('power', tensor([-0.8118]))])
epoch：1316	 i:0 	 global-step:26320	 l-p:0.05644701048731804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[27.0791, 29.8307, 29.7352],
        [27.0791, 31.2387, 32.0495],
        [27.0791, 28.2754, 27.7839],
        [27.0791, 27.2727, 27.1151]], grad_fn=<SliceBackward0>)

training epoch:1317, step:0 
model_pd.l_p.mean(): 0.05644682049751282 
model_pd.l_d.mean(): -1.3322750329971313 
model_pd.lagr.mean(): -1.275828242301941 
model_pd.lambdas: dict_items([('pout', tensor([1.6105])), ('power', tensor([0.0793]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7869])), ('power', tensor([-0.8111]))])
epoch：1317	 i:0 	 global-step:26340	 l-p:0.05644682049751282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[27.0798, 27.4461, 27.1814],
        [27.0798, 27.0799, 27.0798],
        [27.0798, 27.4513, 27.1838],
        [27.0798, 28.0914, 27.6161]], grad_fn=<SliceBackward0>)

training epoch:1318, step:0 
model_pd.l_p.mean(): 0.056446623057127 
model_pd.l_d.mean(): -1.3316258192062378 
model_pd.lagr.mean(): -1.275179147720337 
model_pd.lambdas: dict_items([('pout', tensor([1.6097])), ('power', tensor([0.0793]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7869])), ('power', tensor([-0.8105]))])
epoch：1318	 i:0 	 global-step:26360	 l-p:0.056446623057127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[27.0804, 27.1107, 27.0823],
        [27.0804, 28.2770, 27.7853],
        [27.0804, 27.4520, 27.1845],
        [27.0804, 36.7211, 43.4633]], grad_fn=<SliceBackward0>)

training epoch:1319, step:0 
model_pd.l_p.mean(): 0.056446436792612076 
model_pd.l_d.mean(): -1.3309764862060547 
model_pd.lagr.mean(): -1.274530053138733 
model_pd.lambdas: dict_items([('pout', tensor([1.6089])), ('power', tensor([0.0793]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7870])), ('power', tensor([-0.8098]))])
epoch：1319	 i:0 	 global-step:26380	 l-p:0.056446436792612076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[27.0811, 27.7744, 27.3702],
        [27.0811, 27.0829, 27.0811],
        [27.0811, 35.0875, 39.7679],
        [27.0811, 27.4397, 27.1793]], grad_fn=<SliceBackward0>)

training epoch:1320, step:0 
model_pd.l_p.mean(): 0.056446246802806854 
model_pd.l_d.mean(): -1.330326795578003 
model_pd.lagr.mean(): -1.2738806009292603 
model_pd.lambdas: dict_items([('pout', tensor([1.6081])), ('power', tensor([0.0792]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7870])), ('power', tensor([-0.8091]))])
epoch：1320	 i:0 	 global-step:26400	 l-p:0.056446246802806854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[27.0817, 27.0817, 27.0817],
        [27.0817, 35.0885, 39.7692],
        [27.0817, 29.8341, 29.7386],
        [27.0817, 30.4223, 30.6576]], grad_fn=<SliceBackward0>)

training epoch:1321, step:0 
model_pd.l_p.mean(): 0.05644607171416283 
model_pd.l_d.mean(): -1.329677700996399 
model_pd.lagr.mean(): -1.2732316255569458 
model_pd.lambdas: dict_items([('pout', tensor([1.6073])), ('power', tensor([0.0792]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7870])), ('power', tensor([-0.8085]))])
epoch：1321	 i:0 	 global-step:26420	 l-p:0.05644607171416283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[27.0824, 27.3003, 27.1259],
        [27.0824, 30.0272, 30.0313],
        [27.0824, 30.4231, 30.6584],
        [27.0824, 32.8603, 35.1015]], grad_fn=<SliceBackward0>)

training epoch:1322, step:0 
model_pd.l_p.mean(): 0.0564458891749382 
model_pd.l_d.mean(): -1.3290282487869263 
model_pd.lagr.mean(): -1.2725824117660522 
model_pd.lambdas: dict_items([('pout', tensor([1.6066])), ('power', tensor([0.0791]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7871])), ('power', tensor([-0.8079]))])
epoch：1322	 i:0 	 global-step:26440	 l-p:0.0564458891749382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[27.0830, 27.0830, 27.0830],
        [27.0830, 27.0835, 27.0830],
        [27.0830, 35.0905, 39.7717],
        [27.0830, 27.1965, 27.0982]], grad_fn=<SliceBackward0>)

training epoch:1323, step:0 
model_pd.l_p.mean(): 0.05644571781158447 
model_pd.l_d.mean(): -1.328378438949585 
model_pd.lagr.mean(): -1.2719327211380005 
model_pd.lambdas: dict_items([('pout', tensor([1.6058])), ('power', tensor([0.0791]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7871])), ('power', tensor([-0.8073]))])
epoch：1323	 i:0 	 global-step:26460	 l-p:0.05644571781158447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[27.0836, 31.5463, 32.5974],
        [27.0836, 27.4533, 27.1868],
        [27.0836, 31.2449, 32.0563],
        [27.0836, 36.0512, 41.9181]], grad_fn=<SliceBackward0>)

training epoch:1324, step:0 
model_pd.l_p.mean(): 0.05644554644823074 
model_pd.l_d.mean(): -1.327729344367981 
model_pd.lagr.mean(): -1.271283745765686 
model_pd.lambdas: dict_items([('pout', tensor([1.6050])), ('power', tensor([0.0790]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7871])), ('power', tensor([-0.8067]))])
epoch：1324	 i:0 	 global-step:26480	 l-p:0.05644554644823074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]])
 pt:tensor([[27.0842, 28.0362, 27.5697],
        [27.0842, 32.3658, 34.1324],
        [27.0842, 27.0842, 27.0842],
        [27.0842, 27.4429, 27.1824]], grad_fn=<SliceBackward0>)

training epoch:1325, step:0 
model_pd.l_p.mean(): 0.056445375084877014 
model_pd.l_d.mean(): -1.3270798921585083 
model_pd.lagr.mean(): -1.2706345319747925 
model_pd.lambdas: dict_items([('pout', tensor([1.6042])), ('power', tensor([0.0790]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7871])), ('power', tensor([-0.8061]))])
epoch：1325	 i:0 	 global-step:26500	 l-p:0.056445375084877014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[27.0847, 27.1266, 27.0878],
        [27.0847, 28.6312, 28.1520],
        [27.0847, 28.9977, 28.5848],
        [27.0847, 30.0303, 30.0345]], grad_fn=<SliceBackward0>)

training epoch:1326, step:0 
model_pd.l_p.mean(): 0.05644520744681358 
model_pd.l_d.mean(): -1.3264302015304565 
model_pd.lagr.mean(): -1.2699849605560303 
model_pd.lambdas: dict_items([('pout', tensor([1.6034])), ('power', tensor([0.0790]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7872])), ('power', tensor([-0.8055]))])
epoch：1326	 i:0 	 global-step:26520	 l-p:0.05644520744681358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[27.0853, 27.0858, 27.0853],
        [27.0853, 27.0853, 27.0853],
        [27.0853, 32.8181, 35.0143],
        [27.0853, 27.0892, 27.0854]], grad_fn=<SliceBackward0>)

training epoch:1327, step:0 
model_pd.l_p.mean(): 0.05644505098462105 
model_pd.l_d.mean(): -1.3257811069488525 
model_pd.lagr.mean(): -1.2693361043930054 
model_pd.lambdas: dict_items([('pout', tensor([1.6026])), ('power', tensor([0.0789]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7872])), ('power', tensor([-0.8049]))])
epoch：1327	 i:0 	 global-step:26540	 l-p:0.05644505098462105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[27.0859, 35.0952, 39.7778],
        [27.0859, 27.2797, 27.1219],
        [27.0859, 30.4278, 30.6634],
        [27.0859, 28.0982, 27.6227]], grad_fn=<SliceBackward0>)

training epoch:1328, step:0 
model_pd.l_p.mean(): 0.05644489452242851 
model_pd.l_d.mean(): -1.3251315355300903 
model_pd.lagr.mean(): -1.2686866521835327 
model_pd.lambdas: dict_items([('pout', tensor([1.6018])), ('power', tensor([0.0789]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7872])), ('power', tensor([-0.8044]))])
epoch：1328	 i:0 	 global-step:26560	 l-p:0.05644489452242851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[27.0864, 27.0883, 27.0865],
        [27.0864, 30.4286, 30.6642],
        [27.0864, 27.7802, 27.3757],
        [27.0864, 30.2237, 30.3354]], grad_fn=<SliceBackward0>)

training epoch:1329, step:0 
model_pd.l_p.mean(): 0.05644473060965538 
model_pd.l_d.mean(): -1.3244819641113281 
model_pd.lagr.mean(): -1.26803719997406 
model_pd.lambdas: dict_items([('pout', tensor([1.6010])), ('power', tensor([0.0788]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7873])), ('power', tensor([-0.8038]))])
epoch：1329	 i:0 	 global-step:26580	 l-p:0.05644473060965538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.0870, 34.7498, 39.0228],
        [27.0870, 29.0004, 28.5874],
        [27.0870, 27.1414, 27.0917],
        [27.0870, 27.0870, 27.0870]], grad_fn=<SliceBackward0>)

training epoch:1330, step:0 
model_pd.l_p.mean(): 0.05644457787275314 
model_pd.l_d.mean(): -1.323832631111145 
model_pd.lagr.mean(): -1.267388105392456 
model_pd.lambdas: dict_items([('pout', tensor([1.6003])), ('power', tensor([0.0788]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7873])), ('power', tensor([-0.8033]))])
epoch：1330	 i:0 	 global-step:26600	 l-p:0.05644457787275314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[27.0875, 32.3707, 34.1382],
        [27.0875, 27.4575, 27.1908],
        [27.0875, 30.4300, 30.6658],
        [27.0875, 27.0877, 27.0875]], grad_fn=<SliceBackward0>)

training epoch:1331, step:0 
model_pd.l_p.mean(): 0.056444425135850906 
model_pd.l_d.mean(): -1.3231834173202515 
model_pd.lagr.mean(): -1.266739010810852 
model_pd.lambdas: dict_items([('pout', tensor([1.5995])), ('power', tensor([0.0788]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7873])), ('power', tensor([-0.8028]))])
epoch：1331	 i:0 	 global-step:26620	 l-p:0.056444425135850906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1332
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]])
 pt:tensor([[27.0880, 36.0587, 41.9280],
        [27.0880, 29.8423, 29.7470],
        [27.0880, 36.7339, 43.4806],
        [27.0880, 34.7515, 39.0250]], grad_fn=<SliceBackward0>)

training epoch:1332, step:0 
model_pd.l_p.mean(): 0.056444283574819565 
model_pd.l_d.mean(): -1.3225338459014893 
model_pd.lagr.mean(): -1.2660895586013794 
model_pd.lambdas: dict_items([('pout', tensor([1.5987])), ('power', tensor([0.0787]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7873])), ('power', tensor([-0.8022]))])
epoch：1332	 i:0 	 global-step:26640	 l-p:0.056444283574819565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[27.0885, 27.0885, 27.0885],
        [27.0885, 34.7523, 39.0261],
        [27.0885, 32.8230, 35.0201],
        [27.0885, 27.0887, 27.0885]], grad_fn=<SliceBackward0>)

training epoch:1333, step:0 
model_pd.l_p.mean(): 0.056444134563207626 
model_pd.l_d.mean(): -1.3218845129013062 
model_pd.lagr.mean(): -1.2654403448104858 
model_pd.lambdas: dict_items([('pout', tensor([1.5979])), ('power', tensor([0.0787]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7874])), ('power', tensor([-0.8017]))])
epoch：1333	 i:0 	 global-step:26660	 l-p:0.056444134563207626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.0890, 29.8436, 29.7484],
        [27.0890, 27.2829, 27.1250],
        [27.0890, 32.3731, 34.1409],
        [27.0890, 27.9150, 27.4738]], grad_fn=<SliceBackward0>)

training epoch:1334, step:0 
model_pd.l_p.mean(): 0.056443993002176285 
model_pd.l_d.mean(): -1.321235179901123 
model_pd.lagr.mean(): -1.2647911310195923 
model_pd.lambdas: dict_items([('pout', tensor([1.5971])), ('power', tensor([0.0786]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7874])), ('power', tensor([-0.8012]))])
epoch：1334	 i:0 	 global-step:26680	 l-p:0.056443993002176285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[27.0895, 28.0422, 27.5755],
        [27.0895, 30.0367, 30.0411],
        [27.0895, 27.4486, 27.1878],
        [27.0895, 27.0895, 27.0895]], grad_fn=<SliceBackward0>)

training epoch:1335, step:0 
model_pd.l_p.mean(): 0.056443843990564346 
model_pd.l_d.mean(): -1.3205856084823608 
model_pd.lagr.mean(): -1.2641417980194092 
model_pd.lambdas: dict_items([('pout', tensor([1.5963])), ('power', tensor([0.0786]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7874])), ('power', tensor([-0.8007]))])
epoch：1335	 i:0 	 global-step:26700	 l-p:0.056443843990564346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1336
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[27.0900, 28.1029, 27.6271],
        [27.0900, 27.4491, 27.1883],
        [27.0900, 32.8253, 35.0229],
        [27.0900, 27.3082, 27.1336]], grad_fn=<SliceBackward0>)

training epoch:1336, step:0 
model_pd.l_p.mean(): 0.0564437098801136 
model_pd.l_d.mean(): -1.3199363946914673 
model_pd.lagr.mean(): -1.2634927034378052 
model_pd.lambdas: dict_items([('pout', tensor([1.5955])), ('power', tensor([0.0786]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7875])), ('power', tensor([-0.8002]))])
epoch：1336	 i:0 	 global-step:26720	 l-p:0.0564437098801136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.0905, 27.9166, 27.4754],
        [27.0905, 29.0048, 28.5917],
        [27.0905, 27.0905, 27.0905],
        [27.0905, 27.0921, 27.0905]], grad_fn=<SliceBackward0>)

training epoch:1337, step:0 
model_pd.l_p.mean(): 0.05644357576966286 
model_pd.l_d.mean(): -1.319286823272705 
model_pd.lagr.mean(): -1.2628432512283325 
model_pd.lambdas: dict_items([('pout', tensor([1.5948])), ('power', tensor([0.0785]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7875])), ('power', tensor([-0.7998]))])
epoch：1337	 i:0 	 global-step:26740	 l-p:0.05644357576966286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.0910, 27.0910, 27.0910],
        [27.0910, 27.0910, 27.0910],
        [27.0910, 27.2046, 27.1062],
        [27.0910, 27.0934, 27.0910]], grad_fn=<SliceBackward0>)

training epoch:1338, step:0 
model_pd.l_p.mean(): 0.056443434208631516 
model_pd.l_d.mean(): -1.318637490272522 
model_pd.lagr.mean(): -1.262194037437439 
model_pd.lambdas: dict_items([('pout', tensor([1.5940])), ('power', tensor([0.0785]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7875])), ('power', tensor([-0.7993]))])
epoch：1338	 i:0 	 global-step:26760	 l-p:0.056443434208631516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.0914, 27.4636, 27.1956],
        [27.0914, 27.9176, 27.4764],
        [27.0914, 27.4583, 27.1933],
        [27.0914, 30.1326, 30.1886]], grad_fn=<SliceBackward0>)

training epoch:1339, step:0 
model_pd.l_p.mean(): 0.056443314999341965 
model_pd.l_d.mean(): -1.3179877996444702 
model_pd.lagr.mean(): -1.2615444660186768 
model_pd.lambdas: dict_items([('pout', tensor([1.5932])), ('power', tensor([0.0784]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7875])), ('power', tensor([-0.7988]))])
epoch：1339	 i:0 	 global-step:26780	 l-p:0.056443314999341965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.0919, 27.4588, 27.1937],
        [27.0919, 32.3774, 34.1460],
        [27.0919, 27.1024, 27.0922],
        [27.0919, 27.0935, 27.0919]], grad_fn=<SliceBackward0>)

training epoch:1340, step:0 
model_pd.l_p.mean(): 0.05644318088889122 
model_pd.l_d.mean(): -1.317338466644287 
model_pd.lagr.mean(): -1.2608952522277832 
model_pd.lambdas: dict_items([('pout', tensor([1.5924])), ('power', tensor([0.0784]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7876])), ('power', tensor([-0.7984]))])
epoch：1340	 i:0 	 global-step:26800	 l-p:0.05644318088889122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.0923, 27.0962, 27.0924],
        [27.0923, 27.4515, 27.1907],
        [27.0923, 28.4383, 27.9449],
        [27.0923, 32.8289, 35.0272]], grad_fn=<SliceBackward0>)

training epoch:1341, step:0 
model_pd.l_p.mean(): 0.056443050503730774 
model_pd.l_d.mean(): -1.3166892528533936 
model_pd.lagr.mean(): -1.2602461576461792 
model_pd.lambdas: dict_items([('pout', tensor([1.5916])), ('power', tensor([0.0784]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7876])), ('power', tensor([-0.7979]))])
epoch：1341	 i:0 	 global-step:26820	 l-p:0.056443050503730774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[27.0927, 27.0927, 27.0927],
        [27.0927, 27.3110, 27.1364],
        [27.0927, 28.6409, 28.1613],
        [27.0927, 27.0928, 27.0927]], grad_fn=<SliceBackward0>)

training epoch:1342, step:0 
model_pd.l_p.mean(): 0.056442927569150925 
model_pd.l_d.mean(): -1.316039800643921 
model_pd.lagr.mean(): -1.259596824645996 
model_pd.lambdas: dict_items([('pout', tensor([1.5908])), ('power', tensor([0.0783]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7876])), ('power', tensor([-0.7975]))])
epoch：1342	 i:0 	 global-step:26840	 l-p:0.056442927569150925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.0932, 27.3114, 27.1368],
        [27.0932, 31.2588, 32.0717],
        [27.0932, 30.2330, 30.3451],
        [27.0932, 28.6414, 28.1618]], grad_fn=<SliceBackward0>)

training epoch:1343, step:0 
model_pd.l_p.mean(): 0.056442808359861374 
model_pd.l_d.mean(): -1.3153903484344482 
model_pd.lagr.mean(): -1.258947491645813 
model_pd.lambdas: dict_items([('pout', tensor([1.5900])), ('power', tensor([0.0783]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7876])), ('power', tensor([-0.7971]))])
epoch：1343	 i:0 	 global-step:26860	 l-p:0.056442808359861374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1344
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.0936, 27.4606, 27.1955],
        [27.0936, 31.8131, 33.0770],
        [27.0936, 28.1071, 27.6311],
        [27.0936, 30.1357, 30.1918]], grad_fn=<SliceBackward0>)

training epoch:1344, step:0 
model_pd.l_p.mean(): 0.056442685425281525 
model_pd.l_d.mean(): -1.3147406578063965 
model_pd.lagr.mean(): -1.2582979202270508 
model_pd.lambdas: dict_items([('pout', tensor([1.5892])), ('power', tensor([0.0782]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7876])), ('power', tensor([-0.7967]))])
epoch：1344	 i:0 	 global-step:26880	 l-p:0.056442685425281525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[27.0940, 27.1359, 27.0971],
        [27.0940, 28.1076, 27.6315],
        [27.0940, 32.8784, 35.1231],
        [27.0940, 27.3123, 27.1377]], grad_fn=<SliceBackward0>)

training epoch:1345, step:0 
model_pd.l_p.mean(): 0.05644256994128227 
model_pd.l_d.mean(): -1.314091682434082 
model_pd.lagr.mean(): -1.2576490640640259 
model_pd.lambdas: dict_items([('pout', tensor([1.5884])), ('power', tensor([0.0782]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7877])), ('power', tensor([-0.7963]))])
epoch：1345	 i:0 	 global-step:26900	 l-p:0.05644256994128227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1346
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]])
 pt:tensor([[27.0944, 27.7891, 27.3841],
        [27.0944, 29.0098, 28.5966],
        [27.0944, 29.1034, 28.7150],
        [27.0944, 36.7453, 43.4963]], grad_fn=<SliceBackward0>)

training epoch:1346, step:0 
model_pd.l_p.mean(): 0.05644245445728302 
model_pd.l_d.mean(): -1.3134422302246094 
model_pd.lagr.mean(): -1.2569997310638428 
model_pd.lambdas: dict_items([('pout', tensor([1.5877])), ('power', tensor([0.0782]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7877])), ('power', tensor([-0.7958]))])
epoch：1346	 i:0 	 global-step:26920	 l-p:0.05644245445728302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[27.0948, 27.2889, 27.1309],
        [27.0948, 27.0986, 27.0949],
        [27.0948, 34.7630, 39.0399],
        [27.0948, 28.1085, 27.6324]], grad_fn=<SliceBackward0>)

training epoch:1347, step:0 
model_pd.l_p.mean(): 0.05644233524799347 
model_pd.l_d.mean(): -1.3127926588058472 
model_pd.lagr.mean(): -1.2563502788543701 
model_pd.lambdas: dict_items([('pout', tensor([1.5869])), ('power', tensor([0.0781]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7877])), ('power', tensor([-0.7954]))])
epoch：1347	 i:0 	 global-step:26940	 l-p:0.05644233524799347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.0952, 30.2358, 30.3482],
        [27.0952, 36.0713, 41.9451],
        [27.0952, 27.0970, 27.0952],
        [27.0952, 27.0968, 27.0952]], grad_fn=<SliceBackward0>)

training epoch:1348, step:0 
model_pd.l_p.mean(): 0.05644223093986511 
model_pd.l_d.mean(): -1.3121434450149536 
model_pd.lagr.mean(): -1.2557011842727661 
model_pd.lambdas: dict_items([('pout', tensor([1.5861])), ('power', tensor([0.0781]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7877])), ('power', tensor([-0.7951]))])
epoch：1348	 i:0 	 global-step:26960	 l-p:0.05644223093986511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[27.0956, 29.1050, 28.7165],
        [27.0956, 27.4661, 27.1990],
        [27.0956, 27.0956, 27.0956],
        [27.0956, 27.0956, 27.0956]], grad_fn=<SliceBackward0>)

training epoch:1349, step:0 
model_pd.l_p.mean(): 0.05644211173057556 
model_pd.l_d.mean(): -1.3114937543869019 
model_pd.lagr.mean(): -1.255051612854004 
model_pd.lambdas: dict_items([('pout', tensor([1.5853])), ('power', tensor([0.0780]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7877])), ('power', tensor([-0.7947]))])
epoch：1349	 i:0 	 global-step:26980	 l-p:0.05644211173057556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[27.0959, 30.2369, 30.3493],
        [27.0959, 27.0984, 27.0960],
        [27.0959, 28.9297, 28.4974],
        [27.0959, 30.4420, 30.6785]], grad_fn=<SliceBackward0>)

training epoch:1350, step:0 
model_pd.l_p.mean(): 0.056442007422447205 
model_pd.l_d.mean(): -1.3108443021774292 
model_pd.lagr.mean(): -1.2544022798538208 
model_pd.lambdas: dict_items([('pout', tensor([1.5845])), ('power', tensor([0.0780]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7878])), ('power', tensor([-0.7943]))])
epoch：1350	 i:0 	 global-step:27000	 l-p:0.056442007422447205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[27.0963, 27.0963, 27.0963],
        [27.0963, 27.4668, 27.1998],
        [27.0963, 27.0968, 27.0963],
        [27.0963, 27.0963, 27.0963]], grad_fn=<SliceBackward0>)

training epoch:1351, step:0 
model_pd.l_p.mean(): 0.056441906839609146 
model_pd.l_d.mean(): -1.310194969177246 
model_pd.lagr.mean(): -1.2537530660629272 
model_pd.lambdas: dict_items([('pout', tensor([1.5837])), ('power', tensor([0.0780]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7878])), ('power', tensor([-0.7939]))])
epoch：1351	 i:0 	 global-step:27020	 l-p:0.056441906839609146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.0967, 27.0967, 27.0967],
        [27.0967, 29.1064, 28.7180],
        [27.0967, 27.1073, 27.0970],
        [27.0967, 27.4672, 27.2001]], grad_fn=<SliceBackward0>)

training epoch:1352, step:0 
model_pd.l_p.mean(): 0.05644179880619049 
model_pd.l_d.mean(): -1.3095457553863525 
model_pd.lagr.mean(): -1.2531039714813232 
model_pd.lambdas: dict_items([('pout', tensor([1.5829])), ('power', tensor([0.0779]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7878])), ('power', tensor([-0.7936]))])
epoch：1352	 i:0 	 global-step:27040	 l-p:0.05644179880619049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]])
 pt:tensor([[27.0970, 28.1111, 27.6349],
        [27.0970, 32.3857, 34.1559],
        [27.0970, 27.1516, 27.1017],
        [27.0970, 35.1143, 39.8029]], grad_fn=<SliceBackward0>)

training epoch:1353, step:0 
model_pd.l_p.mean(): 0.05644169822335243 
model_pd.l_d.mean(): -1.3088960647583008 
model_pd.lagr.mean(): -1.252454400062561 
model_pd.lambdas: dict_items([('pout', tensor([1.5821])), ('power', tensor([0.0779]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7878])), ('power', tensor([-0.7932]))])
epoch：1353	 i:0 	 global-step:27060	 l-p:0.05644169822335243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1354
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228]])
 pt:tensor([[27.0974, 34.7676, 39.0460],
        [27.0974, 32.8840, 35.1299],
        [27.0974, 31.8191, 33.0839],
        [27.0974, 32.3863, 34.1566]], grad_fn=<SliceBackward0>)

training epoch:1354, step:0 
model_pd.l_p.mean(): 0.056441593915224075 
model_pd.l_d.mean(): -1.3082466125488281 
model_pd.lagr.mean(): -1.251805067062378 
model_pd.lambdas: dict_items([('pout', tensor([1.5814])), ('power', tensor([0.0779]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7878])), ('power', tensor([-0.7929]))])
epoch：1354	 i:0 	 global-step:27080	 l-p:0.056441593915224075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.0977, 27.1016, 27.0978],
        [27.0977, 29.1078, 28.7194],
        [27.0977, 27.0977, 27.0977],
        [27.0977, 28.6471, 28.1673]], grad_fn=<SliceBackward0>)

training epoch:1355, step:0 
model_pd.l_p.mean(): 0.056441497057676315 
model_pd.l_d.mean(): -1.3075973987579346 
model_pd.lagr.mean(): -1.2511558532714844 
model_pd.lambdas: dict_items([('pout', tensor([1.5806])), ('power', tensor([0.0778]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7879])), ('power', tensor([-0.7925]))])
epoch：1355	 i:0 	 global-step:27100	 l-p:0.056441497057676315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1356
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]])
 pt:tensor([[27.0981, 36.0766, 41.9525],
        [27.0981, 28.1123, 27.6360],
        [27.0981, 30.4451, 30.6819],
        [27.0981, 30.1421, 30.1986]], grad_fn=<SliceBackward0>)

training epoch:1356, step:0 
model_pd.l_p.mean(): 0.056441403925418854 
model_pd.l_d.mean(): -1.3069478273391724 
model_pd.lagr.mean(): -1.2505064010620117 
model_pd.lambdas: dict_items([('pout', tensor([1.5798])), ('power', tensor([0.0778]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7879])), ('power', tensor([-0.7922]))])
epoch：1356	 i:0 	 global-step:27120	 l-p:0.056441403925418854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[27.0984, 27.0984, 27.0984],
        [27.0984, 29.8566, 29.7617],
        [27.0984, 27.1403, 27.1015],
        [27.0984, 36.0773, 41.9534]], grad_fn=<SliceBackward0>)

training epoch:1357, step:0 
model_pd.l_p.mean(): 0.056441303342580795 
model_pd.l_d.mean(): -1.3062986135482788 
model_pd.lagr.mean(): -1.2498573064804077 
model_pd.lambdas: dict_items([('pout', tensor([1.5790])), ('power', tensor([0.0777]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7879])), ('power', tensor([-0.7918]))])
epoch：1357	 i:0 	 global-step:27140	 l-p:0.056441303342580795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[27.0987, 29.5926, 29.3758],
        [27.0987, 27.4714, 27.2031],
        [27.0987, 34.6216, 38.7286],
        [27.0987, 30.2410, 30.3537]], grad_fn=<SliceBackward0>)

training epoch:1358, step:0 
model_pd.l_p.mean(): 0.056441210210323334 
model_pd.l_d.mean(): -1.3056490421295166 
model_pd.lagr.mean(): -1.249207854270935 
model_pd.lambdas: dict_items([('pout', tensor([1.5782])), ('power', tensor([0.0777]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7879])), ('power', tensor([-0.7915]))])
epoch：1358	 i:0 	 global-step:27160	 l-p:0.056441210210323334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[27.0990, 27.1536, 27.1037],
        [27.0990, 29.8575, 29.7627],
        [27.0990, 27.0990, 27.0990],
        [27.0990, 27.4665, 27.2010]], grad_fn=<SliceBackward0>)

training epoch:1359, step:0 
model_pd.l_p.mean(): 0.05644112080335617 
model_pd.l_d.mean(): -1.304999828338623 
model_pd.lagr.mean(): -1.248558759689331 
model_pd.lambdas: dict_items([('pout', tensor([1.5774])), ('power', tensor([0.0777]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7879])), ('power', tensor([-0.7912]))])
epoch：1359	 i:0 	 global-step:27180	 l-p:0.05644112080335617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[27.0993, 27.1009, 27.0994],
        [27.0993, 28.4470, 27.9531],
        [27.0993, 32.8874, 35.1342],
        [27.0993, 27.4668, 27.2014]], grad_fn=<SliceBackward0>)

training epoch:1360, step:0 
model_pd.l_p.mean(): 0.05644103139638901 
model_pd.l_d.mean(): -1.3043501377105713 
model_pd.lagr.mean(): -1.2479090690612793 
model_pd.lambdas: dict_items([('pout', tensor([1.5766])), ('power', tensor([0.0776]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7880])), ('power', tensor([-0.7909]))])
epoch：1360	 i:0 	 global-step:27200	 l-p:0.05644103139638901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.0996, 27.0997, 27.0997],
        [27.0996, 32.3902, 34.1612],
        [27.0996, 31.2688, 32.0831],
        [27.0996, 27.4724, 27.2041]], grad_fn=<SliceBackward0>)

training epoch:1361, step:0 
model_pd.l_p.mean(): 0.05644094571471214 
model_pd.l_d.mean(): -1.3037008047103882 
model_pd.lagr.mean(): -1.2472598552703857 
model_pd.lambdas: dict_items([('pout', tensor([1.5758])), ('power', tensor([0.0776]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7880])), ('power', tensor([-0.7906]))])
epoch：1361	 i:0 	 global-step:27220	 l-p:0.05644094571471214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[27.0999, 27.4707, 27.2035],
        [27.0999, 27.0999, 27.0999],
        [27.0999, 27.1303, 27.1018],
        [27.0999, 27.1005, 27.0999]], grad_fn=<SliceBackward0>)

training epoch:1362, step:0 
model_pd.l_p.mean(): 0.05644085258245468 
model_pd.l_d.mean(): -1.303051471710205 
model_pd.lagr.mean(): -1.2466106414794922 
model_pd.lambdas: dict_items([('pout', tensor([1.5751])), ('power', tensor([0.0775]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7880])), ('power', tensor([-0.7903]))])
epoch：1362	 i:0 	 global-step:27240	 l-p:0.05644085258245468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[27.1002, 35.1204, 39.8111],
        [27.1002, 29.1112, 28.7227],
        [27.1002, 34.7729, 39.0532],
        [27.1002, 27.1002, 27.1002]], grad_fn=<SliceBackward0>)

training epoch:1363, step:0 
model_pd.l_p.mean(): 0.056440774351358414 
model_pd.l_d.mean(): -1.3024020195007324 
model_pd.lagr.mean(): -1.2459611892700195 
model_pd.lambdas: dict_items([('pout', tensor([1.5743])), ('power', tensor([0.0775]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7880])), ('power', tensor([-0.7900]))])
epoch：1363	 i:0 	 global-step:27260	 l-p:0.056440774351358414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[27.1005, 30.0523, 30.0575],
        [27.1005, 32.8428, 35.0442],
        [27.1005, 34.7735, 39.0539],
        [27.1005, 27.1005, 27.1005]], grad_fn=<SliceBackward0>)

training epoch:1364, step:0 
model_pd.l_p.mean(): 0.05644068866968155 
model_pd.l_d.mean(): -1.3017525672912598 
model_pd.lagr.mean(): -1.2453118562698364 
model_pd.lambdas: dict_items([('pout', tensor([1.5735])), ('power', tensor([0.0775]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7880])), ('power', tensor([-0.7897]))])
epoch：1364	 i:0 	 global-step:27280	 l-p:0.05644068866968155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.1008, 27.1033, 27.1008],
        [27.1008, 34.7740, 39.0546],
        [27.1008, 31.2708, 32.0853],
        [27.1008, 28.6511, 28.1711]], grad_fn=<SliceBackward0>)

training epoch:1365, step:0 
model_pd.l_p.mean(): 0.05644061043858528 
model_pd.l_d.mean(): -1.3011033535003662 
model_pd.lagr.mean(): -1.2446627616882324 
model_pd.lambdas: dict_items([('pout', tensor([1.5727])), ('power', tensor([0.0774]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7880])), ('power', tensor([-0.7894]))])
epoch：1365	 i:0 	 global-step:27300	 l-p:0.05644061043858528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1011, 27.1027, 27.1011],
        [27.1011, 28.9365, 28.5040],
        [27.1011, 32.3927, 34.1643],
        [27.1011, 27.1430, 27.1042]], grad_fn=<SliceBackward0>)

training epoch:1366, step:0 
model_pd.l_p.mean(): 0.056440528482198715 
model_pd.l_d.mean(): -1.3004536628723145 
model_pd.lagr.mean(): -1.2440131902694702 
model_pd.lambdas: dict_items([('pout', tensor([1.5719])), ('power', tensor([0.0774]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7891]))])
epoch：1366	 i:0 	 global-step:27320	 l-p:0.056440528482198715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1367
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[27.1013, 34.6266, 38.7354],
        [27.1013, 32.8443, 35.0461],
        [27.1013, 27.2957, 27.1375],
        [27.1013, 27.1014, 27.1014]], grad_fn=<SliceBackward0>)

training epoch:1367, step:0 
model_pd.l_p.mean(): 0.056440453976392746 
model_pd.l_d.mean(): -1.2998040914535522 
model_pd.lagr.mean(): -1.243363618850708 
model_pd.lambdas: dict_items([('pout', tensor([1.5711])), ('power', tensor([0.0773]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7889]))])
epoch：1367	 i:0 	 global-step:27340	 l-p:0.056440453976392746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[27.1016, 27.2960, 27.1377],
        [27.1016, 36.7593, 43.5164],
        [27.1016, 29.5968, 29.3802],
        [27.1016, 27.1122, 27.1020]], grad_fn=<SliceBackward0>)

training epoch:1368, step:0 
model_pd.l_p.mean(): 0.05644036829471588 
model_pd.l_d.mean(): -1.2991548776626587 
model_pd.lagr.mean(): -1.242714524269104 
model_pd.lambdas: dict_items([('pout', tensor([1.5703])), ('power', tensor([0.0773]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7886]))])
epoch：1368	 i:0 	 global-step:27360	 l-p:0.05644036829471588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[27.1019, 34.6277, 38.7368],
        [27.1019, 36.7598, 43.5172],
        [27.1019, 27.1037, 27.1019],
        [27.1019, 27.7976, 27.3921]], grad_fn=<SliceBackward0>)

training epoch:1369, step:0 
model_pd.l_p.mean(): 0.05644029751420021 
model_pd.l_d.mean(): -1.2985053062438965 
model_pd.lagr.mean(): -1.2420649528503418 
model_pd.lambdas: dict_items([('pout', tensor([1.5695])), ('power', tensor([0.0773]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7883]))])
epoch：1369	 i:0 	 global-step:27380	 l-p:0.05644029751420021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.1021, 27.4731, 27.2057],
        [27.1021, 32.8861, 35.1280],
        [27.1021, 27.2965, 27.1382],
        [27.1021, 34.7767, 39.0583]], grad_fn=<SliceBackward0>)

training epoch:1370, step:0 
model_pd.l_p.mean(): 0.056440215557813644 
model_pd.l_d.mean(): -1.2978559732437134 
model_pd.lagr.mean(): -1.2414157390594482 
model_pd.lambdas: dict_items([('pout', tensor([1.5688])), ('power', tensor([0.0772]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7881]))])
epoch：1370	 i:0 	 global-step:27400	 l-p:0.056440215557813644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[27.1024, 30.0552, 30.0606],
        [27.1024, 27.4623, 27.2010],
        [27.1024, 27.9303, 27.4883],
        [27.1024, 36.7609, 43.5188]], grad_fn=<SliceBackward0>)

training epoch:1371, step:0 
model_pd.l_p.mean(): 0.056440144777297974 
model_pd.l_d.mean(): -1.2972062826156616 
model_pd.lagr.mean(): -1.240766167640686 
model_pd.lambdas: dict_items([('pout', tensor([1.5680])), ('power', tensor([0.0772]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7878]))])
epoch：1371	 i:0 	 global-step:27420	 l-p:0.056440144777297974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.1026, 27.1026, 27.1026],
        [27.1026, 27.1026, 27.1026],
        [27.1026, 27.1051, 27.1027],
        [27.1026, 27.9306, 27.4886]], grad_fn=<SliceBackward0>)

training epoch:1372, step:0 
model_pd.l_p.mean(): 0.0564400777220726 
model_pd.l_d.mean(): -1.296556830406189 
model_pd.lagr.mean(): -1.2401167154312134 
model_pd.lambdas: dict_items([('pout', tensor([1.5672])), ('power', tensor([0.0771]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7881])), ('power', tensor([-0.7875]))])
epoch：1372	 i:0 	 global-step:27440	 l-p:0.0564400777220726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.1029, 27.1067, 27.1029],
        [27.1029, 27.2973, 27.1390],
        [27.1029, 27.1047, 27.1029],
        [27.1029, 27.4740, 27.2065]], grad_fn=<SliceBackward0>)

training epoch:1373, step:0 
model_pd.l_p.mean(): 0.056439999490976334 
model_pd.l_d.mean(): -1.2959076166152954 
model_pd.lagr.mean(): -1.2394676208496094 
model_pd.lambdas: dict_items([('pout', tensor([1.5664])), ('power', tensor([0.0771]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7873]))])
epoch：1373	 i:0 	 global-step:27460	 l-p:0.056439999490976334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[27.1031, 27.4709, 27.2052],
        [27.1031, 28.9394, 28.5068],
        [27.1031, 27.4762, 27.2076],
        [27.1031, 28.1185, 27.6418]], grad_fn=<SliceBackward0>)

training epoch:1374, step:0 
model_pd.l_p.mean(): 0.05643993243575096 
model_pd.l_d.mean(): -1.2952581644058228 
model_pd.lagr.mean(): -1.2388182878494263 
model_pd.lambdas: dict_items([('pout', tensor([1.5656])), ('power', tensor([0.0771]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7871]))])
epoch：1374	 i:0 	 global-step:27480	 l-p:0.05643993243575096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.1033, 30.0568, 30.0622],
        [27.1033, 31.5770, 32.6326],
        [27.1033, 27.9315, 27.4894],
        [27.1033, 27.1049, 27.1034]], grad_fn=<SliceBackward0>)

training epoch:1375, step:0 
model_pd.l_p.mean(): 0.05643986538052559 
model_pd.l_d.mean(): -1.2946085929870605 
model_pd.lagr.mean(): -1.238168716430664 
model_pd.lambdas: dict_items([('pout', tensor([1.5648])), ('power', tensor([0.0770]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7868]))])
epoch：1375	 i:0 	 global-step:27500	 l-p:0.05643986538052559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.1036, 28.9401, 28.5074],
        [27.1036, 27.1456, 27.1067],
        [27.1036, 34.7796, 39.0624],
        [27.1036, 27.1061, 27.1036]], grad_fn=<SliceBackward0>)

training epoch:1376, step:0 
model_pd.l_p.mean(): 0.05643979460000992 
model_pd.l_d.mean(): -1.2939592599868774 
model_pd.lagr.mean(): -1.2375195026397705 
model_pd.lambdas: dict_items([('pout', tensor([1.5640])), ('power', tensor([0.0770]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7866]))])
epoch：1376	 i:0 	 global-step:27520	 l-p:0.05643979460000992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1377
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4430,  0.3377,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228]])
 pt:tensor([[27.1038, 32.8490, 35.0521],
        [27.1038, 28.1194, 27.6426],
        [27.1038, 28.4528, 27.9586],
        [27.1038, 31.2760, 32.0914]], grad_fn=<SliceBackward0>)

training epoch:1377, step:0 
model_pd.l_p.mean(): 0.056439727544784546 
model_pd.l_d.mean(): -1.2933098077774048 
model_pd.lagr.mean(): -1.2368700504302979 
model_pd.lambdas: dict_items([('pout', tensor([1.5632])), ('power', tensor([0.0769]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7864]))])
epoch：1377	 i:0 	 global-step:27540	 l-p:0.056439727544784546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.1040, 27.1065, 27.1041],
        [27.1040, 28.1197, 27.6428],
        [27.1040, 29.8651, 29.7707],
        [27.1040, 27.4752, 27.2077]], grad_fn=<SliceBackward0>)

training epoch:1378, step:0 
model_pd.l_p.mean(): 0.05643966794013977 
model_pd.l_d.mean(): -1.2926603555679321 
model_pd.lagr.mean(): -1.2362207174301147 
model_pd.lambdas: dict_items([('pout', tensor([1.5624])), ('power', tensor([0.0769]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7861]))])
epoch：1378	 i:0 	 global-step:27560	 l-p:0.05643966794013977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[27.1042, 36.0892, 41.9707],
        [27.1042, 27.8004, 27.3947],
        [27.1042, 29.6008, 29.3843],
        [27.1042, 27.1048, 27.1042]], grad_fn=<SliceBackward0>)

training epoch:1379, step:0 
model_pd.l_p.mean(): 0.0564396008849144 
model_pd.l_d.mean(): -1.29201078414917 
model_pd.lagr.mean(): -1.2355711460113525 
model_pd.lambdas: dict_items([('pout', tensor([1.5617])), ('power', tensor([0.0769]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7859]))])
epoch：1379	 i:0 	 global-step:27580	 l-p:0.0564396008849144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.1045, 27.1063, 27.1045],
        [27.1045, 29.1173, 28.7287],
        [27.1045, 34.7815, 39.0650],
        [27.1045, 29.8659, 29.7715]], grad_fn=<SliceBackward0>)

training epoch:1380, step:0 
model_pd.l_p.mean(): 0.056439537554979324 
model_pd.l_d.mean(): -1.2913614511489868 
model_pd.lagr.mean(): -1.234921932220459 
model_pd.lambdas: dict_items([('pout', tensor([1.5609])), ('power', tensor([0.0768]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7882])), ('power', tensor([-0.7857]))])
epoch：1380	 i:0 	 global-step:27600	 l-p:0.056439537554979324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.1047, 27.1047, 27.1047],
        [27.1047, 27.1048, 27.1047],
        [27.1047, 36.0902, 41.9721],
        [27.1047, 28.6564, 28.1762]], grad_fn=<SliceBackward0>)

training epoch:1381, step:0 
model_pd.l_p.mean(): 0.05643947422504425 
model_pd.l_d.mean(): -1.2907118797302246 
model_pd.lagr.mean(): -1.2342723608016968 
model_pd.lambdas: dict_items([('pout', tensor([1.5601])), ('power', tensor([0.0768]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7855]))])
epoch：1381	 i:0 	 global-step:27620	 l-p:0.05643947422504425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[27.1049, 27.1049, 27.1049],
        [27.1049, 27.1595, 27.1096],
        [27.1049, 34.7824, 39.0663],
        [27.1049, 27.1067, 27.1049]], grad_fn=<SliceBackward0>)

training epoch:1382, step:0 
model_pd.l_p.mean(): 0.056439414620399475 
model_pd.l_d.mean(): -1.290062427520752 
model_pd.lagr.mean(): -1.2336230278015137 
model_pd.lambdas: dict_items([('pout', tensor([1.5593])), ('power', tensor([0.0767]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7853]))])
epoch：1382	 i:0 	 global-step:27640	 l-p:0.056439414620399475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[27.1051, 34.7829, 39.0669],
        [27.1051, 30.4565, 30.6944],
        [27.1051, 29.1183, 28.7297],
        [27.1051, 27.1157, 27.1055]], grad_fn=<SliceBackward0>)

training epoch:1383, step:0 
model_pd.l_p.mean(): 0.0564393550157547 
model_pd.l_d.mean(): -1.2894129753112793 
model_pd.lagr.mean(): -1.232973575592041 
model_pd.lambdas: dict_items([('pout', tensor([1.5585])), ('power', tensor([0.0767]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7850]))])
epoch：1383	 i:0 	 global-step:27660	 l-p:0.0564393550157547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.1053, 27.1599, 27.1100],
        [27.1053, 27.4656, 27.2040],
        [27.1053, 27.1357, 27.1071],
        [27.1053, 32.8519, 35.0559]], grad_fn=<SliceBackward0>)

training epoch:1384, step:0 
model_pd.l_p.mean(): 0.056439295411109924 
model_pd.l_d.mean(): -1.288763403892517 
model_pd.lagr.mean(): -1.2323241233825684 
model_pd.lambdas: dict_items([('pout', tensor([1.5577])), ('power', tensor([0.0767]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7848]))])
epoch：1384	 i:0 	 global-step:27680	 l-p:0.056439295411109924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1385
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[27.1055, 30.4572, 30.6951],
        [27.1055, 28.6576, 28.1773],
        [27.1055, 27.9342, 27.4918],
        [27.1055, 31.5810, 32.6374]], grad_fn=<SliceBackward0>)

training epoch:1385, step:0 
model_pd.l_p.mean(): 0.056439243257045746 
model_pd.l_d.mean(): -1.2881141901016235 
model_pd.lagr.mean(): -1.2316749095916748 
model_pd.lambdas: dict_items([('pout', tensor([1.5569])), ('power', tensor([0.0766]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7846]))])
epoch：1385	 i:0 	 global-step:27700	 l-p:0.056439243257045746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[27.1057, 27.1075, 27.1057],
        [27.1057, 36.0925, 41.9755],
        [27.1057, 35.1317, 39.8271],
        [27.1057, 31.2794, 32.0954]], grad_fn=<SliceBackward0>)

training epoch:1386, step:0 
model_pd.l_p.mean(): 0.05643917992711067 
model_pd.l_d.mean(): -1.2874644994735718 
model_pd.lagr.mean(): -1.2310253381729126 
model_pd.lambdas: dict_items([('pout', tensor([1.5561])), ('power', tensor([0.0766]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7844]))])
epoch：1386	 i:0 	 global-step:27720	 l-p:0.05643917992711067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01]])
 pt:tensor([[27.1059, 27.1077, 27.1059],
        [27.1059, 27.1362, 27.1077],
        [27.1059, 30.1545, 30.2119],
        [27.1059, 27.1059, 27.1059]], grad_fn=<SliceBackward0>)

training epoch:1387, step:0 
model_pd.l_p.mean(): 0.056439124047756195 
model_pd.l_d.mean(): -1.2868150472640991 
model_pd.lagr.mean(): -1.23037588596344 
model_pd.lambdas: dict_items([('pout', tensor([1.5554])), ('power', tensor([0.0766]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7842]))])
epoch：1387	 i:0 	 global-step:27740	 l-p:0.056439124047756195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.1060, 36.7692, 43.5313],
        [27.1060, 27.4664, 27.2048],
        [27.1060, 30.1549, 30.2123],
        [27.1060, 27.1061, 27.1060]], grad_fn=<SliceBackward0>)

training epoch:1388, step:0 
model_pd.l_p.mean(): 0.05643907189369202 
model_pd.l_d.mean(): -1.2861655950546265 
model_pd.lagr.mean(): -1.2297265529632568 
model_pd.lambdas: dict_items([('pout', tensor([1.5546])), ('power', tensor([0.0765]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7883])), ('power', tensor([-0.7841]))])
epoch：1388	 i:0 	 global-step:27760	 l-p:0.05643907189369202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[27.1062, 34.6369, 38.7497],
        [27.1062, 31.8350, 33.1030],
        [27.1062, 27.2203, 27.1215],
        [27.1062, 28.4561, 27.9617]], grad_fn=<SliceBackward0>)

training epoch:1389, step:0 
model_pd.l_p.mean(): 0.05643901228904724 
model_pd.l_d.mean(): -1.2855161428451538 
model_pd.lagr.mean(): -1.2290771007537842 
model_pd.lambdas: dict_items([('pout', tensor([1.5538])), ('power', tensor([0.0765]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7839]))])
epoch：1389	 i:0 	 global-step:27780	 l-p:0.05643901228904724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[27.1064, 31.5829, 32.6397],
        [27.1064, 27.4779, 27.2101],
        [27.1064, 28.4564, 27.9619],
        [27.1064, 30.4589, 30.6970]], grad_fn=<SliceBackward0>)

training epoch:1390, step:0 
model_pd.l_p.mean(): 0.05643896758556366 
model_pd.l_d.mean(): -1.2848666906356812 
model_pd.lagr.mean(): -1.228427767753601 
model_pd.lambdas: dict_items([('pout', tensor([1.5530])), ('power', tensor([0.0764]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7837]))])
epoch：1390	 i:0 	 global-step:27800	 l-p:0.05643896758556366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]])
 pt:tensor([[27.1066, 30.0623, 30.0681],
        [27.1066, 27.3013, 27.1428],
        [27.1066, 31.5832, 32.6401],
        [27.1066, 27.1172, 27.1070]], grad_fn=<SliceBackward0>)

training epoch:1391, step:0 
model_pd.l_p.mean(): 0.05643891170620918 
model_pd.l_d.mean(): -1.284217357635498 
model_pd.lagr.mean(): -1.227778434753418 
model_pd.lambdas: dict_items([('pout', tensor([1.5522])), ('power', tensor([0.0764]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7835]))])
epoch：1391	 i:0 	 global-step:27820	 l-p:0.05643891170620918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]])
 pt:tensor([[27.1068, 27.1488, 27.1099],
        [27.1068, 27.1371, 27.1086],
        [27.1068, 27.3258, 27.1506],
        [27.1068, 32.4038, 34.1783]], grad_fn=<SliceBackward0>)

training epoch:1392, step:0 
model_pd.l_p.mean(): 0.056438855826854706 
model_pd.l_d.mean(): -1.2835679054260254 
model_pd.lagr.mean(): -1.2271291017532349 
model_pd.lambdas: dict_items([('pout', tensor([1.5514])), ('power', tensor([0.0764]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7833]))])
epoch：1392	 i:0 	 global-step:27840	 l-p:0.056438855826854706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[27.1069, 27.8037, 27.3977],
        [27.1069, 29.8700, 29.7759],
        [27.1069, 28.4571, 27.9626],
        [27.1069, 27.1616, 27.1116]], grad_fn=<SliceBackward0>)

training epoch:1393, step:0 
model_pd.l_p.mean(): 0.05643880367279053 
model_pd.l_d.mean(): -1.2829182147979736 
model_pd.lagr.mean(): -1.226479411125183 
model_pd.lambdas: dict_items([('pout', tensor([1.5506])), ('power', tensor([0.0763]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7831]))])
epoch：1393	 i:0 	 global-step:27860	 l-p:0.05643880367279053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.1071, 27.1071, 27.1071],
        [27.1071, 31.2823, 32.0988],
        [27.1071, 31.5843, 32.6414],
        [27.1071, 30.1567, 30.2143]], grad_fn=<SliceBackward0>)

training epoch:1394, step:0 
model_pd.l_p.mean(): 0.056438758969306946 
model_pd.l_d.mean(): -1.2822686433792114 
model_pd.lagr.mean(): -1.225829839706421 
model_pd.lambdas: dict_items([('pout', tensor([1.5498])), ('power', tensor([0.0763]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7830]))])
epoch：1394	 i:0 	 global-step:27880	 l-p:0.056438758969306946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[27.1073, 27.1088, 27.1073],
        [27.1073, 29.1217, 28.7330],
        [27.1073, 28.3096, 27.8161],
        [27.1073, 27.1619, 27.1120]], grad_fn=<SliceBackward0>)

training epoch:1395, step:0 
model_pd.l_p.mean(): 0.05643870681524277 
model_pd.l_d.mean(): -1.2816191911697388 
model_pd.lagr.mean(): -1.2251805067062378 
model_pd.lambdas: dict_items([('pout', tensor([1.5490])), ('power', tensor([0.0762]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7828]))])
epoch：1395	 i:0 	 global-step:27900	 l-p:0.05643870681524277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[27.1074, 27.1099, 27.1075],
        [27.1074, 27.3022, 27.1436],
        [27.1074, 34.7883, 39.0747],
        [27.1074, 27.8043, 27.3982]], grad_fn=<SliceBackward0>)

training epoch:1396, step:0 
model_pd.l_p.mean(): 0.05643865466117859 
model_pd.l_d.mean(): -1.2809696197509766 
model_pd.lagr.mean(): -1.2245309352874756 
model_pd.lambdas: dict_items([('pout', tensor([1.5483])), ('power', tensor([0.0762]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7826]))])
epoch：1396	 i:0 	 global-step:27920	 l-p:0.05643865466117859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.1076, 30.2557, 30.3697],
        [27.1076, 27.1076, 27.1076],
        [27.1076, 32.8973, 35.1426],
        [27.1076, 27.4812, 27.2123]], grad_fn=<SliceBackward0>)

training epoch:1397, step:0 
model_pd.l_p.mean(): 0.056438613682985306 
model_pd.l_d.mean(): -1.2803200483322144 
model_pd.lagr.mean(): -1.223881483078003 
model_pd.lambdas: dict_items([('pout', tensor([1.5475])), ('power', tensor([0.0762]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7825]))])
epoch：1397	 i:0 	 global-step:27940	 l-p:0.056438613682985306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]])
 pt:tensor([[27.1077, 28.3102, 27.8167],
        [27.1077, 27.1102, 27.1078],
        [27.1077, 29.8715, 29.7774],
        [27.1077, 27.1096, 27.1078]], grad_fn=<SliceBackward0>)

training epoch:1398, step:0 
model_pd.l_p.mean(): 0.056438565254211426 
model_pd.l_d.mean(): -1.2796707153320312 
model_pd.lagr.mean(): -1.2232321500778198 
model_pd.lambdas: dict_items([('pout', tensor([1.5467])), ('power', tensor([0.0761]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7884])), ('power', tensor([-0.7823]))])
epoch：1398	 i:0 	 global-step:27960	 l-p:0.056438565254211426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.1079, 29.6069, 29.3906],
        [27.1079, 27.1383, 27.1097],
        [27.1079, 27.2220, 27.1231],
        [27.1079, 27.9372, 27.4945]], grad_fn=<SliceBackward0>)

training epoch:1399, step:0 
model_pd.l_p.mean(): 0.056438520550727844 
model_pd.l_d.mean(): -1.2790212631225586 
model_pd.lagr.mean(): -1.2225826978683472 
model_pd.lambdas: dict_items([('pout', tensor([1.5459])), ('power', tensor([0.0761]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7822]))])
epoch：1399	 i:0 	 global-step:27980	 l-p:0.056438520550727844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1080, 28.0646, 27.5963],
        [27.1080, 27.1080, 27.1080],
        [27.1080, 27.1627, 27.1127],
        [27.1080, 27.1501, 27.1111]], grad_fn=<SliceBackward0>)

training epoch:1400, step:0 
model_pd.l_p.mean(): 0.05643847584724426 
model_pd.l_d.mean(): -1.2783715724945068 
model_pd.lagr.mean(): -1.221933126449585 
model_pd.lambdas: dict_items([('pout', tensor([1.5451])), ('power', tensor([0.0760]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7820]))])
epoch：1400	 i:0 	 global-step:28000	 l-p:0.05643847584724426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[27.1081, 27.1087, 27.1082],
        [27.1081, 30.1588, 30.2166],
        [27.1081, 34.6416, 38.7565],
        [27.1081, 36.0986, 41.9849]], grad_fn=<SliceBackward0>)

training epoch:1401, step:0 
model_pd.l_p.mean(): 0.05643844231963158 
model_pd.l_d.mean(): -1.2777222394943237 
model_pd.lagr.mean(): -1.2212837934494019 
model_pd.lambdas: dict_items([('pout', tensor([1.5443])), ('power', tensor([0.0760]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7819]))])
epoch：1401	 i:0 	 global-step:28020	 l-p:0.05643844231963158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[27.1083, 27.1083, 27.1083],
        [27.1083, 31.5869, 32.6446],
        [27.1083, 30.0655, 30.0716],
        [27.1083, 27.1088, 27.1083]], grad_fn=<SliceBackward0>)

training epoch:1402, step:0 
model_pd.l_p.mean(): 0.0564383901655674 
model_pd.l_d.mean(): -1.277072548866272 
model_pd.lagr.mean(): -1.22063410282135 
model_pd.lambdas: dict_items([('pout', tensor([1.5435])), ('power', tensor([0.0760]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7817]))])
epoch：1402	 i:0 	 global-step:28040	 l-p:0.0564383901655674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01]])
 pt:tensor([[27.1084, 27.1084, 27.1084],
        [27.1084, 28.4594, 27.9647],
        [27.1084, 28.6620, 28.1815],
        [27.1084, 36.7753, 43.5410]], grad_fn=<SliceBackward0>)

training epoch:1403, step:0 
model_pd.l_p.mean(): 0.05643835663795471 
model_pd.l_d.mean(): -1.2764230966567993 
model_pd.lagr.mean(): -1.219984769821167 
model_pd.lambdas: dict_items([('pout', tensor([1.5427])), ('power', tensor([0.0759]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7816]))])
epoch：1403	 i:0 	 global-step:28060	 l-p:0.05643835663795471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]])
 pt:tensor([[27.1086, 30.2577, 30.3719],
        [27.1086, 27.4823, 27.2133],
        [27.1086, 31.5875, 32.6454],
        [27.1086, 27.1633, 27.1133]], grad_fn=<SliceBackward0>)

training epoch:1404, step:0 
model_pd.l_p.mean(): 0.05643830820918083 
model_pd.l_d.mean(): -1.275773525238037 
model_pd.lagr.mean(): -1.2193351984024048 
model_pd.lambdas: dict_items([('pout', tensor([1.5419])), ('power', tensor([0.0759]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7815]))])
epoch：1404	 i:0 	 global-step:28080	 l-p:0.05643830820918083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1087, 34.7916, 39.0796],
        [27.1087, 28.1260, 27.6485],
        [27.1087, 29.6084, 29.3922],
        [27.1087, 27.1508, 27.1118]], grad_fn=<SliceBackward0>)

training epoch:1405, step:0 
model_pd.l_p.mean(): 0.056438278406858444 
model_pd.l_d.mean(): -1.2751243114471436 
model_pd.lagr.mean(): -1.2186859846115112 
model_pd.lambdas: dict_items([('pout', tensor([1.5412])), ('power', tensor([0.0758]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7813]))])
epoch：1405	 i:0 	 global-step:28100	 l-p:0.056438278406858444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.1088, 31.8407, 33.1102],
        [27.1088, 27.1127, 27.1089],
        [27.1088, 30.0666, 30.0728],
        [27.1088, 27.4807, 27.2127]], grad_fn=<SliceBackward0>)

training epoch:1406, step:0 
model_pd.l_p.mean(): 0.056438229978084564 
model_pd.l_d.mean(): -1.2744746208190918 
model_pd.lagr.mean(): -1.218036413192749 
model_pd.lambdas: dict_items([('pout', tensor([1.5404])), ('power', tensor([0.0758]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7812]))])
epoch：1406	 i:0 	 global-step:28120	 l-p:0.056438229978084564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1407
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5823,  0.4862,  1.0000,  0.4060,
          1.0000,  0.8350, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228]])
 pt:tensor([[27.1089, 29.6089, 29.3927],
        [27.1089, 32.9007, 35.1472],
        [27.1089, 34.7923, 39.0807],
        [27.1089, 28.4602, 27.9654]], grad_fn=<SliceBackward0>)

training epoch:1407, step:0 
model_pd.l_p.mean(): 0.05643819272518158 
model_pd.l_d.mean(): -1.2738251686096191 
model_pd.lagr.mean(): -1.2173869609832764 
model_pd.lambdas: dict_items([('pout', tensor([1.5396])), ('power', tensor([0.0758]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7810]))])
epoch：1407	 i:0 	 global-step:28140	 l-p:0.05643819272518158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[27.1091, 27.1129, 27.1091],
        [27.1091, 28.0661, 27.5976],
        [27.1091, 31.5887, 32.6469],
        [27.1091, 28.1265, 27.6490]], grad_fn=<SliceBackward0>)

training epoch:1408, step:0 
model_pd.l_p.mean(): 0.05643815919756889 
model_pd.l_d.mean(): -1.2731757164001465 
model_pd.lagr.mean(): -1.2167375087738037 
model_pd.lambdas: dict_items([('pout', tensor([1.5388])), ('power', tensor([0.0757]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7809]))])
epoch：1408	 i:0 	 global-step:28160	 l-p:0.05643815919756889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.1092, 27.1110, 27.1092],
        [27.1092, 27.1092, 27.1092],
        [27.1092, 29.0311, 28.6175],
        [27.1092, 34.7929, 39.0817]], grad_fn=<SliceBackward0>)

training epoch:1409, step:0 
model_pd.l_p.mean(): 0.05643811821937561 
model_pd.l_d.mean(): -1.2725259065628052 
model_pd.lagr.mean(): -1.216087818145752 
model_pd.lambdas: dict_items([('pout', tensor([1.5380])), ('power', tensor([0.0757]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7808]))])
epoch：1409	 i:0 	 global-step:28180	 l-p:0.05643811821937561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[27.1093, 30.0676, 30.0739],
        [27.1093, 27.9392, 27.4963],
        [27.1093, 27.1132, 27.1094],
        [27.1093, 27.3042, 27.1455]], grad_fn=<SliceBackward0>)

training epoch:1410, step:0 
model_pd.l_p.mean(): 0.056438084691762924 
model_pd.l_d.mean(): -1.2718764543533325 
model_pd.lagr.mean(): -1.2154383659362793 
model_pd.lambdas: dict_items([('pout', tensor([1.5372])), ('power', tensor([0.0757]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7885])), ('power', tensor([-0.7807]))])
epoch：1410	 i:0 	 global-step:28200	 l-p:0.056438084691762924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1411
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[27.1094, 36.7782, 43.5457],
        [27.1094, 34.7936, 39.0827],
        [27.1094, 30.1614, 30.2195],
        [27.1094, 28.6637, 28.1830]], grad_fn=<SliceBackward0>)

training epoch:1411, step:0 
model_pd.l_p.mean(): 0.05643804371356964 
model_pd.l_d.mean(): -1.2712268829345703 
model_pd.lagr.mean(): -1.214788794517517 
model_pd.lambdas: dict_items([('pout', tensor([1.5364])), ('power', tensor([0.0756]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7805]))])
epoch：1411	 i:0 	 global-step:28220	 l-p:0.05643804371356964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[27.1095, 27.1120, 27.1096],
        [27.1095, 27.1516, 27.1126],
        [27.1095, 27.1095, 27.1095],
        [27.1095, 28.0668, 27.5983]], grad_fn=<SliceBackward0>)

training epoch:1412, step:0 
model_pd.l_p.mean(): 0.056438006460666656 
model_pd.l_d.mean(): -1.2705774307250977 
model_pd.lagr.mean(): -1.214139461517334 
model_pd.lambdas: dict_items([('pout', tensor([1.5356])), ('power', tensor([0.0756]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7804]))])
epoch：1412	 i:0 	 global-step:28240	 l-p:0.056438006460666656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[27.1097, 27.1097, 27.1097],
        [27.1097, 27.1097, 27.1097],
        [27.1097, 27.1098, 27.1097],
        [27.1097, 32.9026, 35.1498]], grad_fn=<SliceBackward0>)

training epoch:1413, step:0 
model_pd.l_p.mean(): 0.05643797293305397 
model_pd.l_d.mean(): -1.2699278593063354 
model_pd.lagr.mean(): -1.2134898900985718 
model_pd.lambdas: dict_items([('pout', tensor([1.5349])), ('power', tensor([0.0755]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7803]))])
epoch：1413	 i:0 	 global-step:28260	 l-p:0.05643797293305397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[27.1098, 30.1622, 30.2203],
        [27.1098, 27.1204, 27.1101],
        [27.1098, 27.9399, 27.4968],
        [27.1098, 29.0322, 28.6186]], grad_fn=<SliceBackward0>)

training epoch:1414, step:0 
model_pd.l_p.mean(): 0.056437939405441284 
model_pd.l_d.mean(): -1.2692782878875732 
model_pd.lagr.mean(): -1.2128403186798096 
model_pd.lambdas: dict_items([('pout', tensor([1.5341])), ('power', tensor([0.0755]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7802]))])
epoch：1414	 i:0 	 global-step:28280	 l-p:0.056437939405441284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[27.1099, 27.3292, 27.1538],
        [27.1099, 27.4820, 27.2138],
        [27.1099, 29.0324, 28.6188],
        [27.1099, 31.8433, 33.1136]], grad_fn=<SliceBackward0>)

training epoch:1415, step:0 
model_pd.l_p.mean(): 0.0564379021525383 
model_pd.l_d.mean(): -1.268628716468811 
model_pd.lagr.mean(): -1.212190866470337 
model_pd.lambdas: dict_items([('pout', tensor([1.5333])), ('power', tensor([0.0755]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7801]))])
epoch：1415	 i:0 	 global-step:28300	 l-p:0.0564379021525383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1416
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228]])
 pt:tensor([[27.1100, 32.9098, 35.1636],
        [27.1100, 28.0674, 27.5988],
        [27.1100, 28.3137, 27.8199],
        [27.1100, 28.1279, 27.6502]], grad_fn=<SliceBackward0>)

training epoch:1416, step:0 
model_pd.l_p.mean(): 0.05643787980079651 
model_pd.l_d.mean(): -1.2679792642593384 
model_pd.lagr.mean(): -1.2115414142608643 
model_pd.lambdas: dict_items([('pout', tensor([1.5325])), ('power', tensor([0.0754]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7800]))])
epoch：1416	 i:0 	 global-step:28320	 l-p:0.05643787980079651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[27.1101, 27.1101, 27.1101],
        [27.1101, 32.8634, 35.0713],
        [27.1101, 28.1280, 27.6504],
        [27.1101, 31.8439, 33.1143]], grad_fn=<SliceBackward0>)

training epoch:1417, step:0 
model_pd.l_p.mean(): 0.056437838822603226 
model_pd.l_d.mean(): -1.2673296928405762 
model_pd.lagr.mean(): -1.210891842842102 
model_pd.lambdas: dict_items([('pout', tensor([1.5317])), ('power', tensor([0.0754]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7798]))])
epoch：1417	 i:0 	 global-step:28340	 l-p:0.056437838822603226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1102, 29.0330, 28.6194],
        [27.1102, 27.1406, 27.1120],
        [27.1102, 27.1107, 27.1102],
        [27.1102, 27.1523, 27.1133]], grad_fn=<SliceBackward0>)

training epoch:1418, step:0 
model_pd.l_p.mean(): 0.05643780529499054 
model_pd.l_d.mean(): -1.2666802406311035 
model_pd.lagr.mean(): -1.2102423906326294 
model_pd.lambdas: dict_items([('pout', tensor([1.5309])), ('power', tensor([0.0753]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7797]))])
epoch：1418	 i:0 	 global-step:28360	 l-p:0.05643780529499054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[27.1103, 27.1108, 27.1103],
        [27.1103, 28.3142, 27.8203],
        [27.1103, 31.2896, 32.1079],
        [27.1103, 27.1103, 27.1103]], grad_fn=<SliceBackward0>)

training epoch:1419, step:0 
model_pd.l_p.mean(): 0.05643777549266815 
model_pd.l_d.mean(): -1.2660306692123413 
model_pd.lagr.mean(): -1.2095929384231567 
model_pd.lambdas: dict_items([('pout', tensor([1.5301])), ('power', tensor([0.0753]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7796]))])
epoch：1419	 i:0 	 global-step:28380	 l-p:0.05643777549266815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.1104, 30.4672, 30.7066],
        [27.1104, 27.4793, 27.2129],
        [27.1104, 27.1104, 27.1104],
        [27.1104, 29.1273, 28.7386]], grad_fn=<SliceBackward0>)

training epoch:1420, step:0 
model_pd.l_p.mean(): 0.056437745690345764 
model_pd.l_d.mean(): -1.2653809785842896 
model_pd.lagr.mean(): -1.208943247795105 
model_pd.lambdas: dict_items([('pout', tensor([1.5293])), ('power', tensor([0.0753]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7795]))])
epoch：1420	 i:0 	 global-step:28400	 l-p:0.056437745690345764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[27.1105, 27.4794, 27.2130],
        [27.1105, 29.1275, 28.7388],
        [27.1105, 28.4627, 27.9678],
        [27.1105, 31.8450, 33.1157]], grad_fn=<SliceBackward0>)

training epoch:1421, step:0 
model_pd.l_p.mean(): 0.05643771216273308 
model_pd.l_d.mean(): -1.264731526374817 
model_pd.lagr.mean(): -1.2082937955856323 
model_pd.lambdas: dict_items([('pout', tensor([1.5285])), ('power', tensor([0.0752]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7794]))])
epoch：1421	 i:0 	 global-step:28420	 l-p:0.05643771216273308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]])
 pt:tensor([[27.1106, 27.1212, 27.1109],
        [27.1106, 30.4677, 30.7071],
        [27.1106, 30.0704, 30.0771],
        [27.1106, 27.3057, 27.1468]], grad_fn=<SliceBackward0>)

training epoch:1422, step:0 
model_pd.l_p.mean(): 0.05643768608570099 
model_pd.l_d.mean(): -1.2640819549560547 
model_pd.lagr.mean(): -1.2076442241668701 
model_pd.lambdas: dict_items([('pout', tensor([1.5278])), ('power', tensor([0.0752]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7793]))])
epoch：1422	 i:0 	 global-step:28440	 l-p:0.05643768608570099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[27.1107, 36.7822, 43.5524],
        [27.1107, 31.5928, 32.6522],
        [27.1107, 27.4796, 27.2132],
        [27.1107, 27.1108, 27.1107]], grad_fn=<SliceBackward0>)

training epoch:1423, step:0 
model_pd.l_p.mean(): 0.0564376562833786 
model_pd.l_d.mean(): -1.263432502746582 
model_pd.lagr.mean(): -1.206994891166687 
model_pd.lambdas: dict_items([('pout', tensor([1.5270])), ('power', tensor([0.0751]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7792]))])
epoch：1423	 i:0 	 global-step:28460	 l-p:0.0564376562833786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]])
 pt:tensor([[27.1108, 27.1214, 27.1111],
        [27.1108, 28.3150, 27.8210],
        [27.1108, 27.1146, 27.1108],
        [27.1108, 31.5931, 32.6526]], grad_fn=<SliceBackward0>)

training epoch:1424, step:0 
model_pd.l_p.mean(): 0.05643762648105621 
model_pd.l_d.mean(): -1.2627826929092407 
model_pd.lagr.mean(): -1.2063450813293457 
model_pd.lambdas: dict_items([('pout', tensor([1.5262])), ('power', tensor([0.0751]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7886])), ('power', tensor([-0.7791]))])
epoch：1424	 i:0 	 global-step:28480	 l-p:0.05643762648105621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.1108, 27.4851, 27.2158],
        [27.1108, 27.3304, 27.1548],
        [27.1108, 27.1133, 27.1109],
        [27.1108, 29.1283, 28.7396]], grad_fn=<SliceBackward0>)

training epoch:1425, step:0 
model_pd.l_p.mean(): 0.056437600404024124 
model_pd.l_d.mean(): -1.2621334791183472 
model_pd.lagr.mean(): -1.2056958675384521 
model_pd.lambdas: dict_items([('pout', tensor([1.5254])), ('power', tensor([0.0751]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7790]))])
epoch：1425	 i:0 	 global-step:28500	 l-p:0.056437600404024124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[27.1109, 27.4852, 27.2159],
        [27.1109, 27.1134, 27.1110],
        [27.1109, 35.1460, 39.8489],
        [27.1109, 31.2914, 32.1101]], grad_fn=<SliceBackward0>)

training epoch:1426, step:0 
model_pd.l_p.mean(): 0.05643757805228233 
model_pd.l_d.mean(): -1.2614837884902954 
model_pd.lagr.mean(): -1.2050461769104004 
model_pd.lambdas: dict_items([('pout', tensor([1.5246])), ('power', tensor([0.0750]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7789]))])
epoch：1426	 i:0 	 global-step:28520	 l-p:0.05643757805228233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]])
 pt:tensor([[27.1110, 27.4801, 27.2136],
        [27.1110, 27.9417, 27.4984],
        [27.1110, 28.0691, 27.6002],
        [27.1110, 27.1126, 27.1111]], grad_fn=<SliceBackward0>)

training epoch:1427, step:0 
model_pd.l_p.mean(): 0.05643754452466965 
model_pd.l_d.mean(): -1.260833978652954 
model_pd.lagr.mean(): -1.2043964862823486 
model_pd.lambdas: dict_items([('pout', tensor([1.5238])), ('power', tensor([0.0750]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7788]))])
epoch：1427	 i:0 	 global-step:28540	 l-p:0.05643754452466965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1428
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.1111, 31.5941, 32.6539],
        [27.1111, 27.3063, 27.1474],
        [27.1111, 31.8468, 33.1182],
        [27.1111, 27.9419, 27.4986]], grad_fn=<SliceBackward0>)

training epoch:1428, step:0 
model_pd.l_p.mean(): 0.05643751472234726 
model_pd.l_d.mean(): -1.260184407234192 
model_pd.lagr.mean(): -1.2037469148635864 
model_pd.lambdas: dict_items([('pout', tensor([1.5230])), ('power', tensor([0.0749]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7787]))])
epoch：1428	 i:0 	 global-step:28560	 l-p:0.05643751472234726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.1112, 27.3307, 27.1551],
        [27.1112, 28.6669, 28.1860],
        [27.1112, 27.1128, 27.1112],
        [27.1112, 29.1290, 28.7403]], grad_fn=<SliceBackward0>)

training epoch:1429, step:0 
model_pd.l_p.mean(): 0.05643749237060547 
model_pd.l_d.mean(): -1.2595348358154297 
model_pd.lagr.mean(): -1.2030973434448242 
model_pd.lambdas: dict_items([('pout', tensor([1.5222])), ('power', tensor([0.0749]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7787]))])
epoch：1429	 i:0 	 global-step:28580	 l-p:0.05643749237060547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.1113, 27.1113, 27.1113],
        [27.1113, 27.1118, 27.1113],
        [27.1113, 27.1534, 27.1144],
        [27.1113, 27.1137, 27.1113]], grad_fn=<SliceBackward0>)

training epoch:1430, step:0 
model_pd.l_p.mean(): 0.05643747001886368 
model_pd.l_d.mean(): -1.258885383605957 
model_pd.lagr.mean(): -1.2024478912353516 
model_pd.lambdas: dict_items([('pout', tensor([1.5214])), ('power', tensor([0.0749]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7786]))])
epoch：1430	 i:0 	 global-step:28600	 l-p:0.05643747001886368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.1113, 27.1115, 27.1114],
        [27.1113, 27.2257, 27.1266],
        [27.1113, 28.1301, 27.6521],
        [27.1113, 30.1660, 30.2247]], grad_fn=<SliceBackward0>)

training epoch:1431, step:0 
model_pd.l_p.mean(): 0.05643743649125099 
model_pd.l_d.mean(): -1.2582358121871948 
model_pd.lagr.mean(): -1.2017983198165894 
model_pd.lambdas: dict_items([('pout', tensor([1.5207])), ('power', tensor([0.0748]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7785]))])
epoch：1431	 i:0 	 global-step:28620	 l-p:0.05643743649125099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.1114, 36.7849, 43.5571],
        [27.1114, 27.4859, 27.2164],
        [27.1114, 29.8795, 29.7861],
        [27.1114, 32.8675, 35.0772]], grad_fn=<SliceBackward0>)

training epoch:1432, step:0 
model_pd.l_p.mean(): 0.0564374178647995 
model_pd.l_d.mean(): -1.257586121559143 
model_pd.lagr.mean(): -1.2011487483978271 
model_pd.lambdas: dict_items([('pout', tensor([1.5199])), ('power', tensor([0.0748]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7784]))])
epoch：1432	 i:0 	 global-step:28640	 l-p:0.0564374178647995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]])
 pt:tensor([[27.1115, 29.6145, 29.3986],
        [27.1115, 27.4860, 27.2165],
        [27.1115, 28.6675, 28.1865],
        [27.1115, 27.4840, 27.2156]], grad_fn=<SliceBackward0>)

training epoch:1433, step:0 
model_pd.l_p.mean(): 0.05643739178776741 
model_pd.l_d.mean(): -1.2569364309310913 
model_pd.lagr.mean(): -1.2004990577697754 
model_pd.lambdas: dict_items([('pout', tensor([1.5191])), ('power', tensor([0.0748]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7783]))])
epoch：1433	 i:0 	 global-step:28660	 l-p:0.05643739178776741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1434
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.1116, 27.4860, 27.2165],
        [27.1116, 31.2933, 32.1125],
        [27.1116, 28.4647, 27.9695],
        [27.1116, 34.8006, 39.0937]], grad_fn=<SliceBackward0>)

training epoch:1434, step:0 
model_pd.l_p.mean(): 0.05643737316131592 
model_pd.l_d.mean(): -1.256286859512329 
model_pd.lagr.mean(): -1.1998494863510132 
model_pd.lambdas: dict_items([('pout', tensor([1.5183])), ('power', tensor([0.0747]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7782]))])
epoch：1434	 i:0 	 global-step:28680	 l-p:0.05643737316131592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01]])
 pt:tensor([[27.1116, 36.7858, 43.5586],
        [27.1116, 27.3069, 27.1479],
        [27.1116, 31.8485, 33.1205],
        [27.1116, 32.8683, 35.0784]], grad_fn=<SliceBackward0>)

training epoch:1435, step:0 
model_pd.l_p.mean(): 0.05643735080957413 
model_pd.l_d.mean(): -1.255637288093567 
model_pd.lagr.mean(): -1.199199914932251 
model_pd.lambdas: dict_items([('pout', tensor([1.5175])), ('power', tensor([0.0747]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7782]))])
epoch：1435	 i:0 	 global-step:28700	 l-p:0.05643735080957413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[27.1117, 27.1119, 27.1117],
        [27.1117, 29.0362, 28.6225],
        [27.1117, 29.1302, 28.7415],
        [27.1117, 27.1117, 27.1117]], grad_fn=<SliceBackward0>)

training epoch:1436, step:0 
model_pd.l_p.mean(): 0.05643732473254204 
model_pd.l_d.mean(): -1.2549877166748047 
model_pd.lagr.mean(): -1.1985503435134888 
model_pd.lambdas: dict_items([('pout', tensor([1.5167])), ('power', tensor([0.0746]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7781]))])
epoch：1436	 i:0 	 global-step:28720	 l-p:0.05643732473254204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]])
 pt:tensor([[27.1118, 31.5963, 32.6567],
        [27.1118, 27.1118, 27.1118],
        [27.1118, 29.8804, 29.7872],
        [27.1118, 30.2654, 30.3806]], grad_fn=<SliceBackward0>)

training epoch:1437, step:0 
model_pd.l_p.mean(): 0.05643730238080025 
model_pd.l_d.mean(): -1.2543381452560425 
model_pd.lagr.mean(): -1.1979008913040161 
model_pd.lambdas: dict_items([('pout', tensor([1.5159])), ('power', tensor([0.0746]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7780]))])
epoch：1437	 i:0 	 global-step:28740	 l-p:0.05643730238080025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.1118, 27.1118, 27.1118],
        [27.1118, 30.1674, 30.2263],
        [27.1118, 36.1106, 42.0045],
        [27.1118, 27.1143, 27.1119]], grad_fn=<SliceBackward0>)

training epoch:1438, step:0 
model_pd.l_p.mean(): 0.056437283754348755 
model_pd.l_d.mean(): -1.2536884546279907 
model_pd.lagr.mean(): -1.1972512006759644 
model_pd.lambdas: dict_items([('pout', tensor([1.5151])), ('power', tensor([0.0746]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7779]))])
epoch：1438	 i:0 	 global-step:28760	 l-p:0.056437283754348755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1439
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]])
 pt:tensor([[27.1119, 29.0367, 28.6229],
        [27.1119, 36.1109, 42.0049],
        [27.1119, 31.5967, 32.6573],
        [27.1119, 30.0739, 30.0810]], grad_fn=<SliceBackward0>)

training epoch:1439, step:0 
model_pd.l_p.mean(): 0.05643725395202637 
model_pd.l_d.mean(): -1.253039002418518 
model_pd.lagr.mean(): -1.1966017484664917 
model_pd.lambdas: dict_items([('pout', tensor([1.5143])), ('power', tensor([0.0745]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7779]))])
epoch：1439	 i:0 	 global-step:28780	 l-p:0.05643725395202637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.1120, 27.1541, 27.1151],
        [27.1120, 30.2660, 30.3813],
        [27.1120, 29.1309, 28.7421],
        [27.1120, 29.8810, 29.7878]], grad_fn=<SliceBackward0>)

training epoch:1440, step:0 
model_pd.l_p.mean(): 0.056437235325574875 
model_pd.l_d.mean(): -1.2523893117904663 
model_pd.lagr.mean(): -1.19595205783844 
model_pd.lambdas: dict_items([('pout', tensor([1.5136])), ('power', tensor([0.0745]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7778]))])
epoch：1440	 i:0 	 global-step:28800	 l-p:0.056437235325574875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[27.1120, 34.8025, 39.0967],
        [27.1120, 32.4181, 34.1977],
        [27.1120, 30.1680, 30.2269],
        [27.1120, 27.3317, 27.1560]], grad_fn=<SliceBackward0>)

training epoch:1441, step:0 
model_pd.l_p.mean(): 0.05643722042441368 
model_pd.l_d.mean(): -1.2517399787902832 
model_pd.lagr.mean(): -1.1953027248382568 
model_pd.lambdas: dict_items([('pout', tensor([1.5128])), ('power', tensor([0.0744]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7887])), ('power', tensor([-0.7777]))])
epoch：1441	 i:0 	 global-step:28820	 l-p:0.05643722042441368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]])
 pt:tensor([[27.1121, 28.3175, 27.8232],
        [27.1121, 30.0745, 30.0817],
        [27.1121, 28.6688, 28.1877],
        [27.1121, 27.1146, 27.1121]], grad_fn=<SliceBackward0>)

training epoch:1442, step:0 
model_pd.l_p.mean(): 0.05643719807267189 
model_pd.l_d.mean(): -1.2510902881622314 
model_pd.lagr.mean(): -1.194653034210205 
model_pd.lambdas: dict_items([('pout', tensor([1.5120])), ('power', tensor([0.0744]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7777]))])
epoch：1442	 i:0 	 global-step:28840	 l-p:0.05643719807267189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.1121, 32.9107, 35.1615],
        [27.1121, 30.1684, 30.2274],
        [27.1121, 27.1121, 27.1122],
        [27.1121, 29.8815, 29.7884]], grad_fn=<SliceBackward0>)

training epoch:1443, step:0 
model_pd.l_p.mean(): 0.0564371794462204 
model_pd.l_d.mean(): -1.2504403591156006 
model_pd.lagr.mean(): -1.1940032243728638 
model_pd.lambdas: dict_items([('pout', tensor([1.5112])), ('power', tensor([0.0744]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7776]))])
epoch：1443	 i:0 	 global-step:28860	 l-p:0.0564371794462204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.1122, 34.8033, 39.0980],
        [27.1122, 30.2668, 30.3822],
        [27.1122, 27.1122, 27.1122],
        [27.1122, 27.1122, 27.1122]], grad_fn=<SliceBackward0>)

training epoch:1444, step:0 
model_pd.l_p.mean(): 0.05643715709447861 
model_pd.l_d.mean(): -1.2497907876968384 
model_pd.lagr.mean(): -1.1933536529541016 
model_pd.lambdas: dict_items([('pout', tensor([1.5104])), ('power', tensor([0.0743]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7775]))])
epoch：1444	 i:0 	 global-step:28880	 l-p:0.05643715709447861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[27.1123, 28.0713, 27.6021],
        [27.1123, 27.1161, 27.1123],
        [27.1123, 29.6166, 29.4009],
        [27.1123, 29.0376, 28.6239]], grad_fn=<SliceBackward0>)

training epoch:1445, step:0 
model_pd.l_p.mean(): 0.05643713101744652 
model_pd.l_d.mean(): -1.2491412162780762 
model_pd.lagr.mean(): -1.1927040815353394 
model_pd.lambdas: dict_items([('pout', tensor([1.5096])), ('power', tensor([0.0743]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7775]))])
epoch：1445	 i:0 	 global-step:28900	 l-p:0.05643713101744652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1446
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1123, 28.0714, 27.6022],
        [27.1123, 31.5983, 32.6594],
        [27.1123, 27.1672, 27.1171],
        [27.1123, 27.1545, 27.1154]], grad_fn=<SliceBackward0>)

training epoch:1446, step:0 
model_pd.l_p.mean(): 0.056437116116285324 
model_pd.l_d.mean(): -1.2484915256500244 
model_pd.lagr.mean(): -1.1920543909072876 
model_pd.lambdas: dict_items([('pout', tensor([1.5088])), ('power', tensor([0.0742]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7774]))])
epoch：1446	 i:0 	 global-step:28920	 l-p:0.056437116116285324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[27.1124, 30.1691, 30.2283],
        [27.1124, 31.5985, 32.6597],
        [27.1124, 27.4852, 27.2166],
        [27.1124, 31.2962, 32.1163]], grad_fn=<SliceBackward0>)

training epoch:1447, step:0 
model_pd.l_p.mean(): 0.056437090039253235 
model_pd.l_d.mean(): -1.2478418350219727 
model_pd.lagr.mean(): -1.1914047002792358 
model_pd.lambdas: dict_items([('pout', tensor([1.5080])), ('power', tensor([0.0742]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7773]))])
epoch：1447	 i:0 	 global-step:28940	 l-p:0.056437090039253235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[27.1124, 31.8515, 33.1245],
        [27.1124, 27.1149, 27.1125],
        [27.1124, 27.4872, 27.2175],
        [27.1124, 30.4732, 30.7136]], grad_fn=<SliceBackward0>)

training epoch:1448, step:0 
model_pd.l_p.mean(): 0.05643707141280174 
model_pd.l_d.mean(): -1.247192144393921 
model_pd.lagr.mean(): -1.1907551288604736 
model_pd.lambdas: dict_items([('pout', tensor([1.5072])), ('power', tensor([0.0742]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7773]))])
epoch：1448	 i:0 	 global-step:28960	 l-p:0.05643707141280174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.1125, 27.1232, 27.1129],
        [27.1125, 27.1143, 27.1125],
        [27.1125, 29.1323, 28.7435],
        [27.1125, 34.8046, 39.1001]], grad_fn=<SliceBackward0>)

training epoch:1449, step:0 
model_pd.l_p.mean(): 0.056437063962221146 
model_pd.l_d.mean(): -1.2465426921844482 
model_pd.lagr.mean(): -1.190105676651001 
model_pd.lambdas: dict_items([('pout', tensor([1.5065])), ('power', tensor([0.0741]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7772]))])
epoch：1449	 i:0 	 global-step:28980	 l-p:0.056437063962221146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1450
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228]])
 pt:tensor([[27.1125, 34.8049, 39.1006],
        [27.1125, 32.4202, 34.2007],
        [27.1125, 28.9560, 28.5228],
        [27.1125, 31.5991, 32.6606]], grad_fn=<SliceBackward0>)

training epoch:1450, step:0 
model_pd.l_p.mean(): 0.05643703415989876 
model_pd.l_d.mean(): -1.245892882347107 
model_pd.lagr.mean(): -1.1894558668136597 
model_pd.lambdas: dict_items([('pout', tensor([1.5057])), ('power', tensor([0.0741]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7771]))])
epoch：1450	 i:0 	 global-step:29000	 l-p:0.05643703415989876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1451
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[27.1126, 28.3186, 27.8241],
        [27.1126, 32.4205, 34.2011],
        [27.1126, 32.9190, 35.1768],
        [27.1126, 29.8829, 29.7900]], grad_fn=<SliceBackward0>)

training epoch:1451, step:0 
model_pd.l_p.mean(): 0.05643702670931816 
model_pd.l_d.mean(): -1.2452431917190552 
model_pd.lagr.mean(): -1.188806176185608 
model_pd.lambdas: dict_items([('pout', tensor([1.5049])), ('power', tensor([0.0741]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7771]))])
epoch：1451	 i:0 	 global-step:29020	 l-p:0.05643702670931816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.1126, 27.9445, 27.5007],
        [27.1126, 30.1700, 30.2293],
        [27.1126, 27.4823, 27.2154],
        [27.1126, 27.1126, 27.1126]], grad_fn=<SliceBackward0>)

training epoch:1452, step:0 
model_pd.l_p.mean(): 0.05643700435757637 
model_pd.l_d.mean(): -1.2445935010910034 
model_pd.lagr.mean(): -1.1881564855575562 
model_pd.lambdas: dict_items([('pout', tensor([1.5041])), ('power', tensor([0.0740]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7770]))])
epoch：1452	 i:0 	 global-step:29040	 l-p:0.05643700435757637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[27.1127, 27.1132, 27.1127],
        [27.1127, 27.4823, 27.2154],
        [27.1127, 27.1234, 27.1131],
        [27.1127, 28.3188, 27.8243]], grad_fn=<SliceBackward0>)

training epoch:1453, step:0 
model_pd.l_p.mean(): 0.05643699690699577 
model_pd.l_d.mean(): -1.2439439296722412 
model_pd.lagr.mean(): -1.187506914138794 
model_pd.lambdas: dict_items([('pout', tensor([1.5033])), ('power', tensor([0.0740]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7770]))])
epoch：1453	 i:0 	 global-step:29060	 l-p:0.05643699690699577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1454
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228]])
 pt:tensor([[27.1127, 28.4673, 27.9718],
        [27.1127, 28.1328, 27.6543],
        [27.1127, 28.9565, 28.5233],
        [27.1127, 29.1330, 28.7442]], grad_fn=<SliceBackward0>)

training epoch:1454, step:0 
model_pd.l_p.mean(): 0.05643697828054428 
model_pd.l_d.mean(): -1.243294358253479 
model_pd.lagr.mean(): -1.1868573427200317 
model_pd.lambdas: dict_items([('pout', tensor([1.5025])), ('power', tensor([0.0739]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7769]))])
epoch：1454	 i:0 	 global-step:29080	 l-p:0.05643697828054428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[27.1128, 32.4213, 34.2023],
        [27.1128, 27.1133, 27.1128],
        [27.1128, 31.6002, 32.6620],
        [27.1128, 34.6575, 38.7813]], grad_fn=<SliceBackward0>)

training epoch:1455, step:0 
model_pd.l_p.mean(): 0.05643695965409279 
model_pd.l_d.mean(): -1.2426444292068481 
model_pd.lagr.mean(): -1.1862074136734009 
model_pd.lambdas: dict_items([('pout', tensor([1.5017])), ('power', tensor([0.0739]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7769]))])
epoch：1455	 i:0 	 global-step:29100	 l-p:0.05643695965409279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]])
 pt:tensor([[27.1128, 27.1130, 27.1128],
        [27.1128, 29.8838, 29.7909],
        [27.1128, 27.1128, 27.1128],
        [27.1128, 27.1128, 27.1128]], grad_fn=<SliceBackward0>)

training epoch:1456, step:0 
model_pd.l_p.mean(): 0.056436941027641296 
model_pd.l_d.mean(): -1.2419949769973755 
model_pd.lagr.mean(): -1.1855580806732178 
model_pd.lambdas: dict_items([('pout', tensor([1.5009])), ('power', tensor([0.0739]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7768]))])
epoch：1456	 i:0 	 global-step:29120	 l-p:0.056436941027641296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[27.1129, 31.8534, 33.1272],
        [27.1129, 27.4826, 27.2156],
        [27.1129, 34.6580, 38.7822],
        [27.1129, 30.0772, 30.0848]], grad_fn=<SliceBackward0>)

training epoch:1457, step:0 
model_pd.l_p.mean(): 0.0564369298517704 
model_pd.l_d.mean(): -1.2413451671600342 
model_pd.lagr.mean(): -1.1849082708358765 
model_pd.lambdas: dict_items([('pout', tensor([1.5001])), ('power', tensor([0.0738]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7768]))])
epoch：1457	 i:0 	 global-step:29140	 l-p:0.0564369298517704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.1129, 27.1168, 27.1130],
        [27.1129, 29.8841, 29.7913],
        [27.1129, 27.4749, 27.2121],
        [27.1129, 30.1711, 30.2305]], grad_fn=<SliceBackward0>)

training epoch:1458, step:0 
model_pd.l_p.mean(): 0.05643691122531891 
model_pd.l_d.mean(): -1.240695595741272 
model_pd.lagr.mean(): -1.1842586994171143 
model_pd.lambdas: dict_items([('pout', tensor([1.4994])), ('power', tensor([0.0738]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7767]))])
epoch：1458	 i:0 	 global-step:29160	 l-p:0.05643691122531891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1129, 34.8071, 39.1042],
        [27.1129, 29.6189, 29.4033],
        [27.1129, 27.1135, 27.1129],
        [27.1129, 27.1552, 27.1161]], grad_fn=<SliceBackward0>)

training epoch:1459, step:0 
model_pd.l_p.mean(): 0.056436896324157715 
model_pd.l_d.mean(): -1.2400459051132202 
model_pd.lagr.mean(): -1.1836090087890625 
model_pd.lambdas: dict_items([('pout', tensor([1.4986])), ('power', tensor([0.0737]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7767]))])
epoch：1459	 i:0 	 global-step:29180	 l-p:0.056436896324157715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.1130, 27.1130, 27.1130],
        [27.1130, 35.1551, 39.8639],
        [27.1130, 29.0397, 28.6259],
        [27.1130, 27.4881, 27.2182]], grad_fn=<SliceBackward0>)

training epoch:1460, step:0 
model_pd.l_p.mean(): 0.05643688142299652 
model_pd.l_d.mean(): -1.2393962144851685 
model_pd.lagr.mean(): -1.1829593181610107 
model_pd.lambdas: dict_items([('pout', tensor([1.4978])), ('power', tensor([0.0737]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7766]))])
epoch：1460	 i:0 	 global-step:29200	 l-p:0.05643688142299652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]])
 pt:tensor([[27.1130, 27.1146, 27.1130],
        [27.1130, 31.2990, 32.1200],
        [27.1130, 27.1130, 27.1130],
        [27.1130, 28.1334, 27.6549]], grad_fn=<SliceBackward0>)

training epoch:1461, step:0 
model_pd.l_p.mean(): 0.05643686652183533 
model_pd.l_d.mean(): -1.2387468814849854 
model_pd.lagr.mean(): -1.1823099851608276 
model_pd.lambdas: dict_items([('pout', tensor([1.4970])), ('power', tensor([0.0737]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7766]))])
epoch：1461	 i:0 	 global-step:29220	 l-p:0.05643686652183533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[27.1131, 30.1717, 30.2313],
        [27.1131, 27.1130, 27.1131],
        [27.1131, 29.8847, 29.7920],
        [27.1131, 28.0730, 27.6034]], grad_fn=<SliceBackward0>)

training epoch:1462, step:0 
model_pd.l_p.mean(): 0.05643685162067413 
model_pd.l_d.mean(): -1.238097071647644 
model_pd.lagr.mean(): -1.1816601753234863 
model_pd.lambdas: dict_items([('pout', tensor([1.4962])), ('power', tensor([0.0736]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7765]))])
epoch：1462	 i:0 	 global-step:29240	 l-p:0.05643685162067413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.1131, 27.4830, 27.2159],
        [27.1131, 27.1147, 27.1131],
        [27.1131, 27.1436, 27.1149],
        [27.1131, 27.9455, 27.5014]], grad_fn=<SliceBackward0>)

training epoch:1463, step:0 
model_pd.l_p.mean(): 0.056436844170093536 
model_pd.l_d.mean(): -1.2374473810195923 
model_pd.lagr.mean(): -1.1810104846954346 
model_pd.lambdas: dict_items([('pout', tensor([1.4954])), ('power', tensor([0.0736]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7888])), ('power', tensor([-0.7765]))])
epoch：1463	 i:0 	 global-step:29260	 l-p:0.056436844170093536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]])
 pt:tensor([[27.1131, 27.1131, 27.1131],
        [27.1131, 28.6716, 28.1902],
        [27.1131, 36.1173, 42.0163],
        [27.1131, 29.1343, 28.7455]], grad_fn=<SliceBackward0>)

training epoch:1464, step:0 
model_pd.l_p.mean(): 0.05643682926893234 
model_pd.l_d.mean(): -1.2367976903915405 
model_pd.lagr.mean(): -1.1803609132766724 
model_pd.lambdas: dict_items([('pout', tensor([1.4946])), ('power', tensor([0.0735]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7765]))])
epoch：1464	 i:0 	 global-step:29280	 l-p:0.05643682926893234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[27.1132, 31.2997, 32.1210],
        [27.1132, 32.9221, 35.1815],
        [27.1132, 27.3088, 27.1495],
        [27.1132, 32.9158, 35.1690]], grad_fn=<SliceBackward0>)

training epoch:1465, step:0 
model_pd.l_p.mean(): 0.05643681064248085 
model_pd.l_d.mean(): -1.2361479997634888 
model_pd.lagr.mean(): -1.1797112226486206 
model_pd.lambdas: dict_items([('pout', tensor([1.4938])), ('power', tensor([0.0735]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7764]))])
epoch：1465	 i:0 	 global-step:29300	 l-p:0.05643681064248085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1132, 27.1681, 27.1179],
        [27.1132, 34.6601, 38.7857],
        [27.1132, 32.9223, 35.1819],
        [27.1132, 27.1554, 27.1163]], grad_fn=<SliceBackward0>)

training epoch:1466, step:0 
model_pd.l_p.mean(): 0.056436799466609955 
model_pd.l_d.mean(): -1.2354984283447266 
model_pd.lagr.mean(): -1.1790616512298584 
model_pd.lambdas: dict_items([('pout', tensor([1.4930])), ('power', tensor([0.0735]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7764]))])
epoch：1466	 i:0 	 global-step:29320	 l-p:0.056436799466609955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]])
 pt:tensor([[27.1132, 27.3333, 27.1573],
        [27.1132, 32.8757, 35.0894],
        [27.1132, 29.6201, 29.4046],
        [27.1132, 30.4766, 30.7178]], grad_fn=<SliceBackward0>)

training epoch:1467, step:0 
model_pd.l_p.mean(): 0.05643678829073906 
model_pd.l_d.mean(): -1.2348486185073853 
model_pd.lagr.mean(): -1.178411841392517 
model_pd.lambdas: dict_items([('pout', tensor([1.4923])), ('power', tensor([0.0734]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7763]))])
epoch：1467	 i:0 	 global-step:29340	 l-p:0.05643678829073906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]])
 pt:tensor([[27.1133, 27.9459, 27.5017],
        [27.1133, 27.1239, 27.1136],
        [27.1133, 36.1183, 42.0180],
        [27.1133, 27.1171, 27.1133]], grad_fn=<SliceBackward0>)

training epoch:1468, step:0 
model_pd.l_p.mean(): 0.056436777114868164 
model_pd.l_d.mean(): -1.2341989278793335 
model_pd.lagr.mean(): -1.1777621507644653 
model_pd.lambdas: dict_items([('pout', tensor([1.4915])), ('power', tensor([0.0734]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7763]))])
epoch：1468	 i:0 	 global-step:29360	 l-p:0.056436777114868164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]])
 pt:tensor([[27.1133, 27.3090, 27.1497],
        [27.1133, 30.4769, 30.7182],
        [27.1133, 27.1149, 27.1133],
        [27.1133, 27.8131, 27.4056]], grad_fn=<SliceBackward0>)

training epoch:1469, step:0 
model_pd.l_p.mean(): 0.05643676221370697 
model_pd.l_d.mean(): -1.2335494756698608 
model_pd.lagr.mean(): -1.1771126985549927 
model_pd.lambdas: dict_items([('pout', tensor([1.4907])), ('power', tensor([0.0734]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7762]))])
epoch：1469	 i:0 	 global-step:29380	 l-p:0.05643676221370697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]])
 pt:tensor([[27.1133, 32.4245, 34.2070],
        [27.1133, 30.1730, 30.2328],
        [27.1133, 27.1135, 27.1133],
        [27.1133, 27.9460, 27.5019]], grad_fn=<SliceBackward0>)

training epoch:1470, step:0 
model_pd.l_p.mean(): 0.05643675848841667 
model_pd.l_d.mean(): -1.2328996658325195 
model_pd.lagr.mean(): -1.1764628887176514 
model_pd.lambdas: dict_items([('pout', tensor([1.4899])), ('power', tensor([0.0733]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7762]))])
epoch：1470	 i:0 	 global-step:29400	 l-p:0.05643675848841667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[27.1133, 27.1172, 27.1134],
        [27.1133, 34.6612, 38.7876],
        [27.1133, 27.1135, 27.1134],
        [27.1133, 28.9587, 28.5253]], grad_fn=<SliceBackward0>)

training epoch:1471, step:0 
model_pd.l_p.mean(): 0.05643673986196518 
model_pd.l_d.mean(): -1.2322499752044678 
model_pd.lagr.mean(): -1.1758131980895996 
model_pd.lambdas: dict_items([('pout', tensor([1.4891])), ('power', tensor([0.0733]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7762]))])
epoch：1471	 i:0 	 global-step:29420	 l-p:0.05643673986196518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]])
 pt:tensor([[27.1134, 27.8133, 27.4057],
        [27.1134, 27.4756, 27.2127],
        [27.1134, 30.1733, 30.2332],
        [27.1134, 27.1556, 27.1165]], grad_fn=<SliceBackward0>)

training epoch:1472, step:0 
model_pd.l_p.mean(): 0.056436728686094284 
model_pd.l_d.mean(): -1.231600284576416 
model_pd.lagr.mean(): -1.1751635074615479 
model_pd.lambdas: dict_items([('pout', tensor([1.4883])), ('power', tensor([0.0732]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7761]))])
epoch：1472	 i:0 	 global-step:29440	 l-p:0.056436728686094284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]])
 pt:tensor([[27.1134, 27.2281, 27.1288],
        [27.1134, 27.1134, 27.1134],
        [27.1134, 30.2718, 30.3882],
        [27.1134, 28.0739, 27.6041]], grad_fn=<SliceBackward0>)

training epoch:1473, step:0 
model_pd.l_p.mean(): 0.05643672123551369 
model_pd.l_d.mean(): -1.2309503555297852 
model_pd.lagr.mean(): -1.174513578414917 
model_pd.lambdas: dict_items([('pout', tensor([1.4875])), ('power', tensor([0.0732]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7761]))])
epoch：1473	 i:0 	 global-step:29460	 l-p:0.05643672123551369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[27.1134, 27.1134, 27.1134],
        [27.1134, 34.8105, 39.1101],
        [27.1134, 27.4835, 27.2163],
        [27.1134, 27.1136, 27.1134]], grad_fn=<SliceBackward0>)

training epoch:1474, step:0 
model_pd.l_p.mean(): 0.05643670633435249 
model_pd.l_d.mean(): -1.2303005456924438 
model_pd.lagr.mean(): -1.1738638877868652 
model_pd.lambdas: dict_items([('pout', tensor([1.4867])), ('power', tensor([0.0732]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7761]))])
epoch：1474	 i:0 	 global-step:29480	 l-p:0.05643670633435249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[27.1135, 31.3016, 32.1235],
        [27.1135, 27.1135, 27.1135],
        [27.1135, 32.9242, 35.1848],
        [27.1135, 27.2282, 27.1288]], grad_fn=<SliceBackward0>)

training epoch:1475, step:0 
model_pd.l_p.mean(): 0.0564366951584816 
model_pd.l_d.mean(): -1.2296509742736816 
model_pd.lagr.mean(): -1.173214316368103 
model_pd.lambdas: dict_items([('pout', tensor([1.4859])), ('power', tensor([0.0731]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7760]))])
epoch：1475	 i:0 	 global-step:29500	 l-p:0.0564366951584816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01]])
 pt:tensor([[27.1135, 30.4780, 30.7196],
        [27.1135, 27.1135, 27.1135],
        [27.1135, 27.1135, 27.1135],
        [27.1135, 27.1140, 27.1135]], grad_fn=<SliceBackward0>)

training epoch:1476, step:0 
model_pd.l_p.mean(): 0.056436680257320404 
model_pd.l_d.mean(): -1.2290011644363403 
model_pd.lagr.mean(): -1.1725645065307617 
model_pd.lambdas: dict_items([('pout', tensor([1.4852])), ('power', tensor([0.0731]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7760]))])
epoch：1476	 i:0 	 global-step:29520	 l-p:0.056436680257320404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.1135, 32.4259, 34.2091],
        [27.1135, 27.4837, 27.2164],
        [27.1135, 27.1140, 27.1135],
        [27.1135, 30.1741, 30.2341]], grad_fn=<SliceBackward0>)

training epoch:1477, step:0 
model_pd.l_p.mean(): 0.05643667280673981 
model_pd.l_d.mean(): -1.2283512353897095 
model_pd.lagr.mean(): -1.1719145774841309 
model_pd.lambdas: dict_items([('pout', tensor([1.4844])), ('power', tensor([0.0730]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7760]))])
epoch：1477	 i:0 	 global-step:29540	 l-p:0.05643667280673981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[27.1135, 27.1135, 27.1135],
        [27.1135, 32.4261, 34.2094],
        [27.1135, 27.1685, 27.1183],
        [27.1135, 28.9595, 28.5261]], grad_fn=<SliceBackward0>)

training epoch:1478, step:0 
model_pd.l_p.mean(): 0.05643666163086891 
model_pd.l_d.mean(): -1.2277016639709473 
model_pd.lagr.mean(): -1.1712650060653687 
model_pd.lambdas: dict_items([('pout', tensor([1.4836])), ('power', tensor([0.0730]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7759]))])
epoch：1478	 i:0 	 global-step:29560	 l-p:0.05643666163086891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[27.1135, 27.3094, 27.1500],
        [27.1135, 27.8138, 27.4060],
        [27.1135, 28.6732, 28.1916],
        [27.1135, 31.8577, 33.1334]], grad_fn=<SliceBackward0>)

training epoch:1479, step:0 
model_pd.l_p.mean(): 0.056436654180288315 
model_pd.l_d.mean(): -1.2270519733428955 
model_pd.lagr.mean(): -1.170615315437317 
model_pd.lambdas: dict_items([('pout', tensor([1.4828])), ('power', tensor([0.0730]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7759]))])
epoch：1479	 i:0 	 global-step:29580	 l-p:0.056436654180288315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]])
 pt:tensor([[27.1136, 27.2284, 27.1289],
        [27.1136, 27.1136, 27.1136],
        [27.1136, 27.1152, 27.1136],
        [27.1136, 30.0808, 30.0890]], grad_fn=<SliceBackward0>)

training epoch:1480, step:0 
model_pd.l_p.mean(): 0.05643664300441742 
model_pd.l_d.mean(): -1.2264022827148438 
model_pd.lagr.mean(): -1.1699656248092651 
model_pd.lambdas: dict_items([('pout', tensor([1.4820])), ('power', tensor([0.0729]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7759]))])
epoch：1480	 i:0 	 global-step:29600	 l-p:0.05643664300441742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[27.1136, 32.9254, 35.1867],
        [27.1136, 29.0423, 28.6284],
        [27.1136, 27.1152, 27.1136],
        [27.1136, 34.6634, 38.7914]], grad_fn=<SliceBackward0>)

training epoch:1481, step:0 
model_pd.l_p.mean(): 0.05643663927912712 
model_pd.l_d.mean(): -1.225752353668213 
model_pd.lagr.mean(): -1.1693156957626343 
model_pd.lambdas: dict_items([('pout', tensor([1.4812])), ('power', tensor([0.0729]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7758]))])
epoch：1481	 i:0 	 global-step:29620	 l-p:0.05643663927912712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]])
 pt:tensor([[27.1136, 27.4761, 27.2130],
        [27.1136, 32.4269, 34.2106],
        [27.1136, 34.8123, 39.1131],
        [27.1136, 27.1136, 27.1136]], grad_fn=<SliceBackward0>)

training epoch:1482, step:0 
model_pd.l_p.mean(): 0.05643662437796593 
model_pd.l_d.mean(): -1.2251026630401611 
model_pd.lagr.mean(): -1.1686660051345825 
model_pd.lambdas: dict_items([('pout', tensor([1.4804])), ('power', tensor([0.0729]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7758]))])
epoch：1482	 i:0 	 global-step:29640	 l-p:0.05643662437796593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[27.1136, 36.7978, 43.5806],
        [27.1136, 31.3030, 32.1255],
        [27.1136, 27.1161, 27.1137],
        [27.1136, 34.6638, 38.7921]], grad_fn=<SliceBackward0>)

training epoch:1483, step:0 
model_pd.l_p.mean(): 0.05643660947680473 
model_pd.l_d.mean(): -1.2244527339935303 
model_pd.lagr.mean(): -1.1680160760879517 
model_pd.lambdas: dict_items([('pout', tensor([1.4796])), ('power', tensor([0.0728]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7758]))])
epoch：1483	 i:0 	 global-step:29660	 l-p:0.05643660947680473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1484
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[27.1137, 30.0813, 30.0897],
        [27.1137, 27.4840, 27.2166],
        [27.1137, 27.1137, 27.1137],
        [27.1137, 29.8880, 29.7957]], grad_fn=<SliceBackward0>)

training epoch:1484, step:0 
model_pd.l_p.mean(): 0.056436605751514435 
model_pd.l_d.mean(): -1.223803162574768 
model_pd.lagr.mean(): -1.1673665046691895 
model_pd.lambdas: dict_items([('pout', tensor([1.4788])), ('power', tensor([0.0728]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7757]))])
epoch：1484	 i:0 	 global-step:29680	 l-p:0.056436605751514435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1485
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228]])
 pt:tensor([[27.1137, 29.6225, 29.4073],
        [27.1137, 31.8588, 33.1350],
        [27.1137, 30.4794, 30.7214],
        [27.1137, 30.2736, 30.3904]], grad_fn=<SliceBackward0>)

training epoch:1485, step:0 
model_pd.l_p.mean(): 0.05643659085035324 
model_pd.l_d.mean(): -1.2231533527374268 
model_pd.lagr.mean(): -1.1667168140411377 
model_pd.lambdas: dict_items([('pout', tensor([1.4781])), ('power', tensor([0.0727]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7757]))])
epoch：1485	 i:0 	 global-step:29700	 l-p:0.05643659085035324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1486
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1459,  0.0768,  1.0000,  0.0404,
          1.0000,  0.5264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3733,  0.2688,  1.0000,  0.1935,
          1.0000,  0.7200, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[27.1137, 27.9472, 27.5027],
        [27.1137, 28.3219, 27.8268],
        [27.1137, 30.4796, 30.7216],
        [27.1137, 29.8882, 29.7960]], grad_fn=<SliceBackward0>)

training epoch:1486, step:0 
model_pd.l_p.mean(): 0.05643659085035324 
model_pd.l_d.mean(): -1.2225037813186646 
model_pd.lagr.mean(): -1.1660672426223755 
model_pd.lambdas: dict_items([('pout', tensor([1.4773])), ('power', tensor([0.0727]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7757]))])
epoch：1486	 i:0 	 global-step:29720	 l-p:0.05643659085035324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[27.1137, 27.1137, 27.1137],
        [27.1137, 35.1612, 39.8745],
        [27.1137, 28.9604, 28.5270],
        [27.1137, 28.4706, 27.9745]], grad_fn=<SliceBackward0>)

training epoch:1487, step:0 
model_pd.l_p.mean(): 0.05643657594919205 
model_pd.l_d.mean(): -1.2218539714813232 
model_pd.lagr.mean(): -1.1654174327850342 
model_pd.lambdas: dict_items([('pout', tensor([1.4765])), ('power', tensor([0.0727]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7757]))])
epoch：1487	 i:0 	 global-step:29740	 l-p:0.05643657594919205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]])
 pt:tensor([[27.1137, 27.1153, 27.1138],
        [27.1137, 27.4875, 27.2182],
        [27.1137, 36.1228, 42.0263],
        [27.1137, 28.4707, 27.9746]], grad_fn=<SliceBackward0>)

training epoch:1488, step:0 
model_pd.l_p.mean(): 0.05643656849861145 
model_pd.l_d.mean(): -1.221204161643982 
model_pd.lagr.mean(): -1.1647676229476929 
model_pd.lambdas: dict_items([('pout', tensor([1.4757])), ('power', tensor([0.0726]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7756]))])
epoch：1488	 i:0 	 global-step:29760	 l-p:0.05643656849861145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]])
 pt:tensor([[27.1138, 27.3341, 27.1579],
        [27.1138, 27.1138, 27.1138],
        [27.1138, 27.1138, 27.1137],
        [27.1138, 31.8595, 33.1360]], grad_fn=<SliceBackward0>)

training epoch:1489, step:0 
model_pd.l_p.mean(): 0.056436557322740555 
model_pd.l_d.mean(): -1.2205543518066406 
model_pd.lagr.mean(): -1.1641178131103516 
model_pd.lambdas: dict_items([('pout', tensor([1.4749])), ('power', tensor([0.0726]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7756]))])
epoch：1489	 i:0 	 global-step:29780	 l-p:0.056436557322740555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[27.1138, 31.3042, 32.1272],
        [27.1138, 36.7993, 43.5836],
        [27.1138, 27.1138, 27.1138],
        [27.1138, 32.9208, 35.1769]], grad_fn=<SliceBackward0>)

training epoch:1490, step:0 
model_pd.l_p.mean(): 0.05643654987215996 
model_pd.l_d.mean(): -1.2199045419692993 
model_pd.lagr.mean(): -1.1634680032730103 
model_pd.lambdas: dict_items([('pout', tensor([1.4741])), ('power', tensor([0.0725]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7756]))])
epoch：1490	 i:0 	 global-step:29800	 l-p:0.05643654987215996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[27.1138, 35.1620, 39.8761],
        [27.1138, 31.6069, 32.6715],
        [27.1138, 27.1143, 27.1138],
        [27.1138, 36.1234, 42.0275]], grad_fn=<SliceBackward0>)

training epoch:1491, step:0 
model_pd.l_p.mean(): 0.05643653869628906 
model_pd.l_d.mean(): -1.219254970550537 
model_pd.lagr.mean(): -1.162818431854248 
model_pd.lambdas: dict_items([('pout', tensor([1.4733])), ('power', tensor([0.0725]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7756]))])
epoch：1491	 i:0 	 global-step:29820	 l-p:0.05643653869628906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]])
 pt:tensor([[27.1138, 31.3045, 32.1276],
        [27.1138, 27.1163, 27.1139],
        [27.1138, 27.1444, 27.1157],
        [27.1138, 32.9276, 35.1901]], grad_fn=<SliceBackward0>)

training epoch:1492, step:0 
model_pd.l_p.mean(): 0.05643653869628906 
model_pd.l_d.mean(): -1.2186050415039062 
model_pd.lagr.mean(): -1.1621685028076172 
model_pd.lambdas: dict_items([('pout', tensor([1.4725])), ('power', tensor([0.0725]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7889])), ('power', tensor([-0.7755]))])
epoch：1492	 i:0 	 global-step:29840	 l-p:0.05643653869628906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]])
 pt:tensor([[27.1138, 27.8146, 27.4066],
        [27.1138, 27.1245, 27.1142],
        [27.1138, 27.4766, 27.2133],
        [27.1138, 28.3225, 27.8272]], grad_fn=<SliceBackward0>)

training epoch:1493, step:0 
model_pd.l_p.mean(): 0.05643652379512787 
model_pd.l_d.mean(): -1.2179553508758545 
model_pd.lagr.mean(): -1.1615188121795654 
model_pd.lambdas: dict_items([('pout', tensor([1.4717])), ('power', tensor([0.0724]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7755]))])
epoch：1493	 i:0 	 global-step:29860	 l-p:0.05643652379512787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[27.1138, 31.6075, 32.6723],
        [27.1138, 27.4844, 27.2169],
        [27.1138, 31.8604, 33.1374],
        [27.1138, 27.1139, 27.1139]], grad_fn=<SliceBackward0>)

training epoch:1494, step:0 
model_pd.l_p.mean(): 0.056436508893966675 
model_pd.l_d.mean(): -1.2173055410385132 
model_pd.lagr.mean(): -1.1608690023422241 
model_pd.lambdas: dict_items([('pout', tensor([1.4710])), ('power', tensor([0.0724]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7755]))])
epoch：1494	 i:0 	 global-step:29880	 l-p:0.056436508893966675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]])
 pt:tensor([[27.1139, 27.4766, 27.2133],
        [27.1139, 27.8147, 27.4067],
        [27.1139, 32.9218, 35.1785],
        [27.1139, 27.1139, 27.1139]], grad_fn=<SliceBackward0>)

training epoch:1495, step:0 
model_pd.l_p.mean(): 0.056436508893966675 
model_pd.l_d.mean(): -1.2166558504104614 
model_pd.lagr.mean(): -1.1602193117141724 
model_pd.lambdas: dict_items([('pout', tensor([1.4702])), ('power', tensor([0.0723]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7754]))])
epoch：1495	 i:0 	 global-step:29900	 l-p:0.056436508893966675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]])
 pt:tensor([[27.1139, 27.4767, 27.2134],
        [27.1139, 27.1144, 27.1139],
        [27.1139, 29.1382, 28.7493],
        [27.1139, 30.1769, 30.2375]], grad_fn=<SliceBackward0>)

training epoch:1496, step:0 
model_pd.l_p.mean(): 0.05643650144338608 
model_pd.l_d.mean(): -1.2160061597824097 
model_pd.lagr.mean(): -1.1595696210861206 
model_pd.lambdas: dict_items([('pout', tensor([1.4694])), ('power', tensor([0.0723]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7754]))])
epoch：1496	 i:0 	 global-step:29920	 l-p:0.05643650144338608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]])
 pt:tensor([[27.1139, 27.1155, 27.1139],
        [27.1139, 27.4879, 27.2184],
        [27.1139, 36.1247, 42.0300],
        [27.1139, 34.6668, 38.7973]], grad_fn=<SliceBackward0>)

training epoch:1497, step:0 
model_pd.l_p.mean(): 0.05643649399280548 
model_pd.l_d.mean(): -1.2153562307357788 
model_pd.lagr.mean(): -1.1589196920394897 
model_pd.lambdas: dict_items([('pout', tensor([1.4686])), ('power', tensor([0.0723]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7754]))])
epoch：1497	 i:0 	 global-step:29940	 l-p:0.05643649399280548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[27.1139, 28.1364, 27.6570],
        [27.1139, 27.1562, 27.1170],
        [27.1139, 28.4716, 27.9753],
        [27.1139, 34.8157, 39.1191]], grad_fn=<SliceBackward0>)

training epoch:1498, step:0 
model_pd.l_p.mean(): 0.056436486542224884 
model_pd.l_d.mean(): -1.214706540107727 
model_pd.lagr.mean(): -1.158270001411438 
model_pd.lambdas: dict_items([('pout', tensor([1.4678])), ('power', tensor([0.0722]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7754]))])
epoch：1498	 i:0 	 global-step:29960	 l-p:0.056436486542224884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]])
 pt:tensor([[27.1139, 27.4846, 27.2170],
        [27.1139, 27.1445, 27.1158],
        [27.1139, 27.1139, 27.1139],
        [27.1139, 28.6751, 28.1933]], grad_fn=<SliceBackward0>)

training epoch:1499, step:0 
model_pd.l_p.mean(): 0.05643647536635399 
model_pd.l_d.mean(): -1.2140566110610962 
model_pd.lagr.mean(): -1.1576201915740967 
model_pd.lambdas: dict_items([('pout', tensor([1.4670])), ('power', tensor([0.0722]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7890])), ('power', tensor([-0.7754]))])
epoch：1499	 i:0 	 global-step:29980	 l-p:0.05643647536635399
Traceback (most recent call last):
  File "/Applications/programe/GCN-HARQ/Data_model_drive_direct/train/main.py", line 223, in <module>
    train()
  File "/Applications/programe/GCN-HARQ/Data_model_drive_direct/train/main.py", line 195, in train
    from findFuncAnswer import equalAllocation
  File "/Applications/programe/GCN-HARQ/Data_model_drive_direct/train/findFuncAnswer.py", line 6, in <module>
    from args import args
ModuleNotFoundError: No module named 'args'
