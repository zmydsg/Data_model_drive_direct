
bounds:tensor([-1.])	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.]), 'power': tensor([1.])},
vars:{'pout': tensor([0.]), 'power': tensor([0.])}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]])
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.17055293917655945 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0014])), ('power', tensor([0.9990]))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984])), ('power', tensor([-20.6474]))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.17055293917655945
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870694518089294
epoch£º0	 i:2 	 global-step:2	 l-p:-0.27698129415512085
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038808345794678
epoch£º0	 i:4 	 global-step:4	 l-p:-0.7105810046195984
epoch£º0	 i:5 	 global-step:5	 l-p:-2.5309479236602783
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505443096160889
epoch£º0	 i:7 	 global-step:7	 l-p:0.491678923368454
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606543064117432
epoch£º0	 i:9 	 global-step:9	 l-p:0.056774258613586426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]])
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.37725818157196045 
model_pd.l_d.mean(): -18.738481521606445 
model_pd.lagr.mean(): -18.361223220825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0125])), ('power', tensor([0.9888]))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113])), ('power', tensor([-19.8624]))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.37725818157196045
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797576904297
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878846138715744
epoch£º1	 i:3 	 global-step:23	 l-p:0.24290160834789276
epoch£º1	 i:4 	 global-step:24	 l-p:0.13980987668037415
epoch£º1	 i:5 	 global-step:25	 l-p:0.1868826299905777
epoch£º1	 i:6 	 global-step:26	 l-p:0.17738381028175354
epoch£º1	 i:7 	 global-step:27	 l-p:0.21870960295200348
epoch£º1	 i:8 	 global-step:28	 l-p:0.22566404938697815
epoch£º1	 i:9 	 global-step:29	 l-p:0.11669810116291046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]])
 pt:tensor([[4.9156, 5.9892, 6.5152],
        [4.9156, 5.0134, 4.9533],
        [4.9156, 5.7275, 5.9885],
        [4.9156, 4.9976, 4.9438]], grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13358278572559357 
model_pd.l_d.mean(): -20.04267120361328 
model_pd.lagr.mean(): -19.909088134765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0189])), ('power', tensor([0.9785]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446])), ('power', tensor([-20.9243]))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13358278572559357
epoch£º2	 i:1 	 global-step:41	 l-p:0.10819091647863388
epoch£º2	 i:2 	 global-step:42	 l-p:0.10342482477426529
epoch£º2	 i:3 	 global-step:43	 l-p:0.10659711807966232
epoch£º2	 i:4 	 global-step:44	 l-p:0.11851216107606888
epoch£º2	 i:5 	 global-step:45	 l-p:0.10561671108007431
epoch£º2	 i:6 	 global-step:46	 l-p:0.1121269017457962
epoch£º2	 i:7 	 global-step:47	 l-p:0.6113704442977905
epoch£º2	 i:8 	 global-step:48	 l-p:0.10728447139263153
epoch£º2	 i:9 	 global-step:49	 l-p:0.11701168864965439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]])
 pt:tensor([[5.4145, 6.5084, 6.9827],
        [5.4145, 5.4657, 5.4267],
        [5.4145, 5.5209, 5.4549],
        [5.4145, 6.9929, 8.0320]], grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08024413883686066 
model_pd.l_d.mean(): -18.32460594177246 
model_pd.lagr.mean(): -18.244361877441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0226])), ('power', tensor([0.9684]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153])), ('power', tensor([-19.3421]))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08024413883686066
epoch£º3	 i:1 	 global-step:61	 l-p:0.08470957726240158
epoch£º3	 i:2 	 global-step:62	 l-p:-0.10689033567905426
epoch£º3	 i:3 	 global-step:63	 l-p:0.121424600481987
epoch£º3	 i:4 	 global-step:64	 l-p:0.11789830029010773
epoch£º3	 i:5 	 global-step:65	 l-p:0.11609980463981628
epoch£º3	 i:6 	 global-step:66	 l-p:0.11234923452138901
epoch£º3	 i:7 	 global-step:67	 l-p:0.11608634889125824
epoch£º3	 i:8 	 global-step:68	 l-p:0.12469916045665741
epoch£º3	 i:9 	 global-step:69	 l-p:0.12293624132871628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]])
 pt:tensor([[5.0105, 5.7389, 5.9174],
        [5.0105, 5.9001, 6.2238],
        [5.0105, 6.5617, 7.6554],
        [5.0105, 5.0994, 5.0424]], grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11384844034910202 
model_pd.l_d.mean(): -18.45567512512207 
model_pd.lagr.mean(): -18.341827392578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0263])), ('power', tensor([0.9582]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766])), ('power', tensor([-19.7508]))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11384844034910202
epoch£º4	 i:1 	 global-step:81	 l-p:0.13007697463035583
epoch£º4	 i:2 	 global-step:82	 l-p:0.16103409230709076
epoch£º4	 i:3 	 global-step:83	 l-p:0.1309719830751419
epoch£º4	 i:4 	 global-step:84	 l-p:0.15025097131729126
epoch£º4	 i:5 	 global-step:85	 l-p:0.12508077919483185
epoch£º4	 i:6 	 global-step:86	 l-p:0.1283344030380249
epoch£º4	 i:7 	 global-step:87	 l-p:0.13245771825313568
epoch£º4	 i:8 	 global-step:88	 l-p:0.1381160020828247
epoch£º4	 i:9 	 global-step:89	 l-p:0.1484026163816452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]])
 pt:tensor([[4.6940, 4.6942, 4.6940],
        [4.6940, 4.7002, 4.6945],
        [4.6940, 4.7884, 4.7309],
        [4.6940, 4.8293, 4.7606]], grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.1720057874917984 
model_pd.l_d.mean(): -19.251270294189453 
model_pd.lagr.mean(): -19.07926368713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0311])), ('power', tensor([0.9479]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5294])), ('power', tensor([-20.8624]))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.1720057874917984
epoch£º5	 i:1 	 global-step:101	 l-p:0.144602432847023
epoch£º5	 i:2 	 global-step:102	 l-p:0.15315285325050354
epoch£º5	 i:3 	 global-step:103	 l-p:0.0172208733856678
epoch£º5	 i:4 	 global-step:104	 l-p:0.13105958700180054
epoch£º5	 i:5 	 global-step:105	 l-p:0.14825403690338135
epoch£º5	 i:6 	 global-step:106	 l-p:0.11833436787128448
epoch£º5	 i:7 	 global-step:107	 l-p:0.13759227097034454
epoch£º5	 i:8 	 global-step:108	 l-p:0.1352844536304474
epoch£º5	 i:9 	 global-step:109	 l-p:0.13510742783546448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]])
 pt:tensor([[4.8894, 4.9172, 4.8942],
        [4.8894, 4.8894, 4.8894],
        [4.8894, 6.3054, 7.2501],
        [4.8894, 4.8942, 4.8897]], grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13070783019065857 
model_pd.l_d.mean(): -19.493690490722656 
model_pd.lagr.mean(): -19.36298179626465 
model_pd.lambdas: dict_items([('pout', tensor([1.0360])), ('power', tensor([0.9376]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203])), ('power', tensor([-21.2313]))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13070783019065857
epoch£º6	 i:1 	 global-step:121	 l-p:0.1298673450946808
epoch£º6	 i:2 	 global-step:122	 l-p:0.13716676831245422
epoch£º6	 i:3 	 global-step:123	 l-p:0.13697637617588043
epoch£º6	 i:4 	 global-step:124	 l-p:0.12771467864513397
epoch£º6	 i:5 	 global-step:125	 l-p:0.14635062217712402
epoch£º6	 i:6 	 global-step:126	 l-p:0.13424725830554962
epoch£º6	 i:7 	 global-step:127	 l-p:0.1264638453722
epoch£º6	 i:8 	 global-step:128	 l-p:0.1280180662870407
epoch£º6	 i:9 	 global-step:129	 l-p:0.12071310728788376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]])
 pt:tensor([[4.9571, 5.6186, 5.7504],
        [4.9571, 5.0206, 4.9756],
        [4.9571, 4.9571, 4.9571],
        [4.9571, 4.9785, 4.9602]], grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12212312966585159 
model_pd.l_d.mean(): -19.144275665283203 
model_pd.lagr.mean(): -19.022151947021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0406])), ('power', tensor([0.9274]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4173])), ('power', tensor([-21.0879]))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12212312966585159
epoch£º7	 i:1 	 global-step:141	 l-p:0.11341287940740585
epoch£º7	 i:2 	 global-step:142	 l-p:0.12858566641807556
epoch£º7	 i:3 	 global-step:143	 l-p:0.12901030480861664
epoch£º7	 i:4 	 global-step:144	 l-p:0.14053748548030853
epoch£º7	 i:5 	 global-step:145	 l-p:0.18343999981880188
epoch£º7	 i:6 	 global-step:146	 l-p:0.12439677119255066
epoch£º7	 i:7 	 global-step:147	 l-p:0.13204345107078552
epoch£º7	 i:8 	 global-step:148	 l-p:0.14843469858169556
epoch£º7	 i:9 	 global-step:149	 l-p:0.1348680853843689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]])
 pt:tensor([[4.7970, 4.7970, 4.7970],
        [4.7970, 4.7971, 4.7970],
        [4.7970, 5.6147, 5.8991],
        [4.7970, 4.9434, 4.8721]], grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1275399625301361 
model_pd.l_d.mean(): -17.36314582824707 
model_pd.lagr.mean(): -17.235605239868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0454])), ('power', tensor([0.9172]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5430])), ('power', tensor([-19.5287]))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.1275399625301361
epoch£º8	 i:1 	 global-step:161	 l-p:0.25699564814567566
epoch£º8	 i:2 	 global-step:162	 l-p:0.13293129205703735
epoch£º8	 i:3 	 global-step:163	 l-p:0.1356596052646637
epoch£º8	 i:4 	 global-step:164	 l-p:0.12099883705377579
epoch£º8	 i:5 	 global-step:165	 l-p:0.1383485347032547
epoch£º8	 i:6 	 global-step:166	 l-p:0.12211734801530838
epoch£º8	 i:7 	 global-step:167	 l-p:0.11606931686401367
epoch£º8	 i:8 	 global-step:168	 l-p:0.13333597779273987
epoch£º8	 i:9 	 global-step:169	 l-p:0.13291341066360474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]])
 pt:tensor([[4.9562, 4.9580, 4.9562],
        [4.9562, 4.9582, 4.9562],
        [4.9562, 5.1072, 5.0335],
        [4.9562, 4.9774, 4.9593]], grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.12783139944076538 
model_pd.l_d.mean(): -16.168113708496094 
model_pd.lagr.mean(): -16.040283203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0501])), ('power', tensor([0.9070]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5306])), ('power', tensor([-18.4214]))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.12783139944076538
epoch£º9	 i:1 	 global-step:181	 l-p:0.12110179662704468
epoch£º9	 i:2 	 global-step:182	 l-p:0.1334223449230194
epoch£º9	 i:3 	 global-step:183	 l-p:0.1254665106534958
epoch£º9	 i:4 	 global-step:184	 l-p:0.11723863333463669
epoch£º9	 i:5 	 global-step:185	 l-p:0.14049024879932404
epoch£º9	 i:6 	 global-step:186	 l-p:0.1481664478778839
epoch£º9	 i:7 	 global-step:187	 l-p:0.12391210347414017
epoch£º9	 i:8 	 global-step:188	 l-p:0.12250911444425583
epoch£º9	 i:9 	 global-step:189	 l-p:0.10549129545688629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]])
 pt:tensor([[4.8670, 6.1089, 6.8449],
        [4.8670, 5.7141, 6.0203],
        [4.8670, 4.8721, 4.8673],
        [4.8670, 4.8670, 4.8670]], grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.1452721357345581 
model_pd.l_d.mean(): -18.279727935791016 
model_pd.lagr.mean(): -18.134456634521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0545])), ('power', tensor([0.8966]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4666])), ('power', tensor([-20.9112]))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.1452721357345581
epoch£º10	 i:1 	 global-step:201	 l-p:0.1342070996761322
epoch£º10	 i:2 	 global-step:202	 l-p:0.11234893649816513
epoch£º10	 i:3 	 global-step:203	 l-p:0.1279735565185547
epoch£º10	 i:4 	 global-step:204	 l-p:0.12984216213226318
epoch£º10	 i:5 	 global-step:205	 l-p:0.13331376016139984
epoch£º10	 i:6 	 global-step:206	 l-p:0.13947650790214539
epoch£º10	 i:7 	 global-step:207	 l-p:0.15099400281906128
epoch£º10	 i:8 	 global-step:208	 l-p:0.1854323446750641
epoch£º10	 i:9 	 global-step:209	 l-p:0.13565851747989655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]])
 pt:tensor([[4.8955, 5.4279, 5.4751],
        [4.8955, 6.2353, 7.0875],
        [4.8955, 5.6243, 5.8207],
        [4.8955, 4.9191, 4.8992]], grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.10967277735471725 
model_pd.l_d.mean(): -17.938800811767578 
model_pd.lagr.mean(): -17.82912826538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0593])), ('power', tensor([0.8864]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658])), ('power', tensor([-20.7703]))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.10967277735471725
epoch£º11	 i:1 	 global-step:221	 l-p:0.14238715171813965
epoch£º11	 i:2 	 global-step:222	 l-p:0.11821668595075607
epoch£º11	 i:3 	 global-step:223	 l-p:0.11465343832969666
epoch£º11	 i:4 	 global-step:224	 l-p:0.1287897378206253
epoch£º11	 i:5 	 global-step:225	 l-p:0.13083766400814056
epoch£º11	 i:6 	 global-step:226	 l-p:0.11910483986139297
epoch£º11	 i:7 	 global-step:227	 l-p:0.151666522026062
epoch£º11	 i:8 	 global-step:228	 l-p:0.11703786253929138
epoch£º11	 i:9 	 global-step:229	 l-p:0.14556296169757843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]])
 pt:tensor([[4.9956, 5.4058, 5.3796],
        [4.9956, 5.0330, 5.0035],
        [4.9956, 5.0273, 5.0016],
        [4.9956, 5.1692, 5.0927]], grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.11751232296228409 
model_pd.l_d.mean(): -17.062349319458008 
model_pd.lagr.mean(): -16.94483757019043 
model_pd.lambdas: dict_items([('pout', tensor([1.0639])), ('power', tensor([0.8762]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4977])), ('power', tensor([-20.0546]))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.11751232296228409
epoch£º12	 i:1 	 global-step:241	 l-p:0.1330374926328659
epoch£º12	 i:2 	 global-step:242	 l-p:0.11821037530899048
epoch£º12	 i:3 	 global-step:243	 l-p:0.13416442275047302
epoch£º12	 i:4 	 global-step:244	 l-p:0.1347993016242981
epoch£º12	 i:5 	 global-step:245	 l-p:0.13663992285728455
epoch£º12	 i:6 	 global-step:246	 l-p:0.13386349380016327
epoch£º12	 i:7 	 global-step:247	 l-p:0.13030798733234406
epoch£º12	 i:8 	 global-step:248	 l-p:0.1338690221309662
epoch£º12	 i:9 	 global-step:249	 l-p:0.09318527579307556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]])
 pt:tensor([[4.9364, 6.1023, 6.7399],
        [4.9364, 4.9364, 4.9364],
        [4.9364, 4.9398, 4.9366],
        [4.9364, 5.3393, 5.3134]], grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.11847709864377975 
model_pd.l_d.mean(): -16.30927085876465 
model_pd.lagr.mean(): -16.190793991088867 
model_pd.lambdas: dict_items([('pout', tensor([1.0684])), ('power', tensor([0.8660]))]) 
model_pd.vars: dict_items([('pout', tensor([0.5031])), ('power', tensor([-19.4319]))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.11847709864377975
epoch£º13	 i:1 	 global-step:261	 l-p:0.1539144366979599
epoch£º13	 i:2 	 global-step:262	 l-p:0.12063947319984436
epoch£º13	 i:3 	 global-step:263	 l-p:0.12584997713565826
epoch£º13	 i:4 	 global-step:264	 l-p:0.1320597231388092
epoch£º13	 i:5 	 global-step:265	 l-p:0.13293148577213287
epoch£º13	 i:6 	 global-step:266	 l-p:0.1062014028429985
epoch£º13	 i:7 	 global-step:267	 l-p:0.12257244437932968
epoch£º13	 i:8 	 global-step:268	 l-p:0.1259191483259201
epoch£º13	 i:9 	 global-step:269	 l-p:0.11576381325721741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]])
 pt:tensor([[4.9871, 4.9871, 4.9871],
        [4.9871, 5.9177, 6.2949],
        [4.9871, 5.0224, 4.9943],
        [4.9871, 5.8493, 6.1596]], grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.13664408028125763 
model_pd.l_d.mean(): -16.009490966796875 
model_pd.lagr.mean(): -15.872846603393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0727])), ('power', tensor([0.8558]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788])), ('power', tensor([-19.2862]))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.13664408028125763
epoch£º14	 i:1 	 global-step:281	 l-p:0.11622811108827591
epoch£º14	 i:2 	 global-step:282	 l-p:0.1473616659641266
epoch£º14	 i:3 	 global-step:283	 l-p:0.12748900055885315
epoch£º14	 i:4 	 global-step:284	 l-p:0.12673760950565338
epoch£º14	 i:5 	 global-step:285	 l-p:0.11566461622714996
epoch£º14	 i:6 	 global-step:286	 l-p:0.1085280254483223
epoch£º14	 i:7 	 global-step:287	 l-p:0.11951415985822678
epoch£º14	 i:8 	 global-step:288	 l-p:0.11050494015216827
epoch£º14	 i:9 	 global-step:289	 l-p:0.14763498306274414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]])
 pt:tensor([[4.9416, 4.9741, 4.9480],
        [4.9416, 4.9455, 4.9418],
        [4.9416, 5.1209, 5.0455],
        [4.9416, 4.9527, 4.9428]], grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.1380920112133026 
model_pd.l_d.mean(): -17.45781898498535 
model_pd.lagr.mean(): -17.319726943969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0771])), ('power', tensor([0.8454]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4145])), ('power', tensor([-21.1510]))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.1380920112133026
epoch£º15	 i:1 	 global-step:301	 l-p:0.13601703941822052
epoch£º15	 i:2 	 global-step:302	 l-p:0.10884493589401245
epoch£º15	 i:3 	 global-step:303	 l-p:0.13043412566184998
epoch£º15	 i:4 	 global-step:304	 l-p:0.1386989951133728
epoch£º15	 i:5 	 global-step:305	 l-p:0.1294151395559311
epoch£º15	 i:6 	 global-step:306	 l-p:0.10909561812877655
epoch£º15	 i:7 	 global-step:307	 l-p:0.13085292279720306
epoch£º15	 i:8 	 global-step:308	 l-p:0.1261949986219406
epoch£º15	 i:9 	 global-step:309	 l-p:0.141516774892807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]])
 pt:tensor([[4.9816, 4.9836, 4.9817],
        [4.9816, 5.0085, 4.9863],
        [4.9816, 4.9820, 4.9816],
        [4.9816, 5.3495, 5.3085]], grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.11294922977685928 
model_pd.l_d.mean(): -15.955671310424805 
model_pd.lagr.mean(): -15.842721939086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0817])), ('power', tensor([0.8353]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4541])), ('power', tensor([-19.6672]))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.11294922977685928
epoch£º16	 i:1 	 global-step:321	 l-p:0.11009015887975693
epoch£º16	 i:2 	 global-step:322	 l-p:0.14144456386566162
epoch£º16	 i:3 	 global-step:323	 l-p:0.12719210982322693
epoch£º16	 i:4 	 global-step:324	 l-p:0.11710184067487717
epoch£º16	 i:5 	 global-step:325	 l-p:0.10888238251209259
epoch£º16	 i:6 	 global-step:326	 l-p:0.1244579404592514
epoch£º16	 i:7 	 global-step:327	 l-p:0.13425426185131073
epoch£º16	 i:8 	 global-step:328	 l-p:0.12269697338342667
epoch£º16	 i:9 	 global-step:329	 l-p:0.12243319302797318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]])
 pt:tensor([[5.0588, 6.0214, 6.4246],
        [5.0588, 5.3848, 5.3267],
        [5.0588, 5.0589, 5.0588],
        [5.0588, 6.0088, 6.3992]], grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.13457804918289185 
model_pd.l_d.mean(): -16.721620559692383 
model_pd.lagr.mean(): -16.5870418548584 
model_pd.lambdas: dict_items([('pout', tensor([1.0859])), ('power', tensor([0.8250]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116])), ('power', tensor([-20.7843]))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.13457804918289185
epoch£º17	 i:1 	 global-step:341	 l-p:0.11440306901931763
epoch£º17	 i:2 	 global-step:342	 l-p:0.12820769846439362
epoch£º17	 i:3 	 global-step:343	 l-p:0.12485244125127792
epoch£º17	 i:4 	 global-step:344	 l-p:0.12331695854663849
epoch£º17	 i:5 	 global-step:345	 l-p:0.11871825158596039
epoch£º17	 i:6 	 global-step:346	 l-p:0.12797503173351288
epoch£º17	 i:7 	 global-step:347	 l-p:0.13184484839439392
epoch£º17	 i:8 	 global-step:348	 l-p:0.11971892416477203
epoch£º17	 i:9 	 global-step:349	 l-p:0.12284131348133087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]])
 pt:tensor([[4.9387, 4.9890, 4.9518],
        [4.9387, 4.9389, 4.9387],
        [4.9387, 5.0554, 4.9907],
        [4.9387, 4.9516, 4.9401]], grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.1274343729019165 
model_pd.l_d.mean(): -16.398231506347656 
model_pd.lagr.mean(): -16.270797729492188 
model_pd.lambdas: dict_items([('pout', tensor([1.0904])), ('power', tensor([0.8148]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619])), ('power', tensor([-20.7180]))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.1274343729019165
epoch£º18	 i:1 	 global-step:361	 l-p:0.14928466081619263
epoch£º18	 i:2 	 global-step:362	 l-p:0.12172546237707138
epoch£º18	 i:3 	 global-step:363	 l-p:0.1406782567501068
epoch£º18	 i:4 	 global-step:364	 l-p:0.11478005349636078
epoch£º18	 i:5 	 global-step:365	 l-p:0.12241888046264648
epoch£º18	 i:6 	 global-step:366	 l-p:0.12147253006696701
epoch£º18	 i:7 	 global-step:367	 l-p:0.12800124287605286
epoch£º18	 i:8 	 global-step:368	 l-p:0.11947079002857208
epoch£º18	 i:9 	 global-step:369	 l-p:0.12146022170782089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]])
 pt:tensor([[5.0219, 5.2052, 5.1293],
        [5.0219, 5.0532, 5.0279],
        [5.0219, 5.0219, 5.0219],
        [5.0219, 5.2398, 5.1639]], grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.12954045832157135 
model_pd.l_d.mean(): -16.55906105041504 
model_pd.lagr.mean(): -16.429519653320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0947])), ('power', tensor([0.8045]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3864])), ('power', tensor([-21.0810]))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.12954045832157135
epoch£º19	 i:1 	 global-step:381	 l-p:0.12365110218524933
epoch£º19	 i:2 	 global-step:382	 l-p:0.11882485449314117
epoch£º19	 i:3 	 global-step:383	 l-p:0.1188560277223587
epoch£º19	 i:4 	 global-step:384	 l-p:0.11091868579387665
epoch£º19	 i:5 	 global-step:385	 l-p:0.12287477403879166
epoch£º19	 i:6 	 global-step:386	 l-p:0.11708088219165802
epoch£º19	 i:7 	 global-step:387	 l-p:0.11340494453907013
epoch£º19	 i:8 	 global-step:388	 l-p:0.0991290882229805
epoch£º19	 i:9 	 global-step:389	 l-p:0.13743706047534943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]])
 pt:tensor([[5.1477, 5.1505, 5.1478],
        [5.1477, 5.5937, 5.5819],
        [5.1477, 5.1923, 5.1582],
        [5.1477, 5.2121, 5.1668]], grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.11412137001752853 
model_pd.l_d.mean(): -16.30812644958496 
model_pd.lagr.mean(): -16.194005966186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0989])), ('power', tensor([0.7943]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3647])), ('power', tensor([-21.0081]))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.11412137001752853
epoch£º20	 i:1 	 global-step:401	 l-p:0.11554767936468124
epoch£º20	 i:2 	 global-step:402	 l-p:0.11031584441661835
epoch£º20	 i:3 	 global-step:403	 l-p:0.1341753602027893
epoch£º20	 i:4 	 global-step:404	 l-p:0.12906259298324585
epoch£º20	 i:5 	 global-step:405	 l-p:0.12351125478744507
epoch£º20	 i:6 	 global-step:406	 l-p:0.11649370193481445
epoch£º20	 i:7 	 global-step:407	 l-p:0.11655127257108688
epoch£º20	 i:8 	 global-step:408	 l-p:0.11524517089128494
epoch£º20	 i:9 	 global-step:409	 l-p:0.1107892245054245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]])
 pt:tensor([[5.1090, 6.5883, 7.5855],
        [5.1090, 5.1735, 5.1284],
        [5.1090, 5.1090, 5.1090],
        [5.1090, 5.2140, 5.1519]], grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.12140897661447525 
model_pd.l_d.mean(): -15.969846725463867 
model_pd.lagr.mean(): -15.848437309265137 
model_pd.lambdas: dict_items([('pout', tensor([1.1031])), ('power', tensor([0.7841]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3914])), ('power', tensor([-20.8900]))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.12140897661447525
epoch£º21	 i:1 	 global-step:421	 l-p:0.11608541756868362
epoch£º21	 i:2 	 global-step:422	 l-p:0.12436261773109436
epoch£º21	 i:3 	 global-step:423	 l-p:0.11515066027641296
epoch£º21	 i:4 	 global-step:424	 l-p:0.13087406754493713
epoch£º21	 i:5 	 global-step:425	 l-p:0.11492504179477692
epoch£º21	 i:6 	 global-step:426	 l-p:0.12985993921756744
epoch£º21	 i:7 	 global-step:427	 l-p:0.15200626850128174
epoch£º21	 i:8 	 global-step:428	 l-p:0.11755523085594177
epoch£º21	 i:9 	 global-step:429	 l-p:0.1208374947309494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[4.9902, 5.0939, 5.0331],
        [4.9902, 5.2922, 5.2319],
        [4.9902, 4.9925, 4.9903],
        [4.9902, 5.0227, 4.9967]], grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13586996495723724 
model_pd.l_d.mean(): -15.901676177978516 
model_pd.lagr.mean(): -15.765806198120117 
model_pd.lambdas: dict_items([('pout', tensor([1.1074])), ('power', tensor([0.7739]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4043])), ('power', tensor([-21.0982]))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13586996495723724
epoch£º22	 i:1 	 global-step:441	 l-p:0.12376661598682404
epoch£º22	 i:2 	 global-step:442	 l-p:0.12214837968349457
epoch£º22	 i:3 	 global-step:443	 l-p:0.12448476999998093
epoch£º22	 i:4 	 global-step:444	 l-p:0.10098809003829956
epoch£º22	 i:5 	 global-step:445	 l-p:0.12878675758838654
epoch£º22	 i:6 	 global-step:446	 l-p:0.10992832481861115
epoch£º22	 i:7 	 global-step:447	 l-p:0.12861141562461853
epoch£º22	 i:8 	 global-step:448	 l-p:0.12274428457021713
epoch£º22	 i:9 	 global-step:449	 l-p:0.09828468412160873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]])
 pt:tensor([[5.2184, 6.7284, 7.7437],
        [5.2184, 5.8037, 5.8717],
        [5.2184, 5.2190, 5.2184],
        [5.2184, 6.2006, 6.6083]], grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.08608147501945496 
model_pd.l_d.mean(): -15.294421195983887 
model_pd.lagr.mean(): -15.20833969116211 
model_pd.lambdas: dict_items([('pout', tensor([1.1116])), ('power', tensor([0.7637]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3868])), ('power', tensor([-20.5629]))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.08608147501945496
epoch£º23	 i:1 	 global-step:461	 l-p:0.11855056136846542
epoch£º23	 i:2 	 global-step:462	 l-p:0.12101618945598602
epoch£º23	 i:3 	 global-step:463	 l-p:0.09896611422300339
epoch£º23	 i:4 	 global-step:464	 l-p:0.11787407100200653
epoch£º23	 i:5 	 global-step:465	 l-p:0.10931495577096939
epoch£º23	 i:6 	 global-step:466	 l-p:0.09520898014307022
epoch£º23	 i:7 	 global-step:467	 l-p:0.10992184281349182
epoch£º23	 i:8 	 global-step:468	 l-p:0.12078878283500671
epoch£º23	 i:9 	 global-step:469	 l-p:0.12276770919561386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]])
 pt:tensor([[5.1988, 5.6896, 5.7010],
        [5.1988, 5.5096, 5.4454],
        [5.1988, 5.2867, 5.2307],
        [5.1988, 5.1988, 5.1988]], grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.12197691947221756 
model_pd.l_d.mean(): -15.785090446472168 
model_pd.lagr.mean(): -15.663113594055176 
model_pd.lambdas: dict_items([('pout', tensor([1.1153])), ('power', tensor([0.7534]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3044])), ('power', tensor([-21.3707]))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.12197691947221756
epoch£º24	 i:1 	 global-step:481	 l-p:0.11418996751308441
epoch£º24	 i:2 	 global-step:482	 l-p:0.1065484806895256
epoch£º24	 i:3 	 global-step:483	 l-p:0.09728135168552399
epoch£º24	 i:4 	 global-step:484	 l-p:0.1119534969329834
epoch£º24	 i:5 	 global-step:485	 l-p:0.10319497436285019
epoch£º24	 i:6 	 global-step:486	 l-p:0.12057969719171524
epoch£º24	 i:7 	 global-step:487	 l-p:0.12351810187101364
epoch£º24	 i:8 	 global-step:488	 l-p:0.12615250051021576
epoch£º24	 i:9 	 global-step:489	 l-p:0.10796461999416351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]])
 pt:tensor([[5.1790, 5.1790, 5.1790],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 6.3921, 7.0561]], grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.11529259383678436 
model_pd.l_d.mean(): -13.646267890930176 
model_pd.lagr.mean(): -13.530975341796875 
model_pd.lambdas: dict_items([('pout', tensor([1.1194])), ('power', tensor([0.7434]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993])), ('power', tensor([-19.0843]))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.11529259383678436
epoch£º25	 i:1 	 global-step:501	 l-p:0.11731074005365372
epoch£º25	 i:2 	 global-step:502	 l-p:0.12022020667791367
epoch£º25	 i:3 	 global-step:503	 l-p:0.10987748205661774
epoch£º25	 i:4 	 global-step:504	 l-p:0.12923933565616608
epoch£º25	 i:5 	 global-step:505	 l-p:0.12474820017814636
epoch£º25	 i:6 	 global-step:506	 l-p:0.116696797311306
epoch£º25	 i:7 	 global-step:507	 l-p:0.10702009499073029
epoch£º25	 i:8 	 global-step:508	 l-p:0.12808609008789062
epoch£º25	 i:9 	 global-step:509	 l-p:0.1162215918302536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]])
 pt:tensor([[5.0837, 5.2865, 5.2107],
        [5.0837, 5.7214, 5.8398],
        [5.0837, 5.1468, 5.1027],
        [5.0837, 5.6338, 5.6902]], grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.13162235915660858 
model_pd.l_d.mean(): -15.073952674865723 
model_pd.lagr.mean(): -14.942330360412598 
model_pd.lambdas: dict_items([('pout', tensor([1.1234])), ('power', tensor([0.7331]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3785])), ('power', tensor([-21.1125]))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.13162235915660858
epoch£º26	 i:1 	 global-step:521	 l-p:0.1172744557261467
epoch£º26	 i:2 	 global-step:522	 l-p:0.10891608148813248
epoch£º26	 i:3 	 global-step:523	 l-p:0.11299803107976913
epoch£º26	 i:4 	 global-step:524	 l-p:0.11221401393413544
epoch£º26	 i:5 	 global-step:525	 l-p:0.12962451577186584
epoch£º26	 i:6 	 global-step:526	 l-p:0.12130329757928848
epoch£º26	 i:7 	 global-step:527	 l-p:0.12078435719013214
epoch£º26	 i:8 	 global-step:528	 l-p:0.12175323814153671
epoch£º26	 i:9 	 global-step:529	 l-p:0.11175708472728729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]])
 pt:tensor([[5.2347, 5.3622, 5.2933],
        [5.2347, 5.5978, 5.5494],
        [5.2347, 5.2347, 5.2347],
        [5.2347, 5.2375, 5.2349]], grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.10817527025938034 
model_pd.l_d.mean(): -13.220855712890625 
model_pd.lagr.mean(): -13.112680435180664 
model_pd.lambdas: dict_items([('pout', tensor([1.1276])), ('power', tensor([0.7230]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4419])), ('power', tensor([-18.9515]))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.10817527025938034
epoch£º27	 i:1 	 global-step:541	 l-p:0.11035981774330139
epoch£º27	 i:2 	 global-step:542	 l-p:0.095889151096344
epoch£º27	 i:3 	 global-step:543	 l-p:0.11369254440069199
epoch£º27	 i:4 	 global-step:544	 l-p:0.10885408520698547
epoch£º27	 i:5 	 global-step:545	 l-p:0.11129140853881836
epoch£º27	 i:6 	 global-step:546	 l-p:0.05435978248715401
epoch£º27	 i:7 	 global-step:547	 l-p:0.11320775747299194
epoch£º27	 i:8 	 global-step:548	 l-p:0.11491820961236954
epoch£º27	 i:9 	 global-step:549	 l-p:0.0936649814248085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]])
 pt:tensor([[5.3853, 5.6127, 5.5321],
        [5.3853, 6.0651, 6.1905],
        [5.3853, 5.4404, 5.3999],
        [5.3853, 6.8756, 7.8362]], grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.08639420568943024 
model_pd.l_d.mean(): -14.139654159545898 
model_pd.lagr.mean(): -14.05325984954834 
model_pd.lambdas: dict_items([('pout', tensor([1.1310])), ('power', tensor([0.7127]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3567])), ('power', tensor([-20.3756]))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.08639420568943024
epoch£º28	 i:1 	 global-step:561	 l-p:0.12282612919807434
epoch£º28	 i:2 	 global-step:562	 l-p:0.10943107306957245
epoch£º28	 i:3 	 global-step:563	 l-p:0.05643932893872261
epoch£º28	 i:4 	 global-step:564	 l-p:0.1115880236029625
epoch£º28	 i:5 	 global-step:565	 l-p:0.09093745052814484
epoch£º28	 i:6 	 global-step:566	 l-p:0.098836749792099
epoch£º28	 i:7 	 global-step:567	 l-p:0.10614234954118729
epoch£º28	 i:8 	 global-step:568	 l-p:0.11169829219579697
epoch£º28	 i:9 	 global-step:569	 l-p:0.12041676789522171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]])
 pt:tensor([[5.3915, 5.3915, 5.3915],
        [5.3915, 5.6172, 5.5365],
        [5.3915, 6.1048, 6.2556],
        [5.3915, 5.3963, 5.3918]], grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.11112741380929947 
model_pd.l_d.mean(): -13.793549537658691 
model_pd.lagr.mean(): -13.682421684265137 
model_pd.lambdas: dict_items([('pout', tensor([1.1344])), ('power', tensor([0.7026]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3516])), ('power', tensor([-20.1711]))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.11112741380929947
epoch£º29	 i:1 	 global-step:581	 l-p:0.07742342352867126
epoch£º29	 i:2 	 global-step:582	 l-p:0.07341588288545609
epoch£º29	 i:3 	 global-step:583	 l-p:0.1160036250948906
epoch£º29	 i:4 	 global-step:584	 l-p:0.09307543933391571
epoch£º29	 i:5 	 global-step:585	 l-p:0.11639875173568726
epoch£º29	 i:6 	 global-step:586	 l-p:0.10867931693792343
epoch£º29	 i:7 	 global-step:587	 l-p:0.1036926656961441
epoch£º29	 i:8 	 global-step:588	 l-p:0.10821042209863663
epoch£º29	 i:9 	 global-step:589	 l-p:0.11150038987398148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]])
 pt:tensor([[5.3868, 5.3886, 5.3868],
        [5.3868, 6.8611, 7.8027],
        [5.3868, 5.6308, 5.5514],
        [5.3868, 6.7258, 7.5039]], grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.10502998530864716 
model_pd.l_d.mean(): -13.083578109741211 
model_pd.lagr.mean(): -12.978548049926758 
model_pd.lambdas: dict_items([('pout', tensor([1.1380])), ('power', tensor([0.6925]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177])), ('power', tensor([-19.5526]))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.10502998530864716
epoch£º30	 i:1 	 global-step:601	 l-p:0.09952212125062943
epoch£º30	 i:2 	 global-step:602	 l-p:0.10810583829879761
epoch£º30	 i:3 	 global-step:603	 l-p:0.08747520297765732
epoch£º30	 i:4 	 global-step:604	 l-p:0.10830474644899368
epoch£º30	 i:5 	 global-step:605	 l-p:0.11918646842241287
epoch£º30	 i:6 	 global-step:606	 l-p:0.10634446889162064
epoch£º30	 i:7 	 global-step:607	 l-p:0.09925434738397598
epoch£º30	 i:8 	 global-step:608	 l-p:0.10271215438842773
epoch£º30	 i:9 	 global-step:609	 l-p:0.11814453452825546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]])
 pt:tensor([[5.2433, 5.2433, 5.2433],
        [5.2433, 5.2524, 5.2441],
        [5.2433, 6.7469, 7.7553],
        [5.2433, 5.3053, 5.2614]], grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.10060146450996399 
model_pd.l_d.mean(): -12.313494682312012 
model_pd.lagr.mean(): -12.21289348602295 
model_pd.lambdas: dict_items([('pout', tensor([1.1416])), ('power', tensor([0.6823]))]) 
model_pd.vars: dict_items([('pout', tensor([0.4949])), ('power', tensor([-18.8475]))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.10060146450996399
epoch£º31	 i:1 	 global-step:621	 l-p:0.09915567189455032
epoch£º31	 i:2 	 global-step:622	 l-p:0.09277866035699844
epoch£º31	 i:3 	 global-step:623	 l-p:0.1129334345459938
epoch£º31	 i:4 	 global-step:624	 l-p:0.10635114461183548
epoch£º31	 i:5 	 global-step:625	 l-p:0.10461898893117905
epoch£º31	 i:6 	 global-step:626	 l-p:0.11531196534633636
epoch£º31	 i:7 	 global-step:627	 l-p:0.08455603569746017
epoch£º31	 i:8 	 global-step:628	 l-p:0.11851351708173752
epoch£º31	 i:9 	 global-step:629	 l-p:0.10939977318048477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]])
 pt:tensor([[5.3678, 7.0105, 8.1725],
        [5.3678, 6.4364, 6.9185],
        [5.3678, 5.4333, 5.3873],
        [5.3678, 5.3695, 5.3679]], grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.10797212272882462 
model_pd.l_d.mean(): -13.331659317016602 
model_pd.lagr.mean(): -13.223687171936035 
model_pd.lambdas: dict_items([('pout', tensor([1.1449])), ('power', tensor([0.6721]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3348])), ('power', tensor([-20.3746]))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.10797212272882462
epoch£º32	 i:1 	 global-step:641	 l-p:0.1026749387383461
epoch£º32	 i:2 	 global-step:642	 l-p:0.09816659986972809
epoch£º32	 i:3 	 global-step:643	 l-p:0.09731559455394745
epoch£º32	 i:4 	 global-step:644	 l-p:0.09096457809209824
epoch£º32	 i:5 	 global-step:645	 l-p:0.11436966806650162
epoch£º32	 i:6 	 global-step:646	 l-p:0.12060103565454483
epoch£º32	 i:7 	 global-step:647	 l-p:0.11068037152290344
epoch£º32	 i:8 	 global-step:648	 l-p:0.09909046441316605
epoch£º32	 i:9 	 global-step:649	 l-p:0.11085397750139236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]])
 pt:tensor([[5.3844, 5.3847, 5.3844],
        [5.3844, 5.6621, 5.5876],
        [5.3844, 5.3852, 5.3844],
        [5.3844, 5.6624, 5.5879]], grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.039325255900621414 
model_pd.l_d.mean(): -13.396242141723633 
model_pd.lagr.mean(): -13.356916427612305 
model_pd.lambdas: dict_items([('pout', tensor([1.1485])), ('power', tensor([0.6619]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3230])), ('power', tensor([-20.7654]))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.039325255900621414
epoch£º33	 i:1 	 global-step:661	 l-p:0.12201925367116928
epoch£º33	 i:2 	 global-step:662	 l-p:0.11464390158653259
epoch£º33	 i:3 	 global-step:663	 l-p:0.09335852414369583
epoch£º33	 i:4 	 global-step:664	 l-p:0.10373082011938095
epoch£º33	 i:5 	 global-step:665	 l-p:0.10819459706544876
epoch£º33	 i:6 	 global-step:666	 l-p:0.10723632574081421
epoch£º33	 i:7 	 global-step:667	 l-p:0.1058555617928505
epoch£º33	 i:8 	 global-step:668	 l-p:0.10375982522964478
epoch£º33	 i:9 	 global-step:669	 l-p:0.09866346418857574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]])
 pt:tensor([[5.3979, 6.0809, 6.2101],
        [5.3979, 5.3981, 5.3979],
        [5.3979, 5.7181, 5.6523],
        [5.3979, 5.4616, 5.4165]], grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.10868921130895615 
model_pd.l_d.mean(): -13.202630996704102 
model_pd.lagr.mean(): -13.093941688537598 
model_pd.lambdas: dict_items([('pout', tensor([1.1519])), ('power', tensor([0.6518]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3167])), ('power', tensor([-20.7823]))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.10868921130895615
epoch£º34	 i:1 	 global-step:681	 l-p:0.11289180815219879
epoch£º34	 i:2 	 global-step:682	 l-p:0.08723554760217667
epoch£º34	 i:3 	 global-step:683	 l-p:0.09895160794258118
epoch£º34	 i:4 	 global-step:684	 l-p:0.11535678803920746
epoch£º34	 i:5 	 global-step:685	 l-p:-0.07410326600074768
epoch£º34	 i:6 	 global-step:686	 l-p:0.09116417914628983
epoch£º34	 i:7 	 global-step:687	 l-p:0.10823648422956467
epoch£º34	 i:8 	 global-step:688	 l-p:0.09077586233615875
epoch£º34	 i:9 	 global-step:689	 l-p:0.10612023621797562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]])
 pt:tensor([[5.5478, 6.0798, 6.0974],
        [5.5478, 5.5478, 5.5478],
        [5.5478, 5.7030, 5.6257],
        [5.5478, 7.1237, 8.1625]], grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.09240873157978058 
model_pd.l_d.mean(): -12.675341606140137 
model_pd.lagr.mean(): -12.582932472229004 
model_pd.lambdas: dict_items([('pout', tensor([1.1552])), ('power', tensor([0.6417]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3225])), ('power', tensor([-20.3017]))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.09240873157978058
epoch£º35	 i:1 	 global-step:701	 l-p:0.10451889038085938
epoch£º35	 i:2 	 global-step:702	 l-p:0.07155335694551468
epoch£º35	 i:3 	 global-step:703	 l-p:0.09988284111022949
epoch£º35	 i:4 	 global-step:704	 l-p:0.08411251753568649
epoch£º35	 i:5 	 global-step:705	 l-p:0.10834165662527084
epoch£º35	 i:6 	 global-step:706	 l-p:0.11210406571626663
epoch£º35	 i:7 	 global-step:707	 l-p:0.23576101660728455
epoch£º35	 i:8 	 global-step:708	 l-p:0.07480399310588837
epoch£º35	 i:9 	 global-step:709	 l-p:0.11605895310640335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]])
 pt:tensor([[5.7966, 5.7967, 5.7966],
        [5.7966, 5.7966, 5.7966],
        [5.7966, 6.5909, 6.7710],
        [5.7966, 5.8546, 5.8118]], grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.09521570801734924 
model_pd.l_d.mean(): -11.958521842956543 
model_pd.lagr.mean(): -11.863306045532227 
model_pd.lambdas: dict_items([('pout', tensor([1.1580])), ('power', tensor([0.6316]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2868])), ('power', tensor([-19.4280]))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.09521570801734924
epoch£º36	 i:1 	 global-step:721	 l-p:0.07559093087911606
epoch£º36	 i:2 	 global-step:722	 l-p:0.10211849957704544
epoch£º36	 i:3 	 global-step:723	 l-p:0.11290233582258224
epoch£º36	 i:4 	 global-step:724	 l-p:0.0909547433257103
epoch£º36	 i:5 	 global-step:725	 l-p:0.09932800382375717
epoch£º36	 i:6 	 global-step:726	 l-p:0.09643187373876572
epoch£º36	 i:7 	 global-step:727	 l-p:0.12458166480064392
epoch£º36	 i:8 	 global-step:728	 l-p:0.09672307968139648
epoch£º36	 i:9 	 global-step:729	 l-p:0.0010070682037621737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]])
 pt:tensor([[5.8381, 5.8459, 5.8387],
        [5.8381, 7.4716, 8.5242],
        [5.8381, 6.3399, 6.3248],
        [5.8381, 6.4664, 6.5233]], grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.07707498967647552 
model_pd.l_d.mean(): -11.471720695495605 
model_pd.lagr.mean(): -11.394645690917969 
model_pd.lambdas: dict_items([('pout', tensor([1.1603])), ('power', tensor([0.6217]))]) 
model_pd.vars: dict_items([('pout', tensor([0.3184])), ('power', tensor([-19.0186]))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.07707498967647552
epoch£º37	 i:1 	 global-step:741	 l-p:0.13980117440223694
epoch£º37	 i:2 	 global-step:742	 l-p:0.10860119760036469
epoch£º37	 i:3 	 global-step:743	 l-p:0.09458385407924652
epoch£º37	 i:4 	 global-step:744	 l-p:-0.01875327154994011
epoch£º37	 i:5 	 global-step:745	 l-p:0.11170542985200882
epoch£º37	 i:6 	 global-step:746	 l-p:0.09420309215784073
epoch£º37	 i:7 	 global-step:747	 l-p:0.06989774107933044
epoch£º37	 i:8 	 global-step:748	 l-p:0.0783393457531929
epoch£º37	 i:9 	 global-step:749	 l-p:0.11890669912099838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]])
 pt:tensor([[5.8549, 5.9953, 5.9185],
        [5.8549, 5.9385, 5.8822],
        [5.8549, 5.8873, 5.8608],
        [5.8549, 5.9347, 5.8802]], grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): -0.03526902571320534 
model_pd.l_d.mean(): -12.025970458984375 
model_pd.lagr.mean(): -12.061239242553711 
model_pd.lambdas: dict_items([('pout', tensor([1.1626])), ('power', tensor([0.6116]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2503])), ('power', tensor([-20.1064]))])
epoch£º38	 i:0 	 global-step:760	 l-p:-0.03526902571320534
epoch£º38	 i:1 	 global-step:761	 l-p:0.09858018159866333
epoch£º38	 i:2 	 global-step:762	 l-p:0.09558064490556717
epoch£º38	 i:3 	 global-step:763	 l-p:0.07655970007181168
epoch£º38	 i:4 	 global-step:764	 l-p:0.10023213177919388
epoch£º38	 i:5 	 global-step:765	 l-p:0.08703967928886414
epoch£º38	 i:6 	 global-step:766	 l-p:0.106107197701931
epoch£º38	 i:7 	 global-step:767	 l-p:0.0977313444018364
epoch£º38	 i:8 	 global-step:768	 l-p:0.1110883429646492
epoch£º38	 i:9 	 global-step:769	 l-p:0.3436920940876007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]])
 pt:tensor([[5.7373, 5.7373, 5.7373],
        [5.7373, 7.0665, 7.7770],
        [5.7373, 5.7373, 5.7373],
        [5.7373, 5.7399, 5.7374]], grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.07874638587236404 
model_pd.l_d.mean(): -12.338299751281738 
model_pd.lagr.mean(): -12.259552955627441 
model_pd.lambdas: dict_items([('pout', tensor([1.1648])), ('power', tensor([0.6015]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981])), ('power', tensor([-20.8590]))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.07874638587236404
epoch£º39	 i:1 	 global-step:781	 l-p:0.10489963740110397
epoch£º39	 i:2 	 global-step:782	 l-p:0.10821607708930969
epoch£º39	 i:3 	 global-step:783	 l-p:0.08689321577548981
epoch£º39	 i:4 	 global-step:784	 l-p:0.09284193068742752
epoch£º39	 i:5 	 global-step:785	 l-p:0.11150485277175903
epoch£º39	 i:6 	 global-step:786	 l-p:0.02349022403359413
epoch£º39	 i:7 	 global-step:787	 l-p:-0.27504846453666687
epoch£º39	 i:8 	 global-step:788	 l-p:0.09505210071802139
epoch£º39	 i:9 	 global-step:789	 l-p:0.09763778746128082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]])
 pt:tensor([[5.7069, 5.7071, 5.7069],
        [5.7069, 5.7298, 5.7103],
        [5.7069, 5.8449, 5.7700],
        [5.7069, 5.7161, 5.7077]], grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.07463578879833221 
model_pd.l_d.mean(): -12.02744197845459 
model_pd.lagr.mean(): -11.95280647277832 
model_pd.lambdas: dict_items([('pout', tensor([1.1675])), ('power', tensor([0.5915]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2274])), ('power', tensor([-20.7472]))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.07463578879833221
epoch£º40	 i:1 	 global-step:801	 l-p:0.09741005301475525
epoch£º40	 i:2 	 global-step:802	 l-p:0.11611176282167435
epoch£º40	 i:3 	 global-step:803	 l-p:0.1099657416343689
epoch£º40	 i:4 	 global-step:804	 l-p:0.5209036469459534
epoch£º40	 i:5 	 global-step:805	 l-p:0.058538999408483505
epoch£º40	 i:6 	 global-step:806	 l-p:0.09825076907873154
epoch£º40	 i:7 	 global-step:807	 l-p:0.10980402678251266
epoch£º40	 i:8 	 global-step:808	 l-p:0.0985245630145073
epoch£º40	 i:9 	 global-step:809	 l-p:0.10197488963603973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]])
 pt:tensor([[5.8872, 5.8906, 5.8873],
        [5.8872, 5.8876, 5.8872],
        [5.8872, 6.1230, 6.0341],
        [5.8872, 6.0349, 5.9560]], grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10314536839723587 
model_pd.l_d.mean(): -11.750953674316406 
model_pd.lagr.mean(): -11.647808074951172 
model_pd.lambdas: dict_items([('pout', tensor([1.1700])), ('power', tensor([0.5814]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1877])), ('power', tensor([-20.5519]))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10314536839723587
epoch£º41	 i:1 	 global-step:821	 l-p:0.0665428638458252
epoch£º41	 i:2 	 global-step:822	 l-p:0.07031968235969543
epoch£º41	 i:3 	 global-step:823	 l-p:0.1293126344680786
epoch£º41	 i:4 	 global-step:824	 l-p:0.09541326761245728
epoch£º41	 i:5 	 global-step:825	 l-p:0.1051110252737999
epoch£º41	 i:6 	 global-step:826	 l-p:0.09897726774215698
epoch£º41	 i:7 	 global-step:827	 l-p:0.046350426971912384
epoch£º41	 i:8 	 global-step:828	 l-p:0.12871499359607697
epoch£º41	 i:9 	 global-step:829	 l-p:0.10417554527521133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]])
 pt:tensor([[6.1256, 6.5125, 6.4401],
        [6.1256, 6.1256, 6.1256],
        [6.1256, 7.9735, 9.2385],
        [6.1256, 6.1477, 6.1287]], grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.0978097915649414 
model_pd.l_d.mean(): -10.758798599243164 
model_pd.lagr.mean(): -10.660988807678223 
model_pd.lambdas: dict_items([('pout', tensor([1.1722])), ('power', tensor([0.5715]))]) 
model_pd.vars: dict_items([('pout', tensor([0.2103])), ('power', tensor([-19.2243]))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.0978097915649414
epoch£º42	 i:1 	 global-step:841	 l-p:-0.13984204828739166
epoch£º42	 i:2 	 global-step:842	 l-p:0.09611714631319046
epoch£º42	 i:3 	 global-step:843	 l-p:0.09329850971698761
epoch£º42	 i:4 	 global-step:844	 l-p:0.08830343931913376
epoch£º42	 i:5 	 global-step:845	 l-p:0.15479113161563873
epoch£º42	 i:6 	 global-step:846	 l-p:0.09984899312257767
epoch£º42	 i:7 	 global-step:847	 l-p:0.0758063867688179
epoch£º42	 i:8 	 global-step:848	 l-p:0.08396697789430618
epoch£º42	 i:9 	 global-step:849	 l-p:0.0943056270480156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]])
 pt:tensor([[6.2625, 7.9745, 9.0410],
        [6.2625, 7.0444, 7.1743],
        [6.2625, 6.6975, 6.6348],
        [6.2625, 7.8909, 8.8566]], grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.06894480437040329 
model_pd.l_d.mean(): -10.988471031188965 
model_pd.lagr.mean(): -10.919526100158691 
model_pd.lambdas: dict_items([('pout', tensor([1.1737])), ('power', tensor([0.5616]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1578])), ('power', tensor([-19.8626]))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.06894480437040329
epoch£º43	 i:1 	 global-step:861	 l-p:0.11170343309640884
epoch£º43	 i:2 	 global-step:862	 l-p:0.1274789273738861
epoch£º43	 i:3 	 global-step:863	 l-p:0.07523403316736221
epoch£º43	 i:4 	 global-step:864	 l-p:0.09394997358322144
epoch£º43	 i:5 	 global-step:865	 l-p:0.09156659990549088
epoch£º43	 i:6 	 global-step:866	 l-p:0.08468872308731079
epoch£º43	 i:7 	 global-step:867	 l-p:0.1544589251279831
epoch£º43	 i:8 	 global-step:868	 l-p:0.08120527863502502
epoch£º43	 i:9 	 global-step:869	 l-p:0.08357616513967514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]])
 pt:tensor([[6.5846, 6.7758, 6.6807],
        [6.5846, 6.5953, 6.5855],
        [6.5846, 6.5882, 6.5848],
        [6.5846, 6.5847, 6.5846]], grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.09056533873081207 
model_pd.l_d.mean(): -11.017952919006348 
model_pd.lagr.mean(): -10.927387237548828 
model_pd.lambdas: dict_items([('pout', tensor([1.1747])), ('power', tensor([0.5517]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0393])), ('power', tensor([-20.0182]))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.09056533873081207
epoch£º44	 i:1 	 global-step:881	 l-p:0.07723362743854523
epoch£º44	 i:2 	 global-step:882	 l-p:0.09402861446142197
epoch£º44	 i:3 	 global-step:883	 l-p:0.10701851546764374
epoch£º44	 i:4 	 global-step:884	 l-p:0.026818357408046722
epoch£º44	 i:5 	 global-step:885	 l-p:0.08354455977678299
epoch£º44	 i:6 	 global-step:886	 l-p:0.05722418799996376
epoch£º44	 i:7 	 global-step:887	 l-p:0.09444471448659897
epoch£º44	 i:8 	 global-step:888	 l-p:0.09905079007148743
epoch£º44	 i:9 	 global-step:889	 l-p:0.05401879921555519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]])
 pt:tensor([[6.3130, 6.3130, 6.3130],
        [6.3130, 6.3145, 6.3130],
        [6.3130, 6.6489, 6.5591],
        [6.3130, 6.3130, 6.3130]], grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.08701212704181671 
model_pd.l_d.mean(): -11.05613899230957 
model_pd.lagr.mean(): -10.96912670135498 
model_pd.lambdas: dict_items([('pout', tensor([1.1758])), ('power', tensor([0.5418]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0683])), ('power', tensor([-20.5143]))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.08701212704181671
epoch£º45	 i:1 	 global-step:901	 l-p:0.07516055554151535
epoch£º45	 i:2 	 global-step:902	 l-p:0.09134162962436676
epoch£º45	 i:3 	 global-step:903	 l-p:0.13060538470745087
epoch£º45	 i:4 	 global-step:904	 l-p:0.09046033024787903
epoch£º45	 i:5 	 global-step:905	 l-p:0.06979946792125702
epoch£º45	 i:6 	 global-step:906	 l-p:0.09054507315158844
epoch£º45	 i:7 	 global-step:907	 l-p:0.09044576436281204
epoch£º45	 i:8 	 global-step:908	 l-p:0.09206932038068771
epoch£º45	 i:9 	 global-step:909	 l-p:-0.0053701638244092464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]])
 pt:tensor([[6.4281, 6.4344, 6.4285],
        [6.4281, 6.4677, 6.4356],
        [6.4281, 6.4281, 6.4281],
        [6.4281, 6.4668, 6.4353]], grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.0571163184940815 
model_pd.l_d.mean(): -10.287980079650879 
model_pd.lagr.mean(): -10.230863571166992 
model_pd.lambdas: dict_items([('pout', tensor([1.1771])), ('power', tensor([0.5320]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297])), ('power', tensor([-19.5886]))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.0571163184940815
epoch£º46	 i:1 	 global-step:921	 l-p:-0.2899050712585449
epoch£º46	 i:2 	 global-step:922	 l-p:0.10111060738563538
epoch£º46	 i:3 	 global-step:923	 l-p:0.08885709941387177
epoch£º46	 i:4 	 global-step:924	 l-p:0.08449490368366241
epoch£º46	 i:5 	 global-step:925	 l-p:0.09080849587917328
epoch£º46	 i:6 	 global-step:926	 l-p:0.09626191109418869
epoch£º46	 i:7 	 global-step:927	 l-p:0.38728347420692444
epoch£º46	 i:8 	 global-step:928	 l-p:0.1627321094274521
epoch£º46	 i:9 	 global-step:929	 l-p:0.09248557686805725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]])
 pt:tensor([[6.5537, 6.6409, 6.5805],
        [6.5537, 6.7192, 6.6300],
        [6.5537, 6.5565, 6.5538],
        [6.5537, 7.0766, 7.0348]], grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.032081570476293564 
model_pd.l_d.mean(): -9.569703102111816 
model_pd.lagr.mean(): -9.53762149810791 
model_pd.lambdas: dict_items([('pout', tensor([1.1781])), ('power', tensor([0.5222]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297])), ('power', tensor([-18.5836]))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.032081570476293564
epoch£º47	 i:1 	 global-step:941	 l-p:0.07884415984153748
epoch£º47	 i:2 	 global-step:942	 l-p:0.094192735850811
epoch£º47	 i:3 	 global-step:943	 l-p:0.1285812258720398
epoch£º47	 i:4 	 global-step:944	 l-p:0.08418199419975281
epoch£º47	 i:5 	 global-step:945	 l-p:0.1323878914117813
epoch£º47	 i:6 	 global-step:946	 l-p:0.09745341539382935
epoch£º47	 i:7 	 global-step:947	 l-p:0.07190411537885666
epoch£º47	 i:8 	 global-step:948	 l-p:0.0923372432589531
epoch£º47	 i:9 	 global-step:949	 l-p:0.12086859345436096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]])
 pt:tensor([[6.6100, 7.1419, 7.1012],
        [6.6100, 6.8970, 6.7950],
        [6.6100, 6.6100, 6.6100],
        [6.6100, 6.9743, 6.8810]], grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.09433824568986893 
model_pd.l_d.mean(): -10.403449058532715 
model_pd.lagr.mean(): -10.309110641479492 
model_pd.lambdas: dict_items([('pout', tensor([1.1787])), ('power', tensor([0.5124]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0097])), ('power', tensor([-20.2865]))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.09433824568986893
epoch£º48	 i:1 	 global-step:961	 l-p:0.09036660194396973
epoch£º48	 i:2 	 global-step:962	 l-p:0.08922349661588669
epoch£º48	 i:3 	 global-step:963	 l-p:-0.7644792795181274
epoch£º48	 i:4 	 global-step:964	 l-p:0.07917939871549606
epoch£º48	 i:5 	 global-step:965	 l-p:0.2909865379333496
epoch£º48	 i:6 	 global-step:966	 l-p:0.10427583754062653
epoch£º48	 i:7 	 global-step:967	 l-p:0.09407643973827362
epoch£º48	 i:8 	 global-step:968	 l-p:0.08958805352449417
epoch£º48	 i:9 	 global-step:969	 l-p:0.11799100786447525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]])
 pt:tensor([[6.3851, 6.4365, 6.3967],
        [6.3851, 6.3871, 6.3852],
        [6.3851, 6.6185, 6.5211],
        [6.3851, 6.3944, 6.3859]], grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.10952906310558319 
model_pd.l_d.mean(): -10.255091667175293 
model_pd.lagr.mean(): -10.145562171936035 
model_pd.lambdas: dict_items([('pout', tensor([1.1797])), ('power', tensor([0.5025]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0496])), ('power', tensor([-20.4809]))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.10952906310558319
epoch£º49	 i:1 	 global-step:981	 l-p:0.09307030588388443
epoch£º49	 i:2 	 global-step:982	 l-p:0.08798348158597946
epoch£º49	 i:3 	 global-step:983	 l-p:0.09821125119924545
epoch£º49	 i:4 	 global-step:984	 l-p:0.08876923471689224
epoch£º49	 i:5 	 global-step:985	 l-p:0.028235532343387604
epoch£º49	 i:6 	 global-step:986	 l-p:0.1226026713848114
epoch£º49	 i:7 	 global-step:987	 l-p:0.09936404973268509
epoch£º49	 i:8 	 global-step:988	 l-p:0.015318844467401505
epoch£º49	 i:9 	 global-step:989	 l-p:0.07434842735528946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]])
 pt:tensor([[6.0901, 7.2447, 7.7179],
        [6.0901, 6.0902, 6.0901],
        [6.0901, 6.5407, 6.4911],
        [6.0901, 6.0926, 6.0902]], grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.08078841120004654 
model_pd.l_d.mean(): -9.71065616607666 
model_pd.lagr.mean(): -9.629867553710938 
model_pd.lambdas: dict_items([('pout', tensor([1.1815])), ('power', tensor([0.4926]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1829])), ('power', tensor([-20.1089]))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.08078841120004654
epoch£º50	 i:1 	 global-step:1001	 l-p:0.04556873440742493
epoch£º50	 i:2 	 global-step:1002	 l-p:0.07808950543403625
epoch£º50	 i:3 	 global-step:1003	 l-p:0.08242611587047577
epoch£º50	 i:4 	 global-step:1004	 l-p:0.08979898691177368
epoch£º50	 i:5 	 global-step:1005	 l-p:0.09713200479745865
epoch£º50	 i:6 	 global-step:1006	 l-p:0.08263570815324783
epoch£º50	 i:7 	 global-step:1007	 l-p:0.12146390229463577
epoch£º50	 i:8 	 global-step:1008	 l-p:0.09964536875486374
epoch£º50	 i:9 	 global-step:1009	 l-p:0.09413197636604309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]])
 pt:tensor([[6.3995, 6.3995, 6.3995],
        [6.3995, 7.2679, 7.4510],
        [6.3995, 8.2663, 9.4976],
        [6.3995, 6.3996, 6.3995]], grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): -0.03677830100059509 
model_pd.l_d.mean(): -8.908262252807617 
model_pd.lagr.mean(): -8.945040702819824 
model_pd.lambdas: dict_items([('pout', tensor([1.1830])), ('power', tensor([0.4828]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1731])), ('power', tensor([-18.8386]))])
epoch£º51	 i:0 	 global-step:1020	 l-p:-0.03677830100059509
epoch£º51	 i:1 	 global-step:1021	 l-p:0.0541403666138649
epoch£º51	 i:2 	 global-step:1022	 l-p:0.20571036636829376
epoch£º51	 i:3 	 global-step:1023	 l-p:0.10342307388782501
epoch£º51	 i:4 	 global-step:1024	 l-p:0.09611926972866058
epoch£º51	 i:5 	 global-step:1025	 l-p:0.09311337769031525
epoch£º51	 i:6 	 global-step:1026	 l-p:0.08230697363615036
epoch£º51	 i:7 	 global-step:1027	 l-p:0.08645493537187576
epoch£º51	 i:8 	 global-step:1028	 l-p:0.10212938487529755
epoch£º51	 i:9 	 global-step:1029	 l-p:0.12266406416893005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]])
 pt:tensor([[6.6525, 7.5890, 7.8021],
        [6.6525, 7.3321, 7.3650],
        [6.6525, 6.7346, 6.6765],
        [6.6525, 6.6525, 6.6525]], grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.0895310789346695 
model_pd.l_d.mean(): -9.479327201843262 
model_pd.lagr.mean(): -9.389796257019043 
model_pd.lambdas: dict_items([('pout', tensor([1.1836])), ('power', tensor([0.4730]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0139])), ('power', tensor([-20.0352]))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.0895310789346695
epoch£º52	 i:1 	 global-step:1041	 l-p:0.12983307242393494
epoch£º52	 i:2 	 global-step:1042	 l-p:0.10063916444778442
epoch£º52	 i:3 	 global-step:1043	 l-p:0.11351220309734344
epoch£º52	 i:4 	 global-step:1044	 l-p:0.08740291744470596
epoch£º52	 i:5 	 global-step:1045	 l-p:0.08358055353164673
epoch£º52	 i:6 	 global-step:1046	 l-p:0.08530694991350174
epoch£º52	 i:7 	 global-step:1047	 l-p:0.0690980851650238
epoch£º52	 i:8 	 global-step:1048	 l-p:0.1125100702047348
epoch£º52	 i:9 	 global-step:1049	 l-p:0.505916178226471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]])
 pt:tensor([[6.6715, 6.6855, 6.6729],
        [6.6715, 6.6716, 6.6715],
        [6.6715, 6.6725, 6.6715],
        [6.6715, 6.6723, 6.6715]], grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.1152379959821701 
model_pd.l_d.mean(): -8.713516235351562 
model_pd.lagr.mean(): -8.598278045654297 
model_pd.lambdas: dict_items([('pout', tensor([1.1844])), ('power', tensor([0.4632]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0837])), ('power', tensor([-18.9851]))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.1152379959821701
epoch£º53	 i:1 	 global-step:1061	 l-p:0.08649776130914688
epoch£º53	 i:2 	 global-step:1062	 l-p:0.0774480402469635
epoch£º53	 i:3 	 global-step:1063	 l-p:0.08109262585639954
epoch£º53	 i:4 	 global-step:1064	 l-p:0.10739963501691818
epoch£º53	 i:5 	 global-step:1065	 l-p:0.02438516542315483
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12104656547307968
epoch£º53	 i:7 	 global-step:1067	 l-p:0.08961916714906693
epoch£º53	 i:8 	 global-step:1068	 l-p:0.09086665511131287
epoch£º53	 i:9 	 global-step:1069	 l-p:0.08740860968828201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]])
 pt:tensor([[6.8990, 6.8990, 6.8990],
        [6.8990, 8.0107, 8.3459],
        [6.8990, 8.0890, 8.4948],
        [6.8990, 7.0721, 6.9780]], grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 1.4922295808792114 
model_pd.l_d.mean(): -9.090080261230469 
model_pd.lagr.mean(): -7.597850799560547 
model_pd.lambdas: dict_items([('pout', tensor([1.1846])), ('power', tensor([0.4535]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0316])), ('power', tensor([-19.9193]))])
epoch£º54	 i:0 	 global-step:1080	 l-p:1.4922295808792114
epoch£º54	 i:1 	 global-step:1081	 l-p:0.09582594037055969
epoch£º54	 i:2 	 global-step:1082	 l-p:0.07957662642002106
epoch£º54	 i:3 	 global-step:1083	 l-p:0.049337469041347504
epoch£º54	 i:4 	 global-step:1084	 l-p:0.08240913599729538
epoch£º54	 i:5 	 global-step:1085	 l-p:0.07779429107904434
epoch£º54	 i:6 	 global-step:1086	 l-p:0.09208753705024719
epoch£º54	 i:7 	 global-step:1087	 l-p:0.08903712034225464
epoch£º54	 i:8 	 global-step:1088	 l-p:0.11798611283302307
epoch£º54	 i:9 	 global-step:1089	 l-p:0.0904305949807167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]])
 pt:tensor([[6.8435, 8.0220, 8.4238],
        [6.8435, 8.4860, 9.3663],
        [6.8435, 7.5497, 7.5856],
        [6.8435, 8.0361, 8.4508]], grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): -0.006456470582634211 
model_pd.l_d.mean(): -8.65629768371582 
model_pd.lagr.mean(): -8.66275405883789 
model_pd.lambdas: dict_items([('pout', tensor([1.1849])), ('power', tensor([0.4438]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0302])), ('power', tensor([-19.5424]))])
epoch£º55	 i:0 	 global-step:1100	 l-p:-0.006456470582634211
epoch£º55	 i:1 	 global-step:1101	 l-p:0.09036088734865189
epoch£º55	 i:2 	 global-step:1102	 l-p:0.08617639541625977
epoch£º55	 i:3 	 global-step:1103	 l-p:0.08846423774957657
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10867343097925186
epoch£º55	 i:5 	 global-step:1105	 l-p:0.1031220406293869
epoch£º55	 i:6 	 global-step:1106	 l-p:0.09278568625450134
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12141294777393341
epoch£º55	 i:8 	 global-step:1108	 l-p:0.10662463307380676
epoch£º55	 i:9 	 global-step:1109	 l-p:0.08953087031841278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]])
 pt:tensor([[ 6.7437,  8.7368, 10.0560],
        [ 6.7437,  6.7437,  6.7437],
        [ 6.7437,  7.1253,  7.0313],
        [ 6.7437,  6.8687,  6.7909]], grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.05728527903556824 
model_pd.l_d.mean(): -8.459395408630371 
model_pd.lagr.mean(): -8.40211009979248 
model_pd.lambdas: dict_items([('pout', tensor([1.1853])), ('power', tensor([0.4341]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0585])), ('power', tensor([-19.6040]))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.05728527903556824
epoch£º56	 i:1 	 global-step:1121	 l-p:0.04928472638130188
epoch£º56	 i:2 	 global-step:1122	 l-p:0.09384458512067795
epoch£º56	 i:3 	 global-step:1123	 l-p:0.09793036431074142
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12074988335371017
epoch£º56	 i:5 	 global-step:1125	 l-p:0.09303464740514755
epoch£º56	 i:6 	 global-step:1126	 l-p:0.11086509376764297
epoch£º56	 i:7 	 global-step:1127	 l-p:0.08098293095827103
epoch£º56	 i:8 	 global-step:1128	 l-p:0.10270225256681442
epoch£º56	 i:9 	 global-step:1129	 l-p:0.09051680564880371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]])
 pt:tensor([[6.8459, 8.4214, 9.2264],
        [6.8459, 8.5992, 9.6063],
        [6.8459, 6.8460, 6.8459],
        [6.8459, 6.9595, 6.8859]], grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.10334699600934982 
model_pd.l_d.mean(): -8.38417911529541 
model_pd.lagr.mean(): -8.280832290649414 
model_pd.lambdas: dict_items([('pout', tensor([1.1856])), ('power', tensor([0.4243]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0094])), ('power', tensor([-19.7383]))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.10334699600934982
epoch£º57	 i:1 	 global-step:1141	 l-p:0.011653129942715168
epoch£º57	 i:2 	 global-step:1142	 l-p:0.09240622818470001
epoch£º57	 i:3 	 global-step:1143	 l-p:0.10093150287866592
epoch£º57	 i:4 	 global-step:1144	 l-p:0.06832299381494522
epoch£º57	 i:5 	 global-step:1145	 l-p:0.11493692547082901
epoch£º57	 i:6 	 global-step:1146	 l-p:0.10592445731163025
epoch£º57	 i:7 	 global-step:1147	 l-p:0.0954841673374176
epoch£º57	 i:8 	 global-step:1148	 l-p:0.09740052372217178
epoch£º57	 i:9 	 global-step:1149	 l-p:0.059522464871406555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]])
 pt:tensor([[ 6.8356,  7.3426,  7.2815],
        [ 6.8356,  7.1937,  7.0930],
        [ 6.8356,  8.7828, 10.0257],
        [ 6.8356,  6.8356,  6.8356]], grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.011178426444530487 
model_pd.l_d.mean(): -8.322009086608887 
model_pd.lagr.mean(): -8.310831069946289 
model_pd.lambdas: dict_items([('pout', tensor([1.1859])), ('power', tensor([0.4146]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0152])), ('power', tensor([-19.9796]))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.011178426444530487
epoch£º58	 i:1 	 global-step:1161	 l-p:0.0929267480969429
epoch£º58	 i:2 	 global-step:1162	 l-p:0.03307192027568817
epoch£º58	 i:3 	 global-step:1163	 l-p:0.09583678096532822
epoch£º58	 i:4 	 global-step:1164	 l-p:0.10427471250295639
epoch£º58	 i:5 	 global-step:1165	 l-p:0.07544051110744476
epoch£º58	 i:6 	 global-step:1166	 l-p:0.06921828538179398
epoch£º58	 i:7 	 global-step:1167	 l-p:0.08487457036972046
epoch£º58	 i:8 	 global-step:1168	 l-p:0.08838322013616562
epoch£º58	 i:9 	 global-step:1169	 l-p:0.10217771679162979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]])
 pt:tensor([[7.0355, 7.6238, 7.5868],
        [7.0355, 7.0394, 7.0356],
        [7.0355, 7.0355, 7.0355],
        [7.0355, 7.3235, 7.2132]], grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.09346452355384827 
model_pd.l_d.mean(): -7.528125762939453 
model_pd.lagr.mean(): -7.434661388397217 
model_pd.lambdas: dict_items([('pout', tensor([1.1860])), ('power', tensor([0.4050]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0230])), ('power', tensor([-18.6112]))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.09346452355384827
epoch£º59	 i:1 	 global-step:1181	 l-p:0.0572962649166584
epoch£º59	 i:2 	 global-step:1182	 l-p:0.08961766958236694
epoch£º59	 i:3 	 global-step:1183	 l-p:1.950650930404663
epoch£º59	 i:4 	 global-step:1184	 l-p:0.08494313061237335
epoch£º59	 i:5 	 global-step:1185	 l-p:0.07996384799480438
epoch£º59	 i:6 	 global-step:1186	 l-p:0.08579728752374649
epoch£º59	 i:7 	 global-step:1187	 l-p:0.10344211757183075
epoch£º59	 i:8 	 global-step:1188	 l-p:0.11543246358633041
epoch£º59	 i:9 	 global-step:1189	 l-p:0.08413940668106079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]])
 pt:tensor([[6.9296, 7.1105, 7.0141],
        [6.9296, 6.9297, 6.9297],
        [6.9296, 6.9331, 6.9298],
        [6.9296, 7.4932, 7.4505]], grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.00827856920659542 
model_pd.l_d.mean(): -7.067984580993652 
model_pd.lagr.mean(): -7.059706211090088 
model_pd.lambdas: dict_items([('pout', tensor([1.1861])), ('power', tensor([0.3954]))]) 
model_pd.vars: dict_items([('pout', tensor([0.1251])), ('power', tensor([-18.2087]))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.00827856920659542
epoch£º60	 i:1 	 global-step:1201	 l-p:0.08998115360736847
epoch£º60	 i:2 	 global-step:1202	 l-p:0.1156148910522461
epoch£º60	 i:3 	 global-step:1203	 l-p:0.08040846139192581
epoch£º60	 i:4 	 global-step:1204	 l-p:0.09891751408576965
epoch£º60	 i:5 	 global-step:1205	 l-p:0.0785706415772438
epoch£º60	 i:6 	 global-step:1206	 l-p:0.09352419525384903
epoch£º60	 i:7 	 global-step:1207	 l-p:0.09338467568159103
epoch£º60	 i:8 	 global-step:1208	 l-p:0.08357637375593185
epoch£º60	 i:9 	 global-step:1209	 l-p:0.01861540600657463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]])
 pt:tensor([[6.8425, 6.8425, 6.8425],
        [6.8425, 7.8663, 8.1319],
        [6.8425, 6.8541, 6.8435],
        [6.8425, 8.0043, 8.3908]], grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.08557075262069702 
model_pd.l_d.mean(): -7.277769565582275 
model_pd.lagr.mean(): -7.192198753356934 
model_pd.lambdas: dict_items([('pout', tensor([1.1862])), ('power', tensor([0.3857]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0483])), ('power', tensor([-18.9719]))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.08557075262069702
epoch£º61	 i:1 	 global-step:1221	 l-p:0.0892803743481636
epoch£º61	 i:2 	 global-step:1222	 l-p:0.09648048132658005
epoch£º61	 i:3 	 global-step:1223	 l-p:0.10207036137580872
epoch£º61	 i:4 	 global-step:1224	 l-p:0.10817490518093109
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11779526621103287
epoch£º61	 i:6 	 global-step:1226	 l-p:0.09068932384252548
epoch£º61	 i:7 	 global-step:1227	 l-p:-0.02241802215576172
epoch£º61	 i:8 	 global-step:1228	 l-p:0.0758141353726387
epoch£º61	 i:9 	 global-step:1229	 l-p:0.026561575010418892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]])
 pt:tensor([[6.9894, 7.0466, 7.0023],
        [6.9894, 8.5348, 9.2846],
        [6.9894, 7.2216, 7.1154],
        [6.9894, 7.3632, 7.2607]], grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): -0.38606521487236023 
model_pd.l_d.mean(): -7.288411617279053 
model_pd.lagr.mean(): -7.674476623535156 
model_pd.lambdas: dict_items([('pout', tensor([1.1865])), ('power', tensor([0.3760]))]) 
model_pd.vars: dict_items([('pout', tensor([0.0166])), ('power', tensor([-19.3887]))])
epoch£º62	 i:0 	 global-step:1240	 l-p:-0.38606521487236023
epoch£º62	 i:1 	 global-step:1241	 l-p:0.08799035102128983
epoch£º62	 i:2 	 global-step:1242	 l-p:0.10054117441177368
epoch£º62	 i:3 	 global-step:1243	 l-p:0.058487776666879654
epoch£º62	 i:4 	 global-step:1244	 l-p:0.08344349265098572
epoch£º62	 i:5 	 global-step:1245	 l-p:0.04337229207158089
epoch£º62	 i:6 	 global-step:1246	 l-p:0.09970007836818695
epoch£º62	 i:7 	 global-step:1247	 l-p:0.08344994485378265
epoch£º62	 i:8 	 global-step:1248	 l-p:0.013217741623520851
epoch£º62	 i:9 	 global-step:1249	 l-p:0.09143740683794022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]])
 pt:tensor([[ 7.6422,  7.6475,  7.6424],
        [ 7.6422, 10.0345, 11.6727],
        [ 7.6422, 10.0425, 11.6912],
        [ 7.6422,  7.6422,  7.6422]], grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.07351450622081757 
model_pd.l_d.mean(): -7.107572555541992 
model_pd.lagr.mean(): -7.034058094024658 
model_pd.lambdas: dict_items([('pout', tensor([1.1858])), ('power', tensor([0.3664]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1204])), ('power', tensor([-18.9579]))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.07351450622081757
epoch£º63	 i:1 	 global-step:1261	 l-p:0.08675716072320938
epoch£º63	 i:2 	 global-step:1262	 l-p:0.08440078049898148
epoch£º63	 i:3 	 global-step:1263	 l-p:0.07994350790977478
epoch£º63	 i:4 	 global-step:1264	 l-p:0.08156588673591614
epoch£º63	 i:5 	 global-step:1265	 l-p:0.09097439050674438
epoch£º63	 i:6 	 global-step:1266	 l-p:-0.7388408184051514
epoch£º63	 i:7 	 global-step:1267	 l-p:0.08796896785497665
epoch£º63	 i:8 	 global-step:1268	 l-p:0.14428813755512238
epoch£º63	 i:9 	 global-step:1269	 l-p:0.12182604521512985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]])
 pt:tensor([[7.6261, 7.6297, 7.6262],
        [7.6261, 7.6261, 7.6261],
        [7.6261, 7.6825, 7.6378],
        [7.6261, 7.6261, 7.6261]], grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.08842842280864716 
model_pd.l_d.mean(): -6.857448577880859 
model_pd.lagr.mean(): -6.769020080566406 
model_pd.lambdas: dict_items([('pout', tensor([1.1847])), ('power', tensor([0.3570]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0984])), ('power', tensor([-18.8313]))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.08842842280864716
epoch£º64	 i:1 	 global-step:1281	 l-p:0.07763233780860901
epoch£º64	 i:2 	 global-step:1282	 l-p:0.18050003051757812
epoch£º64	 i:3 	 global-step:1283	 l-p:0.07792964577674866
epoch£º64	 i:4 	 global-step:1284	 l-p:0.08583391457796097
epoch£º64	 i:5 	 global-step:1285	 l-p:0.07598213106393814
epoch£º64	 i:6 	 global-step:1286	 l-p:0.0826152041554451
epoch£º64	 i:7 	 global-step:1287	 l-p:0.09402701258659363
epoch£º64	 i:8 	 global-step:1288	 l-p:0.08343477547168732
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11963853985071182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]])
 pt:tensor([[ 7.6302,  7.6311,  7.6302],
        [ 7.6302,  7.6302,  7.6302],
        [ 7.6302,  9.5531, 10.6192],
        [ 7.6302,  7.8628,  7.7486]], grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.2551891803741455 
model_pd.l_d.mean(): -6.737538814544678 
model_pd.lagr.mean(): -6.482349395751953 
model_pd.lambdas: dict_items([('pout', tensor([1.1834])), ('power', tensor([0.3476]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1099])), ('power', tensor([-18.9564]))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.2551891803741455
epoch£º65	 i:1 	 global-step:1301	 l-p:0.08708961308002472
epoch£º65	 i:2 	 global-step:1302	 l-p:0.10836568474769592
epoch£º65	 i:3 	 global-step:1303	 l-p:0.08685228228569031
epoch£º65	 i:4 	 global-step:1304	 l-p:0.08276741951704025
epoch£º65	 i:5 	 global-step:1305	 l-p:0.08105938136577606
epoch£º65	 i:6 	 global-step:1306	 l-p:0.096908338367939
epoch£º65	 i:7 	 global-step:1307	 l-p:0.10537003725767136
epoch£º65	 i:8 	 global-step:1308	 l-p:0.08385886251926422
epoch£º65	 i:9 	 global-step:1309	 l-p:0.07736166566610336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]])
 pt:tensor([[ 7.6675,  7.6675,  7.6675],
        [ 7.6675,  9.5771, 10.6214],
        [ 7.6675,  8.7206,  8.9303],
        [ 7.6675,  7.7190,  7.6776]], grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.16353796422481537 
model_pd.l_d.mean(): -6.6866984367370605 
model_pd.lagr.mean(): -6.523160457611084 
model_pd.lambdas: dict_items([('pout', tensor([1.1822])), ('power', tensor([0.3382]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1541])), ('power', tensor([-19.1786]))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.16353796422481537
epoch£º66	 i:1 	 global-step:1321	 l-p:0.0844593346118927
epoch£º66	 i:2 	 global-step:1322	 l-p:0.10359403491020203
epoch£º66	 i:3 	 global-step:1323	 l-p:0.08402065187692642
epoch£º66	 i:4 	 global-step:1324	 l-p:0.08292776346206665
epoch£º66	 i:5 	 global-step:1325	 l-p:0.07299715280532837
epoch£º66	 i:6 	 global-step:1326	 l-p:0.08344895392656326
epoch£º66	 i:7 	 global-step:1327	 l-p:0.104288749396801
epoch£º66	 i:8 	 global-step:1328	 l-p:0.0751371756196022
epoch£º66	 i:9 	 global-step:1329	 l-p:0.09044279158115387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]])
 pt:tensor([[ 7.7073,  7.7074,  7.7073],
        [ 7.7073,  7.7074,  7.7073],
        [ 7.7073,  9.5165, 10.4413],
        [ 7.7073,  9.1503,  9.6955]], grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.08077090233564377 
model_pd.l_d.mean(): -6.364304542541504 
model_pd.lagr.mean(): -6.283533573150635 
model_pd.lambdas: dict_items([('pout', tensor([1.1809])), ('power', tensor([0.3288]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1244])), ('power', tensor([-18.8539]))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.08077090233564377
epoch£º67	 i:1 	 global-step:1341	 l-p:0.07825062423944473
epoch£º67	 i:2 	 global-step:1342	 l-p:0.08674365282058716
epoch£º67	 i:3 	 global-step:1343	 l-p:0.08088009059429169
epoch£º67	 i:4 	 global-step:1344	 l-p:0.08617880195379257
epoch£º67	 i:5 	 global-step:1345	 l-p:0.07716801017522812
epoch£º67	 i:6 	 global-step:1346	 l-p:0.2789229154586792
epoch£º67	 i:7 	 global-step:1347	 l-p:0.0859902948141098
epoch£º67	 i:8 	 global-step:1348	 l-p:0.028727687895298004
epoch£º67	 i:9 	 global-step:1349	 l-p:0.10030410438776016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]])
 pt:tensor([[ 7.5386,  9.8056, 11.3049],
        [ 7.5386,  9.9025, 11.5261],
        [ 7.5386,  7.5585,  7.5408],
        [ 7.5386,  9.4353, 10.4867]], grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.08859656006097794 
model_pd.l_d.mean(): -5.943766117095947 
model_pd.lagr.mean(): -5.855169773101807 
model_pd.lambdas: dict_items([('pout', tensor([1.1799])), ('power', tensor([0.3194]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0342])), ('power', tensor([-18.4291]))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.08859656006097794
epoch£º68	 i:1 	 global-step:1361	 l-p:0.20353874564170837
epoch£º68	 i:2 	 global-step:1362	 l-p:0.08858853578567505
epoch£º68	 i:3 	 global-step:1363	 l-p:0.08308117836713791
epoch£º68	 i:4 	 global-step:1364	 l-p:0.07252561300992966
epoch£º68	 i:5 	 global-step:1365	 l-p:0.08013288676738739
epoch£º68	 i:6 	 global-step:1366	 l-p:0.08522746711969376
epoch£º68	 i:7 	 global-step:1367	 l-p:0.08389255404472351
epoch£º68	 i:8 	 global-step:1368	 l-p:0.0900251641869545
epoch£º68	 i:9 	 global-step:1369	 l-p:0.13897700607776642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]])
 pt:tensor([[ 7.7176,  8.0540,  7.9310],
        [ 7.7176,  9.7376, 10.9007],
        [ 7.7176,  8.1673,  8.0573],
        [ 7.7176,  7.7176,  7.7176]], grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.07079474627971649 
model_pd.l_d.mean(): -5.655105113983154 
model_pd.lagr.mean(): -5.584310531616211 
model_pd.lambdas: dict_items([('pout', tensor([1.1788])), ('power', tensor([0.3100]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0516])), ('power', tensor([-17.9929]))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.07079474627971649
epoch£º69	 i:1 	 global-step:1381	 l-p:0.08939435333013535
epoch£º69	 i:2 	 global-step:1382	 l-p:0.0845465436577797
epoch£º69	 i:3 	 global-step:1383	 l-p:0.07916280627250671
epoch£º69	 i:4 	 global-step:1384	 l-p:0.09409735351800919
epoch£º69	 i:5 	 global-step:1385	 l-p:0.10168112069368362
epoch£º69	 i:6 	 global-step:1386	 l-p:0.10512927919626236
epoch£º69	 i:7 	 global-step:1387	 l-p:0.07776708900928497
epoch£º69	 i:8 	 global-step:1388	 l-p:0.08425378054380417
epoch£º69	 i:9 	 global-step:1389	 l-p:0.08524154871702194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]])
 pt:tensor([[ 7.8765, 10.4553, 12.2845],
        [ 7.8765,  7.9352,  7.8888],
        [ 7.8765,  7.8765,  7.8765],
        [ 7.8765, 10.2967, 11.9197]], grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.11636096239089966 
model_pd.l_d.mean(): -5.335430145263672 
model_pd.lagr.mean(): -5.219069004058838 
model_pd.lambdas: dict_items([('pout', tensor([1.1774])), ('power', tensor([0.3007]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0442])), ('power', tensor([-17.5202]))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.11636096239089966
epoch£º70	 i:1 	 global-step:1401	 l-p:0.08793116360902786
epoch£º70	 i:2 	 global-step:1402	 l-p:0.08761754631996155
epoch£º70	 i:3 	 global-step:1403	 l-p:0.07457941025495529
epoch£º70	 i:4 	 global-step:1404	 l-p:0.0854731947183609
epoch£º70	 i:5 	 global-step:1405	 l-p:0.07246960699558258
epoch£º70	 i:6 	 global-step:1406	 l-p:0.07836423814296722
epoch£º70	 i:7 	 global-step:1407	 l-p:0.08483205735683441
epoch£º70	 i:8 	 global-step:1408	 l-p:0.07637841254472733
epoch£º70	 i:9 	 global-step:1409	 l-p:0.0817931741476059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]])
 pt:tensor([[7.8834, 8.2878, 8.1658],
        [7.8834, 7.8904, 7.8838],
        [7.8834, 7.8834, 7.8834],
        [7.8834, 9.0957, 9.4124]], grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.08876296132802963 
model_pd.l_d.mean(): -5.736788272857666 
model_pd.lagr.mean(): -5.6480255126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.1756])), ('power', tensor([0.2913]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1737])), ('power', tensor([-18.9309]))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.08876296132802963
epoch£º71	 i:1 	 global-step:1421	 l-p:0.10793092101812363
epoch£º71	 i:2 	 global-step:1422	 l-p:0.07962393015623093
epoch£º71	 i:3 	 global-step:1423	 l-p:0.09058870375156403
epoch£º71	 i:4 	 global-step:1424	 l-p:0.07384466379880905
epoch£º71	 i:5 	 global-step:1425	 l-p:0.08657386898994446
epoch£º71	 i:6 	 global-step:1426	 l-p:0.07796259224414825
epoch£º71	 i:7 	 global-step:1427	 l-p:0.08872950822114944
epoch£º71	 i:8 	 global-step:1428	 l-p:0.08182219415903091
epoch£º71	 i:9 	 global-step:1429	 l-p:0.08502884209156036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]])
 pt:tensor([[7.8315, 7.8315, 7.8315],
        [7.8315, 7.8316, 7.8315],
        [7.8315, 7.8414, 7.8323],
        [7.8315, 7.8316, 7.8315]], grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.09495589137077332 
model_pd.l_d.mean(): -5.340511798858643 
model_pd.lagr.mean(): -5.245555877685547 
model_pd.lambdas: dict_items([('pout', tensor([1.1741])), ('power', tensor([0.2820]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1419])), ('power', tensor([-18.2882]))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.09495589137077332
epoch£º72	 i:1 	 global-step:1441	 l-p:0.07899000495672226
epoch£º72	 i:2 	 global-step:1442	 l-p:0.0796852558851242
epoch£º72	 i:3 	 global-step:1443	 l-p:0.10680610686540604
epoch£º72	 i:4 	 global-step:1444	 l-p:0.07487541437149048
epoch£º72	 i:5 	 global-step:1445	 l-p:0.08256727457046509
epoch£º72	 i:6 	 global-step:1446	 l-p:0.08148893713951111
epoch£º72	 i:7 	 global-step:1447	 l-p:0.0837019756436348
epoch£º72	 i:8 	 global-step:1448	 l-p:0.095613993704319
epoch£º72	 i:9 	 global-step:1449	 l-p:0.08166763931512833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]])
 pt:tensor([[ 7.9158,  7.9158,  7.9158],
        [ 7.9158,  8.2483,  8.1214],
        [ 7.9158,  9.7952, 10.7640],
        [ 7.9158,  8.3773,  8.2636]], grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.08195236325263977 
model_pd.l_d.mean(): -5.199428081512451 
model_pd.lagr.mean(): -5.117475509643555 
model_pd.lambdas: dict_items([('pout', tensor([1.1726])), ('power', tensor([0.2727]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1670])), ('power', tensor([-18.2896]))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.08195236325263977
epoch£º73	 i:1 	 global-step:1461	 l-p:0.09715762734413147
epoch£º73	 i:2 	 global-step:1462	 l-p:0.09573039412498474
epoch£º73	 i:3 	 global-step:1463	 l-p:0.08192428946495056
epoch£º73	 i:4 	 global-step:1464	 l-p:0.08269491046667099
epoch£º73	 i:5 	 global-step:1465	 l-p:0.08465126156806946
epoch£º73	 i:6 	 global-step:1466	 l-p:0.07683327049016953
epoch£º73	 i:7 	 global-step:1467	 l-p:0.07449011504650116
epoch£º73	 i:8 	 global-step:1468	 l-p:0.08086256682872772
epoch£º73	 i:9 	 global-step:1469	 l-p:0.07837367802858353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]])
 pt:tensor([[ 8.0241,  8.0643,  8.0306],
        [ 8.0241,  8.7897,  8.7849],
        [ 8.0241,  9.4829, 10.0036],
        [ 8.0241,  8.4913,  8.3755]], grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.09904705733060837 
model_pd.l_d.mean(): -5.064696788787842 
model_pd.lagr.mean(): -4.965649604797363 
model_pd.lambdas: dict_items([('pout', tensor([1.1709])), ('power', tensor([0.2634]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1514])), ('power', tensor([-18.4931]))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.09904705733060837
epoch£º74	 i:1 	 global-step:1481	 l-p:0.07489046454429626
epoch£º74	 i:2 	 global-step:1482	 l-p:0.07293830811977386
epoch£º74	 i:3 	 global-step:1483	 l-p:0.08208253979682922
epoch£º74	 i:4 	 global-step:1484	 l-p:0.08508536219596863
epoch£º74	 i:5 	 global-step:1485	 l-p:0.0666416808962822
epoch£º74	 i:6 	 global-step:1486	 l-p:0.0827258750796318
epoch£º74	 i:7 	 global-step:1487	 l-p:0.08455679565668106
epoch£º74	 i:8 	 global-step:1488	 l-p:0.07649894058704376
epoch£º74	 i:9 	 global-step:1489	 l-p:0.08836578577756882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]])
 pt:tensor([[ 8.1511,  8.8621,  8.8233],
        [ 8.1511,  8.6085,  8.4871],
        [ 8.1511,  8.1511,  8.1511],
        [ 8.1511,  9.7462, 10.3823]], grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.08254363387823105 
model_pd.l_d.mean(): -4.982569217681885 
model_pd.lagr.mean(): -4.900025367736816 
model_pd.lambdas: dict_items([('pout', tensor([1.1689])), ('power', tensor([0.2541]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2098])), ('power', tensor([-18.5750]))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.08254363387823105
epoch£º75	 i:1 	 global-step:1501	 l-p:0.08222229778766632
epoch£º75	 i:2 	 global-step:1502	 l-p:0.08278211951255798
epoch£º75	 i:3 	 global-step:1503	 l-p:0.07259242981672287
epoch£º75	 i:4 	 global-step:1504	 l-p:0.07971686124801636
epoch£º75	 i:5 	 global-step:1505	 l-p:0.06715282797813416
epoch£º75	 i:6 	 global-step:1506	 l-p:0.08662381768226624
epoch£º75	 i:7 	 global-step:1507	 l-p:0.07390012592077255
epoch£º75	 i:8 	 global-step:1508	 l-p:0.08562950044870377
epoch£º75	 i:9 	 global-step:1509	 l-p:0.084926538169384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]])
 pt:tensor([[ 8.2001,  8.2014,  8.2001],
        [ 8.2001,  8.2001,  8.2001],
        [ 8.2001,  9.7406, 10.3173],
        [ 8.2001,  8.7077,  8.5950]], grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.084038145840168 
model_pd.l_d.mean(): -4.659974098205566 
model_pd.lagr.mean(): -4.5759358406066895 
model_pd.lambdas: dict_items([('pout', tensor([1.1670])), ('power', tensor([0.2449]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1737])), ('power', tensor([-18.1325]))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.084038145840168
epoch£º76	 i:1 	 global-step:1521	 l-p:0.076044961810112
epoch£º76	 i:2 	 global-step:1522	 l-p:0.07905298471450806
epoch£º76	 i:3 	 global-step:1523	 l-p:0.0818655714392662
epoch£º76	 i:4 	 global-step:1524	 l-p:0.05988452956080437
epoch£º76	 i:5 	 global-step:1525	 l-p:0.09463685750961304
epoch£º76	 i:6 	 global-step:1526	 l-p:0.07398904860019684
epoch£º76	 i:7 	 global-step:1527	 l-p:0.08407013863325119
epoch£º76	 i:8 	 global-step:1528	 l-p:0.07274745404720306
epoch£º76	 i:9 	 global-step:1529	 l-p:0.0666644349694252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]])
 pt:tensor([[8.4501, 8.4912, 8.4567],
        [8.4501, 8.4510, 8.4501],
        [8.4501, 8.5492, 8.4774],
        [8.4501, 8.6842, 8.5610]], grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.08446826785802841 
model_pd.l_d.mean(): -4.719156265258789 
model_pd.lagr.mean(): -4.634687900543213 
model_pd.lambdas: dict_items([('pout', tensor([1.1646])), ('power', tensor([0.2357]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2788])), ('power', tensor([-18.5683]))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.08446826785802841
epoch£º77	 i:1 	 global-step:1541	 l-p:0.05611739307641983
epoch£º77	 i:2 	 global-step:1542	 l-p:0.08015331625938416
epoch£º77	 i:3 	 global-step:1543	 l-p:0.07139001786708832
epoch£º77	 i:4 	 global-step:1544	 l-p:0.07848509401082993
epoch£º77	 i:5 	 global-step:1545	 l-p:0.0822509229183197
epoch£º77	 i:6 	 global-step:1546	 l-p:0.08435437828302383
epoch£º77	 i:7 	 global-step:1547	 l-p:0.07074878364801407
epoch£º77	 i:8 	 global-step:1548	 l-p:-4.632687091827393
epoch£º77	 i:9 	 global-step:1549	 l-p:0.07514443248510361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]])
 pt:tensor([[ 8.7187, 10.0816, 10.4391],
        [ 8.7187, 10.6698, 11.5917],
        [ 8.7187,  8.9697,  8.8401],
        [ 8.7187,  8.8484,  8.7600]], grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14679525792598724 
model_pd.l_d.mean(): -4.420210361480713 
model_pd.lagr.mean(): -4.2734150886535645 
model_pd.lambdas: dict_items([('pout', tensor([1.1620])), ('power', tensor([0.2267]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2770])), ('power', tensor([-18.0067]))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14679525792598724
epoch£º78	 i:1 	 global-step:1561	 l-p:0.062049876898527145
epoch£º78	 i:2 	 global-step:1562	 l-p:0.07645051181316376
epoch£º78	 i:3 	 global-step:1563	 l-p:0.06896629929542542
epoch£º78	 i:4 	 global-step:1564	 l-p:0.16690044105052948
epoch£º78	 i:5 	 global-step:1565	 l-p:0.075041264295578
epoch£º78	 i:6 	 global-step:1566	 l-p:0.06639707088470459
epoch£º78	 i:7 	 global-step:1567	 l-p:0.07760415226221085
epoch£º78	 i:8 	 global-step:1568	 l-p:0.07555291056632996
epoch£º78	 i:9 	 global-step:1569	 l-p:0.07420728355646133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]])
 pt:tensor([[ 9.3380, 10.9314, 11.4220],
        [ 9.3380,  9.3380,  9.3380],
        [ 9.3380,  9.3426,  9.3382],
        [ 9.3380,  9.3502,  9.3388]], grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.07648303359746933 
model_pd.l_d.mean(): -4.359833717346191 
model_pd.lagr.mean(): -4.283350467681885 
model_pd.lambdas: dict_items([('pout', tensor([1.1585])), ('power', tensor([0.2178]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4031])), ('power', tensor([-17.7969]))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.07648303359746933
epoch£º79	 i:1 	 global-step:1581	 l-p:0.07483693957328796
epoch£º79	 i:2 	 global-step:1582	 l-p:0.0807439535856247
epoch£º79	 i:3 	 global-step:1583	 l-p:0.050874706357717514
epoch£º79	 i:4 	 global-step:1584	 l-p:0.07654628157615662
epoch£º79	 i:5 	 global-step:1585	 l-p:0.0722540020942688
epoch£º79	 i:6 	 global-step:1586	 l-p:0.05559356138110161
epoch£º79	 i:7 	 global-step:1587	 l-p:0.0906166359782219
epoch£º79	 i:8 	 global-step:1588	 l-p:0.07516859471797943
epoch£º79	 i:9 	 global-step:1589	 l-p:0.07749922573566437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]])
 pt:tensor([[ 9.6591, 12.9509, 15.3154],
        [ 9.6591,  9.7071,  9.6667],
        [ 9.6591,  9.6591,  9.6591],
        [ 9.6591, 12.0196, 13.2405]], grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.07234698534011841 
model_pd.l_d.mean(): -3.9930405616760254 
model_pd.lagr.mean(): -3.9206936359405518 
model_pd.lambdas: dict_items([('pout', tensor([1.1546])), ('power', tensor([0.2092]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4026])), ('power', tensor([-16.7971]))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.07234698534011841
epoch£º80	 i:1 	 global-step:1601	 l-p:0.07798466086387634
epoch£º80	 i:2 	 global-step:1602	 l-p:0.0746774971485138
epoch£º80	 i:3 	 global-step:1603	 l-p:0.08573899418115616
epoch£º80	 i:4 	 global-step:1604	 l-p:0.06714951992034912
epoch£º80	 i:5 	 global-step:1605	 l-p:-0.24089035391807556
epoch£º80	 i:6 	 global-step:1606	 l-p:0.07764121145009995
epoch£º80	 i:7 	 global-step:1607	 l-p:0.06885718554258347
epoch£º80	 i:8 	 global-step:1608	 l-p:0.0787128433585167
epoch£º80	 i:9 	 global-step:1609	 l-p:0.11012022197246552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]])
 pt:tensor([[ 9.9178,  9.9355,  9.9193],
        [ 9.9178,  9.9178,  9.9178],
        [ 9.9178, 12.3578, 13.6251],
        [ 9.9178, 10.1205,  9.9958]], grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.07581561803817749 
model_pd.l_d.mean(): -3.998795509338379 
model_pd.lagr.mean(): -3.9229798316955566 
model_pd.lambdas: dict_items([('pout', tensor([1.1501])), ('power', tensor([0.2006]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4678])), ('power', tensor([-17.1767]))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.07581561803817749
epoch£º81	 i:1 	 global-step:1621	 l-p:0.07264039665460587
epoch£º81	 i:2 	 global-step:1622	 l-p:0.10211587697267532
epoch£º81	 i:3 	 global-step:1623	 l-p:0.07202866673469543
epoch£º81	 i:4 	 global-step:1624	 l-p:0.0848919227719307
epoch£º81	 i:5 	 global-step:1625	 l-p:0.07033966481685638
epoch£º81	 i:6 	 global-step:1626	 l-p:0.07218130677938461
epoch£º81	 i:7 	 global-step:1627	 l-p:0.07380583137273788
epoch£º81	 i:8 	 global-step:1628	 l-p:0.06881813704967499
epoch£º81	 i:9 	 global-step:1629	 l-p:0.07655611634254456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]])
 pt:tensor([[10.2799, 10.4912, 10.3612],
        [10.2799, 12.1156, 12.7170],
        [10.2799, 10.8948, 10.7399],
        [10.2799, 13.1142, 14.7727]], grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.07444646954536438 
model_pd.l_d.mean(): -3.850632667541504 
model_pd.lagr.mean(): -3.776186227798462 
model_pd.lambdas: dict_items([('pout', tensor([1.1453])), ('power', tensor([0.1922]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5194])), ('power', tensor([-16.8634]))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.07444646954536438
epoch£º82	 i:1 	 global-step:1641	 l-p:0.08221591264009476
epoch£º82	 i:2 	 global-step:1642	 l-p:0.071029432117939
epoch£º82	 i:3 	 global-step:1643	 l-p:0.0721668228507042
epoch£º82	 i:4 	 global-step:1644	 l-p:0.07601599395275116
epoch£º82	 i:5 	 global-step:1645	 l-p:0.0711127370595932
epoch£º82	 i:6 	 global-step:1646	 l-p:0.07205475121736526
epoch£º82	 i:7 	 global-step:1647	 l-p:0.07575159519910812
epoch£º82	 i:8 	 global-step:1648	 l-p:0.08367259055376053
epoch£º82	 i:9 	 global-step:1649	 l-p:0.06671068072319031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]])
 pt:tensor([[10.2078, 12.6022, 13.7728],
        [10.2078, 10.8748, 10.7336],
        [10.2078, 10.2098, 10.2079],
        [10.2078, 11.5800, 11.8051]], grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.07431674003601074 
model_pd.l_d.mean(): -3.563364267349243 
model_pd.lagr.mean(): -3.4890475273132324 
model_pd.lambdas: dict_items([('pout', tensor([1.1403])), ('power', tensor([0.1839]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4592])), ('power', tensor([-16.4560]))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.07431674003601074
epoch£º83	 i:1 	 global-step:1661	 l-p:0.07322060316801071
epoch£º83	 i:2 	 global-step:1662	 l-p:0.07128213346004486
epoch£º83	 i:3 	 global-step:1663	 l-p:0.07236525416374207
epoch£º83	 i:4 	 global-step:1664	 l-p:0.07185231149196625
epoch£º83	 i:5 	 global-step:1665	 l-p:0.0779397040605545
epoch£º83	 i:6 	 global-step:1666	 l-p:0.08692571520805359
epoch£º83	 i:7 	 global-step:1667	 l-p:0.07337170839309692
epoch£º83	 i:8 	 global-step:1668	 l-p:0.08196770399808884
epoch£º83	 i:9 	 global-step:1669	 l-p:0.07215987890958786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]])
 pt:tensor([[10.1638, 10.1644, 10.1638],
        [10.1638, 10.1903, 10.1666],
        [10.1638, 10.3855, 10.2525],
        [10.1638, 10.8282, 10.6879]], grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.07331006973981857 
model_pd.l_d.mean(): -3.559324264526367 
model_pd.lagr.mean(): -3.4860141277313232 
model_pd.lambdas: dict_items([('pout', tensor([1.1354])), ('power', tensor([0.1755]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4955])), ('power', tensor([-16.9950]))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.07331006973981857
epoch£º84	 i:1 	 global-step:1681	 l-p:0.06882569193840027
epoch£º84	 i:2 	 global-step:1682	 l-p:0.07527952641248703
epoch£º84	 i:3 	 global-step:1683	 l-p:0.0830318033695221
epoch£º84	 i:4 	 global-step:1684	 l-p:0.0801115334033966
epoch£º84	 i:5 	 global-step:1685	 l-p:0.07212691754102707
epoch£º84	 i:6 	 global-step:1686	 l-p:0.07129363715648651
epoch£º84	 i:7 	 global-step:1687	 l-p:0.07807476818561554
epoch£º84	 i:8 	 global-step:1688	 l-p:0.07218067348003387
epoch£º84	 i:9 	 global-step:1689	 l-p:0.07402972877025604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]])
 pt:tensor([[10.3406, 12.3696, 13.1454],
        [10.3406, 11.6752, 11.8623],
        [10.3406, 12.6375, 13.6840],
        [10.3406, 11.2963, 11.2565]], grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.07663437724113464 
model_pd.l_d.mean(): -3.3695931434631348 
model_pd.lagr.mean(): -3.2929587364196777 
model_pd.lambdas: dict_items([('pout', tensor([1.1304])), ('power', tensor([0.1671]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5033])), ('power', tensor([-16.6733]))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.07663437724113464
epoch£º85	 i:1 	 global-step:1701	 l-p:0.08461280167102814
epoch£º85	 i:2 	 global-step:1702	 l-p:0.08094848692417145
epoch£º85	 i:3 	 global-step:1703	 l-p:0.07229682803153992
epoch£º85	 i:4 	 global-step:1704	 l-p:0.07395976781845093
epoch£º85	 i:5 	 global-step:1705	 l-p:0.06923260539770126
epoch£º85	 i:6 	 global-step:1706	 l-p:0.0721966028213501
epoch£º85	 i:7 	 global-step:1707	 l-p:0.0696679949760437
epoch£º85	 i:8 	 global-step:1708	 l-p:0.07103267312049866
epoch£º85	 i:9 	 global-step:1709	 l-p:0.06258451193571091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]])
 pt:tensor([[10.5184, 12.8713, 13.9506],
        [10.5184, 10.5500, 10.5221],
        [10.5184, 10.5185, 10.5184],
        [10.5184, 11.4384, 11.3729]], grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.07196905463933945 
model_pd.l_d.mean(): -2.8377435207366943 
model_pd.lagr.mean(): -2.7657744884490967 
model_pd.lambdas: dict_items([('pout', tensor([1.1253])), ('power', tensor([0.1589]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3548])), ('power', tensor([-15.2706]))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.07196905463933945
epoch£º86	 i:1 	 global-step:1721	 l-p:0.07098328322172165
epoch£º86	 i:2 	 global-step:1722	 l-p:0.07302533835172653
epoch£º86	 i:3 	 global-step:1723	 l-p:0.07034530490636826
epoch£º86	 i:4 	 global-step:1724	 l-p:0.07151860743761063
epoch£º86	 i:5 	 global-step:1725	 l-p:0.07849773019552231
epoch£º86	 i:6 	 global-step:1726	 l-p:0.0737602487206459
epoch£º86	 i:7 	 global-step:1727	 l-p:0.07060112804174423
epoch£º86	 i:8 	 global-step:1728	 l-p:0.06970556080341339
epoch£º86	 i:9 	 global-step:1729	 l-p:0.06854082643985748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]])
 pt:tensor([[10.7808, 11.0863, 10.9244],
        [10.7808, 10.7958, 10.7819],
        [10.7808, 10.7824, 10.7809],
        [10.7808, 12.3026, 12.5891]], grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.07550449669361115 
model_pd.l_d.mean(): -3.0320231914520264 
model_pd.lagr.mean(): -2.9565186500549316 
model_pd.lambdas: dict_items([('pout', tensor([1.1196])), ('power', tensor([0.1507]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5269])), ('power', tensor([-16.1185]))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.07550449669361115
epoch£º87	 i:1 	 global-step:1741	 l-p:0.07099892944097519
epoch£º87	 i:2 	 global-step:1742	 l-p:0.06727834790945053
epoch£º87	 i:3 	 global-step:1743	 l-p:0.06967037171125412
epoch£º87	 i:4 	 global-step:1744	 l-p:0.069672591984272
epoch£º87	 i:5 	 global-step:1745	 l-p:0.06820941716432571
epoch£º87	 i:6 	 global-step:1746	 l-p:0.06720899790525436
epoch£º87	 i:7 	 global-step:1747	 l-p:0.07323229312896729
epoch£º87	 i:8 	 global-step:1748	 l-p:0.07289717346429825
epoch£º87	 i:9 	 global-step:1749	 l-p:0.06917747855186462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]])
 pt:tensor([[11.0497, 11.0497, 11.0497],
        [11.0497, 11.9802, 11.8932],
        [11.0497, 11.3872, 11.2156],
        [11.0497, 11.0685, 11.0512]], grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.06962671875953674 
model_pd.l_d.mean(): -3.0681262016296387 
model_pd.lagr.mean(): -2.9984993934631348 
model_pd.lambdas: dict_items([('pout', tensor([1.1137])), ('power', tensor([0.1426]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6453])), ('power', tensor([-16.3793]))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.06962671875953674
epoch£º88	 i:1 	 global-step:1761	 l-p:0.07041459530591965
epoch£º88	 i:2 	 global-step:1762	 l-p:0.06424303352832794
epoch£º88	 i:3 	 global-step:1763	 l-p:0.07335080206394196
epoch£º88	 i:4 	 global-step:1764	 l-p:0.07002865523099899
epoch£º88	 i:5 	 global-step:1765	 l-p:0.07053899019956589
epoch£º88	 i:6 	 global-step:1766	 l-p:0.06870300322771072
epoch£º88	 i:7 	 global-step:1767	 l-p:0.06840144842863083
epoch£º88	 i:8 	 global-step:1768	 l-p:0.0685880109667778
epoch£º88	 i:9 	 global-step:1769	 l-p:0.06648701429367065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]])
 pt:tensor([[11.3970, 14.3942, 16.0433],
        [11.3970, 11.4876, 11.4160],
        [11.3970, 11.5187, 11.4277],
        [11.3970, 11.9265, 11.7344]], grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.07415104657411575 
model_pd.l_d.mean(): -2.7265493869781494 
model_pd.lagr.mean(): -2.6523983478546143 
model_pd.lambdas: dict_items([('pout', tensor([1.1076])), ('power', tensor([0.1347]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5673])), ('power', tensor([-15.4854]))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.07415104657411575
epoch£º89	 i:1 	 global-step:1781	 l-p:0.06891844421625137
epoch£º89	 i:2 	 global-step:1782	 l-p:0.06586414575576782
epoch£º89	 i:3 	 global-step:1783	 l-p:0.07306674122810364
epoch£º89	 i:4 	 global-step:1784	 l-p:0.06652238219976425
epoch£º89	 i:5 	 global-step:1785	 l-p:0.06482692807912827
epoch£º89	 i:6 	 global-step:1786	 l-p:0.03881547227501869
epoch£º89	 i:7 	 global-step:1787	 l-p:0.06831660866737366
epoch£º89	 i:8 	 global-step:1788	 l-p:0.06864877045154572
epoch£º89	 i:9 	 global-step:1789	 l-p:0.0669412687420845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]])
 pt:tensor([[11.9418, 15.3614, 17.4093],
        [11.9418, 11.9418, 11.9418],
        [11.9418, 11.9420, 11.9418],
        [11.9418, 12.0819, 11.9791]], grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.06571565568447113 
model_pd.l_d.mean(): -2.6757137775421143 
model_pd.lagr.mean(): -2.6099982261657715 
model_pd.lambdas: dict_items([('pout', tensor([1.1009])), ('power', tensor([0.1270]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6861])), ('power', tensor([-15.0328]))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.06571565568447113
epoch£º90	 i:1 	 global-step:1801	 l-p:0.07059379667043686
epoch£º90	 i:2 	 global-step:1802	 l-p:0.06389547139406204
epoch£º90	 i:3 	 global-step:1803	 l-p:0.0690438523888588
epoch£º90	 i:4 	 global-step:1804	 l-p:0.06925036758184433
epoch£º90	 i:5 	 global-step:1805	 l-p:0.06767432391643524
epoch£º90	 i:6 	 global-step:1806	 l-p:0.0669805258512497
epoch£º90	 i:7 	 global-step:1807	 l-p:0.06722459197044373
epoch£º90	 i:8 	 global-step:1808	 l-p:0.06667698919773102
epoch£º90	 i:9 	 global-step:1809	 l-p:0.07579359412193298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]])
 pt:tensor([[12.5063, 12.5063, 12.5063],
        [12.5063, 12.5080, 12.5064],
        [12.5063, 12.5320, 12.5087],
        [12.5063, 15.1673, 16.2814]], grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.05950846150517464 
model_pd.l_d.mean(): -2.583043336868286 
model_pd.lagr.mean(): -2.5235347747802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0935])), ('power', tensor([0.1195]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7637])), ('power', tensor([-14.5336]))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.05950846150517464
epoch£º91	 i:1 	 global-step:1821	 l-p:0.06650242954492569
epoch£º91	 i:2 	 global-step:1822	 l-p:0.06828565895557404
epoch£º91	 i:3 	 global-step:1823	 l-p:0.06704328954219818
epoch£º91	 i:4 	 global-step:1824	 l-p:0.06602632254362106
epoch£º91	 i:5 	 global-step:1825	 l-p:0.07544109970331192
epoch£º91	 i:6 	 global-step:1826	 l-p:0.06742461025714874
epoch£º91	 i:7 	 global-step:1827	 l-p:0.06630588322877884
epoch£º91	 i:8 	 global-step:1828	 l-p:0.06565270572900772
epoch£º91	 i:9 	 global-step:1829	 l-p:0.06373140215873718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]])
 pt:tensor([[13.0072, 13.0072, 13.0072],
        [13.0072, 16.6235, 18.7080],
        [13.0072, 13.5923, 13.3694],
        [13.0072, 13.2530, 13.0952]], grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.06747207790613174 
model_pd.l_d.mean(): -2.420006275177002 
model_pd.lagr.mean(): -2.352534294128418 
model_pd.lambdas: dict_items([('pout', tensor([1.0857])), ('power', tensor([0.1123]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7573])), ('power', tensor([-14.1373]))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.06747207790613174
epoch£º92	 i:1 	 global-step:1841	 l-p:0.06502574682235718
epoch£º92	 i:2 	 global-step:1842	 l-p:0.0624057911336422
epoch£º92	 i:3 	 global-step:1843	 l-p:0.06170548498630524
epoch£º92	 i:4 	 global-step:1844	 l-p:0.06587272137403488
epoch£º92	 i:5 	 global-step:1845	 l-p:0.06280651688575745
epoch£º92	 i:6 	 global-step:1846	 l-p:0.06491150707006454
epoch£º92	 i:7 	 global-step:1847	 l-p:0.06458966434001923
epoch£º92	 i:8 	 global-step:1848	 l-p:0.07159987837076187
epoch£º92	 i:9 	 global-step:1849	 l-p:0.06601694971323013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]])
 pt:tensor([[13.4529, 13.4529, 13.4529],
        [13.4529, 14.8902, 14.9198],
        [13.4529, 15.0130, 15.1148],
        [13.4529, 14.5520, 14.4213]], grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.05998176708817482 
model_pd.l_d.mean(): -2.372687578201294 
model_pd.lagr.mean(): -2.3127057552337646 
model_pd.lambdas: dict_items([('pout', tensor([1.0773])), ('power', tensor([0.1053]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8487])), ('power', tensor([-13.7587]))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.05998176708817482
epoch£º93	 i:1 	 global-step:1861	 l-p:0.06492706388235092
epoch£º93	 i:2 	 global-step:1862	 l-p:0.06117058917880058
epoch£º93	 i:3 	 global-step:1863	 l-p:0.06622413545846939
epoch£º93	 i:4 	 global-step:1864	 l-p:0.06309649348258972
epoch£º93	 i:5 	 global-step:1865	 l-p:0.06441015005111694
epoch£º93	 i:6 	 global-step:1866	 l-p:0.06787972152233124
epoch£º93	 i:7 	 global-step:1867	 l-p:0.06297864764928818
epoch£º93	 i:8 	 global-step:1868	 l-p:0.06565383076667786
epoch£º93	 i:9 	 global-step:1869	 l-p:0.0660649985074997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]])
 pt:tensor([[13.9469, 16.0721, 16.5346],
        [13.9469, 13.9469, 13.9469],
        [13.9469, 14.1453, 14.0060],
        [13.9469, 14.0132, 13.9569]], grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.0640096440911293 
model_pd.l_d.mean(): -2.38559627532959 
model_pd.lagr.mean(): -2.3215866088867188 
model_pd.lambdas: dict_items([('pout', tensor([1.0684])), ('power', tensor([0.0984]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9617])), ('power', tensor([-13.6918]))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.0640096440911293
epoch£º94	 i:1 	 global-step:1881	 l-p:0.06239799037575722
epoch£º94	 i:2 	 global-step:1882	 l-p:0.06550229340791702
epoch£º94	 i:3 	 global-step:1883	 l-p:0.06510165333747864
epoch£º94	 i:4 	 global-step:1884	 l-p:0.057325173169374466
epoch£º94	 i:5 	 global-step:1885	 l-p:0.06442592293024063
epoch£º94	 i:6 	 global-step:1886	 l-p:0.0645078495144844
epoch£º94	 i:7 	 global-step:1887	 l-p:0.0639064759016037
epoch£º94	 i:8 	 global-step:1888	 l-p:0.05650751665234566
epoch£º94	 i:9 	 global-step:1889	 l-p:0.0663287341594696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]])
 pt:tensor([[14.5131, 14.5134, 14.5131],
        [14.5131, 14.5131, 14.5131],
        [14.5131, 14.5202, 14.5133],
        [14.5131, 14.5894, 14.5252]], grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.053908880800008774 
model_pd.l_d.mean(): -2.1385884284973145 
model_pd.lagr.mean(): -2.08467960357666 
model_pd.lambdas: dict_items([('pout', tensor([1.0592])), ('power', tensor([0.0919]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9137])), ('power', tensor([-12.6422]))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.053908880800008774
epoch£º95	 i:1 	 global-step:1901	 l-p:0.06400521099567413
epoch£º95	 i:2 	 global-step:1902	 l-p:0.06405756622552872
epoch£º95	 i:3 	 global-step:1903	 l-p:0.06090768426656723
epoch£º95	 i:4 	 global-step:1904	 l-p:0.058411821722984314
epoch£º95	 i:5 	 global-step:1905	 l-p:0.0631023496389389
epoch£º95	 i:6 	 global-step:1906	 l-p:0.0634499117732048
epoch£º95	 i:7 	 global-step:1907	 l-p:0.06416957825422287
epoch£º95	 i:8 	 global-step:1908	 l-p:0.06257878243923187
epoch£º95	 i:9 	 global-step:1909	 l-p:0.06269925832748413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]])
 pt:tensor([[15.2001, 19.9858, 23.0709],
        [15.2001, 15.2035, 15.2001],
        [15.2001, 18.4829, 19.8550],
        [15.2001, 15.2001, 15.2001]], grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.016139771789312363 
model_pd.l_d.mean(): -1.9365766048431396 
model_pd.lagr.mean(): -1.9204368591308594 
model_pd.lambdas: dict_items([('pout', tensor([1.0494])), ('power', tensor([0.0857]))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8836])), ('power', tensor([-11.6932]))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.016139771789312363
epoch£º96	 i:1 	 global-step:1921	 l-p:0.0645759329199791
epoch£º96	 i:2 	 global-step:1922	 l-p:0.06256747245788574
epoch£º96	 i:3 	 global-step:1923	 l-p:0.06306111067533493
epoch£º96	 i:4 	 global-step:1924	 l-p:0.062256522476673126
epoch£º96	 i:5 	 global-step:1925	 l-p:0.06061695143580437
epoch£º96	 i:6 	 global-step:1926	 l-p:0.06210361048579216
epoch£º96	 i:7 	 global-step:1927	 l-p:0.05247754603624344
epoch£º96	 i:8 	 global-step:1928	 l-p:0.06222493574023247
epoch£º96	 i:9 	 global-step:1929	 l-p:0.057878680527210236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]])
 pt:tensor([[16.2133, 18.4326, 18.7642],
        [16.2133, 19.9791, 21.7053],
        [16.2133, 16.2134, 16.2133],
        [16.2133, 21.7551, 25.5922]], grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.062095120549201965 
model_pd.l_d.mean(): -2.0862560272216797 
model_pd.lagr.mean(): -2.024160861968994 
model_pd.lambdas: dict_items([('pout', tensor([1.0386])), ('power', tensor([0.0798]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1241])), ('power', tensor([-11.4167]))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.062095120549201965
epoch£º97	 i:1 	 global-step:1941	 l-p:0.062063150107860565
epoch£º97	 i:2 	 global-step:1942	 l-p:0.06291279941797256
epoch£º97	 i:3 	 global-step:1943	 l-p:0.05750694125890732
epoch£º97	 i:4 	 global-step:1944	 l-p:0.06274818629026413
epoch£º97	 i:5 	 global-step:1945	 l-p:0.061478376388549805
epoch£º97	 i:6 	 global-step:1946	 l-p:0.06167910620570183
epoch£º97	 i:7 	 global-step:1947	 l-p:0.06133344769477844
epoch£º97	 i:8 	 global-step:1948	 l-p:-0.02297654002904892
epoch£º97	 i:9 	 global-step:1949	 l-p:0.06314877420663834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]])
 pt:tensor([[17.1347, 19.1303, 19.2439],
        [17.1347, 17.1347, 17.1347],
        [17.1347, 17.3818, 17.2080],
        [17.1347, 20.2518, 21.2229]], grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.06109750643372536 
model_pd.l_d.mean(): -2.008324146270752 
model_pd.lagr.mean(): -1.947226643562317 
model_pd.lambdas: dict_items([('pout', tensor([1.0272])), ('power', tensor([0.0744]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1891])), ('power', tensor([-10.4849]))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.06109750643372536
epoch£º98	 i:1 	 global-step:1961	 l-p:0.095911405980587
epoch£º98	 i:2 	 global-step:1962	 l-p:0.06544278562068939
epoch£º98	 i:3 	 global-step:1963	 l-p:0.05763334780931473
epoch£º98	 i:4 	 global-step:1964	 l-p:0.060304220765829086
epoch£º98	 i:5 	 global-step:1965	 l-p:0.06032223254442215
epoch£º98	 i:6 	 global-step:1966	 l-p:0.061362091451883316
epoch£º98	 i:7 	 global-step:1967	 l-p:0.060882873833179474
epoch£º98	 i:8 	 global-step:1968	 l-p:0.05087624117732048
epoch£º98	 i:9 	 global-step:1969	 l-p:0.05999120697379112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]])
 pt:tensor([[18.5081, 18.5081, 18.5081],
        [18.5081, 18.5089, 18.5081],
        [18.5081, 21.0684, 21.4529],
        [18.5081, 18.5085, 18.5081]], grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.05607330799102783 
model_pd.l_d.mean(): -1.9345500469207764 
model_pd.lagr.mean(): -1.8784767389297485 
model_pd.lambdas: dict_items([('pout', tensor([1.0149])), ('power', tensor([0.0696]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2858])), ('power', tensor([-8.9663]))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.05607330799102783
epoch£º99	 i:1 	 global-step:1981	 l-p:0.06005125865340233
epoch£º99	 i:2 	 global-step:1982	 l-p:0.06018650531768799
epoch£º99	 i:3 	 global-step:1983	 l-p:0.05981508642435074
epoch£º99	 i:4 	 global-step:1984	 l-p:0.06024330109357834
epoch£º99	 i:5 	 global-step:1985	 l-p:0.061834149062633514
epoch£º99	 i:6 	 global-step:1986	 l-p:0.031122636049985886
epoch£º99	 i:7 	 global-step:1987	 l-p:0.06336858868598938
epoch£º99	 i:8 	 global-step:1988	 l-p:0.05974399670958519
epoch£º99	 i:9 	 global-step:1989	 l-p:0.0597621351480484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]])
 pt:tensor([[19.6140, 19.6186, 19.6141],
        [19.6140, 22.5812, 23.1722],
        [19.6140, 20.2067, 19.8937],
        [19.6140, 19.6140, 19.6140]], grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.05915546789765358 
model_pd.l_d.mean(): -1.9552898406982422 
model_pd.lagr.mean(): -1.896134376525879 
model_pd.lambdas: dict_items([('pout', tensor([1.0017])), ('power', tensor([0.0653]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4089])), ('power', tensor([-8.2489]))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.05915546789765358
epoch£º100	 i:1 	 global-step:2001	 l-p:0.059270840138196945
epoch£º100	 i:2 	 global-step:2002	 l-p:0.06241034343838692
epoch£º100	 i:3 	 global-step:2003	 l-p:0.05969378352165222
epoch£º100	 i:4 	 global-step:2004	 l-p:0.05915079638361931
epoch£º100	 i:5 	 global-step:2005	 l-p:0.05976027995347977
epoch£º100	 i:6 	 global-step:2006	 l-p:0.05223849415779114
epoch£º100	 i:7 	 global-step:2007	 l-p:0.05891842767596245
epoch£º100	 i:8 	 global-step:2008	 l-p:0.06022337079048157
epoch£º100	 i:9 	 global-step:2009	 l-p:0.06915616244077682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]])
 pt:tensor([[20.5280, 20.9653, 20.6929],
        [20.5280, 20.5283, 20.5280],
        [20.5280, 20.5443, 20.5288],
        [20.5280, 20.5280, 20.5280]], grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.059241313487291336 
model_pd.l_d.mean(): -1.856447696685791 
model_pd.lagr.mean(): -1.7972064018249512 
model_pd.lambdas: dict_items([('pout', tensor([0.9879])), ('power', tensor([0.0616]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4264])), ('power', tensor([-7.1926]))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.059241313487291336
epoch£º101	 i:1 	 global-step:2021	 l-p:0.059048641473054886
epoch£º101	 i:2 	 global-step:2022	 l-p:0.060170602053403854
epoch£º101	 i:3 	 global-step:2023	 l-p:0.05865723267197609
epoch£º101	 i:4 	 global-step:2024	 l-p:0.04928926005959511
epoch£º101	 i:5 	 global-step:2025	 l-p:0.06518831104040146
epoch£º101	 i:6 	 global-step:2026	 l-p:0.05886746942996979
epoch£º101	 i:7 	 global-step:2027	 l-p:0.058710113167762756
epoch£º101	 i:8 	 global-step:2028	 l-p:0.058547358959913254
epoch£º101	 i:9 	 global-step:2029	 l-p:0.06017114594578743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]])
 pt:tensor([[21.2750, 25.6622, 27.3185],
        [21.2750, 25.6887, 27.3707],
        [21.2750, 28.3992, 33.1724],
        [21.2750, 22.4583, 22.0892]], grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.05862157791852951 
model_pd.l_d.mean(): -1.791573166847229 
model_pd.lagr.mean(): -1.7329516410827637 
model_pd.lambdas: dict_items([('pout', tensor([0.9736])), ('power', tensor([0.0582]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4495])), ('power', tensor([-6.4651]))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.05862157791852951
epoch£º102	 i:1 	 global-step:2041	 l-p:0.05866139382123947
epoch£º102	 i:2 	 global-step:2042	 l-p:0.04101977497339249
epoch£º102	 i:3 	 global-step:2043	 l-p:0.06056488677859306
epoch£º102	 i:4 	 global-step:2044	 l-p:0.05821998044848442
epoch£º102	 i:5 	 global-step:2045	 l-p:0.058258056640625
epoch£º102	 i:6 	 global-step:2046	 l-p:0.05806097015738487
epoch£º102	 i:7 	 global-step:2047	 l-p:0.058398693799972534
epoch£º102	 i:8 	 global-step:2048	 l-p:0.06280551850795746
epoch£º102	 i:9 	 global-step:2049	 l-p:0.058342304080724716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]])
 pt:tensor([[22.0417, 22.7975, 22.4261],
        [22.0417, 24.4958, 24.5531],
        [22.0417, 22.7486, 22.3863],
        [22.0417, 22.1316, 22.0537]], grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.05836823582649231 
model_pd.l_d.mean(): -1.7349311113357544 
model_pd.lagr.mean(): -1.6765629053115845 
model_pd.lambdas: dict_items([('pout', tensor([0.9588])), ('power', tensor([0.0552]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4801])), ('power', tensor([-5.6546]))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.05836823582649231
epoch£º103	 i:1 	 global-step:2061	 l-p:0.05803851783275604
epoch£º103	 i:2 	 global-step:2062	 l-p:0.1695484220981598
epoch£º103	 i:3 	 global-step:2063	 l-p:0.062273941934108734
epoch£º103	 i:4 	 global-step:2064	 l-p:0.05935452878475189
epoch£º103	 i:5 	 global-step:2065	 l-p:0.05786620453000069
epoch£º103	 i:6 	 global-step:2066	 l-p:0.05800444632768631
epoch£º103	 i:7 	 global-step:2067	 l-p:0.057815875858068466
epoch£º103	 i:8 	 global-step:2068	 l-p:0.05812307819724083
epoch£º103	 i:9 	 global-step:2069	 l-p:0.0578056201338768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]])
 pt:tensor([[23.0637, 23.0796, 23.0645],
        [23.0637, 24.1988, 23.7861],
        [23.0637, 23.5358, 23.2363],
        [23.0637, 29.1487, 32.3262]], grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.05759960040450096 
model_pd.l_d.mean(): -1.7896660566329956 
model_pd.lagr.mean(): -1.7320665121078491 
model_pd.lambdas: dict_items([('pout', tensor([0.9434])), ('power', tensor([0.0526]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6192])), ('power', tensor([-4.9116]))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.05759960040450096
epoch£º104	 i:1 	 global-step:2081	 l-p:0.05775744840502739
epoch£º104	 i:2 	 global-step:2082	 l-p:0.057502225041389465
epoch£º104	 i:3 	 global-step:2083	 l-p:0.06882931292057037
epoch£º104	 i:4 	 global-step:2084	 l-p:0.05870411545038223
epoch£º104	 i:5 	 global-step:2085	 l-p:0.057880714535713196
epoch£º104	 i:6 	 global-step:2086	 l-p:0.05745678022503853
epoch£º104	 i:7 	 global-step:2087	 l-p:0.058020636439323425
epoch£º104	 i:8 	 global-step:2088	 l-p:0.05738794431090355
epoch£º104	 i:9 	 global-step:2089	 l-p:0.05825653672218323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]])
 pt:tensor([[23.7831, 23.7831, 23.7831],
        [23.7831, 23.7831, 23.7831],
        [23.7831, 25.4361, 25.0776],
        [23.7831, 27.6702, 28.5932]], grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.05747700110077858 
model_pd.l_d.mean(): -1.7174973487854004 
model_pd.lagr.mean(): -1.660020351409912 
model_pd.lambdas: dict_items([('pout', tensor([0.9276])), ('power', tensor([0.0505]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6254])), ('power', tensor([-4.0901]))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.05747700110077858
epoch£º105	 i:1 	 global-step:2101	 l-p:0.05731264129281044
epoch£º105	 i:2 	 global-step:2102	 l-p:0.06331463158130646
epoch£º105	 i:3 	 global-step:2103	 l-p:0.05719529092311859
epoch£º105	 i:4 	 global-step:2104	 l-p:0.057648662477731705
epoch£º105	 i:5 	 global-step:2105	 l-p:0.05742645263671875
epoch£º105	 i:6 	 global-step:2106	 l-p:0.0573868602514267
epoch£º105	 i:7 	 global-step:2107	 l-p:0.05813722684979439
epoch£º105	 i:8 	 global-step:2108	 l-p:0.059241123497486115
epoch£º105	 i:9 	 global-step:2109	 l-p:0.05817326158285141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]])
 pt:tensor([[24.2798, 24.3072, 24.2815],
        [24.2798, 29.4049, 31.3893],
        [24.2798, 24.4502, 24.3114],
        [24.2798, 30.7249, 34.1044]], grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.05934586003422737 
model_pd.l_d.mean(): -1.5650333166122437 
model_pd.lagr.mean(): -1.5056874752044678 
model_pd.lambdas: dict_items([('pout', tensor([0.9115])), ('power', tensor([0.0486]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5442])), ('power', tensor([-3.1787]))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.05934586003422737
epoch£º106	 i:1 	 global-step:2121	 l-p:0.05716269090771675
epoch£º106	 i:2 	 global-step:2122	 l-p:0.057119108736515045
epoch£º106	 i:3 	 global-step:2123	 l-p:0.05712037533521652
epoch£º106	 i:4 	 global-step:2124	 l-p:0.05723121762275696
epoch£º106	 i:5 	 global-step:2125	 l-p:0.06208664923906326
epoch£º106	 i:6 	 global-step:2126	 l-p:0.05721282958984375
epoch£º106	 i:7 	 global-step:2127	 l-p:0.05709661543369293
epoch£º106	 i:8 	 global-step:2128	 l-p:0.05738048627972603
epoch£º106	 i:9 	 global-step:2129	 l-p:0.058294445276260376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]])
 pt:tensor([[24.6719, 24.6971, 24.6734],
        [24.6719, 24.7217, 24.6762],
        [24.6719, 31.9674, 36.2668],
        [24.6719, 30.7879, 33.7401]], grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.057850729674100876 
model_pd.l_d.mean(): -1.595657229423523 
model_pd.lagr.mean(): -1.537806510925293 
model_pd.lambdas: dict_items([('pout', tensor([0.8950])), ('power', tensor([0.0470]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6207])), ('power', tensor([-3.0226]))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.057850729674100876
epoch£º107	 i:1 	 global-step:2141	 l-p:0.05718051269650459
epoch£º107	 i:2 	 global-step:2142	 l-p:0.05736079812049866
epoch£º107	 i:3 	 global-step:2143	 l-p:0.057086292654275894
epoch£º107	 i:4 	 global-step:2144	 l-p:0.06067769601941109
epoch£º107	 i:5 	 global-step:2145	 l-p:0.05695243924856186
epoch£º107	 i:6 	 global-step:2146	 l-p:0.0569230355322361
epoch£º107	 i:7 	 global-step:2147	 l-p:0.05897209048271179
epoch£º107	 i:8 	 global-step:2148	 l-p:0.0578894279897213
epoch£º107	 i:9 	 global-step:2149	 l-p:0.05700637772679329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]])
 pt:tensor([[25.0202, 25.1233, 25.0340],
        [25.0202, 25.0213, 25.0202],
        [25.0202, 25.9001, 25.4726],
        [25.0202, 25.0204, 25.0202]], grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.060548678040504456 
model_pd.l_d.mean(): -1.5480546951293945 
model_pd.lagr.mean(): -1.4875060319900513 
model_pd.lambdas: dict_items([('pout', tensor([0.8785])), ('power', tensor([0.0455]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6257])), ('power', tensor([-2.5687]))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.060548678040504456
epoch£º108	 i:1 	 global-step:2161	 l-p:0.056880369782447815
epoch£º108	 i:2 	 global-step:2162	 l-p:0.05685547739267349
epoch£º108	 i:3 	 global-step:2163	 l-p:0.05862041935324669
epoch£º108	 i:4 	 global-step:2164	 l-p:0.05766870081424713
epoch£º108	 i:5 	 global-step:2165	 l-p:0.05719508230686188
epoch£º108	 i:6 	 global-step:2166	 l-p:0.05708553269505501
epoch£º108	 i:7 	 global-step:2167	 l-p:0.056806422770023346
epoch£º108	 i:8 	 global-step:2168	 l-p:0.0571829192340374
epoch£º108	 i:9 	 global-step:2169	 l-p:0.057474005967378616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]])
 pt:tensor([[25.3452, 26.1127, 25.7040],
        [25.3452, 25.3610, 25.3459],
        [25.3452, 34.3032, 40.5587],
        [25.3452, 25.3452, 25.3452]], grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.057044558227062225 
model_pd.l_d.mean(): -1.5613288879394531 
model_pd.lagr.mean(): -1.504284381866455 
model_pd.lambdas: dict_items([('pout', tensor([0.8616])), ('power', tensor([0.0442]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6809])), ('power', tensor([-2.4852]))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.057044558227062225
epoch£º109	 i:1 	 global-step:2181	 l-p:0.06028082221746445
epoch£º109	 i:2 	 global-step:2182	 l-p:0.056882552802562714
epoch£º109	 i:3 	 global-step:2183	 l-p:0.05676078423857689
epoch£º109	 i:4 	 global-step:2184	 l-p:0.05697713419795036
epoch£º109	 i:5 	 global-step:2185	 l-p:0.05861635133624077
epoch£º109	 i:6 	 global-step:2186	 l-p:0.05697859823703766
epoch£º109	 i:7 	 global-step:2187	 l-p:0.05754159018397331
epoch£º109	 i:8 	 global-step:2188	 l-p:0.05690334364771843
epoch£º109	 i:9 	 global-step:2189	 l-p:0.056756917387247086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]])
 pt:tensor([[25.6362, 25.6410, 25.6363],
        [25.6362, 25.6362, 25.6362],
        [25.6362, 25.6464, 25.6365],
        [25.6362, 25.8394, 25.6768]], grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.05667924880981445 
model_pd.l_d.mean(): -1.6010518074035645 
model_pd.lagr.mean(): -1.54437255859375 
model_pd.lambdas: dict_items([('pout', tensor([0.8446])), ('power', tensor([0.0431]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7705])), ('power', tensor([-2.3736]))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.05667924880981445
epoch£º110	 i:1 	 global-step:2201	 l-p:0.0567002110183239
epoch£º110	 i:2 	 global-step:2202	 l-p:0.058334365487098694
epoch£º110	 i:3 	 global-step:2203	 l-p:0.05678612366318703
epoch£º110	 i:4 	 global-step:2204	 l-p:0.05689743161201477
epoch£º110	 i:5 	 global-step:2205	 l-p:0.05762539803981781
epoch£º110	 i:6 	 global-step:2206	 l-p:0.05937197059392929
epoch£º110	 i:7 	 global-step:2207	 l-p:0.05736914277076721
epoch£º110	 i:8 	 global-step:2208	 l-p:0.05676303058862686
epoch£º110	 i:9 	 global-step:2209	 l-p:0.05688750371336937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]])
 pt:tensor([[25.8812, 25.9279, 25.8850],
        [25.8812, 31.3147, 33.3907],
        [25.8812, 30.1334, 31.1450],
        [25.8812, 32.6064, 36.0340]], grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.05683136358857155 
model_pd.l_d.mean(): -1.5007452964782715 
model_pd.lagr.mean(): -1.4439139366149902 
model_pd.lambdas: dict_items([('pout', tensor([0.8277])), ('power', tensor([0.0420]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7102])), ('power', tensor([-1.9540]))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.05683136358857155
epoch£º111	 i:1 	 global-step:2221	 l-p:0.05689564719796181
epoch£º111	 i:2 	 global-step:2222	 l-p:0.057352084666490555
epoch£º111	 i:3 	 global-step:2223	 l-p:0.057261571288108826
epoch£º111	 i:4 	 global-step:2224	 l-p:0.056651026010513306
epoch£º111	 i:5 	 global-step:2225	 l-p:0.05670119449496269
epoch£º111	 i:6 	 global-step:2226	 l-p:0.05799957737326622
epoch£º111	 i:7 	 global-step:2227	 l-p:0.05661739781498909
epoch£º111	 i:8 	 global-step:2228	 l-p:0.05680233612656593
epoch£º111	 i:9 	 global-step:2229	 l-p:0.059282220900058746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]])
 pt:tensor([[26.0830, 26.7061, 26.3331],
        [26.0830, 26.6878, 26.3212],
        [26.0830, 26.0832, 26.0830],
        [26.0830, 26.3080, 26.1303]], grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.05663008242845535 
model_pd.l_d.mean(): -1.5043529272079468 
model_pd.lagr.mean(): -1.4477227926254272 
model_pd.lambdas: dict_items([('pout', tensor([0.8104])), ('power', tensor([0.0411]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7554])), ('power', tensor([-1.9081]))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.05663008242845535
epoch£º112	 i:1 	 global-step:2241	 l-p:0.059008464217185974
epoch£º112	 i:2 	 global-step:2242	 l-p:0.05679481849074364
epoch£º112	 i:3 	 global-step:2243	 l-p:0.05665070563554764
epoch£º112	 i:4 	 global-step:2244	 l-p:0.057251084595918655
epoch£º112	 i:5 	 global-step:2245	 l-p:0.0572015754878521
epoch£º112	 i:6 	 global-step:2246	 l-p:0.05687957629561424
epoch£º112	 i:7 	 global-step:2247	 l-p:0.05808786302804947
epoch£º112	 i:8 	 global-step:2248	 l-p:0.056525010615587234
epoch£º112	 i:9 	 global-step:2249	 l-p:0.05669478327035904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]])
 pt:tensor([[26.2462, 26.2466, 26.2463],
        [26.2462, 31.8119, 33.9694],
        [26.2462, 26.8736, 26.4981],
        [26.2462, 26.2467, 26.2463]], grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.05716636776924133 
model_pd.l_d.mean(): -1.4211362600326538 
model_pd.lagr.mean(): -1.3639699220657349 
model_pd.lambdas: dict_items([('pout', tensor([0.7932])), ('power', tensor([0.0403]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7113])), ('power', tensor([-1.5049]))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.05716636776924133
epoch£º113	 i:1 	 global-step:2261	 l-p:0.05722654238343239
epoch£º113	 i:2 	 global-step:2262	 l-p:0.05657035484910011
epoch£º113	 i:3 	 global-step:2263	 l-p:0.05657758191227913
epoch£º113	 i:4 	 global-step:2264	 l-p:0.05685137212276459
epoch£º113	 i:5 	 global-step:2265	 l-p:0.056666940450668335
epoch£º113	 i:6 	 global-step:2266	 l-p:0.05667500197887421
epoch£º113	 i:7 	 global-step:2267	 l-p:0.060096047818660736
epoch£º113	 i:8 	 global-step:2268	 l-p:0.056816231459379196
epoch£º113	 i:9 	 global-step:2269	 l-p:0.05649059638381004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]])
 pt:tensor([[26.3490, 26.3491, 26.3490],
        [26.3490, 26.3668, 26.3498],
        [26.3490, 26.3585, 26.3493],
        [26.3490, 33.3756, 37.0632]], grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.05684458836913109 
model_pd.l_d.mean(): -1.3708486557006836 
model_pd.lagr.mean(): -1.3140040636062622 
model_pd.lambdas: dict_items([('pout', tensor([0.7759])), ('power', tensor([0.0396]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6919])), ('power', tensor([-1.3930]))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.05684458836913109
epoch£º114	 i:1 	 global-step:2281	 l-p:0.05654967203736305
epoch£º114	 i:2 	 global-step:2282	 l-p:0.056710947304964066
epoch£º114	 i:3 	 global-step:2283	 l-p:0.05948744714260101
epoch£º114	 i:4 	 global-step:2284	 l-p:0.056531429290771484
epoch£º114	 i:5 	 global-step:2285	 l-p:0.056491803377866745
epoch£º114	 i:6 	 global-step:2286	 l-p:0.05719047412276268
epoch£º114	 i:7 	 global-step:2287	 l-p:0.05659954249858856
epoch£º114	 i:8 	 global-step:2288	 l-p:0.057868730276823044
epoch£º114	 i:9 	 global-step:2289	 l-p:0.05655622109770775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[26.4042, 26.4042, 26.4042],
        [26.4042, 26.4044, 26.4042],
        [26.4042, 26.4043, 26.4042],
        [26.4042, 28.2500, 27.8493]], grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.05662102624773979 
model_pd.l_d.mean(): -1.3900530338287354 
model_pd.lagr.mean(): -1.3334319591522217 
model_pd.lambdas: dict_items([('pout', tensor([0.7585])), ('power', tensor([0.0388]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7520])), ('power', tensor([-1.4938]))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.05662102624773979
epoch£º115	 i:1 	 global-step:2301	 l-p:0.056722115725278854
epoch£º115	 i:2 	 global-step:2302	 l-p:0.05793433636426926
epoch£º115	 i:3 	 global-step:2303	 l-p:0.056596722453832626
epoch£º115	 i:4 	 global-step:2304	 l-p:0.058762453496456146
epoch£º115	 i:5 	 global-step:2305	 l-p:0.05700332298874855
epoch£º115	 i:6 	 global-step:2306	 l-p:0.0566546767950058
epoch£º115	 i:7 	 global-step:2307	 l-p:0.056548818945884705
epoch£º115	 i:8 	 global-step:2308	 l-p:0.0573650486767292
epoch£º115	 i:9 	 global-step:2309	 l-p:0.05649013817310333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]])
 pt:tensor([[26.4130, 27.2135, 26.7867],
        [26.4130, 26.7977, 26.5255],
        [26.4130, 26.4133, 26.4130],
        [26.4130, 28.3022, 27.9121]], grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.05684076249599457 
model_pd.l_d.mean(): -1.3256560564041138 
model_pd.lagr.mean(): -1.268815279006958 
model_pd.lambdas: dict_items([('pout', tensor([0.7412])), ('power', tensor([0.0381]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7129])), ('power', tensor([-1.3931]))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.05684076249599457
epoch£º116	 i:1 	 global-step:2321	 l-p:0.056549254804849625
epoch£º116	 i:2 	 global-step:2322	 l-p:0.05763261765241623
epoch£º116	 i:3 	 global-step:2323	 l-p:0.0572982132434845
epoch£º116	 i:4 	 global-step:2324	 l-p:0.05654327943921089
epoch£º116	 i:5 	 global-step:2325	 l-p:0.0565151609480381
epoch£º116	 i:6 	 global-step:2326	 l-p:0.05660003051161766
epoch£º116	 i:7 	 global-step:2327	 l-p:0.05660407617688179
epoch£º116	 i:8 	 global-step:2328	 l-p:0.059023451060056686
epoch£º116	 i:9 	 global-step:2329	 l-p:0.05714685842394829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]])
 pt:tensor([[26.3766, 26.3873, 26.3770],
        [26.3766, 26.9874, 26.6168],
        [26.3766, 26.3768, 26.3766],
        [26.3766, 26.3767, 26.3766]], grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.056504227221012115 
model_pd.l_d.mean(): -1.3666168451309204 
model_pd.lagr.mean(): -1.3101125955581665 
model_pd.lambdas: dict_items([('pout', tensor([0.7237])), ('power', tensor([0.0374]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7956])), ('power', tensor([-1.7048]))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.056504227221012115
epoch£º117	 i:1 	 global-step:2341	 l-p:0.056692689657211304
epoch£º117	 i:2 	 global-step:2342	 l-p:0.05658753961324692
epoch£º117	 i:3 	 global-step:2343	 l-p:0.05665315315127373
epoch£º117	 i:4 	 global-step:2344	 l-p:0.05696871131658554
epoch£º117	 i:5 	 global-step:2345	 l-p:0.056533604860305786
epoch£º117	 i:6 	 global-step:2346	 l-p:0.059160538017749786
epoch£º117	 i:7 	 global-step:2347	 l-p:0.05791345238685608
epoch£º117	 i:8 	 global-step:2348	 l-p:0.05734245851635933
epoch£º117	 i:9 	 global-step:2349	 l-p:0.05659409612417221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]])
 pt:tensor([[26.3183, 35.5836, 42.0212],
        [26.3183, 29.1537, 29.1538],
        [26.3183, 26.5456, 26.3662],
        [26.3183, 26.3183, 26.3183]], grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.05683867633342743 
model_pd.l_d.mean(): -1.2614375352859497 
model_pd.lagr.mean(): -1.2045989036560059 
model_pd.lambdas: dict_items([('pout', tensor([0.7064])), ('power', tensor([0.0367]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7050])), ('power', tensor([-1.4708]))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.05683867633342743
epoch£º118	 i:1 	 global-step:2361	 l-p:0.0566820465028286
epoch£º118	 i:2 	 global-step:2362	 l-p:0.05999504029750824
epoch£º118	 i:3 	 global-step:2363	 l-p:0.05655993893742561
epoch£º118	 i:4 	 global-step:2364	 l-p:0.05660174787044525
epoch£º118	 i:5 	 global-step:2365	 l-p:0.05704594776034355
epoch£º118	 i:6 	 global-step:2366	 l-p:0.056740716099739075
epoch£º118	 i:7 	 global-step:2367	 l-p:0.056723784655332565
epoch£º118	 i:8 	 global-step:2368	 l-p:0.05727379024028778
epoch£º118	 i:9 	 global-step:2369	 l-p:0.056697145104408264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]])
 pt:tensor([[26.2543, 28.0923, 27.6946],
        [26.2543, 26.3436, 26.2649],
        [26.2543, 30.7871, 31.9941],
        [26.2543, 31.7721, 33.8812]], grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.05652167275547981 
model_pd.l_d.mean(): -1.296571969985962 
model_pd.lagr.mean(): -1.2400503158569336 
model_pd.lambdas: dict_items([('pout', tensor([0.6890])), ('power', tensor([0.0359]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7863])), ('power', tensor([-1.7394]))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.05652167275547981
epoch£º119	 i:1 	 global-step:2381	 l-p:0.05673135817050934
epoch£º119	 i:2 	 global-step:2382	 l-p:0.05831639841198921
epoch£º119	 i:3 	 global-step:2383	 l-p:0.05669233202934265
epoch£º119	 i:4 	 global-step:2384	 l-p:0.056624069809913635
epoch£º119	 i:5 	 global-step:2385	 l-p:0.05699930712580681
epoch£º119	 i:6 	 global-step:2386	 l-p:0.05677446350455284
epoch£º119	 i:7 	 global-step:2387	 l-p:0.05652465671300888
epoch£º119	 i:8 	 global-step:2388	 l-p:0.0590604692697525
epoch£º119	 i:9 	 global-step:2389	 l-p:0.057239141315221786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]])
 pt:tensor([[26.1718, 26.1728, 26.1718],
        [26.1718, 26.1736, 26.1718],
        [26.1718, 26.1822, 26.1721],
        [26.1718, 29.7218, 30.1741]], grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.05665849149227142 
model_pd.l_d.mean(): -1.2337603569030762 
model_pd.lagr.mean(): -1.1771018505096436 
model_pd.lambdas: dict_items([('pout', tensor([0.6718])), ('power', tensor([0.0351]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7420])), ('power', tensor([-1.7191]))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.05665849149227142
epoch£º120	 i:1 	 global-step:2401	 l-p:0.059024691581726074
epoch£º120	 i:2 	 global-step:2402	 l-p:0.05661046877503395
epoch£º120	 i:3 	 global-step:2403	 l-p:0.05668263882398605
epoch£º120	 i:4 	 global-step:2404	 l-p:0.056675978004932404
epoch£º120	 i:5 	 global-step:2405	 l-p:0.05758602172136307
epoch£º120	 i:6 	 global-step:2406	 l-p:0.056773386895656586
epoch£º120	 i:7 	 global-step:2407	 l-p:0.0580860897898674
epoch£º120	 i:8 	 global-step:2408	 l-p:0.05653998255729675
epoch£º120	 i:9 	 global-step:2409	 l-p:0.05713194981217384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]])
 pt:tensor([[26.1001, 27.9240, 27.5281],
        [26.1001, 26.4025, 26.1767],
        [26.1001, 26.1002, 26.1001],
        [26.1001, 26.1068, 26.1003]], grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.05899190902709961 
model_pd.l_d.mean(): -1.203834891319275 
model_pd.lagr.mean(): -1.1448429822921753 
model_pd.lambdas: dict_items([('pout', tensor([0.6545])), ('power', tensor([0.0342]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7473])), ('power', tensor([-1.6637]))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.05899190902709961
epoch£º121	 i:1 	 global-step:2421	 l-p:0.05663255229592323
epoch£º121	 i:2 	 global-step:2422	 l-p:0.05670245736837387
epoch£º121	 i:3 	 global-step:2423	 l-p:0.05698489770293236
epoch£º121	 i:4 	 global-step:2424	 l-p:0.057142116129398346
epoch£º121	 i:5 	 global-step:2425	 l-p:0.05668618530035019
epoch£º121	 i:6 	 global-step:2426	 l-p:0.05781107023358345
epoch£º121	 i:7 	 global-step:2427	 l-p:0.05679311230778694
epoch£º121	 i:8 	 global-step:2428	 l-p:0.05738850682973862
epoch£º121	 i:9 	 global-step:2429	 l-p:0.05691147223114967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]])
 pt:tensor([[26.0354, 26.3219, 26.1056],
        [26.0354, 26.1432, 26.0498],
        [26.0354, 26.0355, 26.0354],
        [26.0354, 26.1117, 26.0437]], grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.05938299000263214 
model_pd.l_d.mean(): -1.1181045770645142 
model_pd.lagr.mean(): -1.0587215423583984 
model_pd.lambdas: dict_items([('pout', tensor([0.6374])), ('power', tensor([0.0334]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6701])), ('power', tensor([-1.5187]))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.05938299000263214
epoch£º122	 i:1 	 global-step:2441	 l-p:0.057550910860300064
epoch£º122	 i:2 	 global-step:2442	 l-p:0.05673808604478836
epoch£º122	 i:3 	 global-step:2443	 l-p:0.05667911469936371
epoch£º122	 i:4 	 global-step:2444	 l-p:0.05657459422945976
epoch£º122	 i:5 	 global-step:2445	 l-p:0.05669541284441948
epoch£º122	 i:6 	 global-step:2446	 l-p:0.05655791610479355
epoch£º122	 i:7 	 global-step:2447	 l-p:0.05703907087445259
epoch£º122	 i:8 	 global-step:2448	 l-p:0.05685364827513695
epoch£º122	 i:9 	 global-step:2449	 l-p:0.058216437697410583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]])
 pt:tensor([[25.9819, 25.9875, 25.9820],
        [25.9819, 26.5573, 26.2019],
        [25.9819, 26.0518, 25.9891],
        [25.9819, 25.9819, 25.9819]], grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.05666173994541168 
model_pd.l_d.mean(): -1.1519562005996704 
model_pd.lagr.mean(): -1.09529447555542 
model_pd.lambdas: dict_items([('pout', tensor([0.6201])), ('power', tensor([0.0324]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7501])), ('power', tensor([-1.9528]))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.05666173994541168
epoch£º123	 i:1 	 global-step:2461	 l-p:0.05664467439055443
epoch£º123	 i:2 	 global-step:2462	 l-p:0.05675864219665527
epoch£º123	 i:3 	 global-step:2463	 l-p:0.05661337822675705
epoch£º123	 i:4 	 global-step:2464	 l-p:0.05810070410370827
epoch£º123	 i:5 	 global-step:2465	 l-p:0.05673525482416153
epoch£º123	 i:6 	 global-step:2466	 l-p:0.05933175981044769
epoch£º123	 i:7 	 global-step:2467	 l-p:0.057073384523391724
epoch£º123	 i:8 	 global-step:2468	 l-p:0.0566798597574234
epoch£º123	 i:9 	 global-step:2469	 l-p:0.05793871730566025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]])
 pt:tensor([[25.9385, 25.9426, 25.9385],
        [25.9385, 31.9732, 34.6471],
        [25.9385, 26.3233, 26.0525],
        [25.9385, 26.4963, 26.1478]], grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.056889619678258896 
model_pd.l_d.mean(): -1.1040185689926147 
model_pd.lagr.mean(): -1.0471289157867432 
model_pd.lambdas: dict_items([('pout', tensor([0.6030])), ('power', tensor([0.0315]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7238])), ('power', tensor([-1.9464]))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.056889619678258896
epoch£º124	 i:1 	 global-step:2481	 l-p:0.05723632872104645
epoch£º124	 i:2 	 global-step:2482	 l-p:0.05680101364850998
epoch£º124	 i:3 	 global-step:2483	 l-p:0.05927329882979393
epoch£º124	 i:4 	 global-step:2484	 l-p:0.057446908205747604
epoch£º124	 i:5 	 global-step:2485	 l-p:0.05807778239250183
epoch£º124	 i:6 	 global-step:2486	 l-p:0.05661909282207489
epoch£º124	 i:7 	 global-step:2487	 l-p:0.05683503299951553
epoch£º124	 i:8 	 global-step:2488	 l-p:0.05671406909823418
epoch£º124	 i:9 	 global-step:2489	 l-p:0.05674799531698227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]])
 pt:tensor([[25.9263, 26.0737, 25.9503],
        [25.9263, 35.4875, 42.4109],
        [25.9263, 26.1113, 25.9609],
        [25.9263, 29.4577, 29.9165]], grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.0593133345246315 
model_pd.l_d.mean(): -1.048838496208191 
model_pd.lagr.mean(): -0.9895251393318176 
model_pd.lambdas: dict_items([('pout', tensor([0.5859])), ('power', tensor([0.0306]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6951])), ('power', tensor([-1.7218]))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.0593133345246315
epoch£º125	 i:1 	 global-step:2501	 l-p:0.05801219493150711
epoch£º125	 i:2 	 global-step:2502	 l-p:0.05741078779101372
epoch£º125	 i:3 	 global-step:2503	 l-p:0.05668843165040016
epoch£º125	 i:4 	 global-step:2504	 l-p:0.057459574192762375
epoch£º125	 i:5 	 global-step:2505	 l-p:0.05662531778216362
epoch£º125	 i:6 	 global-step:2506	 l-p:0.05693763121962547
epoch£º125	 i:7 	 global-step:2507	 l-p:0.056856442242860794
epoch£º125	 i:8 	 global-step:2508	 l-p:0.0567367747426033
epoch£º125	 i:9 	 global-step:2509	 l-p:0.05661667883396149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]])
 pt:tensor([[25.9276, 31.7384, 34.1831],
        [25.9276, 29.8491, 30.5933],
        [25.9276, 25.9277, 25.9276],
        [25.9276, 25.9931, 25.9341]], grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.05668706074357033 
model_pd.l_d.mean(): -1.07160222530365 
model_pd.lagr.mean(): -1.014915108680725 
model_pd.lambdas: dict_items([('pout', tensor([0.5687])), ('power', tensor([0.0296]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7701])), ('power', tensor([-2.0794]))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.05668706074357033
epoch£º126	 i:1 	 global-step:2521	 l-p:0.05925290659070015
epoch£º126	 i:2 	 global-step:2522	 l-p:0.05703537538647652
epoch£º126	 i:3 	 global-step:2523	 l-p:0.05717591196298599
epoch£º126	 i:4 	 global-step:2524	 l-p:0.05675369128584862
epoch£º126	 i:5 	 global-step:2525	 l-p:0.0567174069583416
epoch£º126	 i:6 	 global-step:2526	 l-p:0.05749301612377167
epoch£º126	 i:7 	 global-step:2527	 l-p:0.05673415958881378
epoch£º126	 i:8 	 global-step:2528	 l-p:0.056808024644851685
epoch£º126	 i:9 	 global-step:2529	 l-p:0.05799773335456848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]])
 pt:tensor([[25.9313, 27.1445, 26.6760],
        [25.9313, 26.7918, 26.3570],
        [25.9313, 27.8669, 27.5067],
        [25.9313, 32.5610, 35.8734]], grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.05734375864267349 
model_pd.l_d.mean(): -1.0035969018936157 
model_pd.lagr.mean(): -0.9462531208992004 
model_pd.lambdas: dict_items([('pout', tensor([0.5516])), ('power', tensor([0.0287]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7181])), ('power', tensor([-1.8391]))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.05734375864267349
epoch£º127	 i:1 	 global-step:2541	 l-p:0.056689515709877014
epoch£º127	 i:2 	 global-step:2542	 l-p:0.056930042803287506
epoch£º127	 i:3 	 global-step:2543	 l-p:0.05717268958687782
epoch£º127	 i:4 	 global-step:2544	 l-p:0.05943905934691429
epoch£º127	 i:5 	 global-step:2545	 l-p:0.05687442421913147
epoch£º127	 i:6 	 global-step:2546	 l-p:0.05661935359239578
epoch£º127	 i:7 	 global-step:2547	 l-p:0.05678350478410721
epoch£º127	 i:8 	 global-step:2548	 l-p:0.056769900023937225
epoch£º127	 i:9 	 global-step:2549	 l-p:0.057990044355392456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]])
 pt:tensor([[25.9500, 29.7427, 30.3875],
        [25.9500, 27.6048, 27.1770],
        [25.9500, 25.9810, 25.9520],
        [25.9500, 25.9577, 25.9502]], grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.05696143954992294 
model_pd.l_d.mean(): -0.9522109627723694 
model_pd.lagr.mean(): -0.8952495455741882 
model_pd.lambdas: dict_items([('pout', tensor([0.5345])), ('power', tensor([0.0278]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6810])), ('power', tensor([-1.8287]))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.05696143954992294
epoch£º128	 i:1 	 global-step:2561	 l-p:0.056901715695858
epoch£º128	 i:2 	 global-step:2562	 l-p:0.05675747990608215
epoch£º128	 i:3 	 global-step:2563	 l-p:0.05931513011455536
epoch£º128	 i:4 	 global-step:2564	 l-p:0.05660552904009819
epoch£º128	 i:5 	 global-step:2565	 l-p:0.05666403844952583
epoch£º128	 i:6 	 global-step:2566	 l-p:0.057422079145908356
epoch£º128	 i:7 	 global-step:2567	 l-p:0.05723727121949196
epoch£º128	 i:8 	 global-step:2568	 l-p:0.05801454558968544
epoch£º128	 i:9 	 global-step:2569	 l-p:0.05663450062274933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]])
 pt:tensor([[25.9788, 25.9892, 25.9792],
        [25.9788, 26.1853, 26.0201],
        [25.9788, 25.9790, 25.9788],
        [25.9788, 27.3273, 26.8612]], grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.05680772289633751 
model_pd.l_d.mean(): -0.9276746511459351 
model_pd.lagr.mean(): -0.8708669543266296 
model_pd.lambdas: dict_items([('pout', tensor([0.5173])), ('power', tensor([0.0268]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6927])), ('power', tensor([-1.8254]))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.05680772289633751
epoch£º129	 i:1 	 global-step:2581	 l-p:0.058408793061971664
epoch£º129	 i:2 	 global-step:2582	 l-p:0.05727825313806534
epoch£º129	 i:3 	 global-step:2583	 l-p:0.05668345466256142
epoch£º129	 i:4 	 global-step:2584	 l-p:0.05707283318042755
epoch£º129	 i:5 	 global-step:2585	 l-p:0.05669701471924782
epoch£º129	 i:6 	 global-step:2586	 l-p:0.05951735004782677
epoch£º129	 i:7 	 global-step:2587	 l-p:0.05658788979053497
epoch£º129	 i:8 	 global-step:2588	 l-p:0.05667593330144882
epoch£º129	 i:9 	 global-step:2589	 l-p:0.05664757639169693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]])
 pt:tensor([[26.0145, 27.4194, 26.9563],
        [26.0145, 26.0148, 26.0145],
        [26.0145, 26.0147, 26.0145],
        [26.0145, 26.0277, 26.0150]], grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.05931714177131653 
model_pd.l_d.mean(): -0.8762557506561279 
model_pd.lagr.mean(): -0.8169386386871338 
model_pd.lambdas: dict_items([('pout', tensor([0.5002])), ('power', tensor([0.0259]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6630])), ('power', tensor([-1.6027]))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.05931714177131653
epoch£º130	 i:1 	 global-step:2601	 l-p:0.05793604999780655
epoch£º130	 i:2 	 global-step:2602	 l-p:0.056666191667318344
epoch£º130	 i:3 	 global-step:2603	 l-p:0.05660069361329079
epoch£º130	 i:4 	 global-step:2604	 l-p:0.05764260143041611
epoch£º130	 i:5 	 global-step:2605	 l-p:0.0568666011095047
epoch£º130	 i:6 	 global-step:2606	 l-p:0.05740075558423996
epoch£º130	 i:7 	 global-step:2607	 l-p:0.05661439523100853
epoch£º130	 i:8 	 global-step:2608	 l-p:0.05658401548862457
epoch£º130	 i:9 	 global-step:2609	 l-p:0.05660048872232437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]])
 pt:tensor([[26.0527, 26.0527, 26.0527],
        [26.0527, 26.8581, 26.4335],
        [26.0527, 26.0540, 26.0527],
        [26.0527, 26.0528, 26.0527]], grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.05697638541460037 
model_pd.l_d.mean(): -0.8675575852394104 
model_pd.lagr.mean(): -0.8105812072753906 
model_pd.lambdas: dict_items([('pout', tensor([0.4829])), ('power', tensor([0.0250]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7011])), ('power', tensor([-1.7174]))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.05697638541460037
epoch£º131	 i:1 	 global-step:2621	 l-p:0.056705232709646225
epoch£º131	 i:2 	 global-step:2622	 l-p:0.05672204867005348
epoch£º131	 i:3 	 global-step:2623	 l-p:0.05712790787220001
epoch£º131	 i:4 	 global-step:2624	 l-p:0.05734676495194435
epoch£º131	 i:5 	 global-step:2625	 l-p:0.05916319787502289
epoch£º131	 i:6 	 global-step:2626	 l-p:0.0566997192800045
epoch£º131	 i:7 	 global-step:2627	 l-p:0.05665627494454384
epoch£º131	 i:8 	 global-step:2628	 l-p:0.057928409427404404
epoch£º131	 i:9 	 global-step:2629	 l-p:0.056768111884593964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]])
 pt:tensor([[26.0760, 26.0764, 26.0760],
        [26.0760, 34.4869, 39.8765],
        [26.0760, 26.0760, 26.0760],
        [26.0760, 29.8371, 30.4474]], grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.05736134946346283 
model_pd.l_d.mean(): -0.8165851831436157 
model_pd.lagr.mean(): -0.7592238187789917 
model_pd.lambdas: dict_items([('pout', tensor([0.4658])), ('power', tensor([0.0241]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6640])), ('power', tensor([-1.6010]))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.05736134946346283
epoch£º132	 i:1 	 global-step:2641	 l-p:0.059140563011169434
epoch£º132	 i:2 	 global-step:2642	 l-p:0.05663485825061798
epoch£º132	 i:3 	 global-step:2643	 l-p:0.05651358515024185
epoch£º132	 i:4 	 global-step:2644	 l-p:0.05671431124210358
epoch£º132	 i:5 	 global-step:2645	 l-p:0.057959459722042084
epoch£º132	 i:6 	 global-step:2646	 l-p:0.056695304811000824
epoch£º132	 i:7 	 global-step:2647	 l-p:0.056684233248233795
epoch£º132	 i:8 	 global-step:2648	 l-p:0.057530272752046585
epoch£º132	 i:9 	 global-step:2649	 l-p:0.05675344914197922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]])
 pt:tensor([[26.1031, 26.1031, 26.1031],
        [26.1031, 26.1031, 26.1031],
        [26.1031, 28.9262, 28.9322],
        [26.1031, 27.2523, 26.7820]], grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.058464232832193375 
model_pd.l_d.mean(): -0.789786696434021 
model_pd.lagr.mean(): -0.7313224673271179 
model_pd.lambdas: dict_items([('pout', tensor([0.4485])), ('power', tensor([0.0233]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6807])), ('power', tensor([-1.4174]))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.058464232832193375
epoch£º133	 i:1 	 global-step:2661	 l-p:0.05690814182162285
epoch£º133	 i:2 	 global-step:2662	 l-p:0.05658658966422081
epoch£º133	 i:3 	 global-step:2663	 l-p:0.0565730519592762
epoch£º133	 i:4 	 global-step:2664	 l-p:0.05919407680630684
epoch£º133	 i:5 	 global-step:2665	 l-p:0.05711023882031441
epoch£º133	 i:6 	 global-step:2666	 l-p:0.056778859347105026
epoch£º133	 i:7 	 global-step:2667	 l-p:0.056711629033088684
epoch£º133	 i:8 	 global-step:2668	 l-p:0.05670160800218582
epoch£º133	 i:9 	 global-step:2669	 l-p:0.056861378252506256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]])
 pt:tensor([[26.1221, 28.8422, 28.7927],
        [26.1221, 26.9716, 26.5369],
        [26.1221, 26.3087, 26.1570],
        [26.1221, 26.1534, 26.1241]], grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.05671287700533867 
model_pd.l_d.mean(): -0.7962055206298828 
model_pd.lagr.mean(): -0.739492654800415 
model_pd.lambdas: dict_items([('pout', tensor([0.4312])), ('power', tensor([0.0224]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7442])), ('power', tensor([-1.8216]))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.05671287700533867
epoch£º134	 i:1 	 global-step:2681	 l-p:0.060358427464962006
epoch£º134	 i:2 	 global-step:2682	 l-p:0.05672039836645126
epoch£º134	 i:3 	 global-step:2683	 l-p:0.05724741145968437
epoch£º134	 i:4 	 global-step:2684	 l-p:0.05657929554581642
epoch£º134	 i:5 	 global-step:2685	 l-p:0.056656818836927414
epoch£º134	 i:6 	 global-step:2686	 l-p:0.057604823261499405
epoch£º134	 i:7 	 global-step:2687	 l-p:0.056597813963890076
epoch£º134	 i:8 	 global-step:2688	 l-p:0.05670922249555588
epoch£º134	 i:9 	 global-step:2689	 l-p:0.05663631856441498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]])
 pt:tensor([[26.1341, 26.1341, 26.1341],
        [26.1341, 34.7557, 40.3992],
        [26.1341, 27.4335, 26.9619],
        [26.1341, 33.4783, 37.5654]], grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.05799099802970886 
model_pd.l_d.mean(): -0.7165125012397766 
model_pd.lagr.mean(): -0.6585215330123901 
model_pd.lambdas: dict_items([('pout', tensor([0.4141])), ('power', tensor([0.0216]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6494])), ('power', tensor([-1.4204]))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.05799099802970886
epoch£º135	 i:1 	 global-step:2701	 l-p:0.056891392916440964
epoch£º135	 i:2 	 global-step:2702	 l-p:0.05674732103943825
epoch£º135	 i:3 	 global-step:2703	 l-p:0.05655413493514061
epoch£º135	 i:4 	 global-step:2704	 l-p:0.056827202439308167
epoch£º135	 i:5 	 global-step:2705	 l-p:0.05668087303638458
epoch£º135	 i:6 	 global-step:2706	 l-p:0.0566251166164875
epoch£º135	 i:7 	 global-step:2707	 l-p:0.057346686720848083
epoch£º135	 i:8 	 global-step:2708	 l-p:0.057187724858522415
epoch£º135	 i:9 	 global-step:2709	 l-p:0.058961495757102966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]])
 pt:tensor([[26.1256, 26.1268, 26.1256],
        [26.1256, 27.6095, 27.1513],
        [26.1256, 26.1350, 26.1259],
        [26.1256, 27.8743, 27.4598]], grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.05651983246207237 
model_pd.l_d.mean(): -0.7593973875045776 
model_pd.lagr.mean(): -0.7028775811195374 
model_pd.lambdas: dict_items([('pout', tensor([0.3967])), ('power', tensor([0.0207]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8039])), ('power', tensor([-1.9471]))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.05651983246207237
epoch£º136	 i:1 	 global-step:2721	 l-p:0.056715380400419235
epoch£º136	 i:2 	 global-step:2722	 l-p:0.05742141231894493
epoch£º136	 i:3 	 global-step:2723	 l-p:0.05684661492705345
epoch£º136	 i:4 	 global-step:2724	 l-p:0.05663922801613808
epoch£º136	 i:5 	 global-step:2725	 l-p:0.0581120029091835
epoch£º136	 i:6 	 global-step:2726	 l-p:0.056644946336746216
epoch£º136	 i:7 	 global-step:2727	 l-p:0.05903726443648338
epoch£º136	 i:8 	 global-step:2728	 l-p:0.056539248675107956
epoch£º136	 i:9 	 global-step:2729	 l-p:0.057377442717552185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]])
 pt:tensor([[26.1175, 26.1175, 26.1175],
        [26.1175, 26.3196, 26.1572],
        [26.1175, 26.1175, 26.1175],
        [26.1175, 34.0399, 38.8172]], grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.056587353348731995 
model_pd.l_d.mean(): -0.7116830945014954 
model_pd.lagr.mean(): -0.6550957560539246 
model_pd.lambdas: dict_items([('pout', tensor([0.3795])), ('power', tensor([0.0199]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7685])), ('power', tensor([-1.8741]))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.056587353348731995
epoch£º137	 i:1 	 global-step:2741	 l-p:0.056673113256692886
epoch£º137	 i:2 	 global-step:2742	 l-p:0.05672625079751015
epoch£º137	 i:3 	 global-step:2743	 l-p:0.058135222643613815
epoch£º137	 i:4 	 global-step:2744	 l-p:0.059079866856336594
epoch£º137	 i:5 	 global-step:2745	 l-p:0.057306572794914246
epoch£º137	 i:6 	 global-step:2746	 l-p:0.056767310947179794
epoch£º137	 i:7 	 global-step:2747	 l-p:0.05713922902941704
epoch£º137	 i:8 	 global-step:2748	 l-p:0.05678265914320946
epoch£º137	 i:9 	 global-step:2749	 l-p:0.05668512359261513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]])
 pt:tensor([[26.1102, 26.9020, 26.4799],
        [26.1102, 26.1102, 26.1102],
        [26.1102, 26.1102, 26.1102],
        [26.1102, 26.1102, 26.1102]], grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.056599508970975876 
model_pd.l_d.mean(): -0.6823028922080994 
model_pd.lagr.mean(): -0.6257033944129944 
model_pd.lambdas: dict_items([('pout', tensor([0.3622])), ('power', tensor([0.0190]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7758])), ('power', tensor([-1.8767]))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.056599508970975876
epoch£º138	 i:1 	 global-step:2761	 l-p:0.05681866779923439
epoch£º138	 i:2 	 global-step:2762	 l-p:0.05848574638366699
epoch£º138	 i:3 	 global-step:2763	 l-p:0.0566788911819458
epoch£º138	 i:4 	 global-step:2764	 l-p:0.05907926708459854
epoch£º138	 i:5 	 global-step:2765	 l-p:0.05749058723449707
epoch£º138	 i:6 	 global-step:2766	 l-p:0.05654742196202278
epoch£º138	 i:7 	 global-step:2767	 l-p:0.05680927261710167
epoch£º138	 i:8 	 global-step:2768	 l-p:0.05662768706679344
epoch£º138	 i:9 	 global-step:2769	 l-p:0.056783467531204224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]])
 pt:tensor([[26.0970, 34.0131, 38.7868],
        [26.0970, 35.2850, 41.6699],
        [26.0970, 26.0970, 26.0970],
        [26.0970, 26.1257, 26.0987]], grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.05922909826040268 
model_pd.l_d.mean(): -0.6050264239311218 
model_pd.lagr.mean(): -0.5457973480224609 
model_pd.lambdas: dict_items([('pout', tensor([0.3451])), ('power', tensor([0.0182]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6670])), ('power', tensor([-1.4750]))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.05922909826040268
epoch£º139	 i:1 	 global-step:2781	 l-p:0.05653124302625656
epoch£º139	 i:2 	 global-step:2782	 l-p:0.05660809949040413
epoch£º139	 i:3 	 global-step:2783	 l-p:0.0570940226316452
epoch£º139	 i:4 	 global-step:2784	 l-p:0.05682443082332611
epoch£º139	 i:5 	 global-step:2785	 l-p:0.05686217173933983
epoch£º139	 i:6 	 global-step:2786	 l-p:0.056804925203323364
epoch£º139	 i:7 	 global-step:2787	 l-p:0.05796939879655838
epoch£º139	 i:8 	 global-step:2788	 l-p:0.05734389275312424
epoch£º139	 i:9 	 global-step:2789	 l-p:0.056708551943302155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]])
 pt:tensor([[26.0783, 34.3841, 39.6411],
        [26.0783, 26.4971, 26.2087],
        [26.0783, 31.3355, 33.2147],
        [26.0783, 34.5153, 39.9369]], grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.05676892772316933 
model_pd.l_d.mean(): -0.5972753167152405 
model_pd.lagr.mean(): -0.5405063629150391 
model_pd.lambdas: dict_items([('pout', tensor([0.3279])), ('power', tensor([0.0173]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7201])), ('power', tensor([-1.7461]))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.05676892772316933
epoch£º140	 i:1 	 global-step:2801	 l-p:0.05668892711400986
epoch£º140	 i:2 	 global-step:2802	 l-p:0.0578642375767231
epoch£º140	 i:3 	 global-step:2803	 l-p:0.05663379281759262
epoch£º140	 i:4 	 global-step:2804	 l-p:0.056571267545223236
epoch£º140	 i:5 	 global-step:2805	 l-p:0.057094551622867584
epoch£º140	 i:6 	 global-step:2806	 l-p:0.057351063936948776
epoch£º140	 i:7 	 global-step:2807	 l-p:0.05914762616157532
epoch£º140	 i:8 	 global-step:2808	 l-p:0.056764692068099976
epoch£º140	 i:9 	 global-step:2809	 l-p:0.057197876274585724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]])
 pt:tensor([[26.0531, 26.0548, 26.0532],
        [26.0531, 26.0547, 26.0532],
        [26.0531, 26.0532, 26.0531],
        [26.0531, 27.4015, 26.9336]], grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.05677754804491997 
model_pd.l_d.mean(): -0.5698482990264893 
model_pd.lagr.mean(): -0.5130707621574402 
model_pd.lambdas: dict_items([('pout', tensor([0.3106])), ('power', tensor([0.0164]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7280])), ('power', tensor([-1.8222]))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.05677754804491997
epoch£º141	 i:1 	 global-step:2821	 l-p:0.056768979877233505
epoch£º141	 i:2 	 global-step:2822	 l-p:0.056608665734529495
epoch£º141	 i:3 	 global-step:2823	 l-p:0.05676528066396713
epoch£º141	 i:4 	 global-step:2824	 l-p:0.05662059783935547
epoch£º141	 i:5 	 global-step:2825	 l-p:0.057117000222206116
epoch£º141	 i:6 	 global-step:2826	 l-p:0.05690306797623634
epoch£º141	 i:7 	 global-step:2827	 l-p:0.05689529329538345
epoch£º141	 i:8 	 global-step:2828	 l-p:0.05659620463848114
epoch£º141	 i:9 	 global-step:2829	 l-p:0.06114077568054199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]])
 pt:tensor([[26.0273, 27.1730, 26.7041],
        [26.0273, 27.8559, 27.4632],
        [26.0273, 27.6899, 27.2610],
        [26.0273, 26.0349, 26.0275]], grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.056763529777526855 
model_pd.l_d.mean(): -0.5397543907165527 
model_pd.lagr.mean(): -0.4829908609390259 
model_pd.lambdas: dict_items([('pout', tensor([0.2934])), ('power', tensor([0.0155]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7302])), ('power', tensor([-1.8602]))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.056763529777526855
epoch£º142	 i:1 	 global-step:2841	 l-p:0.05782166123390198
epoch£º142	 i:2 	 global-step:2842	 l-p:0.05670541897416115
epoch£º142	 i:3 	 global-step:2843	 l-p:0.05666695535182953
epoch£º142	 i:4 	 global-step:2844	 l-p:0.05761589854955673
epoch£º142	 i:5 	 global-step:2845	 l-p:0.056739937514066696
epoch£º142	 i:6 	 global-step:2846	 l-p:0.05933085456490517
epoch£º142	 i:7 	 global-step:2847	 l-p:0.05720818042755127
epoch£º142	 i:8 	 global-step:2848	 l-p:0.056767720729112625
epoch£º142	 i:9 	 global-step:2849	 l-p:0.05663264915347099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]])
 pt:tensor([[26.0208, 26.0208, 26.0208],
        [26.0208, 26.0208, 26.0208],
        [26.0208, 26.0209, 26.0208],
        [26.0208, 32.0884, 34.7833]], grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.05803995206952095 
model_pd.l_d.mean(): -0.48796701431274414 
model_pd.lagr.mean(): -0.4299270510673523 
model_pd.lambdas: dict_items([('pout', tensor([0.2763])), ('power', tensor([0.0146]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6696])), ('power', tensor([-1.6205]))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.05803995206952095
epoch£º143	 i:1 	 global-step:2861	 l-p:0.05676116421818733
epoch£º143	 i:2 	 global-step:2862	 l-p:0.056653328239917755
epoch£º143	 i:3 	 global-step:2863	 l-p:0.05657931789755821
epoch£º143	 i:4 	 global-step:2864	 l-p:0.05695560574531555
epoch£º143	 i:5 	 global-step:2865	 l-p:0.056912876665592194
epoch£º143	 i:6 	 global-step:2866	 l-p:0.05667971074581146
epoch£º143	 i:7 	 global-step:2867	 l-p:0.05655204877257347
epoch£º143	 i:8 	 global-step:2868	 l-p:0.05803750827908516
epoch£º143	 i:9 	 global-step:2869	 l-p:0.059119075536727905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]])
 pt:tensor([[26.0082, 35.5730, 42.4806],
        [26.0082, 31.1279, 32.8859],
        [26.0082, 27.1189, 26.6520],
        [26.0082, 26.0889, 26.0173]], grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.057862550020217896 
model_pd.l_d.mean(): -0.4716567099094391 
model_pd.lagr.mean(): -0.4137941598892212 
model_pd.lambdas: dict_items([('pout', tensor([0.2591])), ('power', tensor([0.0137]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7135])), ('power', tensor([-1.7926]))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.057862550020217896
epoch£º144	 i:1 	 global-step:2881	 l-p:0.056606225669384
epoch£º144	 i:2 	 global-step:2882	 l-p:0.05922392010688782
epoch£º144	 i:3 	 global-step:2883	 l-p:0.05670817196369171
epoch£º144	 i:4 	 global-step:2884	 l-p:0.05666196718811989
epoch£º144	 i:5 	 global-step:2885	 l-p:0.05661166086792946
epoch£º144	 i:6 	 global-step:2886	 l-p:0.056742459535598755
epoch£º144	 i:7 	 global-step:2887	 l-p:0.05820458009839058
epoch£º144	 i:8 	 global-step:2888	 l-p:0.05676422268152237
epoch£º144	 i:9 	 global-step:2889	 l-p:0.05692470446228981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]])
 pt:tensor([[26.0107, 31.9181, 34.4487],
        [26.0107, 32.5707, 35.7930],
        [26.0107, 26.1264, 26.0268],
        [26.0107, 34.4015, 39.7786]], grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.05664776265621185 
model_pd.l_d.mean(): -0.4511224627494812 
model_pd.lagr.mean(): -0.39447468519210815 
model_pd.lambdas: dict_items([('pout', tensor([0.2419])), ('power', tensor([0.0128]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7500])), ('power', tensor([-1.9196]))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.05664776265621185
epoch£º145	 i:1 	 global-step:2901	 l-p:0.059289876371622086
epoch£º145	 i:2 	 global-step:2902	 l-p:0.057135358452796936
epoch£º145	 i:3 	 global-step:2903	 l-p:0.057627514004707336
epoch£º145	 i:4 	 global-step:2904	 l-p:0.05681351572275162
epoch£º145	 i:5 	 global-step:2905	 l-p:0.05672801658511162
epoch£º145	 i:6 	 global-step:2906	 l-p:0.056815918534994125
epoch£º145	 i:7 	 global-step:2907	 l-p:0.058050110936164856
epoch£º145	 i:8 	 global-step:2908	 l-p:0.05662567913532257
epoch£º145	 i:9 	 global-step:2909	 l-p:0.05656380578875542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]])
 pt:tensor([[26.0150, 26.0150, 26.0150],
        [26.0150, 26.4236, 26.1404],
        [26.0150, 28.5833, 28.4641],
        [26.0150, 26.7426, 26.3377]], grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.05720577389001846 
model_pd.l_d.mean(): -0.4076468050479889 
model_pd.lagr.mean(): -0.350441038608551 
model_pd.lambdas: dict_items([('pout', tensor([0.2247])), ('power', tensor([0.0119]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7071])), ('power', tensor([-1.7573]))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.05720577389001846
epoch£º146	 i:1 	 global-step:2921	 l-p:0.05676804855465889
epoch£º146	 i:2 	 global-step:2922	 l-p:0.056903451681137085
epoch£º146	 i:3 	 global-step:2923	 l-p:0.05670808255672455
epoch£º146	 i:4 	 global-step:2924	 l-p:0.05787067487835884
epoch£º146	 i:5 	 global-step:2925	 l-p:0.059396084398031235
epoch£º146	 i:6 	 global-step:2926	 l-p:0.05658509582281113
epoch£º146	 i:7 	 global-step:2927	 l-p:0.05740048736333847
epoch£º146	 i:8 	 global-step:2928	 l-p:0.0566687285900116
epoch£º146	 i:9 	 global-step:2929	 l-p:0.05678565055131912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]])
 pt:tensor([[26.0170, 27.9037, 27.5263],
        [26.0170, 26.0182, 26.0170],
        [26.0170, 26.5941, 26.2378],
        [26.0170, 28.6377, 28.5438]], grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.056598469614982605 
model_pd.l_d.mean(): -0.39436766505241394 
model_pd.lagr.mean(): -0.33776921033859253 
model_pd.lambdas: dict_items([('pout', tensor([0.2075])), ('power', tensor([0.0110]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7784])), ('power', tensor([-2.0020]))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.056598469614982605
epoch£º147	 i:1 	 global-step:2941	 l-p:0.059361428022384644
epoch£º147	 i:2 	 global-step:2942	 l-p:0.056768447160720825
epoch£º147	 i:3 	 global-step:2943	 l-p:0.05667592212557793
epoch£º147	 i:4 	 global-step:2944	 l-p:0.05706853047013283
epoch£º147	 i:5 	 global-step:2945	 l-p:0.05692269280552864
epoch£º147	 i:6 	 global-step:2946	 l-p:0.056907471269369125
epoch£º147	 i:7 	 global-step:2947	 l-p:0.05662138760089874
epoch£º147	 i:8 	 global-step:2948	 l-p:0.05664558336138725
epoch£º147	 i:9 	 global-step:2949	 l-p:0.05869497358798981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]])
 pt:tensor([[26.0247, 26.2964, 26.0890],
        [26.0247, 26.1558, 26.0444],
        [26.0247, 34.6102, 40.2305],
        [26.0247, 26.0247, 26.0247]], grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.05666588246822357 
model_pd.l_d.mean(): -0.35709911584854126 
model_pd.lagr.mean(): -0.3004332184791565 
model_pd.lambdas: dict_items([('pout', tensor([0.1903])), ('power', tensor([0.0101]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7561])), ('power', tensor([-1.9428]))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.05666588246822357
epoch£º148	 i:1 	 global-step:2961	 l-p:0.056744400411844254
epoch£º148	 i:2 	 global-step:2962	 l-p:0.05665384605526924
epoch£º148	 i:3 	 global-step:2963	 l-p:0.0569634772837162
epoch£º148	 i:4 	 global-step:2964	 l-p:0.056655578315258026
epoch£º148	 i:5 	 global-step:2965	 l-p:0.05709374323487282
epoch£º148	 i:6 	 global-step:2966	 l-p:0.0590849407017231
epoch£º148	 i:7 	 global-step:2967	 l-p:0.0577690452337265
epoch£º148	 i:8 	 global-step:2968	 l-p:0.056662820279598236
epoch£º148	 i:9 	 global-step:2969	 l-p:0.057943787425756454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]])
 pt:tensor([[26.0333, 26.8159, 26.3967],
        [26.0333, 26.4340, 26.1547],
        [26.0333, 34.5226, 40.0190],
        [26.0333, 30.3199, 31.3426]], grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.0575358010828495 
model_pd.l_d.mean(): -0.3014287054538727 
model_pd.lagr.mean(): -0.24389290809631348 
model_pd.lambdas: dict_items([('pout', tensor([0.1732])), ('power', tensor([0.0092]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6427])), ('power', tensor([-1.5225]))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.0575358010828495
epoch£º149	 i:1 	 global-step:2981	 l-p:0.056606099009513855
epoch£º149	 i:2 	 global-step:2982	 l-p:0.056591831147670746
epoch£º149	 i:3 	 global-step:2983	 l-p:0.05666607245802879
epoch£º149	 i:4 	 global-step:2984	 l-p:0.05689863860607147
epoch£º149	 i:5 	 global-step:2985	 l-p:0.057260073721408844
epoch£º149	 i:6 	 global-step:2986	 l-p:0.056730326265096664
epoch£º149	 i:7 	 global-step:2987	 l-p:0.0568394660949707
epoch£º149	 i:8 	 global-step:2988	 l-p:0.056707993149757385
epoch£º149	 i:9 	 global-step:2989	 l-p:0.060349732637405396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]])
 pt:tensor([[26.0447, 29.0348, 29.1376],
        [26.0447, 30.0641, 30.8720],
        [26.0447, 28.9813, 29.0534],
        [26.0447, 26.0748, 26.0466]], grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.059324711561203 
model_pd.l_d.mean(): -0.2755116820335388 
model_pd.lagr.mean(): -0.21618697047233582 
model_pd.lambdas: dict_items([('pout', tensor([0.1560])), ('power', tensor([0.0084]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6666])), ('power', tensor([-1.5136]))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.059324711561203
epoch£º150	 i:1 	 global-step:3001	 l-p:0.05722333490848541
epoch£º150	 i:2 	 global-step:3002	 l-p:0.05659114196896553
epoch£º150	 i:3 	 global-step:3003	 l-p:0.05787434056401253
epoch£º150	 i:4 	 global-step:3004	 l-p:0.05663081258535385
epoch£º150	 i:5 	 global-step:3005	 l-p:0.05729477107524872
epoch£º150	 i:6 	 global-step:3006	 l-p:0.05687820911407471
epoch£º150	 i:7 	 global-step:3007	 l-p:0.056869372725486755
epoch£º150	 i:8 	 global-step:3008	 l-p:0.05667516216635704
epoch£º150	 i:9 	 global-step:3009	 l-p:0.05675218999385834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]])
 pt:tensor([[26.0722, 29.5078, 29.8886],
        [26.0722, 26.0722, 26.0722],
        [26.0722, 32.6491, 35.8798],
        [26.0722, 30.4484, 31.5410]], grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.05657551810145378 
model_pd.l_d.mean(): -0.2645527720451355 
model_pd.lagr.mean(): -0.20797725021839142 
model_pd.lambdas: dict_items([('pout', tensor([0.1387])), ('power', tensor([0.0074]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7802])), ('power', tensor([-1.9230]))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.05657551810145378
epoch£º151	 i:1 	 global-step:3021	 l-p:0.05661686882376671
epoch£º151	 i:2 	 global-step:3022	 l-p:0.05658715218305588
epoch£º151	 i:3 	 global-step:3023	 l-p:0.05687233805656433
epoch£º151	 i:4 	 global-step:3024	 l-p:0.05667882412672043
epoch£º151	 i:5 	 global-step:3025	 l-p:0.0577445924282074
epoch£º151	 i:6 	 global-step:3026	 l-p:0.05924439802765846
epoch£º151	 i:7 	 global-step:3027	 l-p:0.05712678283452988
epoch£º151	 i:8 	 global-step:3028	 l-p:0.05795920267701149
epoch£º151	 i:9 	 global-step:3029	 l-p:0.05663064867258072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]])
 pt:tensor([[26.0837, 27.6167, 27.1653],
        [26.0837, 33.8273, 38.3949],
        [26.0837, 27.9979, 27.6255],
        [26.0837, 27.9507, 27.5656]], grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.056808874011039734 
model_pd.l_d.mean(): -0.21894049644470215 
model_pd.lagr.mean(): -0.16213162243366241 
model_pd.lambdas: dict_items([('pout', tensor([0.1216])), ('power', tensor([0.0066]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6845])), ('power', tensor([-1.6985]))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.056808874011039734
epoch£º152	 i:1 	 global-step:3041	 l-p:0.056675754487514496
epoch£º152	 i:2 	 global-step:3042	 l-p:0.05676766484975815
epoch£º152	 i:3 	 global-step:3043	 l-p:0.05919894576072693
epoch£º152	 i:4 	 global-step:3044	 l-p:0.056593723595142365
epoch£º152	 i:5 	 global-step:3045	 l-p:0.05819894000887871
epoch£º152	 i:6 	 global-step:3046	 l-p:0.05657301843166351
epoch£º152	 i:7 	 global-step:3047	 l-p:0.05705514922738075
epoch£º152	 i:8 	 global-step:3048	 l-p:0.057533618062734604
epoch£º152	 i:9 	 global-step:3049	 l-p:0.05656816437840462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]])
 pt:tensor([[26.1006, 26.1583, 26.1059],
        [26.1006, 26.1021, 26.1006],
        [26.1006, 26.2492, 26.1247],
        [26.1006, 26.1006, 26.1006]], grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.05902370437979698 
model_pd.l_d.mean(): -0.19255287945270538 
model_pd.lagr.mean(): -0.1335291713476181 
model_pd.lambdas: dict_items([('pout', tensor([0.1043])), ('power', tensor([0.0057]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7273])), ('power', tensor([-1.6232]))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.05902370437979698
epoch£º153	 i:1 	 global-step:3061	 l-p:0.05779702216386795
epoch£º153	 i:2 	 global-step:3062	 l-p:0.05662950500845909
epoch£º153	 i:3 	 global-step:3063	 l-p:0.05670854449272156
epoch£º153	 i:4 	 global-step:3064	 l-p:0.05664139613509178
epoch£º153	 i:5 	 global-step:3065	 l-p:0.05672943592071533
epoch£º153	 i:6 	 global-step:3066	 l-p:0.056719884276390076
epoch£º153	 i:7 	 global-step:3067	 l-p:0.05746258795261383
epoch£º153	 i:8 	 global-step:3068	 l-p:0.057239510118961334
epoch£º153	 i:9 	 global-step:3069	 l-p:0.056955110281705856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]])
 pt:tensor([[26.1133, 31.6048, 33.7048],
        [26.1133, 27.7844, 27.3543],
        [26.1133, 26.4485, 26.2038],
        [26.1133, 26.1820, 26.1202]], grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.05781687796115875 
model_pd.l_d.mean(): -0.15533839166164398 
model_pd.lagr.mean(): -0.09752151370048523 
model_pd.lambdas: dict_items([('pout', tensor([0.0871])), ('power', tensor([0.0049]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6689])), ('power', tensor([-1.4454]))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.05781687796115875
epoch£º154	 i:1 	 global-step:3081	 l-p:0.0568721741437912
epoch£º154	 i:2 	 global-step:3082	 l-p:0.05673326179385185
epoch£º154	 i:3 	 global-step:3083	 l-p:0.059026818722486496
epoch£º154	 i:4 	 global-step:3084	 l-p:0.05793249234557152
epoch£º154	 i:5 	 global-step:3085	 l-p:0.05667778104543686
epoch£º154	 i:6 	 global-step:3086	 l-p:0.056820183992385864
epoch£º154	 i:7 	 global-step:3087	 l-p:0.05674028769135475
epoch£º154	 i:8 	 global-step:3088	 l-p:0.056644830852746964
epoch£º154	 i:9 	 global-step:3089	 l-p:0.05660870671272278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]])
 pt:tensor([[26.1172, 31.6506, 33.7911],
        [26.1172, 31.0145, 32.5550],
        [26.1172, 26.6580, 26.3151],
        [26.1172, 29.2562, 29.4429]], grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.05687420070171356 
model_pd.l_d.mean(): -0.13006500899791718 
model_pd.lagr.mean(): -0.07319080829620361 
model_pd.lambdas: dict_items([('pout', tensor([0.0698])), ('power', tensor([0.0040]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7185])), ('power', tensor([-1.7331]))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.05687420070171356
epoch£º155	 i:1 	 global-step:3101	 l-p:0.056739602237939835
epoch£º155	 i:2 	 global-step:3102	 l-p:0.057084862142801285
epoch£º155	 i:3 	 global-step:3103	 l-p:0.05658045783638954
epoch£º155	 i:4 	 global-step:3104	 l-p:0.05740632116794586
epoch£º155	 i:5 	 global-step:3105	 l-p:0.056556787341833115
epoch£º155	 i:6 	 global-step:3106	 l-p:0.060344111174345016
epoch£º155	 i:7 	 global-step:3107	 l-p:0.05660446733236313
epoch£º155	 i:8 	 global-step:3108	 l-p:0.05667682737112045
epoch£º155	 i:9 	 global-step:3109	 l-p:0.05701953545212746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]])
 pt:tensor([[26.1090, 35.0821, 41.1823],
        [26.1090, 35.2384, 41.5429],
        [26.1090, 27.8498, 27.4338],
        [26.1090, 35.0597, 41.1308]], grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.05682078003883362 
model_pd.l_d.mean(): -0.09836661070585251 
model_pd.lagr.mean(): -0.04154583066701889 
model_pd.lambdas: dict_items([('pout', tensor([0.0526])), ('power', tensor([0.0032]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7067])), ('power', tensor([-1.7449]))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.05682078003883362
epoch£º156	 i:1 	 global-step:3121	 l-p:0.0580589659512043
epoch£º156	 i:2 	 global-step:3122	 l-p:0.05972810834646225
epoch£º156	 i:3 	 global-step:3123	 l-p:0.05666124448180199
epoch£º156	 i:4 	 global-step:3124	 l-p:0.05669290944933891
epoch£º156	 i:5 	 global-step:3125	 l-p:0.056895047426223755
epoch£º156	 i:6 	 global-step:3126	 l-p:0.05725158751010895
epoch£º156	 i:7 	 global-step:3127	 l-p:0.05652126669883728
epoch£º156	 i:8 	 global-step:3128	 l-p:0.05662798509001732
epoch£º156	 i:9 	 global-step:3129	 l-p:0.05664592236280441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]])
 pt:tensor([[26.1032, 26.9110, 26.4852],
        [26.1032, 30.6983, 31.9733],
        [26.1032, 33.5112, 37.6775],
        [26.1032, 28.5616, 28.3868]], grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.056705817580223083 
model_pd.l_d.mean(): -0.06806742399930954 
model_pd.lagr.mean(): -0.011361606419086456 
model_pd.lambdas: dict_items([('pout', tensor([0.0354])), ('power', tensor([0.0023]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7243])), ('power', tensor([-1.7243]))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.056705817580223083
epoch£º157	 i:1 	 global-step:3141	 l-p:0.05663704872131348
epoch£º157	 i:2 	 global-step:3142	 l-p:0.05729406699538231
epoch£º157	 i:3 	 global-step:3143	 l-p:0.05811722204089165
epoch£º157	 i:4 	 global-step:3144	 l-p:0.05654613673686981
epoch£º157	 i:5 	 global-step:3145	 l-p:0.05729222670197487
epoch£º157	 i:6 	 global-step:3146	 l-p:0.056851472705602646
epoch£º157	 i:7 	 global-step:3147	 l-p:0.05683647096157074
epoch£º157	 i:8 	 global-step:3148	 l-p:0.05907243862748146
epoch£º157	 i:9 	 global-step:3149	 l-p:0.05661657452583313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]])
 pt:tensor([[26.0817, 26.0822, 26.0817],
        [26.0817, 26.2088, 26.1005],
        [26.0817, 26.0820, 26.0817],
        [26.0817, 30.0310, 30.7811]], grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.0577869787812233 
model_pd.l_d.mean(): -0.03680776432156563 
model_pd.lagr.mean(): 0.02097921445965767 
model_pd.lambdas: dict_items([('pout', tensor([0.0181])), ('power', tensor([0.0014]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7241])), ('power', tensor([-1.6820]))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.0577869787812233
epoch£º158	 i:1 	 global-step:3161	 l-p:0.05659474804997444
epoch£º158	 i:2 	 global-step:3162	 l-p:0.05685309320688248
epoch£º158	 i:3 	 global-step:3163	 l-p:0.05753275007009506
epoch£º158	 i:4 	 global-step:3164	 l-p:0.05669282376766205
epoch£º158	 i:5 	 global-step:3165	 l-p:0.05713430419564247
epoch£º158	 i:6 	 global-step:3166	 l-p:0.05936727672815323
epoch£º158	 i:7 	 global-step:3167	 l-p:0.056571170687675476
epoch£º158	 i:8 	 global-step:3168	 l-p:0.0565374530851841
epoch£º158	 i:9 	 global-step:3169	 l-p:0.05697498470544815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]])
 pt:tensor([[26.0635, 27.7313, 27.3021],
        [26.0635, 26.8842, 26.4560],
        [26.0635, 27.3700, 26.8998],
        [26.0635, 26.5478, 26.2290]], grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.05665019154548645 
model_pd.l_d.mean(): -0.005835163872689009 
model_pd.lagr.mean(): 0.050815027207136154 
model_pd.lambdas: dict_items([('pout', tensor([0.0009])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7441])), ('power', tensor([-1.8707]))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.05665019154548645
epoch£º159	 i:1 	 global-step:3181	 l-p:0.0566292442381382
epoch£º159	 i:2 	 global-step:3182	 l-p:0.05690049007534981
epoch£º159	 i:3 	 global-step:3183	 l-p:0.05670588091015816
epoch£º159	 i:4 	 global-step:3184	 l-p:0.057962317019701004
epoch£º159	 i:5 	 global-step:3185	 l-p:0.05656826123595238
epoch£º159	 i:6 	 global-step:3186	 l-p:0.05703098326921463
epoch£º159	 i:7 	 global-step:3187	 l-p:0.057825133204460144
epoch£º159	 i:8 	 global-step:3188	 l-p:0.05674537643790245
epoch£º159	 i:9 	 global-step:3189	 l-p:0.05910984426736832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]])
 pt:tensor([[26.0546, 26.8247, 26.4083],
        [26.0546, 32.3042, 35.1851],
        [26.0546, 26.0546, 26.0546],
        [26.0546, 26.0647, 26.0549]], grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.05655539408326149 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05655539408326149 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7923])), ('power', tensor([-1.9891]))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.05655539408326149
epoch£º160	 i:1 	 global-step:3201	 l-p:0.05665545165538788
epoch£º160	 i:2 	 global-step:3202	 l-p:0.057670339941978455
epoch£º160	 i:3 	 global-step:3203	 l-p:0.05786861106753349
epoch£º160	 i:4 	 global-step:3204	 l-p:0.056650713086128235
epoch£º160	 i:5 	 global-step:3205	 l-p:0.05916342884302139
epoch£º160	 i:6 	 global-step:3206	 l-p:0.0568525493144989
epoch£º160	 i:7 	 global-step:3207	 l-p:0.056676339358091354
epoch£º160	 i:8 	 global-step:3208	 l-p:0.05722134932875633
epoch£º160	 i:9 	 global-step:3209	 l-p:0.05670931935310364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]])
 pt:tensor([[26.1183, 27.7343, 27.2944],
        [26.1183, 26.9760, 26.5395],
        [26.1183, 31.4516, 33.3979],
        [26.1183, 31.9965, 34.4819]], grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.05654328316450119 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05654328316450119 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7892])), ('power', tensor([-1.9504]))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.05654328316450119
epoch£º161	 i:1 	 global-step:3221	 l-p:0.05675555765628815
epoch£º161	 i:2 	 global-step:3222	 l-p:0.0599374882876873
epoch£º161	 i:3 	 global-step:3223	 l-p:0.0565621480345726
epoch£º161	 i:4 	 global-step:3224	 l-p:0.05712795630097389
epoch£º161	 i:5 	 global-step:3225	 l-p:0.056803300976753235
epoch£º161	 i:6 	 global-step:3226	 l-p:0.05667389929294586
epoch£º161	 i:7 	 global-step:3227	 l-p:0.05660029873251915
epoch£º161	 i:8 	 global-step:3228	 l-p:0.058090463280677795
epoch£º161	 i:9 	 global-step:3229	 l-p:0.056616391986608505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]])
 pt:tensor([[26.2087, 31.1442, 32.7085],
        [26.2087, 26.2192, 26.2091],
        [26.2087, 27.3633, 26.8908],
        [26.2087, 32.0854, 34.5564]], grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.056658606976270676 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056658606976270676 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7352])), ('power', tensor([-1.6784]))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.056658606976270676
epoch£º162	 i:1 	 global-step:3241	 l-p:0.059607233852148056
epoch£º162	 i:2 	 global-step:3242	 l-p:0.05655987188220024
epoch£º162	 i:3 	 global-step:3243	 l-p:0.056492291390895844
epoch£º162	 i:4 	 global-step:3244	 l-p:0.056618452072143555
epoch£º162	 i:5 	 global-step:3245	 l-p:0.05711667239665985
epoch£º162	 i:6 	 global-step:3246	 l-p:0.056858595460653305
epoch£º162	 i:7 	 global-step:3247	 l-p:0.05677247419953346
epoch£º162	 i:8 	 global-step:3248	 l-p:0.05699332803487778
epoch£º162	 i:9 	 global-step:3249	 l-p:0.05765272676944733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]])
 pt:tensor([[26.3069, 33.7757, 37.9763],
        [26.3069, 26.3069, 26.3069],
        [26.3069, 29.6240, 29.9089],
        [26.3069, 26.3070, 26.3069]], grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.059049032628536224 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.059049032628536224 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6467])), ('power', tensor([-1.2102]))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.059049032628536224
epoch£º163	 i:1 	 global-step:3261	 l-p:0.05679952725768089
epoch£º163	 i:2 	 global-step:3262	 l-p:0.05655033886432648
epoch£º163	 i:3 	 global-step:3263	 l-p:0.05660953372716904
epoch£º163	 i:4 	 global-step:3264	 l-p:0.05668478086590767
epoch£º163	 i:5 	 global-step:3265	 l-p:0.05653345584869385
epoch£º163	 i:6 	 global-step:3266	 l-p:0.05646621063351631
epoch£º163	 i:7 	 global-step:3267	 l-p:0.05727064609527588
epoch£º163	 i:8 	 global-step:3268	 l-p:0.056773923337459564
epoch£º163	 i:9 	 global-step:3269	 l-p:0.0581975132226944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]])
 pt:tensor([[26.4072, 28.9056, 28.7330],
        [26.4072, 26.4780, 26.4145],
        [26.4072, 26.4230, 26.4079],
        [26.4072, 26.7609, 26.5053]], grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.05667933449149132 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05667933449149132 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7456])), ('power', tensor([-1.4811]))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.05667933449149132
epoch£º164	 i:1 	 global-step:3281	 l-p:0.05805061757564545
epoch£º164	 i:2 	 global-step:3282	 l-p:0.05762903392314911
epoch£º164	 i:3 	 global-step:3283	 l-p:0.05888868495821953
epoch£º164	 i:4 	 global-step:3284	 l-p:0.056553274393081665
epoch£º164	 i:5 	 global-step:3285	 l-p:0.0566561222076416
epoch£º164	 i:6 	 global-step:3286	 l-p:0.056410886347293854
epoch£º164	 i:7 	 global-step:3287	 l-p:0.05654614046216011
epoch£º164	 i:8 	 global-step:3288	 l-p:0.056551914662122726
epoch£º164	 i:9 	 global-step:3289	 l-p:0.0565854087471962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]])
 pt:tensor([[26.5106, 26.5107, 26.5107],
        [26.5106, 33.2999, 36.6935],
        [26.5106, 31.6309, 33.3286],
        [26.5106, 28.7218, 28.4277]], grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.05664016306400299 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05664016306400299 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7495])), ('power', tensor([-1.3954]))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.05664016306400299
epoch£º165	 i:1 	 global-step:3301	 l-p:0.05869866907596588
epoch£º165	 i:2 	 global-step:3302	 l-p:0.05646955594420433
epoch£º165	 i:3 	 global-step:3303	 l-p:0.05657235160470009
epoch£º165	 i:4 	 global-step:3304	 l-p:0.05677347257733345
epoch£º165	 i:5 	 global-step:3305	 l-p:0.05643479526042938
epoch£º165	 i:6 	 global-step:3306	 l-p:0.056748420000076294
epoch£º165	 i:7 	 global-step:3307	 l-p:0.05666685476899147
epoch£º165	 i:8 	 global-step:3308	 l-p:0.05644719675183296
epoch£º165	 i:9 	 global-step:3309	 l-p:0.058681558817625046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]])
 pt:tensor([[26.6063, 26.6383, 26.6084],
        [26.6063, 27.5918, 27.1282],
        [26.6063, 27.7054, 27.2297],
        [26.6063, 26.6067, 26.6063]], grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.05772092938423157 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05772092938423157 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6996])), ('power', tensor([-1.0266]))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.05772092938423157
epoch£º166	 i:1 	 global-step:3321	 l-p:0.05728573352098465
epoch£º166	 i:2 	 global-step:3322	 l-p:0.05638350918889046
epoch£º166	 i:3 	 global-step:3323	 l-p:0.057085465639829636
epoch£º166	 i:4 	 global-step:3324	 l-p:0.056562334299087524
epoch£º166	 i:5 	 global-step:3325	 l-p:0.056581366807222366
epoch£º166	 i:6 	 global-step:3326	 l-p:0.056353021413087845
epoch£º166	 i:7 	 global-step:3327	 l-p:0.058531504124403
epoch£º166	 i:8 	 global-step:3328	 l-p:0.05674855411052704
epoch£º166	 i:9 	 global-step:3329	 l-p:0.05652202293276787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]])
 pt:tensor([[26.7080, 26.7080, 26.7080],
        [26.7080, 28.5793, 28.1736],
        [26.7080, 36.5006, 43.5474],
        [26.7080, 26.9089, 26.7467]], grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.057149287313222885 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057149287313222885 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7272])), ('power', tensor([-1.0485]))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.057149287313222885
epoch£º167	 i:1 	 global-step:3341	 l-p:0.05819471925497055
epoch£º167	 i:2 	 global-step:3342	 l-p:0.05661871284246445
epoch£º167	 i:3 	 global-step:3343	 l-p:0.05649232864379883
epoch£º167	 i:4 	 global-step:3344	 l-p:0.05654069408774376
epoch£º167	 i:5 	 global-step:3345	 l-p:0.05651474371552467
epoch£º167	 i:6 	 global-step:3346	 l-p:0.05643202364444733
epoch£º167	 i:7 	 global-step:3347	 l-p:0.05636401101946831
epoch£º167	 i:8 	 global-step:3348	 l-p:0.05647426098585129
epoch£º167	 i:9 	 global-step:3349	 l-p:0.05861001834273338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]])
 pt:tensor([[26.8069, 26.8200, 26.8074],
        [26.8069, 35.8787, 41.9532],
        [26.8069, 26.8070, 26.8069],
        [26.8069, 26.8071, 26.8069]], grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.05861470103263855 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05861470103263855 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6953])), ('power', tensor([-0.7558]))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.05861470103263855
epoch£º168	 i:1 	 global-step:3361	 l-p:0.0564916767179966
epoch£º168	 i:2 	 global-step:3362	 l-p:0.056464120745658875
epoch£º168	 i:3 	 global-step:3363	 l-p:0.05682386830449104
epoch£º168	 i:4 	 global-step:3364	 l-p:0.0564986951649189
epoch£º168	 i:5 	 global-step:3365	 l-p:0.05655425041913986
epoch£º168	 i:6 	 global-step:3366	 l-p:0.05757228285074234
epoch£º168	 i:7 	 global-step:3367	 l-p:0.056451741605997086
epoch£º168	 i:8 	 global-step:3368	 l-p:0.05644046142697334
epoch£º168	 i:9 	 global-step:3369	 l-p:0.05713222548365593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]])
 pt:tensor([[26.9115, 26.9166, 26.9116],
        [26.9115, 27.2464, 27.0000],
        [26.9115, 28.6783, 28.2426],
        [26.9115, 27.7150, 27.2826]], grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.05637426674365997 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05637426674365997 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8164])), ('power', tensor([-1.1207]))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.05637426674365997
epoch£º169	 i:1 	 global-step:3381	 l-p:0.057352084666490555
epoch£º169	 i:2 	 global-step:3382	 l-p:0.05723856762051582
epoch£º169	 i:3 	 global-step:3383	 l-p:0.056591566652059555
epoch£º169	 i:4 	 global-step:3384	 l-p:0.05703500285744667
epoch£º169	 i:5 	 global-step:3385	 l-p:0.056526411324739456
epoch£º169	 i:6 	 global-step:3386	 l-p:0.0563938282430172
epoch£º169	 i:7 	 global-step:3387	 l-p:0.05654586851596832
epoch£º169	 i:8 	 global-step:3388	 l-p:0.058256424963474274
epoch£º169	 i:9 	 global-step:3389	 l-p:0.05634395778179169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]])
 pt:tensor([[27.0098, 29.4273, 29.1901],
        [27.0098, 27.0287, 27.0107],
        [27.0098, 27.4197, 27.1327],
        [27.0098, 27.0239, 27.0104]], grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.058403030037879944 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058403030037879944 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7176])), ('power', tensor([-0.5480]))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.058403030037879944
epoch£º170	 i:1 	 global-step:3401	 l-p:0.057684969156980515
epoch£º170	 i:2 	 global-step:3402	 l-p:0.056289006024599075
epoch£º170	 i:3 	 global-step:3403	 l-p:0.05641629174351692
epoch£º170	 i:4 	 global-step:3404	 l-p:0.056359466165304184
epoch£º170	 i:5 	 global-step:3405	 l-p:0.056301698088645935
epoch£º170	 i:6 	 global-step:3406	 l-p:0.05634994059801102
epoch£º170	 i:7 	 global-step:3407	 l-p:0.056546635925769806
epoch£º170	 i:8 	 global-step:3408	 l-p:0.05661936104297638
epoch£º170	 i:9 	 global-step:3409	 l-p:0.05735236778855324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]])
 pt:tensor([[27.1136, 32.1314, 33.6658],
        [27.1136, 27.1436, 27.1154],
        [27.1136, 27.5503, 27.2496],
        [27.1136, 27.1140, 27.1136]], grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.05757990851998329 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05757990851998329 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7016])), ('power', tensor([-0.4529]))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.05757990851998329
epoch£º171	 i:1 	 global-step:3421	 l-p:0.056902989745140076
epoch£º171	 i:2 	 global-step:3422	 l-p:0.05630681663751602
epoch£º171	 i:3 	 global-step:3423	 l-p:0.05641801282763481
epoch£º171	 i:4 	 global-step:3424	 l-p:0.05823143944144249
epoch£º171	 i:5 	 global-step:3425	 l-p:0.05652416869997978
epoch£º171	 i:6 	 global-step:3426	 l-p:0.05635889619588852
epoch£º171	 i:7 	 global-step:3427	 l-p:0.05674365162849426
epoch£º171	 i:8 	 global-step:3428	 l-p:0.05632208660244942
epoch£º171	 i:9 	 global-step:3429	 l-p:0.05657118931412697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]])
 pt:tensor([[27.2142, 27.2281, 27.2148],
        [27.2142, 27.3392, 27.2320],
        [27.2142, 30.3467, 30.4550],
        [27.2142, 27.2142, 27.2142]], grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.056272659450769424 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056272659450769424 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8325])), ('power', tensor([-0.8344]))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.056272659450769424
epoch£º172	 i:1 	 global-step:3441	 l-p:0.056411013007164
epoch£º172	 i:2 	 global-step:3442	 l-p:0.056349121034145355
epoch£º172	 i:3 	 global-step:3443	 l-p:0.05630683898925781
epoch£º172	 i:4 	 global-step:3444	 l-p:0.05833529308438301
epoch£º172	 i:5 	 global-step:3445	 l-p:0.05742017924785614
epoch£º172	 i:6 	 global-step:3446	 l-p:0.05710875615477562
epoch£º172	 i:7 	 global-step:3447	 l-p:0.05632030591368675
epoch£º172	 i:8 	 global-step:3448	 l-p:0.05638136714696884
epoch£º172	 i:9 	 global-step:3449	 l-p:0.05670040100812912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]])
 pt:tensor([[27.3125, 36.6903, 43.0495],
        [27.3125, 28.6589, 28.1643],
        [27.3125, 33.6982, 36.5359],
        [27.3125, 28.7931, 28.3055]], grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.05664203688502312 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05664203688502312 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7398])), ('power', tensor([-0.4085]))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.05664203688502312
epoch£º173	 i:1 	 global-step:3461	 l-p:0.0573154054582119
epoch£º173	 i:2 	 global-step:3462	 l-p:0.056319188326597214
epoch£º173	 i:3 	 global-step:3463	 l-p:0.05623546987771988
epoch£º173	 i:4 	 global-step:3464	 l-p:0.05621339753270149
epoch£º173	 i:5 	 global-step:3465	 l-p:0.05639718472957611
epoch£º173	 i:6 	 global-step:3466	 l-p:0.05633476376533508
epoch£º173	 i:7 	 global-step:3467	 l-p:0.05694741755723953
epoch£º173	 i:8 	 global-step:3468	 l-p:0.056755948811769485
epoch£º173	 i:9 	 global-step:3469	 l-p:0.05809754878282547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]])
 pt:tensor([[27.4086, 35.2058, 39.5925],
        [27.4086, 27.6958, 27.4767],
        [27.4086, 28.6198, 28.1244],
        [27.4086, 29.3350, 28.9191]], grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.05637853592634201 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05637853592634201 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7964])), ('power', tensor([-0.5045]))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.05637853592634201
epoch£º174	 i:1 	 global-step:3481	 l-p:0.056649066507816315
epoch£º174	 i:2 	 global-step:3482	 l-p:0.056331563740968704
epoch£º174	 i:3 	 global-step:3483	 l-p:0.056338243186473846
epoch£º174	 i:4 	 global-step:3484	 l-p:0.05739971622824669
epoch£º174	 i:5 	 global-step:3485	 l-p:0.05647606775164604
epoch£º174	 i:6 	 global-step:3486	 l-p:0.0561915785074234
epoch£º174	 i:7 	 global-step:3487	 l-p:0.05791877582669258
epoch£º174	 i:8 	 global-step:3488	 l-p:0.056309252977371216
epoch£º174	 i:9 	 global-step:3489	 l-p:0.056942690163850784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]])
 pt:tensor([[27.5086, 27.5626, 27.5132],
        [27.5086, 27.7181, 27.5493],
        [27.5086, 28.3245, 27.8835],
        [27.5086, 28.6879, 28.1924]], grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.056259412318468094 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056259412318468094 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8464])), ('power', tensor([-0.5521]))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.056259412318468094
epoch£º175	 i:1 	 global-step:3501	 l-p:0.056329235434532166
epoch£º175	 i:2 	 global-step:3502	 l-p:0.05669993907213211
epoch£º175	 i:3 	 global-step:3503	 l-p:0.05621146038174629
epoch£º175	 i:4 	 global-step:3504	 l-p:0.05723118782043457
epoch£º175	 i:5 	 global-step:3505	 l-p:0.057939767837524414
epoch£º175	 i:6 	 global-step:3506	 l-p:0.05632027983665466
epoch£º175	 i:7 	 global-step:3507	 l-p:0.05685710906982422
epoch£º175	 i:8 	 global-step:3508	 l-p:0.05648642033338547
epoch£º175	 i:9 	 global-step:3509	 l-p:0.05627492070198059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]])
 pt:tensor([[27.6103, 27.6103, 27.6103],
        [27.6103, 27.9813, 27.7132],
        [27.6103, 33.1955, 35.1943],
        [27.6103, 30.3180, 30.1785]], grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.05716340243816376 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05716340243816376 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7734])), ('power', tensor([-0.1046]))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.05716340243816376
epoch£º176	 i:1 	 global-step:3521	 l-p:0.056153226643800735
epoch£º176	 i:2 	 global-step:3522	 l-p:0.05635643005371094
epoch£º176	 i:3 	 global-step:3523	 l-p:0.056146878749132156
epoch£º176	 i:4 	 global-step:3524	 l-p:0.0564134456217289
epoch£º176	 i:5 	 global-step:3525	 l-p:0.05617159977555275
epoch£º176	 i:6 	 global-step:3526	 l-p:0.058557938784360886
epoch£º176	 i:7 	 global-step:3527	 l-p:0.056706905364990234
epoch£º176	 i:8 	 global-step:3528	 l-p:0.056304942816495895
epoch£º176	 i:9 	 global-step:3529	 l-p:0.056303467601537704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]])
 pt:tensor([[27.7105, 37.7097, 44.7961],
        [27.7105, 27.7279, 27.7112],
        [27.7105, 29.2910, 28.8033],
        [27.7105, 27.7186, 27.7107]], grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.05647061765193939 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05647061765193939 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7850])), ('power', tensor([-0.1455]))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.05647061765193939
epoch£º177	 i:1 	 global-step:3541	 l-p:0.05796961486339569
epoch£º177	 i:2 	 global-step:3542	 l-p:0.056173764169216156
epoch£º177	 i:3 	 global-step:3543	 l-p:0.056912828236818314
epoch£º177	 i:4 	 global-step:3544	 l-p:0.05624878779053688
epoch£º177	 i:5 	 global-step:3545	 l-p:0.056226830929517746
epoch£º177	 i:6 	 global-step:3546	 l-p:0.056493934243917465
epoch£º177	 i:7 	 global-step:3547	 l-p:0.05611855164170265
epoch£º177	 i:8 	 global-step:3548	 l-p:0.05702582001686096
epoch£º177	 i:9 	 global-step:3549	 l-p:0.05632060021162033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]])
 pt:tensor([[27.8119, 27.8171, 27.8120],
        [27.8119, 27.8124, 27.8119],
        [27.8119, 27.8119, 27.8119],
        [27.8119, 28.9639, 28.4655]], grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.05832023173570633 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05832023173570633 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([1.4054e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7473])), ('power', tensor([0.2811]))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.05832023173570633
epoch£º178	 i:1 	 global-step:3561	 l-p:0.05623345077037811
epoch£º178	 i:2 	 global-step:3562	 l-p:0.05607004091143608
epoch£º178	 i:3 	 global-step:3563	 l-p:0.056525811553001404
epoch£º178	 i:4 	 global-step:3564	 l-p:0.05699042230844498
epoch£º178	 i:5 	 global-step:3565	 l-p:0.05617716908454895
epoch£º178	 i:6 	 global-step:3566	 l-p:0.05674981698393822
epoch£º178	 i:7 	 global-step:3567	 l-p:0.05628732591867447
epoch£º178	 i:8 	 global-step:3568	 l-p:0.0561649464070797
epoch£º178	 i:9 	 global-step:3569	 l-p:0.05612592771649361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]])
 pt:tensor([[27.9132, 27.9441, 27.9151],
        [27.9132, 28.3766, 28.0601],
        [27.9132, 28.3538, 28.0485],
        [27.9132, 27.9262, 27.9137]], grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.056150201708078384 
model_pd.l_d.mean(): -4.3817382788802206e-07 
model_pd.lagr.mean(): 0.056149762123823166 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([2.7923e-06]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8521])), ('power', tensor([-0.0698]))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.056150201708078384
epoch£º179	 i:1 	 global-step:3581	 l-p:0.05667470023036003
epoch£º179	 i:2 	 global-step:3582	 l-p:0.05630124732851982
epoch£º179	 i:3 	 global-step:3583	 l-p:0.056288331747055054
epoch£º179	 i:4 	 global-step:3584	 l-p:0.05649179592728615
epoch£º179	 i:5 	 global-step:3585	 l-p:0.05607196316123009
epoch£º179	 i:6 	 global-step:3586	 l-p:0.057344287633895874
epoch£º179	 i:7 	 global-step:3587	 l-p:0.05627822503447533
epoch£º179	 i:8 	 global-step:3588	 l-p:0.05608999356627464
epoch£º179	 i:9 	 global-step:3589	 l-p:0.0576205775141716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]])
 pt:tensor([[28.0051, 28.0275, 28.0063],
        [28.0051, 28.0209, 28.0058],
        [28.0051, 30.6640, 30.4811],
        [28.0051, 28.0052, 28.0051]], grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.05604212358593941 
model_pd.l_d.mean(): -3.4657298328966135e-06 
model_pd.lagr.mean(): 0.05603865906596184 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([5.1232e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8877])), ('power', tensor([-0.0637]))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.05604212358593941
epoch£º180	 i:1 	 global-step:3601	 l-p:0.056082095950841904
epoch£º180	 i:2 	 global-step:3602	 l-p:0.057168617844581604
epoch£º180	 i:3 	 global-step:3603	 l-p:0.056122731417417526
epoch£º180	 i:4 	 global-step:3604	 l-p:0.05725901201367378
epoch£º180	 i:5 	 global-step:3605	 l-p:0.05614093691110611
epoch£º180	 i:6 	 global-step:3606	 l-p:0.05762413516640663
epoch£º180	 i:7 	 global-step:3607	 l-p:0.05607727915048599
epoch£º180	 i:8 	 global-step:3608	 l-p:0.05637239292263985
epoch£º180	 i:9 	 global-step:3609	 l-p:0.05614935979247093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]])
 pt:tensor([[28.0925, 28.2564, 28.1194],
        [28.0925, 29.3428, 28.8342],
        [28.0925, 32.7042, 33.7867],
        [28.0925, 34.8895, 38.0447]], grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.05611546337604523 
model_pd.l_d.mean(): 1.471026916988194e-05 
model_pd.lagr.mean(): 0.05613017454743385 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8430])), ('power', tensor([0.1000]))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.05611546337604523
epoch£º181	 i:1 	 global-step:3621	 l-p:0.05609351769089699
epoch£º181	 i:2 	 global-step:3622	 l-p:0.05677897110581398
epoch£º181	 i:3 	 global-step:3623	 l-p:0.05618701130151749
epoch£º181	 i:4 	 global-step:3624	 l-p:0.05695318803191185
epoch£º181	 i:5 	 global-step:3625	 l-p:0.05617193132638931
epoch£º181	 i:6 	 global-step:3626	 l-p:0.05615457147359848
epoch£º181	 i:7 	 global-step:3627	 l-p:0.05653243884444237
epoch£º181	 i:8 	 global-step:3628	 l-p:0.057700857520103455
epoch£º181	 i:9 	 global-step:3629	 l-p:0.05611053854227066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]])
 pt:tensor([[28.1534, 28.3536, 28.1906],
        [28.1534, 29.0823, 28.6098],
        [28.1534, 30.0473, 29.5991],
        [28.1534, 28.1534, 28.1534]], grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.05624785274267197 
model_pd.l_d.mean(): 4.878463732893579e-05 
model_pd.lagr.mean(): 0.056296639144420624 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0003]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8528])), ('power', tensor([0.1759]))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.05624785274267197
epoch£º182	 i:1 	 global-step:3641	 l-p:0.056972164660692215
epoch£º182	 i:2 	 global-step:3642	 l-p:0.05605704337358475
epoch£º182	 i:3 	 global-step:3643	 l-p:0.05608260631561279
epoch£º182	 i:4 	 global-step:3644	 l-p:0.05616113543510437
epoch£º182	 i:5 	 global-step:3645	 l-p:0.0578346885740757
epoch£º182	 i:6 	 global-step:3646	 l-p:0.056079231202602386
epoch£º182	 i:7 	 global-step:3647	 l-p:0.056027282029390335
epoch£º182	 i:8 	 global-step:3648	 l-p:0.05612765625119209
epoch£º182	 i:9 	 global-step:3649	 l-p:0.057069454342126846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]])
 pt:tensor([[28.1808, 31.9900, 32.4590],
        [28.1808, 30.2073, 29.7899],
        [28.1808, 28.3702, 28.2148],
        [28.1808, 35.3541, 38.9024]], grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.056143227964639664 
model_pd.l_d.mean(): 9.051823144545779e-05 
model_pd.lagr.mean(): 0.05623374506831169 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0004]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8482])), ('power', tensor([0.2101]))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.056143227964639664
epoch£º183	 i:1 	 global-step:3661	 l-p:0.05607385188341141
epoch£º183	 i:2 	 global-step:3662	 l-p:0.056031979620456696
epoch£º183	 i:3 	 global-step:3663	 l-p:0.056107643991708755
epoch£º183	 i:4 	 global-step:3664	 l-p:0.056899379938840866
epoch£º183	 i:5 	 global-step:3665	 l-p:0.05622555688023567
epoch£º183	 i:6 	 global-step:3666	 l-p:0.056215569376945496
epoch£º183	 i:7 	 global-step:3667	 l-p:0.056110456585884094
epoch£º183	 i:8 	 global-step:3668	 l-p:0.05706839635968208
epoch£º183	 i:9 	 global-step:3669	 l-p:0.057767800986766815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]])
 pt:tensor([[28.1617, 29.4152, 28.9053],
        [28.1617, 35.9585, 40.2112],
        [28.1617, 28.8393, 28.4339],
        [28.1617, 28.1617, 28.1617]], grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.05708621069788933 
model_pd.l_d.mean(): 0.0002762891526799649 
model_pd.lagr.mean(): 0.057362500578165054 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0006]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7793])), ('power', tensor([0.4704]))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.05708621069788933
epoch£º184	 i:1 	 global-step:3681	 l-p:0.05652952566742897
epoch£º184	 i:2 	 global-step:3682	 l-p:0.05757579207420349
epoch£º184	 i:3 	 global-step:3683	 l-p:0.05616719275712967
epoch£º184	 i:4 	 global-step:3684	 l-p:0.056067682802677155
epoch£º184	 i:5 	 global-step:3685	 l-p:0.05671226605772972
epoch£º184	 i:6 	 global-step:3686	 l-p:0.05616626888513565
epoch£º184	 i:7 	 global-step:3687	 l-p:0.056252360343933105
epoch£º184	 i:8 	 global-step:3688	 l-p:0.05614064261317253
epoch£º184	 i:9 	 global-step:3689	 l-p:0.0560426339507103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]])
 pt:tensor([[28.1018, 29.9837, 29.5346],
        [28.1018, 28.1018, 28.1018],
        [28.1018, 28.4524, 28.1945],
        [28.1018, 35.5535, 39.4238]], grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.05667925998568535 
model_pd.l_d.mean(): 0.0002654367999639362 
model_pd.lagr.mean(): 0.056944698095321655 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7823])), ('power', tensor([0.3654]))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.05667925998568535
epoch£º185	 i:1 	 global-step:3701	 l-p:0.05614838749170303
epoch£º185	 i:2 	 global-step:3702	 l-p:0.05760873481631279
epoch£º185	 i:3 	 global-step:3703	 l-p:0.056190572679042816
epoch£º185	 i:4 	 global-step:3704	 l-p:0.056140098720788956
epoch£º185	 i:5 	 global-step:3705	 l-p:0.05701543018221855
epoch£º185	 i:6 	 global-step:3706	 l-p:0.05625351518392563
epoch£º185	 i:7 	 global-step:3707	 l-p:0.05610106512904167
epoch£º185	 i:8 	 global-step:3708	 l-p:0.05674686282873154
epoch£º185	 i:9 	 global-step:3709	 l-p:0.056118737906217575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]])
 pt:tensor([[27.9897, 27.9897, 27.9897],
        [27.9897, 32.3265, 33.1996],
        [27.9897, 31.9309, 32.5081],
        [27.9897, 29.7287, 29.2557]], grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.05620619282126427 
model_pd.l_d.mean(): 7.76733213569969e-05 
model_pd.lagr.mean(): 0.0562838651239872 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0008]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8226])), ('power', tensor([0.0943]))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.05620619282126427
epoch£º186	 i:1 	 global-step:3721	 l-p:0.056963883340358734
epoch£º186	 i:2 	 global-step:3722	 l-p:0.056147500872612
epoch£º186	 i:3 	 global-step:3723	 l-p:0.05611346289515495
epoch£º186	 i:4 	 global-step:3724	 l-p:0.05658704414963722
epoch£º186	 i:5 	 global-step:3725	 l-p:0.05712760612368584
epoch£º186	 i:6 	 global-step:3726	 l-p:0.05604526400566101
epoch£º186	 i:7 	 global-step:3727	 l-p:0.05777328088879585
epoch£º186	 i:8 	 global-step:3728	 l-p:0.05632628872990608
epoch£º186	 i:9 	 global-step:3729	 l-p:0.05613381788134575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]])
 pt:tensor([[27.8413, 27.8429, 27.8413],
        [27.8413, 30.2784, 30.0100],
        [27.8413, 31.6022, 32.0650],
        [27.8413, 27.8414, 27.8413]], grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.05703733116388321 
model_pd.l_d.mean(): 9.89685213426128e-05 
model_pd.lagr.mean(): 0.05713630095124245 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0009]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7919])), ('power', tensor([0.1155]))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.05703733116388321
epoch£º187	 i:1 	 global-step:3741	 l-p:0.056258611381053925
epoch£º187	 i:2 	 global-step:3742	 l-p:0.056394677609205246
epoch£º187	 i:3 	 global-step:3743	 l-p:0.05649732053279877
epoch£º187	 i:4 	 global-step:3744	 l-p:0.05615123361349106
epoch£º187	 i:5 	 global-step:3745	 l-p:0.0561610646545887
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05710598826408386
epoch£º187	 i:7 	 global-step:3747	 l-p:0.05618445575237274
epoch£º187	 i:8 	 global-step:3748	 l-p:0.05628439784049988
epoch£º187	 i:9 	 global-step:3749	 l-p:0.05785732716321945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]])
 pt:tensor([[27.6770, 35.3344, 39.5113],
        [27.6770, 29.0155, 28.5134],
        [27.6770, 27.6774, 27.6770],
        [27.6770, 35.2448, 39.3188]], grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.05618565157055855 
model_pd.l_d.mean(): -0.0002752728760242462 
model_pd.lagr.mean(): 0.0559103786945343 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0008]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8422])), ('power', tensor([-0.3383]))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.05618565157055855
epoch£º188	 i:1 	 global-step:3761	 l-p:0.056240588426589966
epoch£º188	 i:2 	 global-step:3762	 l-p:0.05615876987576485
epoch£º188	 i:3 	 global-step:3763	 l-p:0.05632910132408142
epoch£º188	 i:4 	 global-step:3764	 l-p:0.05687782168388367
epoch£º188	 i:5 	 global-step:3765	 l-p:0.056494034826755524
epoch£º188	 i:6 	 global-step:3766	 l-p:0.056225284934043884
epoch£º188	 i:7 	 global-step:3767	 l-p:0.05636387690901756
epoch£º188	 i:8 	 global-step:3768	 l-p:0.05715940520167351
epoch£º188	 i:9 	 global-step:3769	 l-p:0.05844685062766075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]])
 pt:tensor([[27.5217, 36.6046, 42.5395],
        [27.5217, 36.7171, 42.7955],
        [27.5217, 27.5221, 27.5218],
        [27.5217, 27.5218, 27.5218]], grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.05668564885854721 
model_pd.l_d.mean(): -0.00021775084314867854 
model_pd.lagr.mean(): 0.056467898190021515 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0007]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7716])), ('power', tensor([-0.3154]))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.05668564885854721
epoch£º189	 i:1 	 global-step:3781	 l-p:0.05658259987831116
epoch£º189	 i:2 	 global-step:3782	 l-p:0.056321319192647934
epoch£º189	 i:3 	 global-step:3783	 l-p:0.058049798011779785
epoch£º189	 i:4 	 global-step:3784	 l-p:0.05617183819413185
epoch£º189	 i:5 	 global-step:3785	 l-p:0.05687553808093071
epoch£º189	 i:6 	 global-step:3786	 l-p:0.05629688873887062
epoch£º189	 i:7 	 global-step:3787	 l-p:0.05727459862828255
epoch£º189	 i:8 	 global-step:3788	 l-p:0.056348226964473724
epoch£º189	 i:9 	 global-step:3789	 l-p:0.056289054453372955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]])
 pt:tensor([[27.4063, 27.4064, 27.4063],
        [27.4063, 27.4064, 27.4063],
        [27.4063, 27.4064, 27.4063],
        [27.4063, 27.7477, 27.4965]], grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.05642220377922058 
model_pd.l_d.mean(): -0.00026372200227342546 
model_pd.lagr.mean(): 0.056158483028411865 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0005]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8040])), ('power', tensor([-0.5283]))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.05642220377922058
epoch£º190	 i:1 	 global-step:3801	 l-p:0.05641436576843262
epoch£º190	 i:2 	 global-step:3802	 l-p:0.05722862109541893
epoch£º190	 i:3 	 global-step:3803	 l-p:0.056279078125953674
epoch£º190	 i:4 	 global-step:3804	 l-p:0.05630252882838249
epoch£º190	 i:5 	 global-step:3805	 l-p:0.05886419862508774
epoch£º190	 i:6 	 global-step:3806	 l-p:0.05634114146232605
epoch£º190	 i:7 	 global-step:3807	 l-p:0.056386709213256836
epoch£º190	 i:8 	 global-step:3808	 l-p:0.05664273723959923
epoch£º190	 i:9 	 global-step:3809	 l-p:0.05634576454758644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]])
 pt:tensor([[27.3372, 36.0653, 41.5919],
        [27.3372, 28.3065, 27.8361],
        [27.3372, 27.3387, 27.3372],
        [27.3372, 27.3372, 27.3372]], grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.05707326903939247 
model_pd.l_d.mean(): -6.50282236165367e-05 
model_pd.lagr.mean(): 0.05700824037194252 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.0002]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6932])), ('power', tensor([-0.2497]))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.05707326903939247
epoch£º191	 i:1 	 global-step:3821	 l-p:0.05639178678393364
epoch£º191	 i:2 	 global-step:3822	 l-p:0.05717591196298599
epoch£º191	 i:3 	 global-step:3823	 l-p:0.0567484013736248
epoch£º191	 i:4 	 global-step:3824	 l-p:0.0563296377658844
epoch£º191	 i:5 	 global-step:3825	 l-p:0.05652223154902458
epoch£º191	 i:6 	 global-step:3826	 l-p:0.05634603649377823
epoch£º191	 i:7 	 global-step:3827	 l-p:0.056393373757600784
epoch£º191	 i:8 	 global-step:3828	 l-p:0.0563267283141613
epoch£º191	 i:9 	 global-step:3829	 l-p:0.058056171983480453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]])
 pt:tensor([[27.3312, 27.7387, 27.4520],
        [27.3312, 27.6001, 27.3925],
        [27.3312, 35.4640, 40.2627],
        [27.3312, 27.8740, 27.5243]], grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.05640140548348427 
model_pd.l_d.mean(): -9.685786608315539e-07 
model_pd.lagr.mean(): 0.05640043690800667 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7904])), ('power', tensor([-0.5652]))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.05640140548348427
epoch£º192	 i:1 	 global-step:3841	 l-p:0.056420426815748215
epoch£º192	 i:2 	 global-step:3842	 l-p:0.0565761961042881
epoch£º192	 i:3 	 global-step:3843	 l-p:0.05621809884905815
epoch£º192	 i:4 	 global-step:3844	 l-p:0.057810600847005844
epoch£º192	 i:5 	 global-step:3845	 l-p:0.058167580515146255
epoch£º192	 i:6 	 global-step:3846	 l-p:0.056815195828676224
epoch£º192	 i:7 	 global-step:3847	 l-p:0.05624324828386307
epoch£º192	 i:8 	 global-step:3848	 l-p:0.05631996691226959
epoch£º192	 i:9 	 global-step:3849	 l-p:0.05630107969045639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]])
 pt:tensor([[27.3971, 35.0834, 39.3428],
        [27.3971, 27.7748, 27.5036],
        [27.3971, 27.4579, 27.4027],
        [27.3971, 27.4010, 27.3972]], grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.05627770721912384 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05627770721912384 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8171])), ('power', tensor([-0.5989]))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.05627770721912384
epoch£º193	 i:1 	 global-step:3861	 l-p:0.056165777146816254
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05652245879173279
epoch£º193	 i:3 	 global-step:3863	 l-p:0.05776747688651085
epoch£º193	 i:4 	 global-step:3864	 l-p:0.05635799467563629
epoch£º193	 i:5 	 global-step:3865	 l-p:0.056268833577632904
epoch£º193	 i:6 	 global-step:3866	 l-p:0.05684439837932587
epoch£º193	 i:7 	 global-step:3867	 l-p:0.056439805775880814
epoch£º193	 i:8 	 global-step:3868	 l-p:0.05812424048781395
epoch£º193	 i:9 	 global-step:3869	 l-p:0.05622091144323349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]])
 pt:tensor([[27.4901, 27.4901, 27.4901],
        [27.4901, 33.2868, 35.5049],
        [27.4901, 30.1796, 30.0379],
        [27.4901, 28.1755, 27.7720]], grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.056172046810388565 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056172046810388565 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8573])), ('power', tensor([-0.5682]))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.056172046810388565
epoch£º194	 i:1 	 global-step:3881	 l-p:0.056732840836048126
epoch£º194	 i:2 	 global-step:3882	 l-p:0.056227851659059525
epoch£º194	 i:3 	 global-step:3883	 l-p:0.05680825561285019
epoch£º194	 i:4 	 global-step:3884	 l-p:0.05615469813346863
epoch£º194	 i:5 	 global-step:3885	 l-p:0.05638439953327179
epoch£º194	 i:6 	 global-step:3886	 l-p:0.05665336176753044
epoch£º194	 i:7 	 global-step:3887	 l-p:0.05799058452248573
epoch£º194	 i:8 	 global-step:3888	 l-p:0.05714728683233261
epoch£º194	 i:9 	 global-step:3889	 l-p:0.05638530105352402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]])
 pt:tensor([[27.5950, 28.8149, 28.3160],
        [27.5950, 33.0371, 34.9029],
        [27.5950, 27.6679, 27.6024],
        [27.5950, 32.2412, 33.4025]], grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.05629343166947365 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05629343166947365 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([0.]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8162])), ('power', tensor([-0.3569]))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.05629343166947365
epoch£º195	 i:1 	 global-step:3901	 l-p:0.05727262794971466
epoch£º195	 i:2 	 global-step:3902	 l-p:0.05630912259221077
epoch£º195	 i:3 	 global-step:3903	 l-p:0.05645862966775894
epoch£º195	 i:4 	 global-step:3904	 l-p:0.05668766424059868
epoch£º195	 i:5 	 global-step:3905	 l-p:0.05615772679448128
epoch£º195	 i:6 	 global-step:3906	 l-p:0.05612209439277649
epoch£º195	 i:7 	 global-step:3907	 l-p:0.05640587955713272
epoch£º195	 i:8 	 global-step:3908	 l-p:0.05610613524913788
epoch£º195	 i:9 	 global-step:3909	 l-p:0.05848865956068039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]])
 pt:tensor([[27.7036, 27.7038, 27.7036],
        [27.7036, 27.8815, 27.7346],
        [27.7036, 27.7115, 27.7039],
        [27.7036, 27.8391, 27.7236]], grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.056467317044734955 
model_pd.l_d.mean(): -8.721638664610509e-07 
model_pd.lagr.mean(): 0.056466445326805115 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([8.4662e-06]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7497])), ('power', tensor([-0.0722]))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.056467317044734955
epoch£º196	 i:1 	 global-step:3921	 l-p:0.05630258098244667
epoch£º196	 i:2 	 global-step:3922	 l-p:0.05624667555093765
epoch£º196	 i:3 	 global-step:3923	 l-p:0.05704177916049957
epoch£º196	 i:4 	 global-step:3924	 l-p:0.056369371712207794
epoch£º196	 i:5 	 global-step:3925	 l-p:0.05610229820013046
epoch£º196	 i:6 	 global-step:3926	 l-p:0.05707741156220436
epoch£º196	 i:7 	 global-step:3927	 l-p:0.056354962289333344
epoch£º196	 i:8 	 global-step:3928	 l-p:0.056208740919828415
epoch£º196	 i:9 	 global-step:3929	 l-p:0.05777660757303238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]])
 pt:tensor([[27.8138, 27.8280, 27.8144],
        [27.8138, 32.7720, 34.1759],
        [27.8138, 27.8138, 27.8138],
        [27.8138, 28.1930, 27.9199]], grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.05715424567461014 
model_pd.l_d.mean(): 3.5671681075655215e-07 
model_pd.lagr.mean(): 0.05715460330247879 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([9.5591e-06]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7535])), ('power', tensor([0.1403]))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.05715424567461014
epoch£º197	 i:1 	 global-step:3941	 l-p:0.056171368807554245
epoch£º197	 i:2 	 global-step:3942	 l-p:0.056175075471401215
epoch£º197	 i:3 	 global-step:3943	 l-p:0.05679561197757721
epoch£º197	 i:4 	 global-step:3944	 l-p:0.05629350244998932
epoch£º197	 i:5 	 global-step:3945	 l-p:0.05605396255850792
epoch£º197	 i:6 	 global-step:3946	 l-p:0.05622004345059395
epoch£º197	 i:7 	 global-step:3947	 l-p:0.056520428508520126
epoch£º197	 i:8 	 global-step:3948	 l-p:0.0578787699341774
epoch£º197	 i:9 	 global-step:3949	 l-p:0.05633760988712311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]])
 pt:tensor([[27.9260, 28.4473, 28.1041],
        [27.9260, 27.9299, 27.9261],
        [27.9260, 27.9260, 27.9260],
        [27.9260, 27.9596, 27.9281]], grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.05616302415728569 
model_pd.l_d.mean(): -1.5840182641113643e-06 
model_pd.lagr.mean(): 0.056161440908908844 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([1.2947e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8472])), ('power', tensor([-0.0906]))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.05616302415728569
epoch£º198	 i:1 	 global-step:3961	 l-p:0.05652324855327606
epoch£º198	 i:2 	 global-step:3962	 l-p:0.05698493868112564
epoch£º198	 i:3 	 global-step:3963	 l-p:0.05613972991704941
epoch£º198	 i:4 	 global-step:3964	 l-p:0.05618699640035629
epoch£º198	 i:5 	 global-step:3965	 l-p:0.05614272505044937
epoch£º198	 i:6 	 global-step:3966	 l-p:0.056322142481803894
epoch£º198	 i:7 	 global-step:3967	 l-p:0.05630514770746231
epoch£º198	 i:8 	 global-step:3968	 l-p:0.05627692863345146
epoch£º198	 i:9 	 global-step:3969	 l-p:0.05820218101143837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]])
 pt:tensor([[28.0322, 28.0659, 28.0343],
        [28.0322, 28.3426, 28.1083],
        [28.0322, 28.0323, 28.0322],
        [28.0322, 30.0019, 29.5753]], grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.057724837213754654 
model_pd.l_d.mean(): 2.6047442588605918e-05 
model_pd.lagr.mean(): 0.0577508844435215 
model_pd.lambdas: dict_items([('pout', tensor([0.])), ('power', tensor([9.2682e-05]))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7897])), ('power', tensor([0.3454]))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.057724837213754654
epoch£º199	 i:1 	 global-step:3981	 l-p:0.05616552010178566
epoch£º199	 i:2 	 global-step:3982	 l-p:0.05635285750031471
epoch£º199	 i:3 	 global-step:3983	 l-p:0.056015219539403915
epoch£º199	 i:4 	 global-step:3984	 l-p:0.05608714744448662
epoch£º199	 i:5 	 global-step:3985	 l-p:0.056101828813552856
epoch£º199	 i:6 	 global-step:3986	 l-p:0.05617767944931984
epoch£º199	 i:7 	 global-step:3987	 l-p:0.05670743063092232
epoch£º199	 i:8 	 global-step:3988	 l-p:0.0566670261323452
epoch£º199	 i:9 	 global-step:3989	 l-p:0.056945182383060455
Traceback (most recent call last):
  File "D:\work\FC\Data_model_drive_direct-main\train\main.py", line 224, in <module>
    train()
  File "D:\work\FC\Data_model_drive_direct-main\train\main.py", line 201, in train
    plt.subplot(1,2,1, figsize=(4,5))
  File "D:\python\Lib\site-packages\matplotlib\pyplot.py", line 1311, in subplot
    ax = fig.add_subplot(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\Lib\site-packages\matplotlib\figure.py", line 743, in add_subplot
    ax = projection_class(self, *args, **pkw)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\Lib\site-packages\matplotlib\axes\_base.py", line 697, in __init__
    self._internal_update(kwargs)
  File "D:\python\Lib\site-packages\matplotlib\artist.py", line 1223, in _internal_update
    return self._update_props(
           ^^^^^^^^^^^^^^^^^^^
  File "D:\python\Lib\site-packages\matplotlib\artist.py", line 1197, in _update_props
    raise AttributeError(
AttributeError: Axes.set() got an unexpected keyword argument 'figsize'
