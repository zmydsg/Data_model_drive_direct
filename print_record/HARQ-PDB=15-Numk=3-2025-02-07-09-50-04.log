
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0014], device='cuda:0')), ('power', tensor([0.9990], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.2769811153411865
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802981376648
epoch£º0	 i:4 	 global-step:4	 l-p:-0.710580050945282
epoch£º0	 i:5 	 global-step:5	 l-p:-2.530930519104004
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505427598953247
epoch£º0	 i:7 	 global-step:7	 l-p:0.49168166518211365
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606585383415222
epoch£º0	 i:9 	 global-step:9	 l-p:0.056809842586517334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.37725773453712463 
model_pd.l_d.mean(): -18.738479614257812 
model_pd.lagr.mean(): -18.361221313476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0125], device='cuda:0')), ('power', tensor([0.9888], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.37725773453712463
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797576904297
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878846287727356
epoch£º1	 i:3 	 global-step:23	 l-p:0.24290229380130768
epoch£º1	 i:4 	 global-step:24	 l-p:0.13981004059314728
epoch£º1	 i:5 	 global-step:25	 l-p:0.1868826299905777
epoch£º1	 i:6 	 global-step:26	 l-p:0.177383691072464
epoch£º1	 i:7 	 global-step:27	 l-p:0.21870926022529602
epoch£º1	 i:8 	 global-step:28	 l-p:0.22566375136375427
epoch£º1	 i:9 	 global-step:29	 l-p:0.11669813096523285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9156, 5.9892, 6.5152],
        [4.9156, 5.0134, 4.9533],
        [4.9156, 5.7275, 5.9885],
        [4.9156, 4.9976, 4.9438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13358275592327118 
model_pd.l_d.mean(): -20.042667388916016 
model_pd.lagr.mean(): -19.90908432006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0189], device='cuda:0')), ('power', tensor([0.9785], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446], device='cuda:0')), ('power', tensor([-20.9243], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13358275592327118
epoch£º2	 i:1 	 global-step:41	 l-p:0.10819091647863388
epoch£º2	 i:2 	 global-step:42	 l-p:0.1034247949719429
epoch£º2	 i:3 	 global-step:43	 l-p:0.10659708082675934
epoch£º2	 i:4 	 global-step:44	 l-p:0.11851216852664948
epoch£º2	 i:5 	 global-step:45	 l-p:0.10561671108007431
epoch£º2	 i:6 	 global-step:46	 l-p:0.1121269166469574
epoch£º2	 i:7 	 global-step:47	 l-p:0.6113379001617432
epoch£º2	 i:8 	 global-step:48	 l-p:0.10728442668914795
epoch£º2	 i:9 	 global-step:49	 l-p:0.11701169610023499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4145, 6.5084, 6.9827],
        [5.4145, 5.4657, 5.4267],
        [5.4145, 5.5209, 5.4549],
        [5.4145, 6.9929, 8.0320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08024417608976364 
model_pd.l_d.mean(): -18.32460594177246 
model_pd.lagr.mean(): -18.244361877441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0226], device='cuda:0')), ('power', tensor([0.9684], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153], device='cuda:0')), ('power', tensor([-19.3421], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08024417608976364
epoch£º3	 i:1 	 global-step:61	 l-p:0.0847095251083374
epoch£º3	 i:2 	 global-step:62	 l-p:-0.10688630491495132
epoch£º3	 i:3 	 global-step:63	 l-p:0.1214245930314064
epoch£º3	 i:4 	 global-step:64	 l-p:0.11789831519126892
epoch£º3	 i:5 	 global-step:65	 l-p:0.11609981209039688
epoch£º3	 i:6 	 global-step:66	 l-p:0.11234921962022781
epoch£º3	 i:7 	 global-step:67	 l-p:0.11608638614416122
epoch£º3	 i:8 	 global-step:68	 l-p:0.124699167907238
epoch£º3	 i:9 	 global-step:69	 l-p:0.12293625622987747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0105, 5.7389, 5.9174],
        [5.0105, 5.9001, 6.2238],
        [5.0105, 6.5617, 7.6554],
        [5.0105, 5.0994, 5.0424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11384843289852142 
model_pd.l_d.mean(): -18.45567512512207 
model_pd.lagr.mean(): -18.341827392578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0263], device='cuda:0')), ('power', tensor([0.9582], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-19.7508], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11384843289852142
epoch£º4	 i:1 	 global-step:81	 l-p:0.13007698953151703
epoch£º4	 i:2 	 global-step:82	 l-p:0.1610340029001236
epoch£º4	 i:3 	 global-step:83	 l-p:0.1309719830751419
epoch£º4	 i:4 	 global-step:84	 l-p:0.15025097131729126
epoch£º4	 i:5 	 global-step:85	 l-p:0.12508077919483185
epoch£º4	 i:6 	 global-step:86	 l-p:0.1283344328403473
epoch£º4	 i:7 	 global-step:87	 l-p:0.13245773315429688
epoch£º4	 i:8 	 global-step:88	 l-p:0.1381160169839859
epoch£º4	 i:9 	 global-step:89	 l-p:0.14840257167816162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6940, 4.6942, 4.6940],
        [4.6940, 4.7002, 4.6945],
        [4.6940, 4.7884, 4.7309],
        [4.6940, 4.8293, 4.7606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.1720057874917984 
model_pd.l_d.mean(): -19.251270294189453 
model_pd.lagr.mean(): -19.07926368713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0311], device='cuda:0')), ('power', tensor([0.9479], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5294], device='cuda:0')), ('power', tensor([-20.8624], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.1720057874917984
epoch£º5	 i:1 	 global-step:101	 l-p:0.1446024626493454
epoch£º5	 i:2 	 global-step:102	 l-p:0.15315289795398712
epoch£º5	 i:3 	 global-step:103	 l-p:0.01721961423754692
epoch£º5	 i:4 	 global-step:104	 l-p:0.13105958700180054
epoch£º5	 i:5 	 global-step:105	 l-p:0.14825400710105896
epoch£º5	 i:6 	 global-step:106	 l-p:0.1183343380689621
epoch£º5	 i:7 	 global-step:107	 l-p:0.13759227097034454
epoch£º5	 i:8 	 global-step:108	 l-p:0.13528446853160858
epoch£º5	 i:9 	 global-step:109	 l-p:0.13510741293430328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 4.9172, 4.8943],
        [4.8894, 4.8894, 4.8894],
        [4.8894, 6.3054, 7.2501],
        [4.8894, 4.8942, 4.8897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.130707785487175 
model_pd.l_d.mean(): -19.49369239807129 
model_pd.lagr.mean(): -19.36298370361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0360], device='cuda:0')), ('power', tensor([0.9376], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-21.2313], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.130707785487175
epoch£º6	 i:1 	 global-step:121	 l-p:0.1298673301935196
epoch£º6	 i:2 	 global-step:122	 l-p:0.13716676831245422
epoch£º6	 i:3 	 global-step:123	 l-p:0.1369762420654297
epoch£º6	 i:4 	 global-step:124	 l-p:0.12771470844745636
epoch£º6	 i:5 	 global-step:125	 l-p:0.1463506668806076
epoch£º6	 i:6 	 global-step:126	 l-p:0.13424725830554962
epoch£º6	 i:7 	 global-step:127	 l-p:0.1264638751745224
epoch£º6	 i:8 	 global-step:128	 l-p:0.1280180960893631
epoch£º6	 i:9 	 global-step:129	 l-p:0.12071317434310913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9571, 5.6186, 5.7504],
        [4.9571, 5.0206, 4.9756],
        [4.9571, 4.9571, 4.9571],
        [4.9571, 4.9785, 4.9602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12212314456701279 
model_pd.l_d.mean(): -19.144271850585938 
model_pd.lagr.mean(): -19.02214813232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0406], device='cuda:0')), ('power', tensor([0.9274], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4173], device='cuda:0')), ('power', tensor([-21.0879], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12212314456701279
epoch£º7	 i:1 	 global-step:141	 l-p:0.11341287195682526
epoch£º7	 i:2 	 global-step:142	 l-p:0.12858569622039795
epoch£º7	 i:3 	 global-step:143	 l-p:0.12901030480861664
epoch£º7	 i:4 	 global-step:144	 l-p:0.14053748548030853
epoch£º7	 i:5 	 global-step:145	 l-p:0.18343961238861084
epoch£º7	 i:6 	 global-step:146	 l-p:0.12439680099487305
epoch£º7	 i:7 	 global-step:147	 l-p:0.13204346597194672
epoch£º7	 i:8 	 global-step:148	 l-p:0.14843466877937317
epoch£º7	 i:9 	 global-step:149	 l-p:0.1348680704832077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7970, 4.7970, 4.7970],
        [4.7970, 4.7971, 4.7970],
        [4.7970, 5.6147, 5.8991],
        [4.7970, 4.9434, 4.8721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1275399625301361 
model_pd.l_d.mean(): -17.363147735595703 
model_pd.lagr.mean(): -17.235607147216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0454], device='cuda:0')), ('power', tensor([0.9172], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5430], device='cuda:0')), ('power', tensor([-19.5287], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.1275399625301361
epoch£º8	 i:1 	 global-step:161	 l-p:0.25699350237846375
epoch£º8	 i:2 	 global-step:162	 l-p:0.13293126225471497
epoch£º8	 i:3 	 global-step:163	 l-p:0.1356595903635025
epoch£º8	 i:4 	 global-step:164	 l-p:0.1209988221526146
epoch£º8	 i:5 	 global-step:165	 l-p:0.1383485645055771
epoch£º8	 i:6 	 global-step:166	 l-p:0.12211733311414719
epoch£º8	 i:7 	 global-step:167	 l-p:0.11606936156749725
epoch£º8	 i:8 	 global-step:168	 l-p:0.13333594799041748
epoch£º8	 i:9 	 global-step:169	 l-p:0.13291336596012115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9562, 4.9580, 4.9562],
        [4.9562, 4.9582, 4.9562],
        [4.9562, 5.1072, 5.0335],
        [4.9562, 4.9774, 4.9593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.1278313547372818 
model_pd.l_d.mean(): -16.168113708496094 
model_pd.lagr.mean(): -16.040283203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0501], device='cuda:0')), ('power', tensor([0.9070], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5306], device='cuda:0')), ('power', tensor([-18.4214], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.1278313547372818
epoch£º9	 i:1 	 global-step:181	 l-p:0.12110177427530289
epoch£º9	 i:2 	 global-step:182	 l-p:0.13342233002185822
epoch£º9	 i:3 	 global-step:183	 l-p:0.1254664808511734
epoch£º9	 i:4 	 global-step:184	 l-p:0.11723863333463669
epoch£º9	 i:5 	 global-step:185	 l-p:0.14049015939235687
epoch£º9	 i:6 	 global-step:186	 l-p:0.1481662541627884
epoch£º9	 i:7 	 global-step:187	 l-p:0.12391209602355957
epoch£º9	 i:8 	 global-step:188	 l-p:0.12250912189483643
epoch£º9	 i:9 	 global-step:189	 l-p:0.10549133270978928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8670, 6.1089, 6.8449],
        [4.8670, 5.7141, 6.0203],
        [4.8670, 4.8721, 4.8673],
        [4.8670, 4.8670, 4.8670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.14527221024036407 
model_pd.l_d.mean(): -18.279726028442383 
model_pd.lagr.mean(): -18.13445472717285 
model_pd.lambdas: dict_items([('pout', tensor([1.0545], device='cuda:0')), ('power', tensor([0.8966], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4666], device='cuda:0')), ('power', tensor([-20.9112], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.14527221024036407
epoch£º10	 i:1 	 global-step:201	 l-p:0.13420715928077698
epoch£º10	 i:2 	 global-step:202	 l-p:0.11234896630048752
epoch£º10	 i:3 	 global-step:203	 l-p:0.1279735416173935
epoch£º10	 i:4 	 global-step:204	 l-p:0.12984220683574677
epoch£º10	 i:5 	 global-step:205	 l-p:0.1333138793706894
epoch£º10	 i:6 	 global-step:206	 l-p:0.13947667181491852
epoch£º10	 i:7 	 global-step:207	 l-p:0.1509941667318344
epoch£º10	 i:8 	 global-step:208	 l-p:0.1854327768087387
epoch£º10	 i:9 	 global-step:209	 l-p:0.13565859198570251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8955, 5.4279, 5.4751],
        [4.8955, 6.2353, 7.0875],
        [4.8955, 5.6243, 5.8207],
        [4.8955, 4.9191, 4.8992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.10967278480529785 
model_pd.l_d.mean(): -17.938800811767578 
model_pd.lagr.mean(): -17.82912826538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0593], device='cuda:0')), ('power', tensor([0.8864], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([-20.7703], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.10967278480529785
epoch£º11	 i:1 	 global-step:221	 l-p:0.14238719642162323
epoch£º11	 i:2 	 global-step:222	 l-p:0.11821670085191727
epoch£º11	 i:3 	 global-step:223	 l-p:0.11465343087911606
epoch£º11	 i:4 	 global-step:224	 l-p:0.12878970801830292
epoch£º11	 i:5 	 global-step:225	 l-p:0.13083764910697937
epoch£º11	 i:6 	 global-step:226	 l-p:0.11910480260848999
epoch£º11	 i:7 	 global-step:227	 l-p:0.15166644752025604
epoch£º11	 i:8 	 global-step:228	 l-p:0.11703786253929138
epoch£º11	 i:9 	 global-step:229	 l-p:0.14556296169757843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9956, 5.4058, 5.3796],
        [4.9956, 5.0330, 5.0035],
        [4.9956, 5.0273, 5.0016],
        [4.9956, 5.1692, 5.0927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.1175123006105423 
model_pd.l_d.mean(): -17.062349319458008 
model_pd.lagr.mean(): -16.94483757019043 
model_pd.lambdas: dict_items([('pout', tensor([1.0639], device='cuda:0')), ('power', tensor([0.8762], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4977], device='cuda:0')), ('power', tensor([-20.0546], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.1175123006105423
epoch£º12	 i:1 	 global-step:241	 l-p:0.1330374777317047
epoch£º12	 i:2 	 global-step:242	 l-p:0.11821039766073227
epoch£º12	 i:3 	 global-step:243	 l-p:0.13416442275047302
epoch£º12	 i:4 	 global-step:244	 l-p:0.13479922711849213
epoch£º12	 i:5 	 global-step:245	 l-p:0.1366400122642517
epoch£º12	 i:6 	 global-step:246	 l-p:0.13386352360248566
epoch£º12	 i:7 	 global-step:247	 l-p:0.13030801713466644
epoch£º12	 i:8 	 global-step:248	 l-p:0.13386903703212738
epoch£º12	 i:9 	 global-step:249	 l-p:0.09318532049655914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9364, 6.1023, 6.7399],
        [4.9364, 4.9364, 4.9364],
        [4.9364, 4.9398, 4.9366],
        [4.9364, 5.3393, 5.3134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.11847712844610214 
model_pd.l_d.mean(): -16.30927276611328 
model_pd.lagr.mean(): -16.1907958984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0684], device='cuda:0')), ('power', tensor([0.8660], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5031], device='cuda:0')), ('power', tensor([-19.4319], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.11847712844610214
epoch£º13	 i:1 	 global-step:261	 l-p:0.15391451120376587
epoch£º13	 i:2 	 global-step:262	 l-p:0.12063952535390854
epoch£º13	 i:3 	 global-step:263	 l-p:0.12584997713565826
epoch£º13	 i:4 	 global-step:264	 l-p:0.1320597380399704
epoch£º13	 i:5 	 global-step:265	 l-p:0.13293150067329407
epoch£º13	 i:6 	 global-step:266	 l-p:0.1062014177441597
epoch£º13	 i:7 	 global-step:267	 l-p:0.12257245928049088
epoch£º13	 i:8 	 global-step:268	 l-p:0.1259191483259201
epoch£º13	 i:9 	 global-step:269	 l-p:0.11576381325721741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9871, 4.9871, 4.9871],
        [4.9871, 5.9177, 6.2949],
        [4.9871, 5.0224, 4.9943],
        [4.9871, 5.8493, 6.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.13664411008358002 
model_pd.l_d.mean(): -16.009489059448242 
model_pd.lagr.mean(): -15.872844696044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0727], device='cuda:0')), ('power', tensor([0.8558], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-19.2862], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.13664411008358002
epoch£º14	 i:1 	 global-step:281	 l-p:0.1162281185388565
epoch£º14	 i:2 	 global-step:282	 l-p:0.147361621260643
epoch£º14	 i:3 	 global-step:283	 l-p:0.12748898565769196
epoch£º14	 i:4 	 global-step:284	 l-p:0.126737579703331
epoch£º14	 i:5 	 global-step:285	 l-p:0.11566461622714996
epoch£º14	 i:6 	 global-step:286	 l-p:0.1085280105471611
epoch£º14	 i:7 	 global-step:287	 l-p:0.119514100253582
epoch£º14	 i:8 	 global-step:288	 l-p:0.1105048656463623
epoch£º14	 i:9 	 global-step:289	 l-p:0.14763487875461578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9416, 4.9741, 4.9480],
        [4.9416, 4.9455, 4.9418],
        [4.9416, 5.1209, 5.0455],
        [4.9416, 4.9527, 4.9428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.13809196650981903 
model_pd.l_d.mean(): -17.45781707763672 
model_pd.lagr.mean(): -17.319725036621094 
model_pd.lambdas: dict_items([('pout', tensor([1.0771], device='cuda:0')), ('power', tensor([0.8454], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4145], device='cuda:0')), ('power', tensor([-21.1510], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.13809196650981903
epoch£º15	 i:1 	 global-step:301	 l-p:0.13601703941822052
epoch£º15	 i:2 	 global-step:302	 l-p:0.10884491354227066
epoch£º15	 i:3 	 global-step:303	 l-p:0.13043412566184998
epoch£º15	 i:4 	 global-step:304	 l-p:0.1386989802122116
epoch£º15	 i:5 	 global-step:305	 l-p:0.12941522896289825
epoch£º15	 i:6 	 global-step:306	 l-p:0.10909561067819595
epoch£º15	 i:7 	 global-step:307	 l-p:0.13085293769836426
epoch£º15	 i:8 	 global-step:308	 l-p:0.126195028424263
epoch£º15	 i:9 	 global-step:309	 l-p:0.14151671528816223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9816, 4.9836, 4.9817],
        [4.9816, 5.0085, 4.9863],
        [4.9816, 4.9820, 4.9816],
        [4.9816, 5.3495, 5.3085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.11294925212860107 
model_pd.l_d.mean(): -15.955671310424805 
model_pd.lagr.mean(): -15.842721939086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0817], device='cuda:0')), ('power', tensor([0.8353], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4541], device='cuda:0')), ('power', tensor([-19.6672], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.11294925212860107
epoch£º16	 i:1 	 global-step:321	 l-p:0.11009013652801514
epoch£º16	 i:2 	 global-step:322	 l-p:0.14144456386566162
epoch£º16	 i:3 	 global-step:323	 l-p:0.1271921694278717
epoch£º16	 i:4 	 global-step:324	 l-p:0.11710189282894135
epoch£º16	 i:5 	 global-step:325	 l-p:0.10888242721557617
epoch£º16	 i:6 	 global-step:326	 l-p:0.12445797026157379
epoch£º16	 i:7 	 global-step:327	 l-p:0.13425429165363312
epoch£º16	 i:8 	 global-step:328	 l-p:0.12269699573516846
epoch£º16	 i:9 	 global-step:329	 l-p:0.12243320792913437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0588, 6.0214, 6.4246],
        [5.0588, 5.3848, 5.3267],
        [5.0588, 5.0589, 5.0588],
        [5.0588, 6.0088, 6.3992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.13457800447940826 
model_pd.l_d.mean(): -16.72161865234375 
model_pd.lagr.mean(): -16.587039947509766 
model_pd.lambdas: dict_items([('pout', tensor([1.0859], device='cuda:0')), ('power', tensor([0.8250], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116], device='cuda:0')), ('power', tensor([-20.7843], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.13457800447940826
epoch£º17	 i:1 	 global-step:341	 l-p:0.11440305411815643
epoch£º17	 i:2 	 global-step:342	 l-p:0.12820769846439362
epoch£º17	 i:3 	 global-step:343	 l-p:0.1248524859547615
epoch£º17	 i:4 	 global-step:344	 l-p:0.1233169212937355
epoch£º17	 i:5 	 global-step:345	 l-p:0.11871825903654099
epoch£º17	 i:6 	 global-step:346	 l-p:0.1279750019311905
epoch£º17	 i:7 	 global-step:347	 l-p:0.13184480369091034
epoch£º17	 i:8 	 global-step:348	 l-p:0.11971887201070786
epoch£º17	 i:9 	 global-step:349	 l-p:0.12284132093191147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9387, 4.9890, 4.9518],
        [4.9387, 4.9389, 4.9387],
        [4.9387, 5.0554, 4.9907],
        [4.9387, 4.9516, 4.9401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.1274343878030777 
model_pd.l_d.mean(): -16.398231506347656 
model_pd.lagr.mean(): -16.270797729492188 
model_pd.lambdas: dict_items([('pout', tensor([1.0904], device='cuda:0')), ('power', tensor([0.8148], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.7180], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.1274343878030777
epoch£º18	 i:1 	 global-step:361	 l-p:0.14928469061851501
epoch£º18	 i:2 	 global-step:362	 l-p:0.12172544002532959
epoch£º18	 i:3 	 global-step:363	 l-p:0.140678271651268
epoch£º18	 i:4 	 global-step:364	 l-p:0.11478009819984436
epoch£º18	 i:5 	 global-step:365	 l-p:0.12241889536380768
epoch£º18	 i:6 	 global-step:366	 l-p:0.1214725449681282
epoch£º18	 i:7 	 global-step:367	 l-p:0.12800122797489166
epoch£º18	 i:8 	 global-step:368	 l-p:0.11947080492973328
epoch£º18	 i:9 	 global-step:369	 l-p:0.12146024405956268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0219, 5.2052, 5.1293],
        [5.0219, 5.0532, 5.0279],
        [5.0219, 5.0219, 5.0219],
        [5.0219, 5.2398, 5.1639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.12954048812389374 
model_pd.l_d.mean(): -16.55906105041504 
model_pd.lagr.mean(): -16.429519653320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0947], device='cuda:0')), ('power', tensor([0.8045], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3864], device='cuda:0')), ('power', tensor([-21.0810], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.12954048812389374
epoch£º19	 i:1 	 global-step:381	 l-p:0.12365111708641052
epoch£º19	 i:2 	 global-step:382	 l-p:0.11882483959197998
epoch£º19	 i:3 	 global-step:383	 l-p:0.11885599046945572
epoch£º19	 i:4 	 global-step:384	 l-p:0.11091865599155426
epoch£º19	 i:5 	 global-step:385	 l-p:0.12287484854459763
epoch£º19	 i:6 	 global-step:386	 l-p:0.11708089709281921
epoch£º19	 i:7 	 global-step:387	 l-p:0.11340492963790894
epoch£º19	 i:8 	 global-step:388	 l-p:0.0991291031241417
epoch£º19	 i:9 	 global-step:389	 l-p:0.13743703067302704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1477, 5.1505, 5.1478],
        [5.1477, 5.5937, 5.5819],
        [5.1477, 5.1923, 5.1582],
        [5.1477, 5.2121, 5.1668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.11412138491868973 
model_pd.l_d.mean(): -16.30812644958496 
model_pd.lagr.mean(): -16.194005966186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0989], device='cuda:0')), ('power', tensor([0.7943], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3647], device='cuda:0')), ('power', tensor([-21.0081], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.11412138491868973
epoch£º20	 i:1 	 global-step:401	 l-p:0.11554765701293945
epoch£º20	 i:2 	 global-step:402	 l-p:0.11031583696603775
epoch£º20	 i:3 	 global-step:403	 l-p:0.1341754049062729
epoch£º20	 i:4 	 global-step:404	 l-p:0.12906257808208466
epoch£º20	 i:5 	 global-step:405	 l-p:0.12351127713918686
epoch£º20	 i:6 	 global-step:406	 l-p:0.11649370193481445
epoch£º20	 i:7 	 global-step:407	 l-p:0.11655128002166748
epoch£º20	 i:8 	 global-step:408	 l-p:0.11524517089128494
epoch£º20	 i:9 	 global-step:409	 l-p:0.11078933626413345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1090, 6.5883, 7.5855],
        [5.1090, 5.1735, 5.1284],
        [5.1090, 5.1090, 5.1090],
        [5.1090, 5.2140, 5.1519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.12140897661447525 
model_pd.l_d.mean(): -15.969846725463867 
model_pd.lagr.mean(): -15.848437309265137 
model_pd.lambdas: dict_items([('pout', tensor([1.1031], device='cuda:0')), ('power', tensor([0.7841], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3914], device='cuda:0')), ('power', tensor([-20.8900], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.12140897661447525
epoch£º21	 i:1 	 global-step:421	 l-p:0.1160854697227478
epoch£º21	 i:2 	 global-step:422	 l-p:0.12436264753341675
epoch£º21	 i:3 	 global-step:423	 l-p:0.11515067517757416
epoch£º21	 i:4 	 global-step:424	 l-p:0.13087408244609833
epoch£º21	 i:5 	 global-step:425	 l-p:0.1149250864982605
epoch£º21	 i:6 	 global-step:426	 l-p:0.12985995411872864
epoch£º21	 i:7 	 global-step:427	 l-p:0.15200605988502502
epoch£º21	 i:8 	 global-step:428	 l-p:0.1175551638007164
epoch£º21	 i:9 	 global-step:429	 l-p:0.1208374947309494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9902, 5.0939, 5.0331],
        [4.9902, 5.2922, 5.2319],
        [4.9902, 4.9925, 4.9903],
        [4.9902, 5.0227, 4.9967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.13586996495723724 
model_pd.l_d.mean(): -15.901678085327148 
model_pd.lagr.mean(): -15.76580810546875 
model_pd.lambdas: dict_items([('pout', tensor([1.1074], device='cuda:0')), ('power', tensor([0.7739], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4043], device='cuda:0')), ('power', tensor([-21.0982], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.13586996495723724
epoch£º22	 i:1 	 global-step:441	 l-p:0.12376663088798523
epoch£º22	 i:2 	 global-step:442	 l-p:0.12214835733175278
epoch£º22	 i:3 	 global-step:443	 l-p:0.12448477745056152
epoch£º22	 i:4 	 global-step:444	 l-p:0.10098813474178314
epoch£º22	 i:5 	 global-step:445	 l-p:0.12878677248954773
epoch£º22	 i:6 	 global-step:446	 l-p:0.10992828011512756
epoch£º22	 i:7 	 global-step:447	 l-p:0.12861141562461853
epoch£º22	 i:8 	 global-step:448	 l-p:0.12274430692195892
epoch£º22	 i:9 	 global-step:449	 l-p:0.09828466176986694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2184, 6.7284, 7.7437],
        [5.2184, 5.8037, 5.8717],
        [5.2184, 5.2190, 5.2184],
        [5.2184, 6.2006, 6.6083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.08608138561248779 
model_pd.l_d.mean(): -15.294421195983887 
model_pd.lagr.mean(): -15.20833969116211 
model_pd.lambdas: dict_items([('pout', tensor([1.1116], device='cuda:0')), ('power', tensor([0.7637], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3868], device='cuda:0')), ('power', tensor([-20.5629], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.08608138561248779
epoch£º23	 i:1 	 global-step:461	 l-p:0.11855054646730423
epoch£º23	 i:2 	 global-step:462	 l-p:0.12101615965366364
epoch£º23	 i:3 	 global-step:463	 l-p:0.09896612167358398
epoch£º23	 i:4 	 global-step:464	 l-p:0.11787409335374832
epoch£º23	 i:5 	 global-step:465	 l-p:0.1093149185180664
epoch£º23	 i:6 	 global-step:466	 l-p:0.09520889073610306
epoch£º23	 i:7 	 global-step:467	 l-p:0.10992183536291122
epoch£º23	 i:8 	 global-step:468	 l-p:0.12078875303268433
epoch£º23	 i:9 	 global-step:469	 l-p:0.12276769429445267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1988, 5.6896, 5.7010],
        [5.1988, 5.5096, 5.4454],
        [5.1988, 5.2867, 5.2307],
        [5.1988, 5.1988, 5.1988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.12197691947221756 
model_pd.l_d.mean(): -15.785086631774902 
model_pd.lagr.mean(): -15.66310977935791 
model_pd.lambdas: dict_items([('pout', tensor([1.1153], device='cuda:0')), ('power', tensor([0.7534], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3044], device='cuda:0')), ('power', tensor([-21.3707], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.12197691947221756
epoch£º24	 i:1 	 global-step:481	 l-p:0.11418993771076202
epoch£º24	 i:2 	 global-step:482	 l-p:0.1065484881401062
epoch£º24	 i:3 	 global-step:483	 l-p:0.09728126227855682
epoch£º24	 i:4 	 global-step:484	 l-p:0.1119534820318222
epoch£º24	 i:5 	 global-step:485	 l-p:0.10319492965936661
epoch£º24	 i:6 	 global-step:486	 l-p:0.12057968974113464
epoch£º24	 i:7 	 global-step:487	 l-p:0.12351809442043304
epoch£º24	 i:8 	 global-step:488	 l-p:0.12615247070789337
epoch£º24	 i:9 	 global-step:489	 l-p:0.10796462744474411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1790, 5.1790, 5.1790],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 6.3921, 7.0561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.11529262363910675 
model_pd.l_d.mean(): -13.646269798278809 
model_pd.lagr.mean(): -13.530977249145508 
model_pd.lambdas: dict_items([('pout', tensor([1.1194], device='cuda:0')), ('power', tensor([0.7434], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-19.0843], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.11529262363910675
epoch£º25	 i:1 	 global-step:501	 l-p:0.1173107698559761
epoch£º25	 i:2 	 global-step:502	 l-p:0.12022025138139725
epoch£º25	 i:3 	 global-step:503	 l-p:0.10987750440835953
epoch£º25	 i:4 	 global-step:504	 l-p:0.12923935055732727
epoch£º25	 i:5 	 global-step:505	 l-p:0.12474828213453293
epoch£º25	 i:6 	 global-step:506	 l-p:0.116696797311306
epoch£º25	 i:7 	 global-step:507	 l-p:0.10702008754014969
epoch£º25	 i:8 	 global-step:508	 l-p:0.12808609008789062
epoch£º25	 i:9 	 global-step:509	 l-p:0.116221584379673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0837, 5.2865, 5.2107],
        [5.0837, 5.7214, 5.8398],
        [5.0837, 5.1468, 5.1027],
        [5.0837, 5.6338, 5.6902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.1316223442554474 
model_pd.l_d.mean(): -15.073952674865723 
model_pd.lagr.mean(): -14.942330360412598 
model_pd.lambdas: dict_items([('pout', tensor([1.1234], device='cuda:0')), ('power', tensor([0.7331], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3785], device='cuda:0')), ('power', tensor([-21.1125], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.1316223442554474
epoch£º26	 i:1 	 global-step:521	 l-p:0.11727448552846909
epoch£º26	 i:2 	 global-step:522	 l-p:0.1089160144329071
epoch£º26	 i:3 	 global-step:523	 l-p:0.11299801617860794
epoch£º26	 i:4 	 global-step:524	 l-p:0.11221390217542648
epoch£º26	 i:5 	 global-step:525	 l-p:0.12962447106838226
epoch£º26	 i:6 	 global-step:526	 l-p:0.12130329012870789
epoch£º26	 i:7 	 global-step:527	 l-p:0.12078431993722916
epoch£º26	 i:8 	 global-step:528	 l-p:0.12175322324037552
epoch£º26	 i:9 	 global-step:529	 l-p:0.11175708472728729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2347, 5.3622, 5.2933],
        [5.2347, 5.5978, 5.5494],
        [5.2347, 5.2347, 5.2347],
        [5.2347, 5.2375, 5.2349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.10817529261112213 
model_pd.l_d.mean(): -13.220854759216309 
model_pd.lagr.mean(): -13.112679481506348 
model_pd.lambdas: dict_items([('pout', tensor([1.1276], device='cuda:0')), ('power', tensor([0.7230], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4419], device='cuda:0')), ('power', tensor([-18.9515], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.10817529261112213
epoch£º27	 i:1 	 global-step:541	 l-p:0.11035982519388199
epoch£º27	 i:2 	 global-step:542	 l-p:0.09588917344808578
epoch£º27	 i:3 	 global-step:543	 l-p:0.11369254440069199
epoch£º27	 i:4 	 global-step:544	 l-p:0.10885409265756607
epoch£º27	 i:5 	 global-step:545	 l-p:0.11129139363765717
epoch£º27	 i:6 	 global-step:546	 l-p:0.054359935224056244
epoch£º27	 i:7 	 global-step:547	 l-p:0.11320777982473373
epoch£º27	 i:8 	 global-step:548	 l-p:0.11491820216178894
epoch£º27	 i:9 	 global-step:549	 l-p:0.09366496652364731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3853, 5.6127, 5.5321],
        [5.3853, 6.0651, 6.1905],
        [5.3853, 5.4404, 5.3999],
        [5.3853, 6.8756, 7.8362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.08639419078826904 
model_pd.l_d.mean(): -14.139655113220215 
model_pd.lagr.mean(): -14.053260803222656 
model_pd.lambdas: dict_items([('pout', tensor([1.1310], device='cuda:0')), ('power', tensor([0.7127], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3567], device='cuda:0')), ('power', tensor([-20.3756], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.08639419078826904
epoch£º28	 i:1 	 global-step:561	 l-p:0.12282613664865494
epoch£º28	 i:2 	 global-step:562	 l-p:0.10943107306957245
epoch£º28	 i:3 	 global-step:563	 l-p:0.05643943324685097
epoch£º28	 i:4 	 global-step:564	 l-p:0.11158803850412369
epoch£º28	 i:5 	 global-step:565	 l-p:0.09093745052814484
epoch£º28	 i:6 	 global-step:566	 l-p:0.09883676469326019
epoch£º28	 i:7 	 global-step:567	 l-p:0.10614236444234848
epoch£º28	 i:8 	 global-step:568	 l-p:0.11169826984405518
epoch£º28	 i:9 	 global-step:569	 l-p:0.12041677534580231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3915, 5.3915, 5.3915],
        [5.3915, 5.6172, 5.5366],
        [5.3915, 6.1048, 6.2556],
        [5.3915, 5.3963, 5.3918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.11112739145755768 
model_pd.l_d.mean(): -13.793549537658691 
model_pd.lagr.mean(): -13.682421684265137 
model_pd.lambdas: dict_items([('pout', tensor([1.1344], device='cuda:0')), ('power', tensor([0.7026], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3516], device='cuda:0')), ('power', tensor([-20.1711], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.11112739145755768
epoch£º29	 i:1 	 global-step:581	 l-p:0.0774233341217041
epoch£º29	 i:2 	 global-step:582	 l-p:0.07341570407152176
epoch£º29	 i:3 	 global-step:583	 l-p:0.11600358784198761
epoch£º29	 i:4 	 global-step:584	 l-p:0.09307548403739929
epoch£º29	 i:5 	 global-step:585	 l-p:0.11639872938394547
epoch£º29	 i:6 	 global-step:586	 l-p:0.10867927223443985
epoch£º29	 i:7 	 global-step:587	 l-p:0.1036926656961441
epoch£º29	 i:8 	 global-step:588	 l-p:0.10821042954921722
epoch£º29	 i:9 	 global-step:589	 l-p:0.11150039732456207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3868, 5.3886, 5.3868],
        [5.3868, 6.8611, 7.8027],
        [5.3868, 5.6308, 5.5514],
        [5.3868, 6.7258, 7.5039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.10502997040748596 
model_pd.l_d.mean(): -13.083578109741211 
model_pd.lagr.mean(): -12.978548049926758 
model_pd.lambdas: dict_items([('pout', tensor([1.1380], device='cuda:0')), ('power', tensor([0.6925], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-19.5526], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.10502997040748596
epoch£º30	 i:1 	 global-step:601	 l-p:0.09952211380004883
epoch£º30	 i:2 	 global-step:602	 l-p:0.10810583829879761
epoch£º30	 i:3 	 global-step:603	 l-p:0.08747519552707672
epoch£º30	 i:4 	 global-step:604	 l-p:0.10830472409725189
epoch£º30	 i:5 	 global-step:605	 l-p:0.11918645352125168
epoch£º30	 i:6 	 global-step:606	 l-p:0.10634446889162064
epoch£º30	 i:7 	 global-step:607	 l-p:0.09925433993339539
epoch£º30	 i:8 	 global-step:608	 l-p:0.10271213948726654
epoch£º30	 i:9 	 global-step:609	 l-p:0.11814448982477188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2433, 5.2433, 5.2433],
        [5.2433, 5.2524, 5.2441],
        [5.2433, 6.7469, 7.7554],
        [5.2433, 5.3053, 5.2614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.1006014421582222 
model_pd.l_d.mean(): -12.313494682312012 
model_pd.lagr.mean(): -12.21289348602295 
model_pd.lambdas: dict_items([('pout', tensor([1.1416], device='cuda:0')), ('power', tensor([0.6823], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4949], device='cuda:0')), ('power', tensor([-18.8475], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.1006014421582222
epoch£º31	 i:1 	 global-step:621	 l-p:0.09915563464164734
epoch£º31	 i:2 	 global-step:622	 l-p:0.09277868270874023
epoch£º31	 i:3 	 global-step:623	 l-p:0.112933449447155
epoch£º31	 i:4 	 global-step:624	 l-p:0.10635114461183548
epoch£º31	 i:5 	 global-step:625	 l-p:0.10461900383234024
epoch£º31	 i:6 	 global-step:626	 l-p:0.11531200259923935
epoch£º31	 i:7 	 global-step:627	 l-p:0.08455607295036316
epoch£º31	 i:8 	 global-step:628	 l-p:0.11851351708173752
epoch£º31	 i:9 	 global-step:629	 l-p:0.10939972847700119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3678, 7.0105, 8.1725],
        [5.3678, 6.4364, 6.9185],
        [5.3678, 5.4333, 5.3873],
        [5.3678, 5.3695, 5.3679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.1079721599817276 
model_pd.l_d.mean(): -13.331657409667969 
model_pd.lagr.mean(): -13.223685264587402 
model_pd.lambdas: dict_items([('pout', tensor([1.1449], device='cuda:0')), ('power', tensor([0.6721], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3348], device='cuda:0')), ('power', tensor([-20.3746], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.1079721599817276
epoch£º32	 i:1 	 global-step:641	 l-p:0.1026749387383461
epoch£º32	 i:2 	 global-step:642	 l-p:0.09816663712263107
epoch£º32	 i:3 	 global-step:643	 l-p:0.09731559455394745
epoch£º32	 i:4 	 global-step:644	 l-p:0.09096458554267883
epoch£º32	 i:5 	 global-step:645	 l-p:0.11436967551708221
epoch£º32	 i:6 	 global-step:646	 l-p:0.12060104310512543
epoch£º32	 i:7 	 global-step:647	 l-p:0.11068037152290344
epoch£º32	 i:8 	 global-step:648	 l-p:0.09909044951200485
epoch£º32	 i:9 	 global-step:649	 l-p:0.11085396260023117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3844, 5.3847, 5.3844],
        [5.3844, 5.6621, 5.5876],
        [5.3844, 5.3852, 5.3844],
        [5.3844, 5.6624, 5.5879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.039325252175331116 
model_pd.l_d.mean(): -13.396242141723633 
model_pd.lagr.mean(): -13.356916427612305 
model_pd.lambdas: dict_items([('pout', tensor([1.1485], device='cuda:0')), ('power', tensor([0.6619], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3230], device='cuda:0')), ('power', tensor([-20.7654], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.039325252175331116
epoch£º33	 i:1 	 global-step:661	 l-p:0.1220192238688469
epoch£º33	 i:2 	 global-step:662	 l-p:0.1146438792347908
epoch£º33	 i:3 	 global-step:663	 l-p:0.09335850179195404
epoch£º33	 i:4 	 global-step:664	 l-p:0.10373082011938095
epoch£º33	 i:5 	 global-step:665	 l-p:0.10819457471370697
epoch£º33	 i:6 	 global-step:666	 l-p:0.10723629593849182
epoch£º33	 i:7 	 global-step:667	 l-p:0.1058555394411087
epoch£º33	 i:8 	 global-step:668	 l-p:0.10375980287790298
epoch£º33	 i:9 	 global-step:669	 l-p:0.09866355359554291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3979, 6.0809, 6.2101],
        [5.3979, 5.3981, 5.3979],
        [5.3979, 5.7181, 5.6523],
        [5.3979, 5.4616, 5.4165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.10868924111127853 
model_pd.l_d.mean(): -13.202630996704102 
model_pd.lagr.mean(): -13.093941688537598 
model_pd.lambdas: dict_items([('pout', tensor([1.1519], device='cuda:0')), ('power', tensor([0.6518], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3167], device='cuda:0')), ('power', tensor([-20.7823], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.10868924111127853
epoch£º34	 i:1 	 global-step:681	 l-p:0.11289182305335999
epoch£º34	 i:2 	 global-step:682	 l-p:0.08723552525043488
epoch£º34	 i:3 	 global-step:683	 l-p:0.09895164519548416
epoch£º34	 i:4 	 global-step:684	 l-p:0.11535680294036865
epoch£º34	 i:5 	 global-step:685	 l-p:-0.07410163432359695
epoch£º34	 i:6 	 global-step:686	 l-p:0.09116428345441818
epoch£º34	 i:7 	 global-step:687	 l-p:0.10823648422956467
epoch£º34	 i:8 	 global-step:688	 l-p:0.09077591449022293
epoch£º34	 i:9 	 global-step:689	 l-p:0.1061202883720398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.5478, 6.0798, 6.0974],
        [5.5478, 5.5478, 5.5478],
        [5.5478, 5.7030, 5.6257],
        [5.5478, 7.1237, 8.1625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.09240876883268356 
model_pd.l_d.mean(): -12.675341606140137 
model_pd.lagr.mean(): -12.582932472229004 
model_pd.lambdas: dict_items([('pout', tensor([1.1552], device='cuda:0')), ('power', tensor([0.6417], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3225], device='cuda:0')), ('power', tensor([-20.3017], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.09240876883268356
epoch£º35	 i:1 	 global-step:701	 l-p:0.10451892763376236
epoch£º35	 i:2 	 global-step:702	 l-p:0.07155341655015945
epoch£º35	 i:3 	 global-step:703	 l-p:0.09988290816545486
epoch£º35	 i:4 	 global-step:704	 l-p:0.0841125026345253
epoch£º35	 i:5 	 global-step:705	 l-p:0.10834166407585144
epoch£º35	 i:6 	 global-step:706	 l-p:0.11210408806800842
epoch£º35	 i:7 	 global-step:707	 l-p:0.23576492071151733
epoch£º35	 i:8 	 global-step:708	 l-p:0.07480398565530777
epoch£º35	 i:9 	 global-step:709	 l-p:0.11605896800756454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7966, 5.7967, 5.7966],
        [5.7966, 5.7966, 5.7966],
        [5.7966, 6.5909, 6.7710],
        [5.7966, 5.8546, 5.8118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.09521570801734924 
model_pd.l_d.mean(): -11.95851993560791 
model_pd.lagr.mean(): -11.863304138183594 
model_pd.lambdas: dict_items([('pout', tensor([1.1580], device='cuda:0')), ('power', tensor([0.6316], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2868], device='cuda:0')), ('power', tensor([-19.4280], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.09521570801734924
epoch£º36	 i:1 	 global-step:721	 l-p:0.07559092342853546
epoch£º36	 i:2 	 global-step:722	 l-p:0.10211849957704544
epoch£º36	 i:3 	 global-step:723	 l-p:0.1129024401307106
epoch£º36	 i:4 	 global-step:724	 l-p:0.0909547433257103
epoch£º36	 i:5 	 global-step:725	 l-p:0.09932798147201538
epoch£º36	 i:6 	 global-step:726	 l-p:0.09643188118934631
epoch£º36	 i:7 	 global-step:727	 l-p:0.1245814710855484
epoch£º36	 i:8 	 global-step:728	 l-p:0.0967230424284935
epoch£º36	 i:9 	 global-step:729	 l-p:0.0010063099907711148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8381, 5.8459, 5.8387],
        [5.8381, 7.4716, 8.5242],
        [5.8381, 6.3399, 6.3248],
        [5.8381, 6.4664, 6.5233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.07707491517066956 
model_pd.l_d.mean(): -11.471721649169922 
model_pd.lagr.mean(): -11.394646644592285 
model_pd.lambdas: dict_items([('pout', tensor([1.1603], device='cuda:0')), ('power', tensor([0.6217], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3184], device='cuda:0')), ('power', tensor([-19.0186], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.07707491517066956
epoch£º37	 i:1 	 global-step:741	 l-p:0.13980118930339813
epoch£º37	 i:2 	 global-step:742	 l-p:0.10860120505094528
epoch£º37	 i:3 	 global-step:743	 l-p:0.0945839062333107
epoch£º37	 i:4 	 global-step:744	 l-p:-0.018753238022327423
epoch£º37	 i:5 	 global-step:745	 l-p:0.11170545965433121
epoch£º37	 i:6 	 global-step:746	 l-p:0.09420312941074371
epoch£º37	 i:7 	 global-step:747	 l-p:0.06989776343107224
epoch£º37	 i:8 	 global-step:748	 l-p:0.07833944261074066
epoch£º37	 i:9 	 global-step:749	 l-p:0.11890671402215958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8549, 5.9953, 5.9185],
        [5.8549, 5.9385, 5.8822],
        [5.8549, 5.8873, 5.8608],
        [5.8549, 5.9347, 5.8802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): -0.03526965528726578 
model_pd.l_d.mean(): -12.025969505310059 
model_pd.lagr.mean(): -12.061239242553711 
model_pd.lambdas: dict_items([('pout', tensor([1.1626], device='cuda:0')), ('power', tensor([0.6116], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2503], device='cuda:0')), ('power', tensor([-20.1064], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:-0.03526965528726578
epoch£º38	 i:1 	 global-step:761	 l-p:0.09858020395040512
epoch£º38	 i:2 	 global-step:762	 l-p:0.09558065235614777
epoch£º38	 i:3 	 global-step:763	 l-p:0.07655973732471466
epoch£º38	 i:4 	 global-step:764	 l-p:0.10023215413093567
epoch£º38	 i:5 	 global-step:765	 l-p:0.08703971654176712
epoch£º38	 i:6 	 global-step:766	 l-p:0.10610723495483398
epoch£º38	 i:7 	 global-step:767	 l-p:0.09773135930299759
epoch£º38	 i:8 	 global-step:768	 l-p:0.11108836531639099
epoch£º38	 i:9 	 global-step:769	 l-p:0.34369567036628723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7373, 5.7373, 5.7373],
        [5.7373, 7.0665, 7.7770],
        [5.7373, 5.7373, 5.7373],
        [5.7373, 5.7399, 5.7374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.07874655723571777 
model_pd.l_d.mean(): -12.338299751281738 
model_pd.lagr.mean(): -12.259552955627441 
model_pd.lambdas: dict_items([('pout', tensor([1.1648], device='cuda:0')), ('power', tensor([0.6015], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1981], device='cuda:0')), ('power', tensor([-20.8590], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.07874655723571777
epoch£º39	 i:1 	 global-step:781	 l-p:0.10489974915981293
epoch£º39	 i:2 	 global-step:782	 l-p:0.10821607708930969
epoch£º39	 i:3 	 global-step:783	 l-p:0.08689325302839279
epoch£º39	 i:4 	 global-step:784	 l-p:0.09284190833568573
epoch£º39	 i:5 	 global-step:785	 l-p:0.11150487512350082
epoch£º39	 i:6 	 global-step:786	 l-p:0.02349047176539898
epoch£º39	 i:7 	 global-step:787	 l-p:-0.2750483751296997
epoch£º39	 i:8 	 global-step:788	 l-p:0.09505215287208557
epoch£º39	 i:9 	 global-step:789	 l-p:0.097637839615345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.7069, 5.7071, 5.7069],
        [5.7069, 5.7298, 5.7103],
        [5.7069, 5.8449, 5.7700],
        [5.7069, 5.7161, 5.7077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.0746358260512352 
model_pd.l_d.mean(): -12.02744197845459 
model_pd.lagr.mean(): -11.95280647277832 
model_pd.lambdas: dict_items([('pout', tensor([1.1675], device='cuda:0')), ('power', tensor([0.5915], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2274], device='cuda:0')), ('power', tensor([-20.7472], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.0746358260512352
epoch£º40	 i:1 	 global-step:801	 l-p:0.09741004556417465
epoch£º40	 i:2 	 global-step:802	 l-p:0.11611189693212509
epoch£º40	 i:3 	 global-step:803	 l-p:0.1099657490849495
epoch£º40	 i:4 	 global-step:804	 l-p:0.5209644436836243
epoch£º40	 i:5 	 global-step:805	 l-p:0.058539003133773804
epoch£º40	 i:6 	 global-step:806	 l-p:0.09825076907873154
epoch£º40	 i:7 	 global-step:807	 l-p:0.10980402678251266
epoch£º40	 i:8 	 global-step:808	 l-p:0.0985245481133461
epoch£º40	 i:9 	 global-step:809	 l-p:0.10197485983371735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.8872, 5.8906, 5.8873],
        [5.8872, 5.8876, 5.8872],
        [5.8872, 6.1230, 6.0341],
        [5.8872, 6.0349, 5.9560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.10314533859491348 
model_pd.l_d.mean(): -11.750953674316406 
model_pd.lagr.mean(): -11.647808074951172 
model_pd.lambdas: dict_items([('pout', tensor([1.1700], device='cuda:0')), ('power', tensor([0.5814], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1877], device='cuda:0')), ('power', tensor([-20.5519], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.10314533859491348
epoch£º41	 i:1 	 global-step:821	 l-p:0.06654279679059982
epoch£º41	 i:2 	 global-step:822	 l-p:0.07031947374343872
epoch£º41	 i:3 	 global-step:823	 l-p:0.12931238114833832
epoch£º41	 i:4 	 global-step:824	 l-p:0.09541323781013489
epoch£º41	 i:5 	 global-step:825	 l-p:0.10511092096567154
epoch£º41	 i:6 	 global-step:826	 l-p:0.09897727519273758
epoch£º41	 i:7 	 global-step:827	 l-p:0.04634959250688553
epoch£º41	 i:8 	 global-step:828	 l-p:0.1287148892879486
epoch£º41	 i:9 	 global-step:829	 l-p:0.10417554527521133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.1256, 6.5125, 6.4401],
        [6.1256, 6.1256, 6.1256],
        [6.1256, 7.9735, 9.2385],
        [6.1256, 6.1477, 6.1287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.09780984371900558 
model_pd.l_d.mean(): -10.758797645568848 
model_pd.lagr.mean(): -10.660987854003906 
model_pd.lambdas: dict_items([('pout', tensor([1.1722], device='cuda:0')), ('power', tensor([0.5715], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.2103], device='cuda:0')), ('power', tensor([-19.2243], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.09780984371900558
epoch£º42	 i:1 	 global-step:841	 l-p:-0.13983944058418274
epoch£º42	 i:2 	 global-step:842	 l-p:0.09611717611551285
epoch£º42	 i:3 	 global-step:843	 l-p:0.0932985469698906
epoch£º42	 i:4 	 global-step:844	 l-p:0.08830347657203674
epoch£º42	 i:5 	 global-step:845	 l-p:0.15479260683059692
epoch£º42	 i:6 	 global-step:846	 l-p:0.09984903037548065
epoch£º42	 i:7 	 global-step:847	 l-p:0.07580661028623581
epoch£º42	 i:8 	 global-step:848	 l-p:0.08396744728088379
epoch£º42	 i:9 	 global-step:849	 l-p:0.09430576115846634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[6.2625, 7.9745, 9.0409],
        [6.2625, 7.0444, 7.1743],
        [6.2625, 6.6974, 6.6348],
        [6.2625, 7.8909, 8.8566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.06894538551568985 
model_pd.l_d.mean(): -10.988471031188965 
model_pd.lagr.mean(): -10.919526100158691 
model_pd.lambdas: dict_items([('pout', tensor([1.1737], device='cuda:0')), ('power', tensor([0.5616], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1578], device='cuda:0')), ('power', tensor([-19.8626], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.06894538551568985
epoch£º43	 i:1 	 global-step:861	 l-p:0.11170461773872375
epoch£º43	 i:2 	 global-step:862	 l-p:0.12747998535633087
epoch£º43	 i:3 	 global-step:863	 l-p:0.0752352774143219
epoch£º43	 i:4 	 global-step:864	 l-p:0.09395004063844681
epoch£º43	 i:5 	 global-step:865	 l-p:0.09156666696071625
epoch£º43	 i:6 	 global-step:866	 l-p:0.08468877524137497
epoch£º43	 i:7 	 global-step:867	 l-p:0.15446233749389648
epoch£º43	 i:8 	 global-step:868	 l-p:0.08120553940534592
epoch£º43	 i:9 	 global-step:869	 l-p:0.08357635140419006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5846, 6.7758, 6.6807],
        [6.5846, 6.5953, 6.5855],
        [6.5846, 6.5882, 6.5847],
        [6.5846, 6.5847, 6.5846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.09056541323661804 
model_pd.l_d.mean(): -11.017952919006348 
model_pd.lagr.mean(): -10.927387237548828 
model_pd.lambdas: dict_items([('pout', tensor([1.1747], device='cuda:0')), ('power', tensor([0.5517], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0393], device='cuda:0')), ('power', tensor([-20.0182], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.09056541323661804
epoch£º44	 i:1 	 global-step:881	 l-p:0.07723384350538254
epoch£º44	 i:2 	 global-step:882	 l-p:0.09402867406606674
epoch£º44	 i:3 	 global-step:883	 l-p:0.1070188507437706
epoch£º44	 i:4 	 global-step:884	 l-p:0.026822185143828392
epoch£º44	 i:5 	 global-step:885	 l-p:0.08354463428258896
epoch£º44	 i:6 	 global-step:886	 l-p:0.05722597986459732
epoch£º44	 i:7 	 global-step:887	 l-p:0.09444476664066315
epoch£º44	 i:8 	 global-step:888	 l-p:0.0990508422255516
epoch£º44	 i:9 	 global-step:889	 l-p:0.05401959642767906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3129, 6.3130, 6.3129],
        [6.3129, 6.3145, 6.3130],
        [6.3129, 6.6489, 6.5591],
        [6.3129, 6.3129, 6.3129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.08701222389936447 
model_pd.l_d.mean(): -11.056138038635254 
model_pd.lagr.mean(): -10.969125747680664 
model_pd.lambdas: dict_items([('pout', tensor([1.1758], device='cuda:0')), ('power', tensor([0.5418], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0683], device='cuda:0')), ('power', tensor([-20.5143], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.08701222389936447
epoch£º45	 i:1 	 global-step:901	 l-p:0.07516094297170639
epoch£º45	 i:2 	 global-step:902	 l-p:0.09134190529584885
epoch£º45	 i:3 	 global-step:903	 l-p:0.1306053102016449
epoch£º45	 i:4 	 global-step:904	 l-p:0.09046035259962082
epoch£º45	 i:5 	 global-step:905	 l-p:0.06979965418577194
epoch£º45	 i:6 	 global-step:906	 l-p:0.09054508060216904
epoch£º45	 i:7 	 global-step:907	 l-p:0.09044580161571503
epoch£º45	 i:8 	 global-step:908	 l-p:0.0920693576335907
epoch£º45	 i:9 	 global-step:909	 l-p:-0.005369465332478285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.4281, 6.4344, 6.4285],
        [6.4281, 6.4677, 6.4356],
        [6.4281, 6.4281, 6.4281],
        [6.4281, 6.4668, 6.4353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.057116616517305374 
model_pd.l_d.mean(): -10.287980079650879 
model_pd.lagr.mean(): -10.230863571166992 
model_pd.lambdas: dict_items([('pout', tensor([1.1771], device='cuda:0')), ('power', tensor([0.5320], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297], device='cuda:0')), ('power', tensor([-19.5886], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.057116616517305374
epoch£º46	 i:1 	 global-step:921	 l-p:-0.28985145688056946
epoch£º46	 i:2 	 global-step:922	 l-p:0.10111066699028015
epoch£º46	 i:3 	 global-step:923	 l-p:0.08885729312896729
epoch£º46	 i:4 	 global-step:924	 l-p:0.08449509739875793
epoch£º46	 i:5 	 global-step:925	 l-p:0.09080863744020462
epoch£º46	 i:6 	 global-step:926	 l-p:0.09626195579767227
epoch£º46	 i:7 	 global-step:927	 l-p:0.3873419761657715
epoch£º46	 i:8 	 global-step:928	 l-p:0.16273467242717743
epoch£º46	 i:9 	 global-step:929	 l-p:0.09248562902212143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.5537, 6.6409, 6.5805],
        [6.5537, 6.7192, 6.6300],
        [6.5537, 6.5565, 6.5538],
        [6.5537, 7.0766, 7.0348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.03208453953266144 
model_pd.l_d.mean(): -9.569704055786133 
model_pd.lagr.mean(): -9.537619590759277 
model_pd.lambdas: dict_items([('pout', tensor([1.1781], device='cuda:0')), ('power', tensor([0.5222], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1297], device='cuda:0')), ('power', tensor([-18.5836], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.03208453953266144
epoch£º47	 i:1 	 global-step:941	 l-p:0.0788443386554718
epoch£º47	 i:2 	 global-step:942	 l-p:0.09419278800487518
epoch£º47	 i:3 	 global-step:943	 l-p:0.12858200073242188
epoch£º47	 i:4 	 global-step:944	 l-p:0.08418203890323639
epoch£º47	 i:5 	 global-step:945	 l-p:0.1323893964290619
epoch£º47	 i:6 	 global-step:946	 l-p:0.09745346009731293
epoch£º47	 i:7 	 global-step:947	 l-p:0.0719042420387268
epoch£º47	 i:8 	 global-step:948	 l-p:0.09233734011650085
epoch£º47	 i:9 	 global-step:949	 l-p:0.1208689957857132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6100, 7.1419, 7.1012],
        [6.6100, 6.8970, 6.7950],
        [6.6100, 6.6100, 6.6100],
        [6.6100, 6.9743, 6.8810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.09433826059103012 
model_pd.l_d.mean(): -10.403450012207031 
model_pd.lagr.mean(): -10.309111595153809 
model_pd.lambdas: dict_items([('pout', tensor([1.1787], device='cuda:0')), ('power', tensor([0.5124], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0097], device='cuda:0')), ('power', tensor([-20.2865], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.09433826059103012
epoch£º48	 i:1 	 global-step:961	 l-p:0.09036661684513092
epoch£º48	 i:2 	 global-step:962	 l-p:0.08922339975833893
epoch£º48	 i:3 	 global-step:963	 l-p:-0.76447993516922
epoch£º48	 i:4 	 global-step:964	 l-p:0.07917935401201248
epoch£º48	 i:5 	 global-step:965	 l-p:0.2909722924232483
epoch£º48	 i:6 	 global-step:966	 l-p:0.10427580773830414
epoch£º48	 i:7 	 global-step:967	 l-p:0.09407640993595123
epoch£º48	 i:8 	 global-step:968	 l-p:0.0895879939198494
epoch£º48	 i:9 	 global-step:969	 l-p:0.11799068003892899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3851, 6.4365, 6.3967],
        [6.3851, 6.3872, 6.3852],
        [6.3851, 6.6185, 6.5211],
        [6.3851, 6.3944, 6.3859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.10952898114919662 
model_pd.l_d.mean(): -10.255090713500977 
model_pd.lagr.mean(): -10.145562171936035 
model_pd.lambdas: dict_items([('pout', tensor([1.1797], device='cuda:0')), ('power', tensor([0.5025], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0496], device='cuda:0')), ('power', tensor([-20.4809], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.10952898114919662
epoch£º49	 i:1 	 global-step:981	 l-p:0.09307027608156204
epoch£º49	 i:2 	 global-step:982	 l-p:0.08798343688249588
epoch£º49	 i:3 	 global-step:983	 l-p:0.09821119904518127
epoch£º49	 i:4 	 global-step:984	 l-p:0.08876918256282806
epoch£º49	 i:5 	 global-step:985	 l-p:0.028231898322701454
epoch£º49	 i:6 	 global-step:986	 l-p:0.12260236591100693
epoch£º49	 i:7 	 global-step:987	 l-p:0.0993640273809433
epoch£º49	 i:8 	 global-step:988	 l-p:0.015317663550376892
epoch£º49	 i:9 	 global-step:989	 l-p:0.07434841990470886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.0901, 7.2447, 7.7179],
        [6.0901, 6.0902, 6.0901],
        [6.0901, 6.5407, 6.4911],
        [6.0901, 6.0926, 6.0902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.08078841120004654 
model_pd.l_d.mean(): -9.71065616607666 
model_pd.lagr.mean(): -9.629867553710938 
model_pd.lambdas: dict_items([('pout', tensor([1.1815], device='cuda:0')), ('power', tensor([0.4926], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1829], device='cuda:0')), ('power', tensor([-20.1089], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.08078841120004654
epoch£º50	 i:1 	 global-step:1001	 l-p:0.04556803032755852
epoch£º50	 i:2 	 global-step:1002	 l-p:0.07808960974216461
epoch£º50	 i:3 	 global-step:1003	 l-p:0.08242619037628174
epoch£º50	 i:4 	 global-step:1004	 l-p:0.08979907631874084
epoch£º50	 i:5 	 global-step:1005	 l-p:0.09713207185268402
epoch£º50	 i:6 	 global-step:1006	 l-p:0.08263592422008514
epoch£º50	 i:7 	 global-step:1007	 l-p:0.12146405875682831
epoch£º50	 i:8 	 global-step:1008	 l-p:0.09964574873447418
epoch£º50	 i:9 	 global-step:1009	 l-p:0.09413199871778488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.3995, 6.3995, 6.3995],
        [6.3995, 7.2679, 7.4510],
        [6.3995, 8.2663, 9.4976],
        [6.3995, 6.3996, 6.3995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): -0.036773454397916794 
model_pd.l_d.mean(): -8.908259391784668 
model_pd.lagr.mean(): -8.945033073425293 
model_pd.lambdas: dict_items([('pout', tensor([1.1830], device='cuda:0')), ('power', tensor([0.4828], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1731], device='cuda:0')), ('power', tensor([-18.8386], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:-0.036773454397916794
epoch£º51	 i:1 	 global-step:1021	 l-p:0.054140668362379074
epoch£º51	 i:2 	 global-step:1022	 l-p:0.20571361482143402
epoch£º51	 i:3 	 global-step:1023	 l-p:0.1034231185913086
epoch£º51	 i:4 	 global-step:1024	 l-p:0.09611930698156357
epoch£º51	 i:5 	 global-step:1025	 l-p:0.09311339259147644
epoch£º51	 i:6 	 global-step:1026	 l-p:0.08230698108673096
epoch£º51	 i:7 	 global-step:1027	 l-p:0.08645492047071457
epoch£º51	 i:8 	 global-step:1028	 l-p:0.10212934017181396
epoch£º51	 i:9 	 global-step:1029	 l-p:0.12266401946544647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6525, 7.5890, 7.8021],
        [6.6525, 7.3321, 7.3650],
        [6.6525, 6.7346, 6.6765],
        [6.6525, 6.6525, 6.6525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.0895310789346695 
model_pd.l_d.mean(): -9.479325294494629 
model_pd.lagr.mean(): -9.38979434967041 
model_pd.lambdas: dict_items([('pout', tensor([1.1836], device='cuda:0')), ('power', tensor([0.4730], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0139], device='cuda:0')), ('power', tensor([-20.0352], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.0895310789346695
epoch£º52	 i:1 	 global-step:1041	 l-p:0.1298334300518036
epoch£º52	 i:2 	 global-step:1042	 l-p:0.10063863545656204
epoch£º52	 i:3 	 global-step:1043	 l-p:0.11351218819618225
epoch£º52	 i:4 	 global-step:1044	 l-p:0.08740293234586716
epoch£º52	 i:5 	 global-step:1045	 l-p:0.0835806131362915
epoch£º52	 i:6 	 global-step:1046	 l-p:0.08530694991350174
epoch£º52	 i:7 	 global-step:1047	 l-p:0.06909816712141037
epoch£º52	 i:8 	 global-step:1048	 l-p:0.11251039057970047
epoch£º52	 i:9 	 global-step:1049	 l-p:0.5060779452323914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.6715, 6.6855, 6.6729],
        [6.6715, 6.6716, 6.6715],
        [6.6715, 6.6725, 6.6715],
        [6.6715, 6.6723, 6.6715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.11523821204900742 
model_pd.l_d.mean(): -8.713516235351562 
model_pd.lagr.mean(): -8.598278045654297 
model_pd.lambdas: dict_items([('pout', tensor([1.1844], device='cuda:0')), ('power', tensor([0.4632], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0837], device='cuda:0')), ('power', tensor([-18.9851], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.11523821204900742
epoch£º53	 i:1 	 global-step:1061	 l-p:0.08649786561727524
epoch£º53	 i:2 	 global-step:1062	 l-p:0.07744847238063812
epoch£º53	 i:3 	 global-step:1063	 l-p:0.08109269291162491
epoch£º53	 i:4 	 global-step:1064	 l-p:0.10739991813898087
epoch£º53	 i:5 	 global-step:1065	 l-p:0.024388059973716736
epoch£º53	 i:6 	 global-step:1066	 l-p:0.121047243475914
epoch£º53	 i:7 	 global-step:1067	 l-p:0.0896192118525505
epoch£º53	 i:8 	 global-step:1068	 l-p:0.09086667746305466
epoch£º53	 i:9 	 global-step:1069	 l-p:0.0874086245894432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8990, 6.8990, 6.8990],
        [6.8990, 8.0107, 8.3459],
        [6.8990, 8.0890, 8.4948],
        [6.8990, 7.0721, 6.9780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 1.4921234846115112 
model_pd.l_d.mean(): -9.090078353881836 
model_pd.lagr.mean(): -7.597954750061035 
model_pd.lambdas: dict_items([('pout', tensor([1.1846], device='cuda:0')), ('power', tensor([0.4535], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0316], device='cuda:0')), ('power', tensor([-19.9193], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:1.4921234846115112
epoch£º54	 i:1 	 global-step:1081	 l-p:0.0958259105682373
epoch£º54	 i:2 	 global-step:1082	 l-p:0.07957658916711807
epoch£º54	 i:3 	 global-step:1083	 l-p:0.049335915595293045
epoch£º54	 i:4 	 global-step:1084	 l-p:0.08240894228219986
epoch£º54	 i:5 	 global-step:1085	 l-p:0.0777941420674324
epoch£º54	 i:6 	 global-step:1086	 l-p:0.09208731353282928
epoch£º54	 i:7 	 global-step:1087	 l-p:0.08903706073760986
epoch£º54	 i:8 	 global-step:1088	 l-p:0.11798573285341263
epoch£º54	 i:9 	 global-step:1089	 l-p:0.09043054282665253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[6.8435, 8.0221, 8.4238],
        [6.8435, 8.4860, 9.3663],
        [6.8435, 7.5497, 7.5856],
        [6.8435, 8.0361, 8.4508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): -0.006464481353759766 
model_pd.l_d.mean(): -8.656296730041504 
model_pd.lagr.mean(): -8.662761688232422 
model_pd.lambdas: dict_items([('pout', tensor([1.1849], device='cuda:0')), ('power', tensor([0.4438], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0302], device='cuda:0')), ('power', tensor([-19.5423], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:-0.006464481353759766
epoch£º55	 i:1 	 global-step:1101	 l-p:0.09036082029342651
epoch£º55	 i:2 	 global-step:1102	 l-p:0.08617633581161499
epoch£º55	 i:3 	 global-step:1103	 l-p:0.08846419304609299
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10867292433977127
epoch£º55	 i:5 	 global-step:1105	 l-p:0.1031217947602272
epoch£º55	 i:6 	 global-step:1106	 l-p:0.09278561174869537
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12141287326812744
epoch£º55	 i:8 	 global-step:1108	 l-p:0.10662463307380676
epoch£º55	 i:9 	 global-step:1109	 l-p:0.08953086286783218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.7437,  8.7368, 10.0560],
        [ 6.7437,  6.7437,  6.7437],
        [ 6.7437,  7.1253,  7.0313],
        [ 6.7437,  6.8687,  6.7909]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.05728539451956749 
model_pd.l_d.mean(): -8.459394454956055 
model_pd.lagr.mean(): -8.402109146118164 
model_pd.lambdas: dict_items([('pout', tensor([1.1853], device='cuda:0')), ('power', tensor([0.4341], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0585], device='cuda:0')), ('power', tensor([-19.6040], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.05728539451956749
epoch£º56	 i:1 	 global-step:1121	 l-p:0.04928507283329964
epoch£º56	 i:2 	 global-step:1122	 l-p:0.09384465217590332
epoch£º56	 i:3 	 global-step:1123	 l-p:0.0979304313659668
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12075021117925644
epoch£º56	 i:5 	 global-step:1125	 l-p:0.09303472191095352
epoch£º56	 i:6 	 global-step:1126	 l-p:0.1108655110001564
epoch£º56	 i:7 	 global-step:1127	 l-p:0.08098296821117401
epoch£º56	 i:8 	 global-step:1128	 l-p:0.10270228236913681
epoch£º56	 i:9 	 global-step:1129	 l-p:0.09051681309938431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8459, 8.4214, 9.2264],
        [6.8459, 8.5992, 9.6063],
        [6.8459, 6.8460, 6.8459],
        [6.8459, 6.9595, 6.8859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.10334698110818863 
model_pd.l_d.mean(): -8.38417911529541 
model_pd.lagr.mean(): -8.280832290649414 
model_pd.lambdas: dict_items([('pout', tensor([1.1856], device='cuda:0')), ('power', tensor([0.4243], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0094], device='cuda:0')), ('power', tensor([-19.7383], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.10334698110818863
epoch£º57	 i:1 	 global-step:1141	 l-p:0.011652164161205292
epoch£º57	 i:2 	 global-step:1142	 l-p:0.09240618348121643
epoch£º57	 i:3 	 global-step:1143	 l-p:0.10093141347169876
epoch£º57	 i:4 	 global-step:1144	 l-p:0.06832291185855865
epoch£º57	 i:5 	 global-step:1145	 l-p:0.11493674665689468
epoch£º57	 i:6 	 global-step:1146	 l-p:0.1059243306517601
epoch£º57	 i:7 	 global-step:1147	 l-p:0.09548412263393402
epoch£º57	 i:8 	 global-step:1148	 l-p:0.09740036725997925
epoch£º57	 i:9 	 global-step:1149	 l-p:0.05952221155166626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 6.8356,  7.3426,  7.2815],
        [ 6.8356,  7.1937,  7.0930],
        [ 6.8356,  8.7828, 10.0257],
        [ 6.8356,  6.8356,  6.8356]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.011175589635968208 
model_pd.l_d.mean(): -8.322007179260254 
model_pd.lagr.mean(): -8.310832023620605 
model_pd.lambdas: dict_items([('pout', tensor([1.1859], device='cuda:0')), ('power', tensor([0.4146], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0152], device='cuda:0')), ('power', tensor([-19.9795], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.011175589635968208
epoch£º58	 i:1 	 global-step:1161	 l-p:0.09292664378881454
epoch£º58	 i:2 	 global-step:1162	 l-p:0.033070292323827744
epoch£º58	 i:3 	 global-step:1163	 l-p:0.09583671391010284
epoch£º58	 i:4 	 global-step:1164	 l-p:0.10427459329366684
epoch£º58	 i:5 	 global-step:1165	 l-p:0.07544036954641342
epoch£º58	 i:6 	 global-step:1166	 l-p:0.06921809166669846
epoch£º58	 i:7 	 global-step:1167	 l-p:0.08487455546855927
epoch£º58	 i:8 	 global-step:1168	 l-p:0.08838318288326263
epoch£º58	 i:9 	 global-step:1169	 l-p:0.10217760503292084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.0355, 7.6238, 7.5868],
        [7.0355, 7.0394, 7.0356],
        [7.0355, 7.0355, 7.0355],
        [7.0355, 7.3235, 7.2132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.09346441924571991 
model_pd.l_d.mean(): -7.528124809265137 
model_pd.lagr.mean(): -7.4346604347229 
model_pd.lambdas: dict_items([('pout', tensor([1.1860], device='cuda:0')), ('power', tensor([0.4050], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0230], device='cuda:0')), ('power', tensor([-18.6112], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.09346441924571991
epoch£º59	 i:1 	 global-step:1181	 l-p:0.057296037673950195
epoch£º59	 i:2 	 global-step:1182	 l-p:0.08961766213178635
epoch£º59	 i:3 	 global-step:1183	 l-p:1.9493131637573242
epoch£º59	 i:4 	 global-step:1184	 l-p:0.08494310081005096
epoch£º59	 i:5 	 global-step:1185	 l-p:0.07996377348899841
epoch£º59	 i:6 	 global-step:1186	 l-p:0.08579724282026291
epoch£º59	 i:7 	 global-step:1187	 l-p:0.10344205796718597
epoch£º59	 i:8 	 global-step:1188	 l-p:0.11543230712413788
epoch£º59	 i:9 	 global-step:1189	 l-p:0.08413936197757721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9297, 7.1105, 7.0141],
        [6.9297, 6.9297, 6.9297],
        [6.9297, 6.9331, 6.9298],
        [6.9297, 7.4932, 7.4505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.008276195265352726 
model_pd.l_d.mean(): -7.067983150482178 
model_pd.lagr.mean(): -7.059707164764404 
model_pd.lambdas: dict_items([('pout', tensor([1.1861], device='cuda:0')), ('power', tensor([0.3954], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.1251], device='cuda:0')), ('power', tensor([-18.2087], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.008276195265352726
epoch£º60	 i:1 	 global-step:1201	 l-p:0.08998113125562668
epoch£º60	 i:2 	 global-step:1202	 l-p:0.11561473459005356
epoch£º60	 i:3 	 global-step:1203	 l-p:0.08040838688611984
epoch£º60	 i:4 	 global-step:1204	 l-p:0.09891746193170547
epoch£º60	 i:5 	 global-step:1205	 l-p:0.07857058942317963
epoch£º60	 i:6 	 global-step:1206	 l-p:0.09352418035268784
epoch£º60	 i:7 	 global-step:1207	 l-p:0.09338465332984924
epoch£º60	 i:8 	 global-step:1208	 l-p:0.08357637375593185
epoch£º60	 i:9 	 global-step:1209	 l-p:0.01861509308218956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.8425, 6.8425, 6.8425],
        [6.8425, 7.8663, 8.1319],
        [6.8425, 6.8541, 6.8435],
        [6.8425, 8.0043, 8.3908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.08557073771953583 
model_pd.l_d.mean(): -7.277768611907959 
model_pd.lagr.mean(): -7.192197799682617 
model_pd.lambdas: dict_items([('pout', tensor([1.1862], device='cuda:0')), ('power', tensor([0.3857], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0483], device='cuda:0')), ('power', tensor([-18.9719], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.08557073771953583
epoch£º61	 i:1 	 global-step:1221	 l-p:0.0892803817987442
epoch£º61	 i:2 	 global-step:1222	 l-p:0.09648047387599945
epoch£º61	 i:3 	 global-step:1223	 l-p:0.10207034647464752
epoch£º61	 i:4 	 global-step:1224	 l-p:0.10817493498325348
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11779540777206421
epoch£º61	 i:6 	 global-step:1226	 l-p:0.09068933129310608
epoch£º61	 i:7 	 global-step:1227	 l-p:-0.022416962310671806
epoch£º61	 i:8 	 global-step:1228	 l-p:0.0758141279220581
epoch£º61	 i:9 	 global-step:1229	 l-p:0.0265608262270689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[6.9894, 7.0466, 7.0023],
        [6.9894, 8.5348, 9.2846],
        [6.9894, 7.2216, 7.1154],
        [6.9894, 7.3632, 7.2607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): -0.38604095578193665 
model_pd.l_d.mean(): -7.2884111404418945 
model_pd.lagr.mean(): -7.674452304840088 
model_pd.lambdas: dict_items([('pout', tensor([1.1865], device='cuda:0')), ('power', tensor([0.3760], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.0166], device='cuda:0')), ('power', tensor([-19.3887], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:-0.38604095578193665
epoch£º62	 i:1 	 global-step:1241	 l-p:0.08799033612012863
epoch£º62	 i:2 	 global-step:1242	 l-p:0.10054118931293488
epoch£º62	 i:3 	 global-step:1243	 l-p:0.0584876611828804
epoch£º62	 i:4 	 global-step:1244	 l-p:0.08344347774982452
epoch£º62	 i:5 	 global-step:1245	 l-p:0.0433720201253891
epoch£º62	 i:6 	 global-step:1246	 l-p:0.09970007836818695
epoch£º62	 i:7 	 global-step:1247	 l-p:0.08344995230436325
epoch£º62	 i:8 	 global-step:1248	 l-p:0.013217234052717686
epoch£º62	 i:9 	 global-step:1249	 l-p:0.09143737703561783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.6422,  7.6475,  7.6424],
        [ 7.6422, 10.0345, 11.6727],
        [ 7.6422, 10.0425, 11.6912],
        [ 7.6422,  7.6422,  7.6422]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.07351448386907578 
model_pd.l_d.mean(): -7.107571125030518 
model_pd.lagr.mean(): -7.034056663513184 
model_pd.lambdas: dict_items([('pout', tensor([1.1858], device='cuda:0')), ('power', tensor([0.3664], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1204], device='cuda:0')), ('power', tensor([-18.9579], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.07351448386907578
epoch£º63	 i:1 	 global-step:1261	 l-p:0.08675714582204819
epoch£º63	 i:2 	 global-step:1262	 l-p:0.08440076559782028
epoch£º63	 i:3 	 global-step:1263	 l-p:0.07994348555803299
epoch£º63	 i:4 	 global-step:1264	 l-p:0.08156585693359375
epoch£º63	 i:5 	 global-step:1265	 l-p:0.0909743681550026
epoch£º63	 i:6 	 global-step:1266	 l-p:-0.7390660643577576
epoch£º63	 i:7 	 global-step:1267	 l-p:0.08796894550323486
epoch£º63	 i:8 	 global-step:1268	 l-p:0.14428703486919403
epoch£º63	 i:9 	 global-step:1269	 l-p:0.12182530760765076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.6261, 7.6297, 7.6262],
        [7.6261, 7.6261, 7.6261],
        [7.6261, 7.6825, 7.6378],
        [7.6261, 7.6261, 7.6261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.08842836320400238 
model_pd.l_d.mean(): -6.857448101043701 
model_pd.lagr.mean(): -6.769019603729248 
model_pd.lambdas: dict_items([('pout', tensor([1.1847], device='cuda:0')), ('power', tensor([0.3570], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0984], device='cuda:0')), ('power', tensor([-18.8313], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.08842836320400238
epoch£º64	 i:1 	 global-step:1281	 l-p:0.07763233780860901
epoch£º64	 i:2 	 global-step:1282	 l-p:0.18049737811088562
epoch£º64	 i:3 	 global-step:1283	 l-p:0.07792963087558746
epoch£º64	 i:4 	 global-step:1284	 l-p:0.08583389967679977
epoch£º64	 i:5 	 global-step:1285	 l-p:0.07598210126161575
epoch£º64	 i:6 	 global-step:1286	 l-p:0.0826152041554451
epoch£º64	 i:7 	 global-step:1287	 l-p:0.09402698278427124
epoch£º64	 i:8 	 global-step:1288	 l-p:0.08343476057052612
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11963801831007004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.6302,  7.6311,  7.6302],
        [ 7.6302,  7.6302,  7.6302],
        [ 7.6302,  9.5531, 10.6192],
        [ 7.6302,  7.8628,  7.7486]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.2551797032356262 
model_pd.l_d.mean(): -6.7375383377075195 
model_pd.lagr.mean(): -6.482358455657959 
model_pd.lambdas: dict_items([('pout', tensor([1.1834], device='cuda:0')), ('power', tensor([0.3476], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1099], device='cuda:0')), ('power', tensor([-18.9564], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.2551797032356262
epoch£º65	 i:1 	 global-step:1301	 l-p:0.08708961308002472
epoch£º65	 i:2 	 global-step:1302	 l-p:0.10836533457040787
epoch£º65	 i:3 	 global-step:1303	 l-p:0.08685228228569031
epoch£º65	 i:4 	 global-step:1304	 l-p:0.08276741206645966
epoch£º65	 i:5 	 global-step:1305	 l-p:0.08105938136577606
epoch£º65	 i:6 	 global-step:1306	 l-p:0.09690836071968079
epoch£º65	 i:7 	 global-step:1307	 l-p:0.1053699254989624
epoch£º65	 i:8 	 global-step:1308	 l-p:0.08385889232158661
epoch£º65	 i:9 	 global-step:1309	 l-p:0.07736169546842575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.6675,  7.6675,  7.6675],
        [ 7.6675,  9.5771, 10.6214],
        [ 7.6675,  8.7206,  8.9303],
        [ 7.6675,  7.7190,  7.6776]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.16353937983512878 
model_pd.l_d.mean(): -6.686697959899902 
model_pd.lagr.mean(): -6.523158550262451 
model_pd.lambdas: dict_items([('pout', tensor([1.1822], device='cuda:0')), ('power', tensor([0.3382], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1541], device='cuda:0')), ('power', tensor([-19.1786], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.16353937983512878
epoch£º66	 i:1 	 global-step:1321	 l-p:0.08445940166711807
epoch£º66	 i:2 	 global-step:1322	 l-p:0.10359426587820053
epoch£º66	 i:3 	 global-step:1323	 l-p:0.0840206891298294
epoch£º66	 i:4 	 global-step:1324	 l-p:0.08292777836322784
epoch£º66	 i:5 	 global-step:1325	 l-p:0.07299721240997314
epoch£º66	 i:6 	 global-step:1326	 l-p:0.08344898372888565
epoch£º66	 i:7 	 global-step:1327	 l-p:0.10428890585899353
epoch£º66	 i:8 	 global-step:1328	 l-p:0.0751371905207634
epoch£º66	 i:9 	 global-step:1329	 l-p:0.09044280648231506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.7073,  7.7074,  7.7073],
        [ 7.7073,  7.7074,  7.7073],
        [ 7.7073,  9.5165, 10.4413],
        [ 7.7073,  9.1503,  9.6955]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.08077090978622437 
model_pd.l_d.mean(): -6.364303112030029 
model_pd.lagr.mean(): -6.28353214263916 
model_pd.lambdas: dict_items([('pout', tensor([1.1809], device='cuda:0')), ('power', tensor([0.3288], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1244], device='cuda:0')), ('power', tensor([-18.8539], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.08077090978622437
epoch£º67	 i:1 	 global-step:1341	 l-p:0.07825063169002533
epoch£º67	 i:2 	 global-step:1342	 l-p:0.08674366027116776
epoch£º67	 i:3 	 global-step:1343	 l-p:0.08088009059429169
epoch£º67	 i:4 	 global-step:1344	 l-p:0.08617878705263138
epoch£º67	 i:5 	 global-step:1345	 l-p:0.07716800272464752
epoch£º67	 i:6 	 global-step:1346	 l-p:0.27891117334365845
epoch£º67	 i:7 	 global-step:1347	 l-p:0.08599026501178741
epoch£º67	 i:8 	 global-step:1348	 l-p:0.02872670628130436
epoch£º67	 i:9 	 global-step:1349	 l-p:0.10030406713485718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.5386,  9.8056, 11.3049],
        [ 7.5386,  9.9025, 11.5261],
        [ 7.5386,  7.5585,  7.5408],
        [ 7.5386,  9.4353, 10.4867]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.08859655261039734 
model_pd.l_d.mean(): -5.943766117095947 
model_pd.lagr.mean(): -5.855169773101807 
model_pd.lambdas: dict_items([('pout', tensor([1.1799], device='cuda:0')), ('power', tensor([0.3194], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0342], device='cuda:0')), ('power', tensor([-18.4291], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.08859655261039734
epoch£º68	 i:1 	 global-step:1361	 l-p:0.20353414118289948
epoch£º68	 i:2 	 global-step:1362	 l-p:0.08858850598335266
epoch£º68	 i:3 	 global-step:1363	 l-p:0.08308117091655731
epoch£º68	 i:4 	 global-step:1364	 l-p:0.07252558320760727
epoch£º68	 i:5 	 global-step:1365	 l-p:0.080132856965065
epoch£º68	 i:6 	 global-step:1366	 l-p:0.08522741496562958
epoch£º68	 i:7 	 global-step:1367	 l-p:0.08389253169298172
epoch£º68	 i:8 	 global-step:1368	 l-p:0.0900251641869545
epoch£º68	 i:9 	 global-step:1369	 l-p:0.1389753371477127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.7176,  8.0540,  7.9310],
        [ 7.7176,  9.7376, 10.9007],
        [ 7.7176,  8.1673,  8.0573],
        [ 7.7176,  7.7176,  7.7176]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.0707947313785553 
model_pd.l_d.mean(): -5.655105113983154 
model_pd.lagr.mean(): -5.584310531616211 
model_pd.lambdas: dict_items([('pout', tensor([1.1788], device='cuda:0')), ('power', tensor([0.3100], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0516], device='cuda:0')), ('power', tensor([-17.9929], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.0707947313785553
epoch£º69	 i:1 	 global-step:1381	 l-p:0.08939430117607117
epoch£º69	 i:2 	 global-step:1382	 l-p:0.0845465138554573
epoch£º69	 i:3 	 global-step:1383	 l-p:0.07916278392076492
epoch£º69	 i:4 	 global-step:1384	 l-p:0.09409718960523605
epoch£º69	 i:5 	 global-step:1385	 l-p:0.1016809418797493
epoch£º69	 i:6 	 global-step:1386	 l-p:0.10512901097536087
epoch£º69	 i:7 	 global-step:1387	 l-p:0.07776708155870438
epoch£º69	 i:8 	 global-step:1388	 l-p:0.08425378799438477
epoch£º69	 i:9 	 global-step:1389	 l-p:0.08524155616760254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.8765, 10.4553, 12.2844],
        [ 7.8765,  7.9352,  7.8887],
        [ 7.8765,  7.8765,  7.8765],
        [ 7.8765, 10.2967, 11.9197]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.11636114120483398 
model_pd.l_d.mean(): -5.335429668426514 
model_pd.lagr.mean(): -5.21906852722168 
model_pd.lambdas: dict_items([('pout', tensor([1.1774], device='cuda:0')), ('power', tensor([0.3007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.0442], device='cuda:0')), ('power', tensor([-17.5202], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.11636114120483398
epoch£º70	 i:1 	 global-step:1401	 l-p:0.08793119341135025
epoch£º70	 i:2 	 global-step:1402	 l-p:0.08761761337518692
epoch£º70	 i:3 	 global-step:1403	 l-p:0.07457942515611649
epoch£º70	 i:4 	 global-step:1404	 l-p:0.08547324687242508
epoch£º70	 i:5 	 global-step:1405	 l-p:0.07246965914964676
epoch£º70	 i:6 	 global-step:1406	 l-p:0.0783642828464508
epoch£º70	 i:7 	 global-step:1407	 l-p:0.0848320946097374
epoch£º70	 i:8 	 global-step:1408	 l-p:0.07637844234704971
epoch£º70	 i:9 	 global-step:1409	 l-p:0.08179318159818649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.8834, 8.2878, 8.1658],
        [7.8834, 7.8904, 7.8838],
        [7.8834, 7.8834, 7.8834],
        [7.8834, 9.0957, 9.4124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.08876300603151321 
model_pd.l_d.mean(): -5.736788272857666 
model_pd.lagr.mean(): -5.648025035858154 
model_pd.lambdas: dict_items([('pout', tensor([1.1756], device='cuda:0')), ('power', tensor([0.2913], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1737], device='cuda:0')), ('power', tensor([-18.9309], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.08876300603151321
epoch£º71	 i:1 	 global-step:1421	 l-p:0.1079312413930893
epoch£º71	 i:2 	 global-step:1422	 l-p:0.07962395250797272
epoch£º71	 i:3 	 global-step:1423	 l-p:0.0905887708067894
epoch£º71	 i:4 	 global-step:1424	 l-p:0.07384467869997025
epoch£º71	 i:5 	 global-step:1425	 l-p:0.08657380938529968
epoch£º71	 i:6 	 global-step:1426	 l-p:0.07796260714530945
epoch£º71	 i:7 	 global-step:1427	 l-p:0.08872951567173004
epoch£º71	 i:8 	 global-step:1428	 l-p:0.08182219415903091
epoch£º71	 i:9 	 global-step:1429	 l-p:0.08502883464097977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[7.8315, 7.8315, 7.8315],
        [7.8315, 7.8316, 7.8315],
        [7.8315, 7.8414, 7.8323],
        [7.8315, 7.8316, 7.8315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.09495582431554794 
model_pd.l_d.mean(): -5.340510845184326 
model_pd.lagr.mean(): -5.2455549240112305 
model_pd.lambdas: dict_items([('pout', tensor([1.1741], device='cuda:0')), ('power', tensor([0.2820], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1419], device='cuda:0')), ('power', tensor([-18.2882], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.09495582431554794
epoch£º72	 i:1 	 global-step:1441	 l-p:0.07898997515439987
epoch£º72	 i:2 	 global-step:1442	 l-p:0.07968520373106003
epoch£º72	 i:3 	 global-step:1443	 l-p:0.10680566728115082
epoch£º72	 i:4 	 global-step:1444	 l-p:0.07487539947032928
epoch£º72	 i:5 	 global-step:1445	 l-p:0.0825672596693039
epoch£º72	 i:6 	 global-step:1446	 l-p:0.08148892968893051
epoch£º72	 i:7 	 global-step:1447	 l-p:0.0837019681930542
epoch£º72	 i:8 	 global-step:1448	 l-p:0.0956140086054802
epoch£º72	 i:9 	 global-step:1449	 l-p:0.08166763931512833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 7.9158,  7.9158,  7.9158],
        [ 7.9158,  8.2483,  8.1214],
        [ 7.9158,  9.7952, 10.7640],
        [ 7.9158,  8.3773,  8.2636]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.08195237815380096 
model_pd.l_d.mean(): -5.199429988861084 
model_pd.lagr.mean(): -5.1174774169921875 
model_pd.lambdas: dict_items([('pout', tensor([1.1726], device='cuda:0')), ('power', tensor([0.2727], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1670], device='cuda:0')), ('power', tensor([-18.2896], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.08195237815380096
epoch£º73	 i:1 	 global-step:1461	 l-p:0.09715764969587326
epoch£º73	 i:2 	 global-step:1462	 l-p:0.09573040902614594
epoch£º73	 i:3 	 global-step:1463	 l-p:0.08192429691553116
epoch£º73	 i:4 	 global-step:1464	 l-p:0.08269493281841278
epoch£º73	 i:5 	 global-step:1465	 l-p:0.08465133607387543
epoch£º73	 i:6 	 global-step:1466	 l-p:0.07683329284191132
epoch£º73	 i:7 	 global-step:1467	 l-p:0.07449014484882355
epoch£º73	 i:8 	 global-step:1468	 l-p:0.08086259663105011
epoch£º73	 i:9 	 global-step:1469	 l-p:0.07837370038032532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.0241,  8.0643,  8.0306],
        [ 8.0241,  8.7897,  8.7849],
        [ 8.0241,  9.4829, 10.0036],
        [ 8.0241,  8.4913,  8.3755]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.09904712438583374 
model_pd.l_d.mean(): -5.064696788787842 
model_pd.lagr.mean(): -4.965649604797363 
model_pd.lambdas: dict_items([('pout', tensor([1.1709], device='cuda:0')), ('power', tensor([0.2634], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1514], device='cuda:0')), ('power', tensor([-18.4931], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.09904712438583374
epoch£º74	 i:1 	 global-step:1481	 l-p:0.07489047944545746
epoch£º74	 i:2 	 global-step:1482	 l-p:0.07293832302093506
epoch£º74	 i:3 	 global-step:1483	 l-p:0.08208255469799042
epoch£º74	 i:4 	 global-step:1484	 l-p:0.08508537709712982
epoch£º74	 i:5 	 global-step:1485	 l-p:0.06664170324802399
epoch£º74	 i:6 	 global-step:1486	 l-p:0.0827258974313736
epoch£º74	 i:7 	 global-step:1487	 l-p:0.08455679565668106
epoch£º74	 i:8 	 global-step:1488	 l-p:0.07649894058704376
epoch£º74	 i:9 	 global-step:1489	 l-p:0.08836577832698822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.1511,  8.8621,  8.8233],
        [ 8.1511,  8.6085,  8.4871],
        [ 8.1511,  8.1511,  8.1511],
        [ 8.1511,  9.7462, 10.3823]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.08254359662532806 
model_pd.l_d.mean(): -4.982569217681885 
model_pd.lagr.mean(): -4.900025844573975 
model_pd.lambdas: dict_items([('pout', tensor([1.1689], device='cuda:0')), ('power', tensor([0.2541], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2098], device='cuda:0')), ('power', tensor([-18.5750], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.08254359662532806
epoch£º75	 i:1 	 global-step:1501	 l-p:0.08222229778766632
epoch£º75	 i:2 	 global-step:1502	 l-p:0.08278213441371918
epoch£º75	 i:3 	 global-step:1503	 l-p:0.07259242981672287
epoch£º75	 i:4 	 global-step:1504	 l-p:0.07971686869859695
epoch£º75	 i:5 	 global-step:1505	 l-p:0.06715282052755356
epoch£º75	 i:6 	 global-step:1506	 l-p:0.08662381023168564
epoch£º75	 i:7 	 global-step:1507	 l-p:0.07390008866786957
epoch£º75	 i:8 	 global-step:1508	 l-p:0.08562950044870377
epoch£º75	 i:9 	 global-step:1509	 l-p:0.08492650836706161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.2001,  8.2014,  8.2001],
        [ 8.2001,  8.2001,  8.2001],
        [ 8.2001,  9.7406, 10.3173],
        [ 8.2001,  8.7077,  8.5950]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.08403812348842621 
model_pd.l_d.mean(): -4.659973621368408 
model_pd.lagr.mean(): -4.575935363769531 
model_pd.lambdas: dict_items([('pout', tensor([1.1670], device='cuda:0')), ('power', tensor([0.2449], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.1737], device='cuda:0')), ('power', tensor([-18.1325], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.08403812348842621
epoch£º76	 i:1 	 global-step:1521	 l-p:0.076044961810112
epoch£º76	 i:2 	 global-step:1522	 l-p:0.07905296981334686
epoch£º76	 i:3 	 global-step:1523	 l-p:0.08186556398868561
epoch£º76	 i:4 	 global-step:1524	 l-p:0.059884414076805115
epoch£º76	 i:5 	 global-step:1525	 l-p:0.09463682025671005
epoch£º76	 i:6 	 global-step:1526	 l-p:0.07398903369903564
epoch£º76	 i:7 	 global-step:1527	 l-p:0.08407013863325119
epoch£º76	 i:8 	 global-step:1528	 l-p:0.07274743169546127
epoch£º76	 i:9 	 global-step:1529	 l-p:0.06666440516710281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[8.4501, 8.4912, 8.4567],
        [8.4501, 8.4510, 8.4501],
        [8.4501, 8.5492, 8.4774],
        [8.4501, 8.6842, 8.5610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.08446824550628662 
model_pd.l_d.mean(): -4.719156265258789 
model_pd.lagr.mean(): -4.634687900543213 
model_pd.lambdas: dict_items([('pout', tensor([1.1646], device='cuda:0')), ('power', tensor([0.2357], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2788], device='cuda:0')), ('power', tensor([-18.5683], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.08446824550628662
epoch£º77	 i:1 	 global-step:1541	 l-p:0.05611731857061386
epoch£º77	 i:2 	 global-step:1542	 l-p:0.08015330135822296
epoch£º77	 i:3 	 global-step:1543	 l-p:0.07138995826244354
epoch£º77	 i:4 	 global-step:1544	 l-p:0.07848507910966873
epoch£º77	 i:5 	 global-step:1545	 l-p:0.08225090801715851
epoch£º77	 i:6 	 global-step:1546	 l-p:0.08435436338186264
epoch£º77	 i:7 	 global-step:1547	 l-p:0.07074876874685287
epoch£º77	 i:8 	 global-step:1548	 l-p:-4.646953105926514
epoch£º77	 i:9 	 global-step:1549	 l-p:0.07514441013336182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 8.7187, 10.0816, 10.4392],
        [ 8.7187, 10.6698, 11.5917],
        [ 8.7187,  8.9697,  8.8401],
        [ 8.7187,  8.8484,  8.7600]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14679253101348877 
model_pd.l_d.mean(): -4.420209884643555 
model_pd.lagr.mean(): -4.2734174728393555 
model_pd.lambdas: dict_items([('pout', tensor([1.1620], device='cuda:0')), ('power', tensor([0.2267], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.2770], device='cuda:0')), ('power', tensor([-18.0067], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14679253101348877
epoch£º78	 i:1 	 global-step:1561	 l-p:0.0620497465133667
epoch£º78	 i:2 	 global-step:1562	 l-p:0.07645049691200256
epoch£º78	 i:3 	 global-step:1563	 l-p:0.06896624714136124
epoch£º78	 i:4 	 global-step:1564	 l-p:0.1668958216905594
epoch£º78	 i:5 	 global-step:1565	 l-p:0.07504124194383621
epoch£º78	 i:6 	 global-step:1566	 l-p:0.06639694422483444
epoch£º78	 i:7 	 global-step:1567	 l-p:0.07760414481163025
epoch£º78	 i:8 	 global-step:1568	 l-p:0.07555288821458817
epoch£º78	 i:9 	 global-step:1569	 l-p:0.07420726120471954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.3380, 10.9314, 11.4220],
        [ 9.3380,  9.3380,  9.3380],
        [ 9.3380,  9.3426,  9.3382],
        [ 9.3380,  9.3502,  9.3388]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.07648301124572754 
model_pd.l_d.mean(): -4.35983419418335 
model_pd.lagr.mean(): -4.283350944519043 
model_pd.lambdas: dict_items([('pout', tensor([1.1585], device='cuda:0')), ('power', tensor([0.2178], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4031], device='cuda:0')), ('power', tensor([-17.7969], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.07648301124572754
epoch£º79	 i:1 	 global-step:1581	 l-p:0.07483691722154617
epoch£º79	 i:2 	 global-step:1582	 l-p:0.08074392378330231
epoch£º79	 i:3 	 global-step:1583	 l-p:0.050874192267656326
epoch£º79	 i:4 	 global-step:1584	 l-p:0.07654625922441483
epoch£º79	 i:5 	 global-step:1585	 l-p:0.07225397229194641
epoch£º79	 i:6 	 global-step:1586	 l-p:0.05559292063117027
epoch£º79	 i:7 	 global-step:1587	 l-p:0.09061616659164429
epoch£º79	 i:8 	 global-step:1588	 l-p:0.07516855746507645
epoch£º79	 i:9 	 global-step:1589	 l-p:0.07749917358160019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.6591, 12.9509, 15.3154],
        [ 9.6591,  9.7071,  9.6667],
        [ 9.6591,  9.6591,  9.6591],
        [ 9.6591, 12.0196, 13.2405]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.07234694063663483 
model_pd.l_d.mean(): -3.9930405616760254 
model_pd.lagr.mean(): -3.9206936359405518 
model_pd.lambdas: dict_items([('pout', tensor([1.1546], device='cuda:0')), ('power', tensor([0.2092], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4027], device='cuda:0')), ('power', tensor([-16.7971], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.07234694063663483
epoch£º80	 i:1 	 global-step:1601	 l-p:0.07798460125923157
epoch£º80	 i:2 	 global-step:1602	 l-p:0.07467744499444962
epoch£º80	 i:3 	 global-step:1603	 l-p:0.0857386663556099
epoch£º80	 i:4 	 global-step:1604	 l-p:0.06714946776628494
epoch£º80	 i:5 	 global-step:1605	 l-p:-0.24106326699256897
epoch£º80	 i:6 	 global-step:1606	 l-p:0.07764112204313278
epoch£º80	 i:7 	 global-step:1607	 l-p:0.06885715574026108
epoch£º80	 i:8 	 global-step:1608	 l-p:0.07871276140213013
epoch£º80	 i:9 	 global-step:1609	 l-p:0.11011792719364166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[ 9.9178,  9.9355,  9.9193],
        [ 9.9178,  9.9178,  9.9178],
        [ 9.9178, 12.3578, 13.6251],
        [ 9.9178, 10.1206,  9.9958]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.07581556588411331 
model_pd.l_d.mean(): -3.9987947940826416 
model_pd.lagr.mean(): -3.9229791164398193 
model_pd.lambdas: dict_items([('pout', tensor([1.1501], device='cuda:0')), ('power', tensor([0.2006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4678], device='cuda:0')), ('power', tensor([-17.1767], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.07581556588411331
epoch£º81	 i:1 	 global-step:1621	 l-p:0.07264038175344467
epoch£º81	 i:2 	 global-step:1622	 l-p:0.10211482644081116
epoch£º81	 i:3 	 global-step:1623	 l-p:0.07202865183353424
epoch£º81	 i:4 	 global-step:1624	 l-p:0.08489178121089935
epoch£º81	 i:5 	 global-step:1625	 l-p:0.07033965736627579
epoch£º81	 i:6 	 global-step:1626	 l-p:0.0721813216805458
epoch£º81	 i:7 	 global-step:1627	 l-p:0.07380585372447968
epoch£º81	 i:8 	 global-step:1628	 l-p:0.06881819665431976
epoch£º81	 i:9 	 global-step:1629	 l-p:0.07655617594718933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.2799, 10.4912, 10.3612],
        [10.2799, 12.1156, 12.7170],
        [10.2799, 10.8948, 10.7398],
        [10.2799, 13.1141, 14.7727]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.07444652169942856 
model_pd.l_d.mean(): -3.850632429122925 
model_pd.lagr.mean(): -3.776185989379883 
model_pd.lambdas: dict_items([('pout', tensor([1.1453], device='cuda:0')), ('power', tensor([0.1922], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5194], device='cuda:0')), ('power', tensor([-16.8634], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.07444652169942856
epoch£º82	 i:1 	 global-step:1641	 l-p:0.08221610635519028
epoch£º82	 i:2 	 global-step:1642	 l-p:0.07102946937084198
epoch£º82	 i:3 	 global-step:1643	 l-p:0.07216685265302658
epoch£º82	 i:4 	 global-step:1644	 l-p:0.07601608335971832
epoch£º82	 i:5 	 global-step:1645	 l-p:0.07111278176307678
epoch£º82	 i:6 	 global-step:1646	 l-p:0.07205480337142944
epoch£º82	 i:7 	 global-step:1647	 l-p:0.07575168460607529
epoch£º82	 i:8 	 global-step:1648	 l-p:0.08367288112640381
epoch£º82	 i:9 	 global-step:1649	 l-p:0.06671076267957687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.2078, 12.6022, 13.7728],
        [10.2078, 10.8748, 10.7336],
        [10.2078, 10.2098, 10.2079],
        [10.2078, 11.5799, 11.8051]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.07431682199239731 
model_pd.l_d.mean(): -3.5633645057678223 
model_pd.lagr.mean(): -3.4890477657318115 
model_pd.lambdas: dict_items([('pout', tensor([1.1403], device='cuda:0')), ('power', tensor([0.1839], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4592], device='cuda:0')), ('power', tensor([-16.4560], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.07431682199239731
epoch£º83	 i:1 	 global-step:1661	 l-p:0.07322065532207489
epoch£º83	 i:2 	 global-step:1662	 l-p:0.07128217816352844
epoch£º83	 i:3 	 global-step:1663	 l-p:0.07236529886722565
epoch£º83	 i:4 	 global-step:1664	 l-p:0.07185235619544983
epoch£º83	 i:5 	 global-step:1665	 l-p:0.07793977856636047
epoch£º83	 i:6 	 global-step:1666	 l-p:0.08692606538534164
epoch£º83	 i:7 	 global-step:1667	 l-p:0.0733717530965805
epoch£º83	 i:8 	 global-step:1668	 l-p:0.08196786791086197
epoch£º83	 i:9 	 global-step:1669	 l-p:0.07215990126132965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.1638, 10.1644, 10.1638],
        [10.1638, 10.1903, 10.1666],
        [10.1638, 10.3855, 10.2525],
        [10.1638, 10.8282, 10.6879]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.07331008464097977 
model_pd.l_d.mean(): -3.559324264526367 
model_pd.lagr.mean(): -3.4860141277313232 
model_pd.lambdas: dict_items([('pout', tensor([1.1354], device='cuda:0')), ('power', tensor([0.1755], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.4955], device='cuda:0')), ('power', tensor([-16.9950], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.07331008464097977
epoch£º84	 i:1 	 global-step:1681	 l-p:0.06882569938898087
epoch£º84	 i:2 	 global-step:1682	 l-p:0.07527952641248703
epoch£º84	 i:3 	 global-step:1683	 l-p:0.08303181827068329
epoch£º84	 i:4 	 global-step:1684	 l-p:0.0801115557551384
epoch£º84	 i:5 	 global-step:1685	 l-p:0.07212691754102707
epoch£º84	 i:6 	 global-step:1686	 l-p:0.07129362970590591
epoch£º84	 i:7 	 global-step:1687	 l-p:0.07807476818561554
epoch£º84	 i:8 	 global-step:1688	 l-p:0.07218067348003387
epoch£º84	 i:9 	 global-step:1689	 l-p:0.07402973622083664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[10.3406, 12.3696, 13.1454],
        [10.3406, 11.6752, 11.8623],
        [10.3406, 12.6375, 13.6840],
        [10.3406, 11.2963, 11.2565]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.07663434743881226 
model_pd.l_d.mean(): -3.3695931434631348 
model_pd.lagr.mean(): -3.2929587364196777 
model_pd.lambdas: dict_items([('pout', tensor([1.1304], device='cuda:0')), ('power', tensor([0.1671], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5033], device='cuda:0')), ('power', tensor([-16.6733], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.07663434743881226
epoch£º85	 i:1 	 global-step:1701	 l-p:0.08461274206638336
epoch£º85	 i:2 	 global-step:1702	 l-p:0.08094839006662369
epoch£º85	 i:3 	 global-step:1703	 l-p:0.07229682803153992
epoch£º85	 i:4 	 global-step:1704	 l-p:0.07395974546670914
epoch£º85	 i:5 	 global-step:1705	 l-p:0.06923259049654007
epoch£º85	 i:6 	 global-step:1706	 l-p:0.0721965879201889
epoch£º85	 i:7 	 global-step:1707	 l-p:0.06966798007488251
epoch£º85	 i:8 	 global-step:1708	 l-p:0.07103265821933746
epoch£º85	 i:9 	 global-step:1709	 l-p:0.06258446723222733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.5184, 12.8713, 13.9506],
        [10.5184, 10.5500, 10.5221],
        [10.5184, 10.5185, 10.5184],
        [10.5184, 11.4384, 11.3729]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.07196904718875885 
model_pd.l_d.mean(): -2.8377435207366943 
model_pd.lagr.mean(): -2.7657744884490967 
model_pd.lambdas: dict_items([('pout', tensor([1.1253], device='cuda:0')), ('power', tensor([0.1589], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.3548], device='cuda:0')), ('power', tensor([-15.2706], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.07196904718875885
epoch£º86	 i:1 	 global-step:1721	 l-p:0.07098318636417389
epoch£º86	 i:2 	 global-step:1722	 l-p:0.07302532345056534
epoch£º86	 i:3 	 global-step:1723	 l-p:0.07034528255462646
epoch£º86	 i:4 	 global-step:1724	 l-p:0.07151859253644943
epoch£º86	 i:5 	 global-step:1725	 l-p:0.07849769294261932
epoch£º86	 i:6 	 global-step:1726	 l-p:0.07376023381948471
epoch£º86	 i:7 	 global-step:1727	 l-p:0.07060112059116364
epoch£º86	 i:8 	 global-step:1728	 l-p:0.0697055533528328
epoch£º86	 i:9 	 global-step:1729	 l-p:0.06854081153869629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[10.7809, 11.0863, 10.9244],
        [10.7809, 10.7958, 10.7819],
        [10.7809, 10.7824, 10.7809],
        [10.7809, 12.3026, 12.5891]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.07550446689128876 
model_pd.l_d.mean(): -3.0320231914520264 
model_pd.lagr.mean(): -2.9565186500549316 
model_pd.lambdas: dict_items([('pout', tensor([1.1196], device='cuda:0')), ('power', tensor([0.1507], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5269], device='cuda:0')), ('power', tensor([-16.1185], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.07550446689128876
epoch£º87	 i:1 	 global-step:1741	 l-p:0.0709989070892334
epoch£º87	 i:2 	 global-step:1742	 l-p:0.06727834045886993
epoch£º87	 i:3 	 global-step:1743	 l-p:0.06967036426067352
epoch£º87	 i:4 	 global-step:1744	 l-p:0.0696725845336914
epoch£º87	 i:5 	 global-step:1745	 l-p:0.06820940971374512
epoch£º87	 i:6 	 global-step:1746	 l-p:0.06720895320177078
epoch£º87	 i:7 	 global-step:1747	 l-p:0.07323229312896729
epoch£º87	 i:8 	 global-step:1748	 l-p:0.07289716601371765
epoch£º87	 i:9 	 global-step:1749	 l-p:0.06917747110128403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[11.0497, 11.0497, 11.0497],
        [11.0497, 11.9802, 11.8932],
        [11.0497, 11.3872, 11.2156],
        [11.0497, 11.0685, 11.0512]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.06962671875953674 
model_pd.l_d.mean(): -3.0681262016296387 
model_pd.lagr.mean(): -2.9984993934631348 
model_pd.lambdas: dict_items([('pout', tensor([1.1137], device='cuda:0')), ('power', tensor([0.1426], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6453], device='cuda:0')), ('power', tensor([-16.3793], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.06962671875953674
epoch£º88	 i:1 	 global-step:1761	 l-p:0.07041460275650024
epoch£º88	 i:2 	 global-step:1762	 l-p:0.06424295902252197
epoch£º88	 i:3 	 global-step:1763	 l-p:0.07335079461336136
epoch£º88	 i:4 	 global-step:1764	 l-p:0.07002865523099899
epoch£º88	 i:5 	 global-step:1765	 l-p:0.07053899019956589
epoch£º88	 i:6 	 global-step:1766	 l-p:0.06870299577713013
epoch£º88	 i:7 	 global-step:1767	 l-p:0.06840144097805023
epoch£º88	 i:8 	 global-step:1768	 l-p:0.0685880109667778
epoch£º88	 i:9 	 global-step:1769	 l-p:0.06648701429367065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[11.3970, 14.3942, 16.0433],
        [11.3970, 11.4876, 11.4160],
        [11.3970, 11.5187, 11.4277],
        [11.3970, 11.9265, 11.7344]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.07415103912353516 
model_pd.l_d.mean(): -2.7265498638153076 
model_pd.lagr.mean(): -2.6523988246917725 
model_pd.lambdas: dict_items([('pout', tensor([1.1076], device='cuda:0')), ('power', tensor([0.1347], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.5673], device='cuda:0')), ('power', tensor([-15.4854], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.07415103912353516
epoch£º89	 i:1 	 global-step:1781	 l-p:0.06891845166683197
epoch£º89	 i:2 	 global-step:1782	 l-p:0.06586415320634842
epoch£º89	 i:3 	 global-step:1783	 l-p:0.07306674122810364
epoch£º89	 i:4 	 global-step:1784	 l-p:0.06652238965034485
epoch£º89	 i:5 	 global-step:1785	 l-p:0.06482692807912827
epoch£º89	 i:6 	 global-step:1786	 l-p:0.03881483152508736
epoch£º89	 i:7 	 global-step:1787	 l-p:0.06831660866737366
epoch£º89	 i:8 	 global-step:1788	 l-p:0.06864876300096512
epoch£º89	 i:9 	 global-step:1789	 l-p:0.0669412687420845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[11.9418, 15.3614, 17.4093],
        [11.9418, 11.9418, 11.9418],
        [11.9418, 11.9420, 11.9418],
        [11.9418, 12.0819, 11.9791]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.06571565568447113 
model_pd.l_d.mean(): -2.6757142543792725 
model_pd.lagr.mean(): -2.6099987030029297 
model_pd.lambdas: dict_items([('pout', tensor([1.1009], device='cuda:0')), ('power', tensor([0.1270], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.6861], device='cuda:0')), ('power', tensor([-15.0328], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.06571565568447113
epoch£º90	 i:1 	 global-step:1801	 l-p:0.07059378921985626
epoch£º90	 i:2 	 global-step:1802	 l-p:0.06389547884464264
epoch£º90	 i:3 	 global-step:1803	 l-p:0.0690438523888588
epoch£º90	 i:4 	 global-step:1804	 l-p:0.06925036758184433
epoch£º90	 i:5 	 global-step:1805	 l-p:0.06767431646585464
epoch£º90	 i:6 	 global-step:1806	 l-p:0.0669805258512497
epoch£º90	 i:7 	 global-step:1807	 l-p:0.06722459197044373
epoch£º90	 i:8 	 global-step:1808	 l-p:0.06667698919773102
epoch£º90	 i:9 	 global-step:1809	 l-p:0.07579346746206284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[12.5063, 12.5063, 12.5063],
        [12.5063, 12.5080, 12.5064],
        [12.5063, 12.5320, 12.5087],
        [12.5063, 15.1673, 16.2814]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.059508465230464935 
model_pd.l_d.mean(): -2.5830435752868652 
model_pd.lagr.mean(): -2.5235350131988525 
model_pd.lambdas: dict_items([('pout', tensor([1.0935], device='cuda:0')), ('power', tensor([0.1195], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7637], device='cuda:0')), ('power', tensor([-14.5336], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.059508465230464935
epoch£º91	 i:1 	 global-step:1821	 l-p:0.06650242954492569
epoch£º91	 i:2 	 global-step:1822	 l-p:0.06828566640615463
epoch£º91	 i:3 	 global-step:1823	 l-p:0.06704328954219818
epoch£º91	 i:4 	 global-step:1824	 l-p:0.06602632254362106
epoch£º91	 i:5 	 global-step:1825	 l-p:0.07544105499982834
epoch£º91	 i:6 	 global-step:1826	 l-p:0.06742460280656815
epoch£º91	 i:7 	 global-step:1827	 l-p:0.06630588322877884
epoch£º91	 i:8 	 global-step:1828	 l-p:0.06565270572900772
epoch£º91	 i:9 	 global-step:1829	 l-p:0.06373140960931778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[13.0072, 13.0072, 13.0072],
        [13.0072, 16.6235, 18.7080],
        [13.0072, 13.5923, 13.3694],
        [13.0072, 13.2530, 13.0952]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.06747207790613174 
model_pd.l_d.mean(): -2.420006275177002 
model_pd.lagr.mean(): -2.352534294128418 
model_pd.lambdas: dict_items([('pout', tensor([1.0857], device='cuda:0')), ('power', tensor([0.1123], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.7573], device='cuda:0')), ('power', tensor([-14.1373], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.06747207790613174
epoch£º92	 i:1 	 global-step:1841	 l-p:0.06502574682235718
epoch£º92	 i:2 	 global-step:1842	 l-p:0.06240580603480339
epoch£º92	 i:3 	 global-step:1843	 l-p:0.06170548126101494
epoch£º92	 i:4 	 global-step:1844	 l-p:0.06587272137403488
epoch£º92	 i:5 	 global-step:1845	 l-p:0.06280651688575745
epoch£º92	 i:6 	 global-step:1846	 l-p:0.06491150707006454
epoch£º92	 i:7 	 global-step:1847	 l-p:0.06458965688943863
epoch£º92	 i:8 	 global-step:1848	 l-p:0.07159986346960068
epoch£º92	 i:9 	 global-step:1849	 l-p:0.06601694971323013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[13.4529, 13.4529, 13.4529],
        [13.4529, 14.8902, 14.9198],
        [13.4529, 15.0130, 15.1148],
        [13.4529, 14.5520, 14.4213]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.05998176336288452 
model_pd.l_d.mean(): -2.372687816619873 
model_pd.lagr.mean(): -2.3127059936523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0773], device='cuda:0')), ('power', tensor([0.1053], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8487], device='cuda:0')), ('power', tensor([-13.7587], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.05998176336288452
epoch£º93	 i:1 	 global-step:1861	 l-p:0.06492706388235092
epoch£º93	 i:2 	 global-step:1862	 l-p:0.06117058917880058
epoch£º93	 i:3 	 global-step:1863	 l-p:0.06622414290904999
epoch£º93	 i:4 	 global-step:1864	 l-p:0.06309648603200912
epoch£º93	 i:5 	 global-step:1865	 l-p:0.06441014260053635
epoch£º93	 i:6 	 global-step:1866	 l-p:0.06787972152233124
epoch£º93	 i:7 	 global-step:1867	 l-p:0.06297864019870758
epoch£º93	 i:8 	 global-step:1868	 l-p:0.06565383076667786
epoch£º93	 i:9 	 global-step:1869	 l-p:0.06606500595808029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[13.9469, 16.0721, 16.5346],
        [13.9469, 13.9469, 13.9469],
        [13.9469, 14.1453, 14.0060],
        [13.9469, 14.0132, 13.9569]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.0640096440911293 
model_pd.l_d.mean(): -2.385596513748169 
model_pd.lagr.mean(): -2.321586847305298 
model_pd.lambdas: dict_items([('pout', tensor([1.0684], device='cuda:0')), ('power', tensor([0.0984], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9617], device='cuda:0')), ('power', tensor([-13.6918], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.0640096440911293
epoch£º94	 i:1 	 global-step:1881	 l-p:0.062397994101047516
epoch£º94	 i:2 	 global-step:1882	 l-p:0.06550229340791702
epoch£º94	 i:3 	 global-step:1883	 l-p:0.06510165333747864
epoch£º94	 i:4 	 global-step:1884	 l-p:0.05732516944408417
epoch£º94	 i:5 	 global-step:1885	 l-p:0.06442591547966003
epoch£º94	 i:6 	 global-step:1886	 l-p:0.064507856965065
epoch£º94	 i:7 	 global-step:1887	 l-p:0.0639064759016037
epoch£º94	 i:8 	 global-step:1888	 l-p:0.05650752782821655
epoch£º94	 i:9 	 global-step:1889	 l-p:0.0663287341594696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[14.5131, 14.5134, 14.5131],
        [14.5131, 14.5130, 14.5131],
        [14.5131, 14.5202, 14.5133],
        [14.5131, 14.5894, 14.5252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.05390889197587967 
model_pd.l_d.mean(): -2.1385886669158936 
model_pd.lagr.mean(): -2.0846798419952393 
model_pd.lambdas: dict_items([('pout', tensor([1.0592], device='cuda:0')), ('power', tensor([0.0919], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.9137], device='cuda:0')), ('power', tensor([-12.6422], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.05390889197587967
epoch£º95	 i:1 	 global-step:1901	 l-p:0.06400521099567413
epoch£º95	 i:2 	 global-step:1902	 l-p:0.06405758112668991
epoch£º95	 i:3 	 global-step:1903	 l-p:0.06090768799185753
epoch£º95	 i:4 	 global-step:1904	 l-p:0.05841183662414551
epoch£º95	 i:5 	 global-step:1905	 l-p:0.0631023496389389
epoch£º95	 i:6 	 global-step:1906	 l-p:0.0634499117732048
epoch£º95	 i:7 	 global-step:1907	 l-p:0.06416957825422287
epoch£º95	 i:8 	 global-step:1908	 l-p:0.06257877498865128
epoch£º95	 i:9 	 global-step:1909	 l-p:0.06269925832748413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[15.2001, 19.9858, 23.0709],
        [15.2001, 15.2035, 15.2001],
        [15.2001, 18.4829, 19.8550],
        [15.2001, 15.2000, 15.2001]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.016139769926667213 
model_pd.l_d.mean(): -1.9365767240524292 
model_pd.lagr.mean(): -1.920436978340149 
model_pd.lambdas: dict_items([('pout', tensor([1.0494], device='cuda:0')), ('power', tensor([0.0857], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-0.8836], device='cuda:0')), ('power', tensor([-11.6932], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.016139769926667213
epoch£º96	 i:1 	 global-step:1921	 l-p:0.0645759254693985
epoch£º96	 i:2 	 global-step:1922	 l-p:0.06256747990846634
epoch£º96	 i:3 	 global-step:1923	 l-p:0.06306111812591553
epoch£º96	 i:4 	 global-step:1924	 l-p:0.062256526201963425
epoch£º96	 i:5 	 global-step:1925	 l-p:0.06061695143580437
epoch£º96	 i:6 	 global-step:1926	 l-p:0.06210361421108246
epoch£º96	 i:7 	 global-step:1927	 l-p:0.05247761309146881
epoch£º96	 i:8 	 global-step:1928	 l-p:0.06222495064139366
epoch£º96	 i:9 	 global-step:1929	 l-p:0.057878680527210236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[16.2133, 18.4326, 18.7642],
        [16.2133, 19.9791, 21.7053],
        [16.2133, 16.2134, 16.2133],
        [16.2133, 21.7551, 25.5922]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.062095124274492264 
model_pd.l_d.mean(): -2.086256265640259 
model_pd.lagr.mean(): -2.0241611003875732 
model_pd.lambdas: dict_items([('pout', tensor([1.0386], device='cuda:0')), ('power', tensor([0.0798], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1241], device='cuda:0')), ('power', tensor([-11.4167], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.062095124274492264
epoch£º97	 i:1 	 global-step:1941	 l-p:0.062063150107860565
epoch£º97	 i:2 	 global-step:1942	 l-p:0.06291279941797256
epoch£º97	 i:3 	 global-step:1943	 l-p:0.05750694125890732
epoch£º97	 i:4 	 global-step:1944	 l-p:0.06274818629026413
epoch£º97	 i:5 	 global-step:1945	 l-p:0.0614783801138401
epoch£º97	 i:6 	 global-step:1946	 l-p:0.06167910620570183
epoch£º97	 i:7 	 global-step:1947	 l-p:0.061333440244197845
epoch£º97	 i:8 	 global-step:1948	 l-p:-0.02297077886760235
epoch£º97	 i:9 	 global-step:1949	 l-p:0.06314876675605774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[17.1347, 19.1303, 19.2439],
        [17.1347, 17.1347, 17.1347],
        [17.1347, 17.3818, 17.2080],
        [17.1347, 20.2518, 21.2229]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.06109750643372536 
model_pd.l_d.mean(): -2.008324384689331 
model_pd.lagr.mean(): -1.947226881980896 
model_pd.lambdas: dict_items([('pout', tensor([1.0272], device='cuda:0')), ('power', tensor([0.0744], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.1891], device='cuda:0')), ('power', tensor([-10.4849], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.06109750643372536
epoch£º98	 i:1 	 global-step:1961	 l-p:0.09591273218393326
epoch£º98	 i:2 	 global-step:1962	 l-p:0.06544279307126999
epoch£º98	 i:3 	 global-step:1963	 l-p:0.057633351534605026
epoch£º98	 i:4 	 global-step:1964	 l-p:0.060304220765829086
epoch£º98	 i:5 	 global-step:1965	 l-p:0.06032223626971245
epoch£º98	 i:6 	 global-step:1966	 l-p:0.061362095177173615
epoch£º98	 i:7 	 global-step:1967	 l-p:0.060882873833179474
epoch£º98	 i:8 	 global-step:1968	 l-p:0.05087623372673988
epoch£º98	 i:9 	 global-step:1969	 l-p:0.05999120697379112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[18.5081, 18.5081, 18.5081],
        [18.5081, 18.5089, 18.5081],
        [18.5081, 21.0684, 21.4529],
        [18.5081, 18.5085, 18.5081]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.05607330799102783 
model_pd.l_d.mean(): -1.934550404548645 
model_pd.lagr.mean(): -1.8784770965576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0149], device='cuda:0')), ('power', tensor([0.0696], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.2858], device='cuda:0')), ('power', tensor([-8.9663], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.05607330799102783
epoch£º99	 i:1 	 global-step:1981	 l-p:0.06005125865340233
epoch£º99	 i:2 	 global-step:1982	 l-p:0.06018650904297829
epoch£º99	 i:3 	 global-step:1983	 l-p:0.05981508642435074
epoch£º99	 i:4 	 global-step:1984	 l-p:0.06024330109357834
epoch£º99	 i:5 	 global-step:1985	 l-p:0.061834149062633514
epoch£º99	 i:6 	 global-step:1986	 l-p:0.03112252801656723
epoch£º99	 i:7 	 global-step:1987	 l-p:0.06336861103773117
epoch£º99	 i:8 	 global-step:1988	 l-p:0.05974399298429489
epoch£º99	 i:9 	 global-step:1989	 l-p:0.0597621314227581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[19.6140, 19.6186, 19.6141],
        [19.6140, 22.5812, 23.1722],
        [19.6140, 20.2067, 19.8937],
        [19.6140, 19.6140, 19.6140]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.05915546789765358 
model_pd.l_d.mean(): -1.9552898406982422 
model_pd.lagr.mean(): -1.896134376525879 
model_pd.lambdas: dict_items([('pout', tensor([1.0017], device='cuda:0')), ('power', tensor([0.0653], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4089], device='cuda:0')), ('power', tensor([-8.2489], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.05915546789765358
epoch£º100	 i:1 	 global-step:2001	 l-p:0.059270843863487244
epoch£º100	 i:2 	 global-step:2002	 l-p:0.06241035461425781
epoch£º100	 i:3 	 global-step:2003	 l-p:0.05969378352165222
epoch£º100	 i:4 	 global-step:2004	 l-p:0.05915078893303871
epoch£º100	 i:5 	 global-step:2005	 l-p:0.05976027250289917
epoch£º100	 i:6 	 global-step:2006	 l-p:0.05223848298192024
epoch£º100	 i:7 	 global-step:2007	 l-p:0.05891842767596245
epoch£º100	 i:8 	 global-step:2008	 l-p:0.06022336333990097
epoch£º100	 i:9 	 global-step:2009	 l-p:0.06915616244077682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[20.5280, 20.9653, 20.6929],
        [20.5280, 20.5283, 20.5280],
        [20.5280, 20.5443, 20.5288],
        [20.5280, 20.5280, 20.5280]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.059241313487291336 
model_pd.l_d.mean(): -1.8564479351043701 
model_pd.lagr.mean(): -1.7972066402435303 
model_pd.lambdas: dict_items([('pout', tensor([0.9879], device='cuda:0')), ('power', tensor([0.0616], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4264], device='cuda:0')), ('power', tensor([-7.1926], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.059241313487291336
epoch£º101	 i:1 	 global-step:2021	 l-p:0.059048641473054886
epoch£º101	 i:2 	 global-step:2022	 l-p:0.06017060577869415
epoch£º101	 i:3 	 global-step:2023	 l-p:0.05865722522139549
epoch£º101	 i:4 	 global-step:2024	 l-p:0.04928923398256302
epoch£º101	 i:5 	 global-step:2025	 l-p:0.06518831104040146
epoch£º101	 i:6 	 global-step:2026	 l-p:0.05886746197938919
epoch£º101	 i:7 	 global-step:2027	 l-p:0.05871010571718216
epoch£º101	 i:8 	 global-step:2028	 l-p:0.05854735150933266
epoch£º101	 i:9 	 global-step:2029	 l-p:0.06017114967107773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[21.2750, 25.6622, 27.3185],
        [21.2750, 25.6887, 27.3707],
        [21.2750, 28.3992, 33.1724],
        [21.2750, 22.4583, 22.0892]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.05862158536911011 
model_pd.l_d.mean(): -1.7915736436843872 
model_pd.lagr.mean(): -1.7329521179199219 
model_pd.lambdas: dict_items([('pout', tensor([0.9736], device='cuda:0')), ('power', tensor([0.0582], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4495], device='cuda:0')), ('power', tensor([-6.4651], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.05862158536911011
epoch£º102	 i:1 	 global-step:2041	 l-p:0.05866139382123947
epoch£º102	 i:2 	 global-step:2042	 l-p:0.04101971164345741
epoch£º102	 i:3 	 global-step:2043	 l-p:0.06056488677859306
epoch£º102	 i:4 	 global-step:2044	 l-p:0.05821997672319412
epoch£º102	 i:5 	 global-step:2045	 l-p:0.0582580603659153
epoch£º102	 i:6 	 global-step:2046	 l-p:0.05806097015738487
epoch£º102	 i:7 	 global-step:2047	 l-p:0.058398693799972534
epoch£º102	 i:8 	 global-step:2048	 l-p:0.06280551105737686
epoch£º102	 i:9 	 global-step:2049	 l-p:0.058342304080724716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[22.0417, 22.7975, 22.4261],
        [22.0417, 24.4959, 24.5531],
        [22.0417, 22.7486, 22.3863],
        [22.0417, 22.1316, 22.0537]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.05836822837591171 
model_pd.l_d.mean(): -1.7349313497543335 
model_pd.lagr.mean(): -1.6765631437301636 
model_pd.lambdas: dict_items([('pout', tensor([0.9588], device='cuda:0')), ('power', tensor([0.0552], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.4801], device='cuda:0')), ('power', tensor([-5.6546], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.05836822837591171
epoch£º103	 i:1 	 global-step:2061	 l-p:0.058038510382175446
epoch£º103	 i:2 	 global-step:2062	 l-p:0.1695435643196106
epoch£º103	 i:3 	 global-step:2063	 l-p:0.06227393448352814
epoch£º103	 i:4 	 global-step:2064	 l-p:0.05935453251004219
epoch£º103	 i:5 	 global-step:2065	 l-p:0.05786620080471039
epoch£º103	 i:6 	 global-step:2066	 l-p:0.05800444632768631
epoch£º103	 i:7 	 global-step:2067	 l-p:0.057815875858068466
epoch£º103	 i:8 	 global-step:2068	 l-p:0.05812307074666023
epoch£º103	 i:9 	 global-step:2069	 l-p:0.0578056238591671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.0637, 23.0796, 23.0645],
        [23.0637, 24.1988, 23.7861],
        [23.0637, 23.5359, 23.2363],
        [23.0637, 29.1487, 32.3262]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.05759960040450096 
model_pd.l_d.mean(): -1.7896662950515747 
model_pd.lagr.mean(): -1.7320667505264282 
model_pd.lambdas: dict_items([('pout', tensor([0.9434], device='cuda:0')), ('power', tensor([0.0526], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6192], device='cuda:0')), ('power', tensor([-4.9116], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.05759960040450096
epoch£º104	 i:1 	 global-step:2081	 l-p:0.05775744467973709
epoch£º104	 i:2 	 global-step:2082	 l-p:0.05750222131609917
epoch£º104	 i:3 	 global-step:2083	 l-p:0.06882927566766739
epoch£º104	 i:4 	 global-step:2084	 l-p:0.058704111725091934
epoch£º104	 i:5 	 global-step:2085	 l-p:0.057880714535713196
epoch£º104	 i:6 	 global-step:2086	 l-p:0.05745677649974823
epoch£º104	 i:7 	 global-step:2087	 l-p:0.058020636439323425
epoch£º104	 i:8 	 global-step:2088	 l-p:0.057387933135032654
epoch£º104	 i:9 	 global-step:2089	 l-p:0.05825652927160263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[23.7831, 23.7831, 23.7831],
        [23.7831, 23.7831, 23.7831],
        [23.7831, 25.4361, 25.0776],
        [23.7831, 27.6702, 28.5932]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.05747700110077858 
model_pd.l_d.mean(): -1.717497706413269 
model_pd.lagr.mean(): -1.6600207090377808 
model_pd.lambdas: dict_items([('pout', tensor([0.9276], device='cuda:0')), ('power', tensor([0.0505], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6254], device='cuda:0')), ('power', tensor([-4.0901], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.05747700110077858
epoch£º105	 i:1 	 global-step:2101	 l-p:0.05731263384222984
epoch£º105	 i:2 	 global-step:2102	 l-p:0.06331462413072586
epoch£º105	 i:3 	 global-step:2103	 l-p:0.05719529464840889
epoch£º105	 i:4 	 global-step:2104	 l-p:0.057648658752441406
epoch£º105	 i:5 	 global-step:2105	 l-p:0.05742646008729935
epoch£º105	 i:6 	 global-step:2106	 l-p:0.0573868602514267
epoch£º105	 i:7 	 global-step:2107	 l-p:0.05813722312450409
epoch£º105	 i:8 	 global-step:2108	 l-p:0.059241123497486115
epoch£º105	 i:9 	 global-step:2109	 l-p:0.05817325785756111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.2798, 24.3072, 24.2815],
        [24.2798, 29.4049, 31.3893],
        [24.2798, 24.4501, 24.3114],
        [24.2798, 30.7249, 34.1044]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.05934586003422737 
model_pd.l_d.mean(): -1.5650335550308228 
model_pd.lagr.mean(): -1.5056877136230469 
model_pd.lambdas: dict_items([('pout', tensor([0.9115], device='cuda:0')), ('power', tensor([0.0486], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.5442], device='cuda:0')), ('power', tensor([-3.1788], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.05934586003422737
epoch£º106	 i:1 	 global-step:2121	 l-p:0.05716268718242645
epoch£º106	 i:2 	 global-step:2122	 l-p:0.05711910128593445
epoch£º106	 i:3 	 global-step:2123	 l-p:0.05712037906050682
epoch£º106	 i:4 	 global-step:2124	 l-p:0.057231221348047256
epoch£º106	 i:5 	 global-step:2125	 l-p:0.06208665668964386
epoch£º106	 i:6 	 global-step:2126	 l-p:0.05721282958984375
epoch£º106	 i:7 	 global-step:2127	 l-p:0.057096611708402634
epoch£º106	 i:8 	 global-step:2128	 l-p:0.05738049000501633
epoch£º106	 i:9 	 global-step:2129	 l-p:0.05829444155097008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[24.6719, 24.6971, 24.6734],
        [24.6719, 24.7217, 24.6762],
        [24.6719, 31.9674, 36.2668],
        [24.6719, 30.7879, 33.7401]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.05785072594881058 
model_pd.l_d.mean(): -1.5956575870513916 
model_pd.lagr.mean(): -1.5378068685531616 
model_pd.lambdas: dict_items([('pout', tensor([0.8950], device='cuda:0')), ('power', tensor([0.0470], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6207], device='cuda:0')), ('power', tensor([-3.0226], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.05785072594881058
epoch£º107	 i:1 	 global-step:2141	 l-p:0.05718051269650459
epoch£º107	 i:2 	 global-step:2142	 l-p:0.05736079439520836
epoch£º107	 i:3 	 global-step:2143	 l-p:0.05708629637956619
epoch£º107	 i:4 	 global-step:2144	 l-p:0.060677699744701385
epoch£º107	 i:5 	 global-step:2145	 l-p:0.05695243552327156
epoch£º107	 i:6 	 global-step:2146	 l-p:0.0569230355322361
epoch£º107	 i:7 	 global-step:2147	 l-p:0.05897210165858269
epoch£º107	 i:8 	 global-step:2148	 l-p:0.0578894279897213
epoch£º107	 i:9 	 global-step:2149	 l-p:0.05700637772679329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.0202, 25.1233, 25.0340],
        [25.0202, 25.0213, 25.0202],
        [25.0202, 25.9001, 25.4726],
        [25.0202, 25.0203, 25.0202]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.06054868549108505 
model_pd.l_d.mean(): -1.5480550527572632 
model_pd.lagr.mean(): -1.48750638961792 
model_pd.lambdas: dict_items([('pout', tensor([0.8785], device='cuda:0')), ('power', tensor([0.0455], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6257], device='cuda:0')), ('power', tensor([-2.5687], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.06054868549108505
epoch£º108	 i:1 	 global-step:2161	 l-p:0.05688037723302841
epoch£º108	 i:2 	 global-step:2162	 l-p:0.05685547739267349
epoch£º108	 i:3 	 global-step:2163	 l-p:0.058620426803827286
epoch£º108	 i:4 	 global-step:2164	 l-p:0.05766870453953743
epoch£º108	 i:5 	 global-step:2165	 l-p:0.05719507858157158
epoch£º108	 i:6 	 global-step:2166	 l-p:0.05708552524447441
epoch£º108	 i:7 	 global-step:2167	 l-p:0.05680641904473305
epoch£º108	 i:8 	 global-step:2168	 l-p:0.0571829192340374
epoch£º108	 i:9 	 global-step:2169	 l-p:0.057474009692668915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.3452, 26.1127, 25.7040],
        [25.3452, 25.3610, 25.3459],
        [25.3452, 34.3032, 40.5587],
        [25.3452, 25.3452, 25.3452]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.057044561952352524 
model_pd.l_d.mean(): -1.5613290071487427 
model_pd.lagr.mean(): -1.5042845010757446 
model_pd.lambdas: dict_items([('pout', tensor([0.8616], device='cuda:0')), ('power', tensor([0.0442], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6809], device='cuda:0')), ('power', tensor([-2.4852], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.057044561952352524
epoch£º109	 i:1 	 global-step:2181	 l-p:0.06028082221746445
epoch£º109	 i:2 	 global-step:2182	 l-p:0.056882552802562714
epoch£º109	 i:3 	 global-step:2183	 l-p:0.05676078051328659
epoch£º109	 i:4 	 global-step:2184	 l-p:0.05697713419795036
epoch£º109	 i:5 	 global-step:2185	 l-p:0.05861635506153107
epoch£º109	 i:6 	 global-step:2186	 l-p:0.05697859078645706
epoch£º109	 i:7 	 global-step:2187	 l-p:0.05754159763455391
epoch£º109	 i:8 	 global-step:2188	 l-p:0.05690334364771843
epoch£º109	 i:9 	 global-step:2189	 l-p:0.05675691366195679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.6362, 25.6410, 25.6363],
        [25.6362, 25.6362, 25.6362],
        [25.6362, 25.6464, 25.6366],
        [25.6362, 25.8395, 25.6768]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.05667924880981445 
model_pd.l_d.mean(): -1.6010520458221436 
model_pd.lagr.mean(): -1.544372797012329 
model_pd.lambdas: dict_items([('pout', tensor([0.8446], device='cuda:0')), ('power', tensor([0.0431], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7705], device='cuda:0')), ('power', tensor([-2.3736], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.05667924880981445
epoch£º110	 i:1 	 global-step:2201	 l-p:0.0567002110183239
epoch£º110	 i:2 	 global-step:2202	 l-p:0.058334361761808395
epoch£º110	 i:3 	 global-step:2203	 l-p:0.056786127388477325
epoch£º110	 i:4 	 global-step:2204	 l-p:0.05689743533730507
epoch£º110	 i:5 	 global-step:2205	 l-p:0.05762539803981781
epoch£º110	 i:6 	 global-step:2206	 l-p:0.05937197431921959
epoch£º110	 i:7 	 global-step:2207	 l-p:0.05736913904547691
epoch£º110	 i:8 	 global-step:2208	 l-p:0.056763023138046265
epoch£º110	 i:9 	 global-step:2209	 l-p:0.05688750743865967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.8812, 25.9279, 25.8850],
        [25.8812, 31.3147, 33.3907],
        [25.8812, 30.1334, 31.1450],
        [25.8812, 32.6064, 36.0340]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.05683136731386185 
model_pd.l_d.mean(): -1.5007455348968506 
model_pd.lagr.mean(): -1.4439141750335693 
model_pd.lambdas: dict_items([('pout', tensor([0.8277], device='cuda:0')), ('power', tensor([0.0420], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7102], device='cuda:0')), ('power', tensor([-1.9540], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.05683136731386185
epoch£º111	 i:1 	 global-step:2221	 l-p:0.05689564719796181
epoch£º111	 i:2 	 global-step:2222	 l-p:0.05735208839178085
epoch£º111	 i:3 	 global-step:2223	 l-p:0.057261571288108826
epoch£º111	 i:4 	 global-step:2224	 l-p:0.05665102228522301
epoch£º111	 i:5 	 global-step:2225	 l-p:0.05670120194554329
epoch£º111	 i:6 	 global-step:2226	 l-p:0.05799956992268562
epoch£º111	 i:7 	 global-step:2227	 l-p:0.05661739781498909
epoch£º111	 i:8 	 global-step:2228	 l-p:0.056802332401275635
epoch£º111	 i:9 	 global-step:2229	 l-p:0.059282220900058746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0830, 26.7061, 26.3331],
        [26.0830, 26.6878, 26.3212],
        [26.0830, 26.0832, 26.0830],
        [26.0830, 26.3080, 26.1303]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.05663008242845535 
model_pd.l_d.mean(): -1.5043532848358154 
model_pd.lagr.mean(): -1.447723150253296 
model_pd.lambdas: dict_items([('pout', tensor([0.8104], device='cuda:0')), ('power', tensor([0.0411], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7554], device='cuda:0')), ('power', tensor([-1.9081], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.05663008242845535
epoch£º112	 i:1 	 global-step:2241	 l-p:0.05900847166776657
epoch£º112	 i:2 	 global-step:2242	 l-p:0.056794822216033936
epoch£º112	 i:3 	 global-step:2243	 l-p:0.05665070563554764
epoch£º112	 i:4 	 global-step:2244	 l-p:0.05725108087062836
epoch£º112	 i:5 	 global-step:2245	 l-p:0.0572015754878521
epoch£º112	 i:6 	 global-step:2246	 l-p:0.05687957629561424
epoch£º112	 i:7 	 global-step:2247	 l-p:0.05808786302804947
epoch£º112	 i:8 	 global-step:2248	 l-p:0.056525010615587234
epoch£º112	 i:9 	 global-step:2249	 l-p:0.05669478327035904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.2462, 26.2466, 26.2462],
        [26.2462, 31.8119, 33.9694],
        [26.2462, 26.8736, 26.4981],
        [26.2462, 26.2467, 26.2462]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.057166360318660736 
model_pd.l_d.mean(): -1.4211362600326538 
model_pd.lagr.mean(): -1.3639699220657349 
model_pd.lambdas: dict_items([('pout', tensor([0.7932], device='cuda:0')), ('power', tensor([0.0403], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7113], device='cuda:0')), ('power', tensor([-1.5049], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.057166360318660736
epoch£º113	 i:1 	 global-step:2261	 l-p:0.05722654238343239
epoch£º113	 i:2 	 global-step:2262	 l-p:0.05657036229968071
epoch£º113	 i:3 	 global-step:2263	 l-p:0.05657757818698883
epoch£º113	 i:4 	 global-step:2264	 l-p:0.056851375848054886
epoch£º113	 i:5 	 global-step:2265	 l-p:0.05666694417595863
epoch£º113	 i:6 	 global-step:2266	 l-p:0.05667499825358391
epoch£º113	 i:7 	 global-step:2267	 l-p:0.060096051543951035
epoch£º113	 i:8 	 global-step:2268	 l-p:0.0568162240087986
epoch£º113	 i:9 	 global-step:2269	 l-p:0.05649059638381004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3490, 26.3491, 26.3490],
        [26.3490, 26.3667, 26.3498],
        [26.3490, 26.3585, 26.3493],
        [26.3490, 33.3757, 37.0632]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.05684458464384079 
model_pd.l_d.mean(): -1.3708488941192627 
model_pd.lagr.mean(): -1.3140043020248413 
model_pd.lambdas: dict_items([('pout', tensor([0.7759], device='cuda:0')), ('power', tensor([0.0396], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6919], device='cuda:0')), ('power', tensor([-1.3930], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.05684458464384079
epoch£º114	 i:1 	 global-step:2281	 l-p:0.05654967203736305
epoch£º114	 i:2 	 global-step:2282	 l-p:0.056710947304964066
epoch£º114	 i:3 	 global-step:2283	 l-p:0.05948744714260101
epoch£º114	 i:4 	 global-step:2284	 l-p:0.056531429290771484
epoch£º114	 i:5 	 global-step:2285	 l-p:0.05649180710315704
epoch£º114	 i:6 	 global-step:2286	 l-p:0.05719047784805298
epoch£º114	 i:7 	 global-step:2287	 l-p:0.05659954994916916
epoch£º114	 i:8 	 global-step:2288	 l-p:0.057868726551532745
epoch£º114	 i:9 	 global-step:2289	 l-p:0.05655621364712715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.4042, 26.4042, 26.4042],
        [26.4042, 26.4044, 26.4042],
        [26.4042, 26.4043, 26.4042],
        [26.4042, 28.2500, 27.8493]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.05662102624773979 
model_pd.l_d.mean(): -1.3900530338287354 
model_pd.lagr.mean(): -1.3334319591522217 
model_pd.lambdas: dict_items([('pout', tensor([0.7585], device='cuda:0')), ('power', tensor([0.0388], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7520], device='cuda:0')), ('power', tensor([-1.4938], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.05662102624773979
epoch£º115	 i:1 	 global-step:2301	 l-p:0.05672212317585945
epoch£º115	 i:2 	 global-step:2302	 l-p:0.05793432891368866
epoch£º115	 i:3 	 global-step:2303	 l-p:0.056596726179122925
epoch£º115	 i:4 	 global-step:2304	 l-p:0.058762453496456146
epoch£º115	 i:5 	 global-step:2305	 l-p:0.05700331926345825
epoch£º115	 i:6 	 global-step:2306	 l-p:0.0566546767950058
epoch£º115	 i:7 	 global-step:2307	 l-p:0.056548822671175
epoch£º115	 i:8 	 global-step:2308	 l-p:0.057365044951438904
epoch£º115	 i:9 	 global-step:2309	 l-p:0.05649014189839363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.4130, 27.2135, 26.7867],
        [26.4130, 26.7977, 26.5255],
        [26.4130, 26.4133, 26.4130],
        [26.4130, 28.3022, 27.9121]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.05684075132012367 
model_pd.l_d.mean(): -1.3256564140319824 
model_pd.lagr.mean(): -1.2688156366348267 
model_pd.lambdas: dict_items([('pout', tensor([0.7412], device='cuda:0')), ('power', tensor([0.0381], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7129], device='cuda:0')), ('power', tensor([-1.3931], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.05684075132012367
epoch£º116	 i:1 	 global-step:2321	 l-p:0.056549251079559326
epoch£º116	 i:2 	 global-step:2322	 l-p:0.05763261765241623
epoch£º116	 i:3 	 global-step:2323	 l-p:0.0572982095181942
epoch£º116	 i:4 	 global-step:2324	 l-p:0.056543271988630295
epoch£º116	 i:5 	 global-step:2325	 l-p:0.0565151572227478
epoch£º116	 i:6 	 global-step:2326	 l-p:0.05660003423690796
epoch£º116	 i:7 	 global-step:2327	 l-p:0.05660406872630119
epoch£º116	 i:8 	 global-step:2328	 l-p:0.059023454785346985
epoch£º116	 i:9 	 global-step:2329	 l-p:0.05714685842394829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3766, 26.3872, 26.3770],
        [26.3766, 26.9874, 26.6168],
        [26.3766, 26.3768, 26.3766],
        [26.3766, 26.3766, 26.3766]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.056504230946302414 
model_pd.l_d.mean(): -1.3666170835494995 
model_pd.lagr.mean(): -1.3101128339767456 
model_pd.lambdas: dict_items([('pout', tensor([0.7237], device='cuda:0')), ('power', tensor([0.0374], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7956], device='cuda:0')), ('power', tensor([-1.7048], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.056504230946302414
epoch£º117	 i:1 	 global-step:2341	 l-p:0.056692689657211304
epoch£º117	 i:2 	 global-step:2342	 l-p:0.056587543338537216
epoch£º117	 i:3 	 global-step:2343	 l-p:0.05665314942598343
epoch£º117	 i:4 	 global-step:2344	 l-p:0.05696871131658554
epoch£º117	 i:5 	 global-step:2345	 l-p:0.05653360113501549
epoch£º117	 i:6 	 global-step:2346	 l-p:0.05916052684187889
epoch£º117	 i:7 	 global-step:2347	 l-p:0.05791344493627548
epoch£º117	 i:8 	 global-step:2348	 l-p:0.05734246224164963
epoch£º117	 i:9 	 global-step:2349	 l-p:0.05659409239888191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3183, 35.5836, 42.0212],
        [26.3183, 29.1537, 29.1538],
        [26.3183, 26.5456, 26.3661],
        [26.3183, 26.3183, 26.3183]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.05683867260813713 
model_pd.l_d.mean(): -1.2614377737045288 
model_pd.lagr.mean(): -1.204599142074585 
model_pd.lambdas: dict_items([('pout', tensor([0.7064], device='cuda:0')), ('power', tensor([0.0367], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7050], device='cuda:0')), ('power', tensor([-1.4708], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.05683867260813713
epoch£º118	 i:1 	 global-step:2361	 l-p:0.0566820427775383
epoch£º118	 i:2 	 global-step:2362	 l-p:0.05999504029750824
epoch£º118	 i:3 	 global-step:2363	 l-p:0.05655993893742561
epoch£º118	 i:4 	 global-step:2364	 l-p:0.05660174414515495
epoch£º118	 i:5 	 global-step:2365	 l-p:0.05704594403505325
epoch£º118	 i:6 	 global-step:2366	 l-p:0.056740716099739075
epoch£º118	 i:7 	 global-step:2367	 l-p:0.056723784655332565
epoch£º118	 i:8 	 global-step:2368	 l-p:0.05727379024028778
epoch£º118	 i:9 	 global-step:2369	 l-p:0.05669713765382767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.2543, 28.0923, 27.6946],
        [26.2543, 26.3436, 26.2649],
        [26.2543, 30.7871, 31.9941],
        [26.2543, 31.7721, 33.8812]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.05652167648077011 
model_pd.l_d.mean(): -1.2965720891952515 
model_pd.lagr.mean(): -1.2400504350662231 
model_pd.lambdas: dict_items([('pout', tensor([0.6890], device='cuda:0')), ('power', tensor([0.0359], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7863], device='cuda:0')), ('power', tensor([-1.7394], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.05652167648077011
epoch£º119	 i:1 	 global-step:2381	 l-p:0.05673135444521904
epoch£º119	 i:2 	 global-step:2382	 l-p:0.058316390961408615
epoch£º119	 i:3 	 global-step:2383	 l-p:0.05669233202934265
epoch£º119	 i:4 	 global-step:2384	 l-p:0.05662406235933304
epoch£º119	 i:5 	 global-step:2385	 l-p:0.05699931085109711
epoch£º119	 i:6 	 global-step:2386	 l-p:0.05677446350455284
epoch£º119	 i:7 	 global-step:2387	 l-p:0.05652465671300888
epoch£º119	 i:8 	 global-step:2388	 l-p:0.059060461819171906
epoch£º119	 i:9 	 global-step:2389	 l-p:0.057239141315221786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1718, 26.1728, 26.1718],
        [26.1718, 26.1736, 26.1718],
        [26.1718, 26.1822, 26.1722],
        [26.1718, 29.7218, 30.1742]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.056658487766981125 
model_pd.l_d.mean(): -1.2337605953216553 
model_pd.lagr.mean(): -1.1771020889282227 
model_pd.lambdas: dict_items([('pout', tensor([0.6718], device='cuda:0')), ('power', tensor([0.0351], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7420], device='cuda:0')), ('power', tensor([-1.7191], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.056658487766981125
epoch£º120	 i:1 	 global-step:2401	 l-p:0.05902469530701637
epoch£º120	 i:2 	 global-step:2402	 l-p:0.05661046877503395
epoch£º120	 i:3 	 global-step:2403	 l-p:0.05668263882398605
epoch£º120	 i:4 	 global-step:2404	 l-p:0.0566759817302227
epoch£º120	 i:5 	 global-step:2405	 l-p:0.05758602172136307
epoch£º120	 i:6 	 global-step:2406	 l-p:0.05677338317036629
epoch£º120	 i:7 	 global-step:2407	 l-p:0.0580860897898674
epoch£º120	 i:8 	 global-step:2408	 l-p:0.056539978832006454
epoch£º120	 i:9 	 global-step:2409	 l-p:0.057131946086883545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1001, 27.9240, 27.5281],
        [26.1001, 26.4025, 26.1767],
        [26.1001, 26.1001, 26.1001],
        [26.1001, 26.1068, 26.1003]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.05899190902709961 
model_pd.l_d.mean(): -1.203835129737854 
model_pd.lagr.mean(): -1.1448432207107544 
model_pd.lambdas: dict_items([('pout', tensor([0.6545], device='cuda:0')), ('power', tensor([0.0342], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7473], device='cuda:0')), ('power', tensor([-1.6637], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.05899190902709961
epoch£º121	 i:1 	 global-step:2421	 l-p:0.05663255229592323
epoch£º121	 i:2 	 global-step:2422	 l-p:0.056702449917793274
epoch£º121	 i:3 	 global-step:2423	 l-p:0.05698489025235176
epoch£º121	 i:4 	 global-step:2424	 l-p:0.05714211240410805
epoch£º121	 i:5 	 global-step:2425	 l-p:0.05668618530035019
epoch£º121	 i:6 	 global-step:2426	 l-p:0.05781106650829315
epoch£º121	 i:7 	 global-step:2427	 l-p:0.056793104857206345
epoch£º121	 i:8 	 global-step:2428	 l-p:0.05738850310444832
epoch£º121	 i:9 	 global-step:2429	 l-p:0.05691147595643997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0354, 26.3219, 26.1057],
        [26.0354, 26.1432, 26.0498],
        [26.0354, 26.0355, 26.0354],
        [26.0354, 26.1117, 26.0437]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.05938299000263214 
model_pd.l_d.mean(): -1.1181045770645142 
model_pd.lagr.mean(): -1.0587215423583984 
model_pd.lambdas: dict_items([('pout', tensor([0.6374], device='cuda:0')), ('power', tensor([0.0334], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6701], device='cuda:0')), ('power', tensor([-1.5187], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.05938299000263214
epoch£º122	 i:1 	 global-step:2441	 l-p:0.057550910860300064
epoch£º122	 i:2 	 global-step:2442	 l-p:0.05673808977007866
epoch£º122	 i:3 	 global-step:2443	 l-p:0.05667911842465401
epoch£º122	 i:4 	 global-step:2444	 l-p:0.056574590504169464
epoch£º122	 i:5 	 global-step:2445	 l-p:0.056695401668548584
epoch£º122	 i:6 	 global-step:2446	 l-p:0.05655791983008385
epoch£º122	 i:7 	 global-step:2447	 l-p:0.05703906714916229
epoch£º122	 i:8 	 global-step:2448	 l-p:0.05685364454984665
epoch£º122	 i:9 	 global-step:2449	 l-p:0.058216437697410583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9819, 25.9875, 25.9820],
        [25.9819, 26.5573, 26.2019],
        [25.9819, 26.0518, 25.9891],
        [25.9819, 25.9819, 25.9818]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.056661732494831085 
model_pd.l_d.mean(): -1.1519564390182495 
model_pd.lagr.mean(): -1.095294713973999 
model_pd.lambdas: dict_items([('pout', tensor([0.6201], device='cuda:0')), ('power', tensor([0.0324], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7501], device='cuda:0')), ('power', tensor([-1.9528], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.056661732494831085
epoch£º123	 i:1 	 global-step:2461	 l-p:0.05664467811584473
epoch£º123	 i:2 	 global-step:2462	 l-p:0.05675864592194557
epoch£º123	 i:3 	 global-step:2463	 l-p:0.05661337822675705
epoch£º123	 i:4 	 global-step:2464	 l-p:0.058100707828998566
epoch£º123	 i:5 	 global-step:2465	 l-p:0.05673525854945183
epoch£º123	 i:6 	 global-step:2466	 l-p:0.05933175981044769
epoch£º123	 i:7 	 global-step:2467	 l-p:0.057073380798101425
epoch£º123	 i:8 	 global-step:2468	 l-p:0.0566798560321331
epoch£º123	 i:9 	 global-step:2469	 l-p:0.05793871730566025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9385, 25.9426, 25.9385],
        [25.9385, 31.9733, 34.6471],
        [25.9385, 26.3233, 26.0525],
        [25.9385, 26.4962, 26.1478]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.056889619678258896 
model_pd.l_d.mean(): -1.1040188074111938 
model_pd.lagr.mean(): -1.0471291542053223 
model_pd.lambdas: dict_items([('pout', tensor([0.6030], device='cuda:0')), ('power', tensor([0.0315], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7238], device='cuda:0')), ('power', tensor([-1.9464], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.056889619678258896
epoch£º124	 i:1 	 global-step:2481	 l-p:0.05723632127046585
epoch£º124	 i:2 	 global-step:2482	 l-p:0.05680101737380028
epoch£º124	 i:3 	 global-step:2483	 l-p:0.05927329882979393
epoch£º124	 i:4 	 global-step:2484	 l-p:0.057446908205747604
epoch£º124	 i:5 	 global-step:2485	 l-p:0.058077771216630936
epoch£º124	 i:6 	 global-step:2486	 l-p:0.056619081646203995
epoch£º124	 i:7 	 global-step:2487	 l-p:0.056835029274225235
epoch£º124	 i:8 	 global-step:2488	 l-p:0.05671406909823418
epoch£º124	 i:9 	 global-step:2489	 l-p:0.05674799904227257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9263, 26.0737, 25.9502],
        [25.9263, 35.4875, 42.4109],
        [25.9263, 26.1113, 25.9609],
        [25.9263, 29.4576, 29.9165]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.0593133382499218 
model_pd.l_d.mean(): -1.0488386154174805 
model_pd.lagr.mean(): -0.9895252585411072 
model_pd.lambdas: dict_items([('pout', tensor([0.5859], device='cuda:0')), ('power', tensor([0.0306], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6951], device='cuda:0')), ('power', tensor([-1.7218], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.0593133382499218
epoch£º125	 i:1 	 global-step:2501	 l-p:0.058012187480926514
epoch£º125	 i:2 	 global-step:2502	 l-p:0.05741078779101372
epoch£º125	 i:3 	 global-step:2503	 l-p:0.05668843165040016
epoch£º125	 i:4 	 global-step:2504	 l-p:0.057459574192762375
epoch£º125	 i:5 	 global-step:2505	 l-p:0.05662531778216362
epoch£º125	 i:6 	 global-step:2506	 l-p:0.056937627494335175
epoch£º125	 i:7 	 global-step:2507	 l-p:0.0568564347922802
epoch£º125	 i:8 	 global-step:2508	 l-p:0.0567367747426033
epoch£º125	 i:9 	 global-step:2509	 l-p:0.05661667138338089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9276, 31.7384, 34.1831],
        [25.9276, 29.8491, 30.5933],
        [25.9276, 25.9277, 25.9276],
        [25.9276, 25.9932, 25.9341]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.05668705701828003 
model_pd.l_d.mean(): -1.0716023445129395 
model_pd.lagr.mean(): -1.0149152278900146 
model_pd.lambdas: dict_items([('pout', tensor([0.5687], device='cuda:0')), ('power', tensor([0.0296], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7701], device='cuda:0')), ('power', tensor([-2.0794], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.05668705701828003
epoch£º126	 i:1 	 global-step:2521	 l-p:0.05925289914011955
epoch£º126	 i:2 	 global-step:2522	 l-p:0.05703536793589592
epoch£º126	 i:3 	 global-step:2523	 l-p:0.05717591196298599
epoch£º126	 i:4 	 global-step:2524	 l-p:0.05675369128584862
epoch£º126	 i:5 	 global-step:2525	 l-p:0.056717399507761
epoch£º126	 i:6 	 global-step:2526	 l-p:0.05749301239848137
epoch£º126	 i:7 	 global-step:2527	 l-p:0.05673415958881378
epoch£º126	 i:8 	 global-step:2528	 l-p:0.05680802837014198
epoch£º126	 i:9 	 global-step:2529	 l-p:0.05799772962927818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[25.9313, 27.1445, 26.6760],
        [25.9313, 26.7918, 26.3570],
        [25.9313, 27.8669, 27.5067],
        [25.9313, 32.5610, 35.8734]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.05734375864267349 
model_pd.l_d.mean(): -1.0035971403121948 
model_pd.lagr.mean(): -0.9462533593177795 
model_pd.lambdas: dict_items([('pout', tensor([0.5516], device='cuda:0')), ('power', tensor([0.0287], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7181], device='cuda:0')), ('power', tensor([-1.8391], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.05734375864267349
epoch£º127	 i:1 	 global-step:2541	 l-p:0.05668951943516731
epoch£º127	 i:2 	 global-step:2542	 l-p:0.05693003907799721
epoch£º127	 i:3 	 global-step:2543	 l-p:0.05717268958687782
epoch£º127	 i:4 	 global-step:2544	 l-p:0.05943905562162399
epoch£º127	 i:5 	 global-step:2545	 l-p:0.05687441676855087
epoch£º127	 i:6 	 global-step:2546	 l-p:0.056619346141815186
epoch£º127	 i:7 	 global-step:2547	 l-p:0.05678350478410721
epoch£º127	 i:8 	 global-step:2548	 l-p:0.05676989257335663
epoch£º127	 i:9 	 global-step:2549	 l-p:0.057990044355392456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9500, 29.7427, 30.3875],
        [25.9500, 27.6048, 27.1770],
        [25.9500, 25.9810, 25.9519],
        [25.9500, 25.9577, 25.9502]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.05696143954992294 
model_pd.l_d.mean(): -0.9522110819816589 
model_pd.lagr.mean(): -0.8952496647834778 
model_pd.lambdas: dict_items([('pout', tensor([0.5345], device='cuda:0')), ('power', tensor([0.0278], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6810], device='cuda:0')), ('power', tensor([-1.8287], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.05696143954992294
epoch£º128	 i:1 	 global-step:2561	 l-p:0.0569017194211483
epoch£º128	 i:2 	 global-step:2562	 l-p:0.056757476180791855
epoch£º128	 i:3 	 global-step:2563	 l-p:0.05931512638926506
epoch£º128	 i:4 	 global-step:2564	 l-p:0.05660552904009819
epoch£º128	 i:5 	 global-step:2565	 l-p:0.056664030998945236
epoch£º128	 i:6 	 global-step:2566	 l-p:0.05742207542061806
epoch£º128	 i:7 	 global-step:2567	 l-p:0.05723727494478226
epoch£º128	 i:8 	 global-step:2568	 l-p:0.05801454558968544
epoch£º128	 i:9 	 global-step:2569	 l-p:0.05663450062274933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[25.9788, 25.9892, 25.9792],
        [25.9788, 26.1853, 26.0201],
        [25.9788, 25.9790, 25.9788],
        [25.9788, 27.3273, 26.8612]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.05680771544575691 
model_pd.l_d.mean(): -0.9276747107505798 
model_pd.lagr.mean(): -0.8708670139312744 
model_pd.lambdas: dict_items([('pout', tensor([0.5173], device='cuda:0')), ('power', tensor([0.0268], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6927], device='cuda:0')), ('power', tensor([-1.8254], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.05680771544575691
epoch£º129	 i:1 	 global-step:2581	 l-p:0.058408793061971664
epoch£º129	 i:2 	 global-step:2582	 l-p:0.05727824941277504
epoch£º129	 i:3 	 global-step:2583	 l-p:0.05668345466256142
epoch£º129	 i:4 	 global-step:2584	 l-p:0.05707283318042755
epoch£º129	 i:5 	 global-step:2585	 l-p:0.05669701471924782
epoch£º129	 i:6 	 global-step:2586	 l-p:0.059517353773117065
epoch£º129	 i:7 	 global-step:2587	 l-p:0.056587886065244675
epoch£º129	 i:8 	 global-step:2588	 l-p:0.05667593330144882
epoch£º129	 i:9 	 global-step:2589	 l-p:0.05664757639169693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0145, 27.4194, 26.9563],
        [26.0145, 26.0148, 26.0145],
        [26.0145, 26.0147, 26.0145],
        [26.0145, 26.0277, 26.0150]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.05931713804602623 
model_pd.l_d.mean(): -0.8762559294700623 
model_pd.lagr.mean(): -0.8169388175010681 
model_pd.lambdas: dict_items([('pout', tensor([0.5002], device='cuda:0')), ('power', tensor([0.0259], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6630], device='cuda:0')), ('power', tensor([-1.6027], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.05931713804602623
epoch£º130	 i:1 	 global-step:2601	 l-p:0.05793604627251625
epoch£º130	 i:2 	 global-step:2602	 l-p:0.056666191667318344
epoch£º130	 i:3 	 global-step:2603	 l-p:0.05660069361329079
epoch£º130	 i:4 	 global-step:2604	 l-p:0.05764260143041611
epoch£º130	 i:5 	 global-step:2605	 l-p:0.0568665973842144
epoch£º130	 i:6 	 global-step:2606	 l-p:0.05740075930953026
epoch£º130	 i:7 	 global-step:2607	 l-p:0.05661439150571823
epoch£º130	 i:8 	 global-step:2608	 l-p:0.05658401921391487
epoch£º130	 i:9 	 global-step:2609	 l-p:0.05660048499703407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0527, 26.0526, 26.0526],
        [26.0527, 26.8581, 26.4335],
        [26.0527, 26.0540, 26.0527],
        [26.0527, 26.0528, 26.0526]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.056976377964019775 
model_pd.l_d.mean(): -0.8675578832626343 
model_pd.lagr.mean(): -0.8105815052986145 
model_pd.lambdas: dict_items([('pout', tensor([0.4829], device='cuda:0')), ('power', tensor([0.0250], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7011], device='cuda:0')), ('power', tensor([-1.7174], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.056976377964019775
epoch£º131	 i:1 	 global-step:2621	 l-p:0.05670522525906563
epoch£º131	 i:2 	 global-step:2622	 l-p:0.056722044944763184
epoch£º131	 i:3 	 global-step:2623	 l-p:0.05712791159749031
epoch£º131	 i:4 	 global-step:2624	 l-p:0.05734676122665405
epoch£º131	 i:5 	 global-step:2625	 l-p:0.05916320160031319
epoch£º131	 i:6 	 global-step:2626	 l-p:0.056699711829423904
epoch£º131	 i:7 	 global-step:2627	 l-p:0.05665626749396324
epoch£º131	 i:8 	 global-step:2628	 l-p:0.057928416877985
epoch£º131	 i:9 	 global-step:2629	 l-p:0.056768111884593964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0760, 26.0764, 26.0760],
        [26.0760, 34.4869, 39.8765],
        [26.0760, 26.0760, 26.0760],
        [26.0760, 29.8371, 30.4474]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.05736134946346283 
model_pd.l_d.mean(): -0.8165853023529053 
model_pd.lagr.mean(): -0.7592239379882812 
model_pd.lambdas: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([0.0241], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6640], device='cuda:0')), ('power', tensor([-1.6010], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.05736134946346283
epoch£º132	 i:1 	 global-step:2641	 l-p:0.059140563011169434
epoch£º132	 i:2 	 global-step:2642	 l-p:0.05663485452532768
epoch£º132	 i:3 	 global-step:2643	 l-p:0.05651358515024185
epoch£º132	 i:4 	 global-step:2644	 l-p:0.05671430751681328
epoch£º132	 i:5 	 global-step:2645	 l-p:0.057959459722042084
epoch£º132	 i:6 	 global-step:2646	 l-p:0.05669530853629112
epoch£º132	 i:7 	 global-step:2647	 l-p:0.0566842295229435
epoch£º132	 i:8 	 global-step:2648	 l-p:0.057530276477336884
epoch£º132	 i:9 	 global-step:2649	 l-p:0.05675344914197922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1031, 26.1031, 26.1031],
        [26.1031, 26.1031, 26.1031],
        [26.1031, 28.9262, 28.9322],
        [26.1031, 27.2523, 26.7820]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.058464229106903076 
model_pd.l_d.mean(): -0.7897868156433105 
model_pd.lagr.mean(): -0.7313225865364075 
model_pd.lambdas: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([0.0233], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6807], device='cuda:0')), ('power', tensor([-1.4174], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.058464229106903076
epoch£º133	 i:1 	 global-step:2661	 l-p:0.056908149272203445
epoch£º133	 i:2 	 global-step:2662	 l-p:0.05658658221364021
epoch£º133	 i:3 	 global-step:2663	 l-p:0.0565730556845665
epoch£º133	 i:4 	 global-step:2664	 l-p:0.05919407680630684
epoch£º133	 i:5 	 global-step:2665	 l-p:0.05711023136973381
epoch£º133	 i:6 	 global-step:2666	 l-p:0.056778859347105026
epoch£º133	 i:7 	 global-step:2667	 l-p:0.056711625307798386
epoch£º133	 i:8 	 global-step:2668	 l-p:0.056701600551605225
epoch£º133	 i:9 	 global-step:2669	 l-p:0.056861381977796555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1221, 28.8422, 28.7927],
        [26.1221, 26.9716, 26.5369],
        [26.1221, 26.3087, 26.1570],
        [26.1221, 26.1534, 26.1241]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.05671287700533867 
model_pd.l_d.mean(): -0.7962056994438171 
model_pd.lagr.mean(): -0.7394928336143494 
model_pd.lambdas: dict_items([('pout', tensor([0.4312], device='cuda:0')), ('power', tensor([0.0224], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7442], device='cuda:0')), ('power', tensor([-1.8216], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.05671287700533867
epoch£º134	 i:1 	 global-step:2681	 l-p:0.060358427464962006
epoch£º134	 i:2 	 global-step:2682	 l-p:0.05672039836645126
epoch£º134	 i:3 	 global-step:2683	 l-p:0.057247407734394073
epoch£º134	 i:4 	 global-step:2684	 l-p:0.05657929182052612
epoch£º134	 i:5 	 global-step:2685	 l-p:0.056656818836927414
epoch£º134	 i:6 	 global-step:2686	 l-p:0.0576048269867897
epoch£º134	 i:7 	 global-step:2687	 l-p:0.05659781023859978
epoch£º134	 i:8 	 global-step:2688	 l-p:0.056709229946136475
epoch£º134	 i:9 	 global-step:2689	 l-p:0.056636322289705276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1341, 26.1340, 26.1341],
        [26.1341, 34.7557, 40.3992],
        [26.1341, 27.4335, 26.9619],
        [26.1341, 33.4783, 37.5654]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.05799099802970886 
model_pd.l_d.mean(): -0.7165126800537109 
model_pd.lagr.mean(): -0.6585216522216797 
model_pd.lambdas: dict_items([('pout', tensor([0.4141], device='cuda:0')), ('power', tensor([0.0216], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6494], device='cuda:0')), ('power', tensor([-1.4204], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.05799099802970886
epoch£º135	 i:1 	 global-step:2701	 l-p:0.056891392916440964
epoch£º135	 i:2 	 global-step:2702	 l-p:0.05674732103943825
epoch£º135	 i:3 	 global-step:2703	 l-p:0.05655413866043091
epoch£º135	 i:4 	 global-step:2704	 l-p:0.05682719871401787
epoch£º135	 i:5 	 global-step:2705	 l-p:0.056680869311094284
epoch£º135	 i:6 	 global-step:2706	 l-p:0.0566251166164875
epoch£º135	 i:7 	 global-step:2707	 l-p:0.057346686720848083
epoch£º135	 i:8 	 global-step:2708	 l-p:0.05718771740794182
epoch£º135	 i:9 	 global-step:2709	 l-p:0.058961499482393265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1256, 26.1268, 26.1256],
        [26.1256, 27.6095, 27.1513],
        [26.1256, 26.1350, 26.1259],
        [26.1256, 27.8743, 27.4598]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.05651983246207237 
model_pd.l_d.mean(): -0.7593975067138672 
model_pd.lagr.mean(): -0.7028777003288269 
model_pd.lambdas: dict_items([('pout', tensor([0.3967], device='cuda:0')), ('power', tensor([0.0207], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8039], device='cuda:0')), ('power', tensor([-1.9471], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.05651983246207237
epoch£º136	 i:1 	 global-step:2721	 l-p:0.056715380400419235
epoch£º136	 i:2 	 global-step:2722	 l-p:0.057421404868364334
epoch£º136	 i:3 	 global-step:2723	 l-p:0.056846607476472855
epoch£º136	 i:4 	 global-step:2724	 l-p:0.05663922056555748
epoch£º136	 i:5 	 global-step:2725	 l-p:0.0581120066344738
epoch£º136	 i:6 	 global-step:2726	 l-p:0.05664494261145592
epoch£º136	 i:7 	 global-step:2727	 l-p:0.059037260711193085
epoch£º136	 i:8 	 global-step:2728	 l-p:0.056539252400398254
epoch£º136	 i:9 	 global-step:2729	 l-p:0.057377442717552185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1175, 26.1175, 26.1175],
        [26.1175, 26.3196, 26.1572],
        [26.1175, 26.1175, 26.1175],
        [26.1175, 34.0399, 38.8173]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): 0.056587353348731995 
model_pd.l_d.mean(): -0.7116832733154297 
model_pd.lagr.mean(): -0.6550959348678589 
model_pd.lambdas: dict_items([('pout', tensor([0.3795], device='cuda:0')), ('power', tensor([0.0199], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7685], device='cuda:0')), ('power', tensor([-1.8741], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:0.056587353348731995
epoch£º137	 i:1 	 global-step:2741	 l-p:0.05667310580611229
epoch£º137	 i:2 	 global-step:2742	 l-p:0.05672624334692955
epoch£º137	 i:3 	 global-step:2743	 l-p:0.05813521891832352
epoch£º137	 i:4 	 global-step:2744	 l-p:0.0590798556804657
epoch£º137	 i:5 	 global-step:2745	 l-p:0.057306572794914246
epoch£º137	 i:6 	 global-step:2746	 l-p:0.056767310947179794
epoch£º137	 i:7 	 global-step:2747	 l-p:0.05713922902941704
epoch£º137	 i:8 	 global-step:2748	 l-p:0.05678265914320946
epoch£º137	 i:9 	 global-step:2749	 l-p:0.056685127317905426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1102, 26.9020, 26.4799],
        [26.1102, 26.1101, 26.1101],
        [26.1102, 26.1101, 26.1102],
        [26.1102, 26.1102, 26.1101]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.05659950152039528 
model_pd.l_d.mean(): -0.6823031306266785 
model_pd.lagr.mean(): -0.6257036328315735 
model_pd.lambdas: dict_items([('pout', tensor([0.3622], device='cuda:0')), ('power', tensor([0.0190], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7758], device='cuda:0')), ('power', tensor([-1.8767], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.05659950152039528
epoch£º138	 i:1 	 global-step:2761	 l-p:0.05681866407394409
epoch£º138	 i:2 	 global-step:2762	 l-p:0.05848574638366699
epoch£º138	 i:3 	 global-step:2763	 l-p:0.0566788911819458
epoch£º138	 i:4 	 global-step:2764	 l-p:0.05907926335930824
epoch£º138	 i:5 	 global-step:2765	 l-p:0.05749058723449707
epoch£º138	 i:6 	 global-step:2766	 l-p:0.05654742196202278
epoch£º138	 i:7 	 global-step:2767	 l-p:0.056809261441230774
epoch£º138	 i:8 	 global-step:2768	 l-p:0.05662768334150314
epoch£º138	 i:9 	 global-step:2769	 l-p:0.056783463805913925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0970, 34.0131, 38.7868],
        [26.0970, 35.2850, 41.6699],
        [26.0970, 26.0970, 26.0970],
        [26.0970, 26.1257, 26.0987]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.05922909453511238 
model_pd.l_d.mean(): -0.6050266623497009 
model_pd.lagr.mean(): -0.54579758644104 
model_pd.lambdas: dict_items([('pout', tensor([0.3451], device='cuda:0')), ('power', tensor([0.0182], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6670], device='cuda:0')), ('power', tensor([-1.4750], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.05922909453511238
epoch£º139	 i:1 	 global-step:2781	 l-p:0.056531235575675964
epoch£º139	 i:2 	 global-step:2782	 l-p:0.05660809949040413
epoch£º139	 i:3 	 global-step:2783	 l-p:0.057094018906354904
epoch£º139	 i:4 	 global-step:2784	 l-p:0.05682443454861641
epoch£º139	 i:5 	 global-step:2785	 l-p:0.05686217173933983
epoch£º139	 i:6 	 global-step:2786	 l-p:0.05680492892861366
epoch£º139	 i:7 	 global-step:2787	 l-p:0.05796939879655838
epoch£º139	 i:8 	 global-step:2788	 l-p:0.05734388530254364
epoch£º139	 i:9 	 global-step:2789	 l-p:0.05670854449272156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0783, 34.3841, 39.6411],
        [26.0783, 26.4971, 26.2087],
        [26.0783, 31.3355, 33.2147],
        [26.0783, 34.5153, 39.9369]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.05676892027258873 
model_pd.l_d.mean(): -0.5972754955291748 
model_pd.lagr.mean(): -0.5405066013336182 
model_pd.lambdas: dict_items([('pout', tensor([0.3279], device='cuda:0')), ('power', tensor([0.0173], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7201], device='cuda:0')), ('power', tensor([-1.7461], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.05676892027258873
epoch£º140	 i:1 	 global-step:2801	 l-p:0.05668892711400986
epoch£º140	 i:2 	 global-step:2802	 l-p:0.0578642301261425
epoch£º140	 i:3 	 global-step:2803	 l-p:0.05663379654288292
epoch£º140	 i:4 	 global-step:2804	 l-p:0.056571267545223236
epoch£º140	 i:5 	 global-step:2805	 l-p:0.057094547897577286
epoch£º140	 i:6 	 global-step:2806	 l-p:0.057351063936948776
epoch£º140	 i:7 	 global-step:2807	 l-p:0.05914762243628502
epoch£º140	 i:8 	 global-step:2808	 l-p:0.05676468834280968
epoch£º140	 i:9 	 global-step:2809	 l-p:0.057197876274585724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0531, 26.0548, 26.0532],
        [26.0531, 26.0547, 26.0532],
        [26.0531, 26.0531, 26.0531],
        [26.0531, 27.4015, 26.9336]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.056777551770210266 
model_pd.l_d.mean(): -0.5698485374450684 
model_pd.lagr.mean(): -0.5130710005760193 
model_pd.lambdas: dict_items([('pout', tensor([0.3106], device='cuda:0')), ('power', tensor([0.0164], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7280], device='cuda:0')), ('power', tensor([-1.8222], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.056777551770210266
epoch£º141	 i:1 	 global-step:2821	 l-p:0.056768979877233505
epoch£º141	 i:2 	 global-step:2822	 l-p:0.056608665734529495
epoch£º141	 i:3 	 global-step:2823	 l-p:0.056765276938676834
epoch£º141	 i:4 	 global-step:2824	 l-p:0.05662059783935547
epoch£º141	 i:5 	 global-step:2825	 l-p:0.057117003947496414
epoch£º141	 i:6 	 global-step:2826	 l-p:0.056903060525655746
epoch£º141	 i:7 	 global-step:2827	 l-p:0.05689529329538345
epoch£º141	 i:8 	 global-step:2828	 l-p:0.05659619718790054
epoch£º141	 i:9 	 global-step:2829	 l-p:0.06114077568054199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0273, 27.1730, 26.7041],
        [26.0273, 27.8559, 27.4632],
        [26.0273, 27.6899, 27.2610],
        [26.0273, 26.0349, 26.0275]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.056763529777526855 
model_pd.l_d.mean(): -0.5397546887397766 
model_pd.lagr.mean(): -0.48299115896224976 
model_pd.lambdas: dict_items([('pout', tensor([0.2934], device='cuda:0')), ('power', tensor([0.0155], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7302], device='cuda:0')), ('power', tensor([-1.8602], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.056763529777526855
epoch£º142	 i:1 	 global-step:2841	 l-p:0.05782165378332138
epoch£º142	 i:2 	 global-step:2842	 l-p:0.05670541524887085
epoch£º142	 i:3 	 global-step:2843	 l-p:0.05666695535182953
epoch£º142	 i:4 	 global-step:2844	 l-p:0.05761589854955673
epoch£º142	 i:5 	 global-step:2845	 l-p:0.056739937514066696
epoch£º142	 i:6 	 global-step:2846	 l-p:0.05933085456490517
epoch£º142	 i:7 	 global-step:2847	 l-p:0.05720818415284157
epoch£º142	 i:8 	 global-step:2848	 l-p:0.056767720729112625
epoch£º142	 i:9 	 global-step:2849	 l-p:0.056632641702890396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0208, 26.0208, 26.0208],
        [26.0208, 26.0208, 26.0208],
        [26.0208, 26.0210, 26.0208],
        [26.0208, 32.0884, 34.7833]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.05803994834423065 
model_pd.l_d.mean(): -0.48796719312667847 
model_pd.lagr.mean(): -0.4299272298812866 
model_pd.lambdas: dict_items([('pout', tensor([0.2763], device='cuda:0')), ('power', tensor([0.0146], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6696], device='cuda:0')), ('power', tensor([-1.6205], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.05803994834423065
epoch£º143	 i:1 	 global-step:2861	 l-p:0.05676116421818733
epoch£º143	 i:2 	 global-step:2862	 l-p:0.056653328239917755
epoch£º143	 i:3 	 global-step:2863	 l-p:0.05657932162284851
epoch£º143	 i:4 	 global-step:2864	 l-p:0.05695560202002525
epoch£º143	 i:5 	 global-step:2865	 l-p:0.056912872940301895
epoch£º143	 i:6 	 global-step:2866	 l-p:0.05667971447110176
epoch£º143	 i:7 	 global-step:2867	 l-p:0.05655204504728317
epoch£º143	 i:8 	 global-step:2868	 l-p:0.058037519454956055
epoch£º143	 i:9 	 global-step:2869	 l-p:0.0591190867125988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0082, 35.5730, 42.4806],
        [26.0082, 31.1279, 32.8859],
        [26.0082, 27.1190, 26.6520],
        [26.0082, 26.0889, 26.0173]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.0578625462949276 
model_pd.l_d.mean(): -0.4716569483280182 
model_pd.lagr.mean(): -0.4137943983078003 
model_pd.lambdas: dict_items([('pout', tensor([0.2591], device='cuda:0')), ('power', tensor([0.0137], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7135], device='cuda:0')), ('power', tensor([-1.7926], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.0578625462949276
epoch£º144	 i:1 	 global-step:2881	 l-p:0.056606225669384
epoch£º144	 i:2 	 global-step:2882	 l-p:0.05922391265630722
epoch£º144	 i:3 	 global-step:2883	 l-p:0.05670817568898201
epoch£º144	 i:4 	 global-step:2884	 l-p:0.05666196718811989
epoch£º144	 i:5 	 global-step:2885	 l-p:0.05661166459321976
epoch£º144	 i:6 	 global-step:2886	 l-p:0.056742455810308456
epoch£º144	 i:7 	 global-step:2887	 l-p:0.05820458382368088
epoch£º144	 i:8 	 global-step:2888	 l-p:0.05676421895623207
epoch£º144	 i:9 	 global-step:2889	 l-p:0.05692470818758011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0107, 31.9181, 34.4487],
        [26.0107, 32.5707, 35.7930],
        [26.0107, 26.1264, 26.0268],
        [26.0107, 34.4015, 39.7786]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.056647758930921555 
model_pd.l_d.mean(): -0.4511226713657379 
model_pd.lagr.mean(): -0.39447492361068726 
model_pd.lambdas: dict_items([('pout', tensor([0.2419], device='cuda:0')), ('power', tensor([0.0128], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7500], device='cuda:0')), ('power', tensor([-1.9196], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.056647758930921555
epoch£º145	 i:1 	 global-step:2901	 l-p:0.059289880096912384
epoch£º145	 i:2 	 global-step:2902	 l-p:0.05713535100221634
epoch£º145	 i:3 	 global-step:2903	 l-p:0.057627514004707336
epoch£º145	 i:4 	 global-step:2904	 l-p:0.056813519448041916
epoch£º145	 i:5 	 global-step:2905	 l-p:0.05672801658511162
epoch£º145	 i:6 	 global-step:2906	 l-p:0.056815918534994125
epoch£º145	 i:7 	 global-step:2907	 l-p:0.058050114661455154
epoch£º145	 i:8 	 global-step:2908	 l-p:0.05662567541003227
epoch£º145	 i:9 	 global-step:2909	 l-p:0.05656380578875542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0150, 26.0150, 26.0150],
        [26.0150, 26.4236, 26.1404],
        [26.0150, 28.5833, 28.4641],
        [26.0150, 26.7426, 26.3377]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.05720578134059906 
model_pd.l_d.mean(): -0.4076470136642456 
model_pd.lagr.mean(): -0.35044121742248535 
model_pd.lambdas: dict_items([('pout', tensor([0.2247], device='cuda:0')), ('power', tensor([0.0119], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7071], device='cuda:0')), ('power', tensor([-1.7573], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.05720578134059906
epoch£º146	 i:1 	 global-step:2921	 l-p:0.05676805227994919
epoch£º146	 i:2 	 global-step:2922	 l-p:0.056903451681137085
epoch£º146	 i:3 	 global-step:2923	 l-p:0.05670808628201485
epoch£º146	 i:4 	 global-step:2924	 l-p:0.05787067115306854
epoch£º146	 i:5 	 global-step:2925	 l-p:0.059396084398031235
epoch£º146	 i:6 	 global-step:2926	 l-p:0.056585099548101425
epoch£º146	 i:7 	 global-step:2927	 l-p:0.05740049108862877
epoch£º146	 i:8 	 global-step:2928	 l-p:0.0566687285900116
epoch£º146	 i:9 	 global-step:2929	 l-p:0.05678565055131912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0170, 27.9037, 27.5263],
        [26.0170, 26.0182, 26.0170],
        [26.0170, 26.5940, 26.2377],
        [26.0170, 28.6377, 28.5438]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.056598465889692307 
model_pd.l_d.mean(): -0.39436784386634827 
model_pd.lagr.mean(): -0.33776938915252686 
model_pd.lambdas: dict_items([('pout', tensor([0.2075], device='cuda:0')), ('power', tensor([0.0110], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7784], device='cuda:0')), ('power', tensor([-2.0020], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.056598465889692307
epoch£º147	 i:1 	 global-step:2941	 l-p:0.05936143174767494
epoch£º147	 i:2 	 global-step:2942	 l-p:0.05676844343543053
epoch£º147	 i:3 	 global-step:2943	 l-p:0.05667592212557793
epoch£º147	 i:4 	 global-step:2944	 l-p:0.05706852674484253
epoch£º147	 i:5 	 global-step:2945	 l-p:0.05692269280552864
epoch£º147	 i:6 	 global-step:2946	 l-p:0.056907474994659424
epoch£º147	 i:7 	 global-step:2947	 l-p:0.05662138760089874
epoch£º147	 i:8 	 global-step:2948	 l-p:0.05664558336138725
epoch£º147	 i:9 	 global-step:2949	 l-p:0.05869496986269951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0247, 26.2964, 26.0890],
        [26.0247, 26.1558, 26.0444],
        [26.0247, 34.6102, 40.2305],
        [26.0247, 26.0247, 26.0246]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.05666588246822357 
model_pd.l_d.mean(): -0.357099324464798 
model_pd.lagr.mean(): -0.3004334568977356 
model_pd.lambdas: dict_items([('pout', tensor([0.1903], device='cuda:0')), ('power', tensor([0.0101], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7561], device='cuda:0')), ('power', tensor([-1.9428], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.05666588246822357
epoch£º148	 i:1 	 global-step:2961	 l-p:0.05674439296126366
epoch£º148	 i:2 	 global-step:2962	 l-p:0.05665384978055954
epoch£º148	 i:3 	 global-step:2963	 l-p:0.0569634772837162
epoch£º148	 i:4 	 global-step:2964	 l-p:0.056655578315258026
epoch£º148	 i:5 	 global-step:2965	 l-p:0.05709373950958252
epoch£º148	 i:6 	 global-step:2966	 l-p:0.0590849369764328
epoch£º148	 i:7 	 global-step:2967	 l-p:0.0577690489590168
epoch£º148	 i:8 	 global-step:2968	 l-p:0.056662824004888535
epoch£º148	 i:9 	 global-step:2969	 l-p:0.05794379115104675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0332, 26.8159, 26.3967],
        [26.0332, 26.4340, 26.1547],
        [26.0332, 34.5226, 40.0190],
        [26.0332, 30.3199, 31.3426]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.057535793632268906 
model_pd.l_d.mean(): -0.3014289438724518 
model_pd.lagr.mean(): -0.24389314651489258 
model_pd.lambdas: dict_items([('pout', tensor([0.1732], device='cuda:0')), ('power', tensor([0.0092], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6427], device='cuda:0')), ('power', tensor([-1.5225], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.057535793632268906
epoch£º149	 i:1 	 global-step:2981	 l-p:0.05660609155893326
epoch£º149	 i:2 	 global-step:2982	 l-p:0.05659182742238045
epoch£º149	 i:3 	 global-step:2983	 l-p:0.05666607245802879
epoch£º149	 i:4 	 global-step:2984	 l-p:0.056898631155490875
epoch£º149	 i:5 	 global-step:2985	 l-p:0.057260073721408844
epoch£º149	 i:6 	 global-step:2986	 l-p:0.056730326265096664
epoch£º149	 i:7 	 global-step:2987	 l-p:0.0568394660949707
epoch£º149	 i:8 	 global-step:2988	 l-p:0.05670798942446709
epoch£º149	 i:9 	 global-step:2989	 l-p:0.060349736362695694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0447, 29.0348, 29.1376],
        [26.0447, 30.0641, 30.8720],
        [26.0447, 28.9813, 29.0534],
        [26.0447, 26.0748, 26.0466]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.059324711561203 
model_pd.l_d.mean(): -0.27551189064979553 
model_pd.lagr.mean(): -0.21618717908859253 
model_pd.lambdas: dict_items([('pout', tensor([0.1560], device='cuda:0')), ('power', tensor([0.0084], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6666], device='cuda:0')), ('power', tensor([-1.5136], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.059324711561203
epoch£º150	 i:1 	 global-step:3001	 l-p:0.05722333863377571
epoch£º150	 i:2 	 global-step:3002	 l-p:0.05659114196896553
epoch£º150	 i:3 	 global-step:3003	 l-p:0.05787433311343193
epoch£º150	 i:4 	 global-step:3004	 l-p:0.05663080886006355
epoch£º150	 i:5 	 global-step:3005	 l-p:0.05729476734995842
epoch£º150	 i:6 	 global-step:3006	 l-p:0.05687820911407471
epoch£º150	 i:7 	 global-step:3007	 l-p:0.05686936900019646
epoch£º150	 i:8 	 global-step:3008	 l-p:0.056675154715776443
epoch£º150	 i:9 	 global-step:3009	 l-p:0.05675218626856804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0722, 29.5078, 29.8886],
        [26.0722, 26.0722, 26.0722],
        [26.0722, 32.6491, 35.8798],
        [26.0722, 30.4484, 31.5410]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.05657552182674408 
model_pd.l_d.mean(): -0.2645529806613922 
model_pd.lagr.mean(): -0.20797745883464813 
model_pd.lambdas: dict_items([('pout', tensor([0.1387], device='cuda:0')), ('power', tensor([0.0074], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7802], device='cuda:0')), ('power', tensor([-1.9230], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.05657552182674408
epoch£º151	 i:1 	 global-step:3021	 l-p:0.05661686882376671
epoch£º151	 i:2 	 global-step:3022	 l-p:0.05658714473247528
epoch£º151	 i:3 	 global-step:3023	 l-p:0.05687234178185463
epoch£º151	 i:4 	 global-step:3024	 l-p:0.05667881667613983
epoch£º151	 i:5 	 global-step:3025	 l-p:0.057744596153497696
epoch£º151	 i:6 	 global-step:3026	 l-p:0.059244394302368164
epoch£º151	 i:7 	 global-step:3027	 l-p:0.05712677910923958
epoch£º151	 i:8 	 global-step:3028	 l-p:0.05795920267701149
epoch£º151	 i:9 	 global-step:3029	 l-p:0.05663064867258072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[26.0837, 27.6167, 27.1653],
        [26.0837, 33.8273, 38.3949],
        [26.0837, 27.9979, 27.6255],
        [26.0837, 27.9507, 27.5656]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.056808870285749435 
model_pd.l_d.mean(): -0.21894069015979767 
model_pd.lagr.mean(): -0.16213181614875793 
model_pd.lambdas: dict_items([('pout', tensor([0.1216], device='cuda:0')), ('power', tensor([0.0066], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6845], device='cuda:0')), ('power', tensor([-1.6985], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.056808870285749435
epoch£º152	 i:1 	 global-step:3041	 l-p:0.0566757470369339
epoch£º152	 i:2 	 global-step:3042	 l-p:0.05676766112446785
epoch£º152	 i:3 	 global-step:3043	 l-p:0.05919894203543663
epoch£º152	 i:4 	 global-step:3044	 l-p:0.056593723595142365
epoch£º152	 i:5 	 global-step:3045	 l-p:0.05819893255829811
epoch£º152	 i:6 	 global-step:3046	 l-p:0.05657301843166351
epoch£º152	 i:7 	 global-step:3047	 l-p:0.05705514922738075
epoch£º152	 i:8 	 global-step:3048	 l-p:0.0575336217880249
epoch£º152	 i:9 	 global-step:3049	 l-p:0.05656816065311432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1006, 26.1583, 26.1059],
        [26.1006, 26.1021, 26.1006],
        [26.1006, 26.2492, 26.1247],
        [26.1006, 26.1006, 26.1006]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.05902368947863579 
model_pd.l_d.mean(): -0.1925531029701233 
model_pd.lagr.mean(): -0.1335294097661972 
model_pd.lambdas: dict_items([('pout', tensor([0.1043], device='cuda:0')), ('power', tensor([0.0057], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7273], device='cuda:0')), ('power', tensor([-1.6232], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.05902368947863579
epoch£º153	 i:1 	 global-step:3061	 l-p:0.057797014713287354
epoch£º153	 i:2 	 global-step:3062	 l-p:0.056629497557878494
epoch£º153	 i:3 	 global-step:3063	 l-p:0.05670854076743126
epoch£º153	 i:4 	 global-step:3064	 l-p:0.05664139613509178
epoch£º153	 i:5 	 global-step:3065	 l-p:0.05672943592071533
epoch£º153	 i:6 	 global-step:3066	 l-p:0.05671987310051918
epoch£º153	 i:7 	 global-step:3067	 l-p:0.05746258422732353
epoch£º153	 i:8 	 global-step:3068	 l-p:0.057239506393671036
epoch£º153	 i:9 	 global-step:3069	 l-p:0.05695510655641556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1133, 31.6048, 33.7048],
        [26.1133, 27.7844, 27.3543],
        [26.1133, 26.4485, 26.2038],
        [26.1133, 26.1820, 26.1203]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.05781686678528786 
model_pd.l_d.mean(): -0.1553386002779007 
model_pd.lagr.mean(): -0.09752173721790314 
model_pd.lambdas: dict_items([('pout', tensor([0.0871], device='cuda:0')), ('power', tensor([0.0049], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6689], device='cuda:0')), ('power', tensor([-1.4454], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.05781686678528786
epoch£º154	 i:1 	 global-step:3081	 l-p:0.0568721666932106
epoch£º154	 i:2 	 global-step:3082	 l-p:0.056733258068561554
epoch£º154	 i:3 	 global-step:3083	 l-p:0.0590268112719059
epoch£º154	 i:4 	 global-step:3084	 l-p:0.05793248489499092
epoch£º154	 i:5 	 global-step:3085	 l-p:0.05667777732014656
epoch£º154	 i:6 	 global-step:3086	 l-p:0.05682017281651497
epoch£º154	 i:7 	 global-step:3087	 l-p:0.05674028396606445
epoch£º154	 i:8 	 global-step:3088	 l-p:0.05664483457803726
epoch£º154	 i:9 	 global-step:3089	 l-p:0.05660870298743248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.1172, 31.6506, 33.7911],
        [26.1172, 31.0145, 32.5550],
        [26.1172, 26.6580, 26.3152],
        [26.1172, 29.2562, 29.4429]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.056874196976423264 
model_pd.l_d.mean(): -0.1300652176141739 
model_pd.lagr.mean(): -0.07319101691246033 
model_pd.lambdas: dict_items([('pout', tensor([0.0698], device='cuda:0')), ('power', tensor([0.0040], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7185], device='cuda:0')), ('power', tensor([-1.7331], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.056874196976423264
epoch£º155	 i:1 	 global-step:3101	 l-p:0.05673959478735924
epoch£º155	 i:2 	 global-step:3102	 l-p:0.057084858417510986
epoch£º155	 i:3 	 global-step:3103	 l-p:0.05658045783638954
epoch£º155	 i:4 	 global-step:3104	 l-p:0.05740632861852646
epoch£º155	 i:5 	 global-step:3105	 l-p:0.05655677989125252
epoch£º155	 i:6 	 global-step:3106	 l-p:0.06034410372376442
epoch£º155	 i:7 	 global-step:3107	 l-p:0.05660445988178253
epoch£º155	 i:8 	 global-step:3108	 l-p:0.056676823645830154
epoch£º155	 i:9 	 global-step:3109	 l-p:0.057019539177417755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[26.1090, 35.0821, 41.1823],
        [26.1090, 35.2384, 41.5429],
        [26.1090, 27.8498, 27.4338],
        [26.1090, 35.0597, 41.1308]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.05682077631354332 
model_pd.l_d.mean(): -0.09836681187152863 
model_pd.lagr.mean(): -0.041546035557985306 
model_pd.lambdas: dict_items([('pout', tensor([0.0526], device='cuda:0')), ('power', tensor([0.0032], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7067], device='cuda:0')), ('power', tensor([-1.7448], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.05682077631354332
epoch£º156	 i:1 	 global-step:3121	 l-p:0.058058962225914
epoch£º156	 i:2 	 global-step:3122	 l-p:0.059728097170591354
epoch£º156	 i:3 	 global-step:3123	 l-p:0.05666123703122139
epoch£º156	 i:4 	 global-step:3124	 l-p:0.05669290944933891
epoch£º156	 i:5 	 global-step:3125	 l-p:0.05689503997564316
epoch£º156	 i:6 	 global-step:3126	 l-p:0.057251591235399246
epoch£º156	 i:7 	 global-step:3127	 l-p:0.05652126297354698
epoch£º156	 i:8 	 global-step:3128	 l-p:0.05662797763943672
epoch£º156	 i:9 	 global-step:3129	 l-p:0.056645914912223816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[26.1032, 26.9110, 26.4852],
        [26.1032, 30.6983, 31.9733],
        [26.1032, 33.5112, 37.6775],
        [26.1032, 28.5616, 28.3868]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.056705813854932785 
model_pd.l_d.mean(): -0.06806764751672745 
model_pd.lagr.mean(): -0.011361833661794662 
model_pd.lambdas: dict_items([('pout', tensor([0.0354], device='cuda:0')), ('power', tensor([0.0023], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7243], device='cuda:0')), ('power', tensor([-1.7243], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.056705813854932785
epoch£º157	 i:1 	 global-step:3141	 l-p:0.05663703754544258
epoch£º157	 i:2 	 global-step:3142	 l-p:0.05729406699538231
epoch£º157	 i:3 	 global-step:3143	 l-p:0.05811722204089165
epoch£º157	 i:4 	 global-step:3144	 l-p:0.056546129286289215
epoch£º157	 i:5 	 global-step:3145	 l-p:0.05729222297668457
epoch£º157	 i:6 	 global-step:3146	 l-p:0.05685146525502205
epoch£º157	 i:7 	 global-step:3147	 l-p:0.056836459785699844
epoch£º157	 i:8 	 global-step:3148	 l-p:0.05907243490219116
epoch£º157	 i:9 	 global-step:3149	 l-p:0.05661656707525253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0817, 26.0822, 26.0817],
        [26.0817, 26.2088, 26.1005],
        [26.0817, 26.0820, 26.0817],
        [26.0817, 30.0310, 30.7811]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.057786975055933 
model_pd.l_d.mean(): -0.03680797293782234 
model_pd.lagr.mean(): 0.020979002118110657 
model_pd.lambdas: dict_items([('pout', tensor([0.0181], device='cuda:0')), ('power', tensor([0.0014], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7241], device='cuda:0')), ('power', tensor([-1.6820], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.057786975055933
epoch£º158	 i:1 	 global-step:3161	 l-p:0.05659474432468414
epoch£º158	 i:2 	 global-step:3162	 l-p:0.05685308948159218
epoch£º158	 i:3 	 global-step:3163	 l-p:0.057532746344804764
epoch£º158	 i:4 	 global-step:3164	 l-p:0.05669282376766205
epoch£º158	 i:5 	 global-step:3165	 l-p:0.05713430419564247
epoch£º158	 i:6 	 global-step:3166	 l-p:0.05936727300286293
epoch£º158	 i:7 	 global-step:3167	 l-p:0.05657116696238518
epoch£º158	 i:8 	 global-step:3168	 l-p:0.056537456810474396
epoch£º158	 i:9 	 global-step:3169	 l-p:0.05697498098015785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0635, 27.7313, 27.3021],
        [26.0635, 26.8842, 26.4559],
        [26.0635, 27.3700, 26.8998],
        [26.0635, 26.5478, 26.2290]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.05665019527077675 
model_pd.l_d.mean(): -0.005835379473865032 
model_pd.lagr.mean(): 0.05081481486558914 
model_pd.lambdas: dict_items([('pout', tensor([0.0009], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7441], device='cuda:0')), ('power', tensor([-1.8707], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.05665019527077675
epoch£º159	 i:1 	 global-step:3181	 l-p:0.0566292479634285
epoch£º159	 i:2 	 global-step:3182	 l-p:0.05690049007534981
epoch£º159	 i:3 	 global-step:3183	 l-p:0.056705884635448456
epoch£º159	 i:4 	 global-step:3184	 l-p:0.057962317019701004
epoch£º159	 i:5 	 global-step:3185	 l-p:0.056568264961242676
epoch£º159	 i:6 	 global-step:3186	 l-p:0.05703098326921463
epoch£º159	 i:7 	 global-step:3187	 l-p:0.05782512575387955
epoch£º159	 i:8 	 global-step:3188	 l-p:0.05674537643790245
epoch£º159	 i:9 	 global-step:3189	 l-p:0.059109851717948914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.0546, 26.8247, 26.4083],
        [26.0546, 32.3042, 35.1851],
        [26.0546, 26.0546, 26.0546],
        [26.0546, 26.0647, 26.0549]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.05655539408326149 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05655539408326149 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7923], device='cuda:0')), ('power', tensor([-1.9891], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.05655539408326149
epoch£º160	 i:1 	 global-step:3201	 l-p:0.05665544793009758
epoch£º160	 i:2 	 global-step:3202	 l-p:0.05767034366726875
epoch£º160	 i:3 	 global-step:3203	 l-p:0.05786861106753349
epoch£º160	 i:4 	 global-step:3204	 l-p:0.056650713086128235
epoch£º160	 i:5 	 global-step:3205	 l-p:0.059163421392440796
epoch£º160	 i:6 	 global-step:3206	 l-p:0.0568525493144989
epoch£º160	 i:7 	 global-step:3207	 l-p:0.056676339358091354
epoch£º160	 i:8 	 global-step:3208	 l-p:0.05722135305404663
epoch£º160	 i:9 	 global-step:3209	 l-p:0.05670931562781334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[26.1183, 27.7343, 27.2944],
        [26.1183, 26.9760, 26.5395],
        [26.1183, 31.4516, 33.3979],
        [26.1183, 31.9965, 34.4819]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.05654328316450119 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05654328316450119 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7892], device='cuda:0')), ('power', tensor([-1.9504], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.05654328316450119
epoch£º161	 i:1 	 global-step:3221	 l-p:0.056755561381578445
epoch£º161	 i:2 	 global-step:3222	 l-p:0.059937484562397
epoch£º161	 i:3 	 global-step:3223	 l-p:0.0565621443092823
epoch£º161	 i:4 	 global-step:3224	 l-p:0.05712796002626419
epoch£º161	 i:5 	 global-step:3225	 l-p:0.056803300976753235
epoch£º161	 i:6 	 global-step:3226	 l-p:0.05667390301823616
epoch£º161	 i:7 	 global-step:3227	 l-p:0.05660030245780945
epoch£º161	 i:8 	 global-step:3228	 l-p:0.0580904595553875
epoch£º161	 i:9 	 global-step:3229	 l-p:0.05661638453602791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.2087, 31.1441, 32.7085],
        [26.2087, 26.2191, 26.2091],
        [26.2087, 27.3633, 26.8908],
        [26.2087, 32.0854, 34.5564]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.056658606976270676 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056658606976270676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7352], device='cuda:0')), ('power', tensor([-1.6785], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.056658606976270676
epoch£º162	 i:1 	 global-step:3241	 l-p:0.059607237577438354
epoch£º162	 i:2 	 global-step:3242	 l-p:0.05655987560749054
epoch£º162	 i:3 	 global-step:3243	 l-p:0.05649229511618614
epoch£º162	 i:4 	 global-step:3244	 l-p:0.056618452072143555
epoch£º162	 i:5 	 global-step:3245	 l-p:0.05711666867136955
epoch£º162	 i:6 	 global-step:3246	 l-p:0.056858595460653305
epoch£º162	 i:7 	 global-step:3247	 l-p:0.05677247419953346
epoch£º162	 i:8 	 global-step:3248	 l-p:0.056993331760168076
epoch£º162	 i:9 	 global-step:3249	 l-p:0.057652730494737625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.3069, 33.7757, 37.9763],
        [26.3069, 26.3069, 26.3068],
        [26.3069, 29.6240, 29.9089],
        [26.3069, 26.3070, 26.3069]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.05904903635382652 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05904903635382652 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6467], device='cuda:0')), ('power', tensor([-1.2102], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.05904903635382652
epoch£º163	 i:1 	 global-step:3261	 l-p:0.056799523532390594
epoch£º163	 i:2 	 global-step:3262	 l-p:0.05655035004019737
epoch£º163	 i:3 	 global-step:3263	 l-p:0.05660953372716904
epoch£º163	 i:4 	 global-step:3264	 l-p:0.056684788316488266
epoch£º163	 i:5 	 global-step:3265	 l-p:0.056533459573984146
epoch£º163	 i:6 	 global-step:3266	 l-p:0.05646621063351631
epoch£º163	 i:7 	 global-step:3267	 l-p:0.05727064609527588
epoch£º163	 i:8 	 global-step:3268	 l-p:0.056773923337459564
epoch£º163	 i:9 	 global-step:3269	 l-p:0.058197516947984695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.4072, 28.9056, 28.7330],
        [26.4072, 26.4780, 26.4145],
        [26.4072, 26.4230, 26.4079],
        [26.4072, 26.7609, 26.5053]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.05667933449149132 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05667933449149132 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7456], device='cuda:0')), ('power', tensor([-1.4811], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.05667933449149132
epoch£º164	 i:1 	 global-step:3281	 l-p:0.058050621300935745
epoch£º164	 i:2 	 global-step:3282	 l-p:0.05762903019785881
epoch£º164	 i:3 	 global-step:3283	 l-p:0.058888692408800125
epoch£º164	 i:4 	 global-step:3284	 l-p:0.056553278118371964
epoch£º164	 i:5 	 global-step:3285	 l-p:0.0566561259329319
epoch£º164	 i:6 	 global-step:3286	 l-p:0.05641089379787445
epoch£º164	 i:7 	 global-step:3287	 l-p:0.05654614418745041
epoch£º164	 i:8 	 global-step:3288	 l-p:0.056551914662122726
epoch£º164	 i:9 	 global-step:3289	 l-p:0.0565854050219059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.5106, 26.5106, 26.5106],
        [26.5106, 33.2998, 36.6935],
        [26.5106, 31.6309, 33.3286],
        [26.5106, 28.7218, 28.4277]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.056640155613422394 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056640155613422394 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7495], device='cuda:0')), ('power', tensor([-1.3954], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.056640155613422394
epoch£º165	 i:1 	 global-step:3301	 l-p:0.05869866535067558
epoch£º165	 i:2 	 global-step:3302	 l-p:0.05646955221891403
epoch£º165	 i:3 	 global-step:3303	 l-p:0.05657234415411949
epoch£º165	 i:4 	 global-step:3304	 l-p:0.05677346885204315
epoch£º165	 i:5 	 global-step:3305	 l-p:0.056434791535139084
epoch£º165	 i:6 	 global-step:3306	 l-p:0.05674842372536659
epoch£º165	 i:7 	 global-step:3307	 l-p:0.05666685104370117
epoch£º165	 i:8 	 global-step:3308	 l-p:0.056447193026542664
epoch£º165	 i:9 	 global-step:3309	 l-p:0.058681562542915344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.6063, 26.6382, 26.6083],
        [26.6063, 27.5918, 27.1282],
        [26.6063, 27.7054, 27.2297],
        [26.6063, 26.6067, 26.6063]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.05772092193365097 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05772092193365097 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6996], device='cuda:0')), ('power', tensor([-1.0266], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.05772092193365097
epoch£º166	 i:1 	 global-step:3321	 l-p:0.05728573724627495
epoch£º166	 i:2 	 global-step:3322	 l-p:0.05638350918889046
epoch£º166	 i:3 	 global-step:3323	 l-p:0.05708546191453934
epoch£º166	 i:4 	 global-step:3324	 l-p:0.05656233802437782
epoch£º166	 i:5 	 global-step:3325	 l-p:0.056581366807222366
epoch£º166	 i:6 	 global-step:3326	 l-p:0.056353017687797546
epoch£º166	 i:7 	 global-step:3327	 l-p:0.0585315078496933
epoch£º166	 i:8 	 global-step:3328	 l-p:0.05674855038523674
epoch£º166	 i:9 	 global-step:3329	 l-p:0.05652201920747757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.7080, 26.7079, 26.7080],
        [26.7080, 28.5793, 28.1736],
        [26.7080, 36.5006, 43.5474],
        [26.7080, 26.9089, 26.7467]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.05714928358793259 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05714928358793259 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7272], device='cuda:0')), ('power', tensor([-1.0485], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.05714928358793259
epoch£º167	 i:1 	 global-step:3341	 l-p:0.05819472670555115
epoch£º167	 i:2 	 global-step:3342	 l-p:0.05661872401833534
epoch£º167	 i:3 	 global-step:3343	 l-p:0.05649232864379883
epoch£º167	 i:4 	 global-step:3344	 l-p:0.05654069781303406
epoch£º167	 i:5 	 global-step:3345	 l-p:0.056514739990234375
epoch£º167	 i:6 	 global-step:3346	 l-p:0.05643202364444733
epoch£º167	 i:7 	 global-step:3347	 l-p:0.05636401101946831
epoch£º167	 i:8 	 global-step:3348	 l-p:0.05647425353527069
epoch£º167	 i:9 	 global-step:3349	 l-p:0.05861001834273338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.8069, 26.8199, 26.8074],
        [26.8069, 35.8787, 41.9532],
        [26.8069, 26.8071, 26.8069],
        [26.8069, 26.8071, 26.8069]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.05861470475792885 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05861470475792885 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6953], device='cuda:0')), ('power', tensor([-0.7558], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.05861470475792885
epoch£º168	 i:1 	 global-step:3361	 l-p:0.056491680443286896
epoch£º168	 i:2 	 global-step:3362	 l-p:0.056464120745658875
epoch£º168	 i:3 	 global-step:3363	 l-p:0.056823860853910446
epoch£º168	 i:4 	 global-step:3364	 l-p:0.0564986914396286
epoch£º168	 i:5 	 global-step:3365	 l-p:0.05655425786972046
epoch£º168	 i:6 	 global-step:3366	 l-p:0.05757228657603264
epoch£º168	 i:7 	 global-step:3367	 l-p:0.05645173788070679
epoch£º168	 i:8 	 global-step:3368	 l-p:0.056440457701683044
epoch£º168	 i:9 	 global-step:3369	 l-p:0.05713222548365593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[26.9115, 26.9166, 26.9116],
        [26.9115, 27.2464, 27.0000],
        [26.9115, 28.6783, 28.2426],
        [26.9115, 27.7150, 27.2826]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.056374263018369675 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056374263018369675 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8164], device='cuda:0')), ('power', tensor([-1.1207], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.056374263018369675
epoch£º169	 i:1 	 global-step:3381	 l-p:0.057352084666490555
epoch£º169	 i:2 	 global-step:3382	 l-p:0.05723856762051582
epoch£º169	 i:3 	 global-step:3383	 l-p:0.056591570377349854
epoch£º169	 i:4 	 global-step:3384	 l-p:0.05703500658273697
epoch£º169	 i:5 	 global-step:3385	 l-p:0.05652642250061035
epoch£º169	 i:6 	 global-step:3386	 l-p:0.056393831968307495
epoch£º169	 i:7 	 global-step:3387	 l-p:0.05654587596654892
epoch£º169	 i:8 	 global-step:3388	 l-p:0.058256424963474274
epoch£º169	 i:9 	 global-step:3389	 l-p:0.056343965232372284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.0098, 29.4273, 29.1901],
        [27.0098, 27.0287, 27.0107],
        [27.0098, 27.4197, 27.1327],
        [27.0098, 27.0239, 27.0104]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.05840303376317024 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05840303376317024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7176], device='cuda:0')), ('power', tensor([-0.5480], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.05840303376317024
epoch£º170	 i:1 	 global-step:3401	 l-p:0.05768497288227081
epoch£º170	 i:2 	 global-step:3402	 l-p:0.056289009749889374
epoch£º170	 i:3 	 global-step:3403	 l-p:0.05641629174351692
epoch£º170	 i:4 	 global-step:3404	 l-p:0.056359462440013885
epoch£º170	 i:5 	 global-step:3405	 l-p:0.056301698088645935
epoch£º170	 i:6 	 global-step:3406	 l-p:0.056349944323301315
epoch£º170	 i:7 	 global-step:3407	 l-p:0.05654662847518921
epoch£º170	 i:8 	 global-step:3408	 l-p:0.05661935731768608
epoch£º170	 i:9 	 global-step:3409	 l-p:0.057352371513843536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.1136, 32.1314, 33.6658],
        [27.1136, 27.1436, 27.1154],
        [27.1136, 27.5503, 27.2496],
        [27.1136, 27.1140, 27.1136]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.05757990851998329 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05757990851998329 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7016], device='cuda:0')), ('power', tensor([-0.4529], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.05757990851998329
epoch£º171	 i:1 	 global-step:3421	 l-p:0.056902989745140076
epoch£º171	 i:2 	 global-step:3422	 l-p:0.05630682036280632
epoch£º171	 i:3 	 global-step:3423	 l-p:0.05641801655292511
epoch£º171	 i:4 	 global-step:3424	 l-p:0.05823144316673279
epoch£º171	 i:5 	 global-step:3425	 l-p:0.056524161249399185
epoch£º171	 i:6 	 global-step:3426	 l-p:0.05635889992117882
epoch£º171	 i:7 	 global-step:3427	 l-p:0.05674365907907486
epoch£º171	 i:8 	 global-step:3428	 l-p:0.05632208660244942
epoch£º171	 i:9 	 global-step:3429	 l-p:0.05657118558883667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2142, 27.2280, 27.2148],
        [27.2142, 27.3392, 27.2319],
        [27.2142, 30.3466, 30.4550],
        [27.2142, 27.2142, 27.2142]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.05627266317605972 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05627266317605972 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8325], device='cuda:0')), ('power', tensor([-0.8344], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.05627266317605972
epoch£º172	 i:1 	 global-step:3441	 l-p:0.0564110092818737
epoch£º172	 i:2 	 global-step:3442	 l-p:0.056349124759435654
epoch£º172	 i:3 	 global-step:3443	 l-p:0.05630683898925781
epoch£º172	 i:4 	 global-step:3444	 l-p:0.05833529308438301
epoch£º172	 i:5 	 global-step:3445	 l-p:0.05742017552256584
epoch£º172	 i:6 	 global-step:3446	 l-p:0.05710875242948532
epoch£º172	 i:7 	 global-step:3447	 l-p:0.056320302188396454
epoch£º172	 i:8 	 global-step:3448	 l-p:0.05638136342167854
epoch£º172	 i:9 	 global-step:3449	 l-p:0.05670040100812912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[27.3125, 36.6903, 43.0495],
        [27.3125, 28.6589, 28.1643],
        [27.3125, 33.6982, 36.5359],
        [27.3125, 28.7931, 28.3055]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.05664203688502312 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05664203688502312 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7398], device='cuda:0')), ('power', tensor([-0.4085], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.05664203688502312
epoch£º173	 i:1 	 global-step:3461	 l-p:0.0573154054582119
epoch£º173	 i:2 	 global-step:3462	 l-p:0.056319188326597214
epoch£º173	 i:3 	 global-step:3463	 l-p:0.05623547360301018
epoch£º173	 i:4 	 global-step:3464	 l-p:0.05621339753270149
epoch£º173	 i:5 	 global-step:3465	 l-p:0.05639718472957611
epoch£º173	 i:6 	 global-step:3466	 l-p:0.056334760040044785
epoch£º173	 i:7 	 global-step:3467	 l-p:0.05694742128252983
epoch£º173	 i:8 	 global-step:3468	 l-p:0.056755952537059784
epoch£º173	 i:9 	 global-step:3469	 l-p:0.05809755250811577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4086, 35.2058, 39.5925],
        [27.4086, 27.6958, 27.4767],
        [27.4086, 28.6198, 28.1244],
        [27.4086, 29.3350, 28.9191]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.05637853592634201 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05637853592634201 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7964], device='cuda:0')), ('power', tensor([-0.5045], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.05637853592634201
epoch£º174	 i:1 	 global-step:3481	 l-p:0.056649062782526016
epoch£º174	 i:2 	 global-step:3482	 l-p:0.056331560015678406
epoch£º174	 i:3 	 global-step:3483	 l-p:0.056338243186473846
epoch£º174	 i:4 	 global-step:3484	 l-p:0.05739971995353699
epoch£º174	 i:5 	 global-step:3485	 l-p:0.05647606775164604
epoch£º174	 i:6 	 global-step:3486	 l-p:0.0561915785074234
epoch£º174	 i:7 	 global-step:3487	 l-p:0.05791877955198288
epoch£º174	 i:8 	 global-step:3488	 l-p:0.05630924925208092
epoch£º174	 i:9 	 global-step:3489	 l-p:0.056942690163850784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5086, 27.5626, 27.5132],
        [27.5086, 27.7181, 27.5493],
        [27.5086, 28.3245, 27.8835],
        [27.5086, 28.6879, 28.1924]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.056259412318468094 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056259412318468094 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8464], device='cuda:0')), ('power', tensor([-0.5521], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.056259412318468094
epoch£º175	 i:1 	 global-step:3501	 l-p:0.056329235434532166
epoch£º175	 i:2 	 global-step:3502	 l-p:0.05669993907213211
epoch£º175	 i:3 	 global-step:3503	 l-p:0.056211456656455994
epoch£º175	 i:4 	 global-step:3504	 l-p:0.05723118782043457
epoch£º175	 i:5 	 global-step:3505	 l-p:0.057939767837524414
epoch£º175	 i:6 	 global-step:3506	 l-p:0.05632027983665466
epoch£º175	 i:7 	 global-step:3507	 l-p:0.05685711279511452
epoch£º175	 i:8 	 global-step:3508	 l-p:0.056486424058675766
epoch£º175	 i:9 	 global-step:3509	 l-p:0.05627491697669029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6103, 27.6103, 27.6103],
        [27.6103, 27.9812, 27.7132],
        [27.6103, 33.1955, 35.1943],
        [27.6103, 30.3180, 30.1785]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.057163409888744354 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057163409888744354 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7734], device='cuda:0')), ('power', tensor([-0.1046], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.057163409888744354
epoch£º176	 i:1 	 global-step:3521	 l-p:0.056153230369091034
epoch£º176	 i:2 	 global-step:3522	 l-p:0.056356433779001236
epoch£º176	 i:3 	 global-step:3523	 l-p:0.056146878749132156
epoch£º176	 i:4 	 global-step:3524	 l-p:0.056413449347019196
epoch£º176	 i:5 	 global-step:3525	 l-p:0.056171588599681854
epoch£º176	 i:6 	 global-step:3526	 l-p:0.058557938784360886
epoch£º176	 i:7 	 global-step:3527	 l-p:0.05670690909028053
epoch£º176	 i:8 	 global-step:3528	 l-p:0.056304946541786194
epoch£º176	 i:9 	 global-step:3529	 l-p:0.056303471326828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7105, 37.7096, 44.7961],
        [27.7105, 27.7279, 27.7113],
        [27.7105, 29.2909, 28.8033],
        [27.7105, 27.7186, 27.7107]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.05647061765193939 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05647061765193939 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7850], device='cuda:0')), ('power', tensor([-0.1455], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.05647061765193939
epoch£º177	 i:1 	 global-step:3541	 l-p:0.05796961486339569
epoch£º177	 i:2 	 global-step:3542	 l-p:0.05617376044392586
epoch£º177	 i:3 	 global-step:3543	 l-p:0.056912824511528015
epoch£º177	 i:4 	 global-step:3544	 l-p:0.05624878779053688
epoch£º177	 i:5 	 global-step:3545	 l-p:0.056226834654808044
epoch£º177	 i:6 	 global-step:3546	 l-p:0.056493934243917465
epoch£º177	 i:7 	 global-step:3547	 l-p:0.056118544191122055
epoch£º177	 i:8 	 global-step:3548	 l-p:0.05702582374215126
epoch£º177	 i:9 	 global-step:3549	 l-p:0.05632060021162033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8119, 27.8171, 27.8120],
        [27.8119, 27.8124, 27.8119],
        [27.8119, 27.8119, 27.8119],
        [27.8119, 28.9639, 28.4655]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.05832022428512573 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05832022428512573 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4054e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7473], device='cuda:0')), ('power', tensor([0.2811], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.05832022428512573
epoch£º178	 i:1 	 global-step:3561	 l-p:0.056233443319797516
epoch£º178	 i:2 	 global-step:3562	 l-p:0.05607004091143608
epoch£º178	 i:3 	 global-step:3563	 l-p:0.056525811553001404
epoch£º178	 i:4 	 global-step:3564	 l-p:0.056990426033735275
epoch£º178	 i:5 	 global-step:3565	 l-p:0.05617716535925865
epoch£º178	 i:6 	 global-step:3566	 l-p:0.05674980953335762
epoch£º178	 i:7 	 global-step:3567	 l-p:0.05628732591867447
epoch£º178	 i:8 	 global-step:3568	 l-p:0.0561649389564991
epoch£º178	 i:9 	 global-step:3569	 l-p:0.05612592399120331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9132, 27.9441, 27.9151],
        [27.9132, 28.3766, 28.0602],
        [27.9132, 28.3538, 28.0485],
        [27.9132, 27.9262, 27.9137]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.056150197982788086 
model_pd.l_d.mean(): -4.381531368835567e-07 
model_pd.lagr.mean(): 0.05614975839853287 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.7916e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8521], device='cuda:0')), ('power', tensor([-0.0698], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.056150197982788086
epoch£º179	 i:1 	 global-step:3581	 l-p:0.056674692779779434
epoch£º179	 i:2 	 global-step:3582	 l-p:0.05630124360322952
epoch£º179	 i:3 	 global-step:3583	 l-p:0.056288328021764755
epoch£º179	 i:4 	 global-step:3584	 l-p:0.05649179220199585
epoch£º179	 i:5 	 global-step:3585	 l-p:0.056071966886520386
epoch£º179	 i:6 	 global-step:3586	 l-p:0.057344283908605576
epoch£º179	 i:7 	 global-step:3587	 l-p:0.056278228759765625
epoch£º179	 i:8 	 global-step:3588	 l-p:0.056089989840984344
epoch£º179	 i:9 	 global-step:3589	 l-p:0.0576205775141716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0051, 28.0275, 28.0063],
        [28.0051, 28.0209, 28.0058],
        [28.0051, 30.6639, 30.4811],
        [28.0051, 28.0051, 28.0051]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.05604211613535881 
model_pd.l_d.mean(): -3.4657309697649907e-06 
model_pd.lagr.mean(): 0.05603865161538124 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.1230e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8877], device='cuda:0')), ('power', tensor([-0.0637], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.05604211613535881
epoch£º180	 i:1 	 global-step:3601	 l-p:0.056082095950841904
epoch£º180	 i:2 	 global-step:3602	 l-p:0.057168614119291306
epoch£º180	 i:3 	 global-step:3603	 l-p:0.056122731417417526
epoch£º180	 i:4 	 global-step:3604	 l-p:0.05725901946425438
epoch£º180	 i:5 	 global-step:3605	 l-p:0.05614094436168671
epoch£º180	 i:6 	 global-step:3606	 l-p:0.05762413889169693
epoch£º180	 i:7 	 global-step:3607	 l-p:0.05607727915048599
epoch£º180	 i:8 	 global-step:3608	 l-p:0.05637239292263985
epoch£º180	 i:9 	 global-step:3609	 l-p:0.056149356067180634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0925, 28.2564, 28.1194],
        [28.0925, 29.3428, 28.8341],
        [28.0925, 32.7042, 33.7867],
        [28.0925, 34.8895, 38.0447]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.05611545965075493 
model_pd.l_d.mean(): 1.4710384675709065e-05 
model_pd.lagr.mean(): 0.056130170822143555 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8430], device='cuda:0')), ('power', tensor([0.1000], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.05611545965075493
epoch£º181	 i:1 	 global-step:3621	 l-p:0.05609351024031639
epoch£º181	 i:2 	 global-step:3622	 l-p:0.05677897483110428
epoch£º181	 i:3 	 global-step:3623	 l-p:0.05618700757622719
epoch£º181	 i:4 	 global-step:3624	 l-p:0.05695319175720215
epoch£º181	 i:5 	 global-step:3625	 l-p:0.05617193505167961
epoch£º181	 i:6 	 global-step:3626	 l-p:0.05615457519888878
epoch£º181	 i:7 	 global-step:3627	 l-p:0.05653243884444237
epoch£º181	 i:8 	 global-step:3628	 l-p:0.057700857520103455
epoch£º181	 i:9 	 global-step:3629	 l-p:0.05611053481698036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1534, 28.3536, 28.1906],
        [28.1534, 29.0823, 28.6098],
        [28.1534, 30.0473, 29.5991],
        [28.1534, 28.1534, 28.1534]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.05624785274267197 
model_pd.l_d.mean(): 4.8783389502204955e-05 
model_pd.lagr.mean(): 0.056296635419130325 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8528], device='cuda:0')), ('power', tensor([0.1759], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.05624785274267197
epoch£º182	 i:1 	 global-step:3641	 l-p:0.056972164660692215
epoch£º182	 i:2 	 global-step:3642	 l-p:0.056057047098875046
epoch£º182	 i:3 	 global-step:3643	 l-p:0.056082598865032196
epoch£º182	 i:4 	 global-step:3644	 l-p:0.05616113543510437
epoch£º182	 i:5 	 global-step:3645	 l-p:0.0578346811234951
epoch£º182	 i:6 	 global-step:3646	 l-p:0.056079234927892685
epoch£º182	 i:7 	 global-step:3647	 l-p:0.056027285754680634
epoch£º182	 i:8 	 global-step:3648	 l-p:0.05612765997648239
epoch£º182	 i:9 	 global-step:3649	 l-p:0.057069454342126846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1808, 31.9900, 32.4590],
        [28.1808, 30.2073, 29.7899],
        [28.1808, 28.3702, 28.2148],
        [28.1808, 35.3541, 38.9024]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.05614322051405907 
model_pd.l_d.mean(): 9.051775123225525e-05 
model_pd.lagr.mean(): 0.056233737617731094 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8482], device='cuda:0')), ('power', tensor([0.2101], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.05614322051405907
epoch£º183	 i:1 	 global-step:3661	 l-p:0.056073855608701706
epoch£º183	 i:2 	 global-step:3662	 l-p:0.0560319684445858
epoch£º183	 i:3 	 global-step:3663	 l-p:0.056107643991708755
epoch£º183	 i:4 	 global-step:3664	 l-p:0.056899379938840866
epoch£º183	 i:5 	 global-step:3665	 l-p:0.05622555688023567
epoch£º183	 i:6 	 global-step:3666	 l-p:0.056215569376945496
epoch£º183	 i:7 	 global-step:3667	 l-p:0.05611046031117439
epoch£º183	 i:8 	 global-step:3668	 l-p:0.057068392634391785
epoch£º183	 i:9 	 global-step:3669	 l-p:0.05776778981089592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1617, 29.4152, 28.9053],
        [28.1617, 35.9585, 40.2112],
        [28.1617, 28.8393, 28.4339],
        [28.1617, 28.1617, 28.1617]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.057086214423179626 
model_pd.l_d.mean(): 0.0002762880176305771 
model_pd.lagr.mean(): 0.05736250430345535 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7793], device='cuda:0')), ('power', tensor([0.4704], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.057086214423179626
epoch£º184	 i:1 	 global-step:3681	 l-p:0.05652952194213867
epoch£º184	 i:2 	 global-step:3682	 l-p:0.05757578834891319
epoch£º184	 i:3 	 global-step:3683	 l-p:0.05616719275712967
epoch£º184	 i:4 	 global-step:3684	 l-p:0.056067679077386856
epoch£º184	 i:5 	 global-step:3685	 l-p:0.05671226233243942
epoch£º184	 i:6 	 global-step:3686	 l-p:0.05616626515984535
epoch£º184	 i:7 	 global-step:3687	 l-p:0.056252364069223404
epoch£º184	 i:8 	 global-step:3688	 l-p:0.05614064261317253
epoch£º184	 i:9 	 global-step:3689	 l-p:0.05604263022542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1018, 29.9837, 29.5346],
        [28.1018, 28.1018, 28.1018],
        [28.1018, 28.4524, 28.1945],
        [28.1018, 35.5535, 39.4238]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.05667925626039505 
model_pd.l_d.mean(): 0.00026543610147200525 
model_pd.lagr.mean(): 0.05694469064474106 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7823], device='cuda:0')), ('power', tensor([0.3654], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.05667925626039505
epoch£º185	 i:1 	 global-step:3701	 l-p:0.056148383766412735
epoch£º185	 i:2 	 global-step:3702	 l-p:0.05760872736573219
epoch£º185	 i:3 	 global-step:3703	 l-p:0.05619056895375252
epoch£º185	 i:4 	 global-step:3704	 l-p:0.056140098720788956
epoch£º185	 i:5 	 global-step:3705	 l-p:0.05701543018221855
epoch£º185	 i:6 	 global-step:3706	 l-p:0.05625351145863533
epoch£º185	 i:7 	 global-step:3707	 l-p:0.05610106512904167
epoch£º185	 i:8 	 global-step:3708	 l-p:0.05674686282873154
epoch£º185	 i:9 	 global-step:3709	 l-p:0.056118741631507874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9897, 27.9897, 27.9897],
        [27.9897, 32.3265, 33.1996],
        [27.9897, 31.9308, 32.5081],
        [27.9897, 29.7287, 29.2557]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.056206196546554565 
model_pd.l_d.mean(): 7.767439819872379e-05 
model_pd.lagr.mean(): 0.056283872574567795 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8226], device='cuda:0')), ('power', tensor([0.0943], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.056206196546554565
epoch£º186	 i:1 	 global-step:3721	 l-p:0.05696388706564903
epoch£º186	 i:2 	 global-step:3722	 l-p:0.0561474934220314
epoch£º186	 i:3 	 global-step:3723	 l-p:0.05611346289515495
epoch£º186	 i:4 	 global-step:3724	 l-p:0.056587040424346924
epoch£º186	 i:5 	 global-step:3725	 l-p:0.05712759867310524
epoch£º186	 i:6 	 global-step:3726	 l-p:0.05604526400566101
epoch£º186	 i:7 	 global-step:3727	 l-p:0.057773277163505554
epoch£º186	 i:8 	 global-step:3728	 l-p:0.056326285004615784
epoch£º186	 i:9 	 global-step:3729	 l-p:0.05613381788134575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8413, 27.8429, 27.8413],
        [27.8413, 30.2784, 30.0100],
        [27.8413, 31.6022, 32.0650],
        [27.8413, 27.8413, 27.8413]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.05703732371330261 
model_pd.l_d.mean(): 9.897159179672599e-05 
model_pd.lagr.mean(): 0.05713629350066185 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7919], device='cuda:0')), ('power', tensor([0.1155], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.05703732371330261
epoch£º187	 i:1 	 global-step:3741	 l-p:0.056258611381053925
epoch£º187	 i:2 	 global-step:3742	 l-p:0.05639467015862465
epoch£º187	 i:3 	 global-step:3743	 l-p:0.05649731680750847
epoch£º187	 i:4 	 global-step:3744	 l-p:0.05615122616291046
epoch£º187	 i:5 	 global-step:3745	 l-p:0.0561610609292984
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05710598826408386
epoch£º187	 i:7 	 global-step:3747	 l-p:0.05618445575237274
epoch£º187	 i:8 	 global-step:3748	 l-p:0.05628438666462898
epoch£º187	 i:9 	 global-step:3749	 l-p:0.057857319712638855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6770, 35.3344, 39.5113],
        [27.6770, 29.0154, 28.5134],
        [27.6770, 27.6774, 27.6770],
        [27.6770, 35.2448, 39.3188]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.05618564411997795 
model_pd.l_d.mean(): -0.0002752730797510594 
model_pd.lagr.mean(): 0.055910371243953705 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8422], device='cuda:0')), ('power', tensor([-0.3383], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.05618564411997795
epoch£º188	 i:1 	 global-step:3761	 l-p:0.05624058470129967
epoch£º188	 i:2 	 global-step:3762	 l-p:0.05615876615047455
epoch£º188	 i:3 	 global-step:3763	 l-p:0.05632909759879112
epoch£º188	 i:4 	 global-step:3764	 l-p:0.05687782168388367
epoch£º188	 i:5 	 global-step:3765	 l-p:0.056494034826755524
epoch£º188	 i:6 	 global-step:3766	 l-p:0.056225281208753586
epoch£º188	 i:7 	 global-step:3767	 l-p:0.05636387690901756
epoch£º188	 i:8 	 global-step:3768	 l-p:0.05715940520167351
epoch£º188	 i:9 	 global-step:3769	 l-p:0.05844685062766075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5217, 36.6046, 42.5395],
        [27.5217, 36.7171, 42.7955],
        [27.5217, 27.5222, 27.5217],
        [27.5217, 27.5217, 27.5217]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.05668564513325691 
model_pd.l_d.mean(): -0.00021774927154183388 
model_pd.lagr.mean(): 0.056467894464731216 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7716], device='cuda:0')), ('power', tensor([-0.3154], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.05668564513325691
epoch£º189	 i:1 	 global-step:3781	 l-p:0.05658259242773056
epoch£º189	 i:2 	 global-step:3782	 l-p:0.05632132291793823
epoch£º189	 i:3 	 global-step:3783	 l-p:0.058049798011779785
epoch£º189	 i:4 	 global-step:3784	 l-p:0.05617183446884155
epoch£º189	 i:5 	 global-step:3785	 l-p:0.05687554180622101
epoch£º189	 i:6 	 global-step:3786	 l-p:0.056296881288290024
epoch£º189	 i:7 	 global-step:3787	 l-p:0.05727459862828255
epoch£º189	 i:8 	 global-step:3788	 l-p:0.056348226964473724
epoch£º189	 i:9 	 global-step:3789	 l-p:0.05628905072808266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4063, 27.4064, 27.4063],
        [27.4063, 27.4063, 27.4063],
        [27.4063, 27.4064, 27.4063],
        [27.4063, 27.7477, 27.4966]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.056422192603349686 
model_pd.l_d.mean(): -0.0002637213619891554 
model_pd.lagr.mean(): 0.05615847185254097 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8040], device='cuda:0')), ('power', tensor([-0.5283], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.056422192603349686
epoch£º190	 i:1 	 global-step:3801	 l-p:0.05641436576843262
epoch£º190	 i:2 	 global-step:3802	 l-p:0.05722862109541893
epoch£º190	 i:3 	 global-step:3803	 l-p:0.056279078125953674
epoch£º190	 i:4 	 global-step:3804	 l-p:0.056302521377801895
epoch£º190	 i:5 	 global-step:3805	 l-p:0.058864202350378036
epoch£º190	 i:6 	 global-step:3806	 l-p:0.05634114146232605
epoch£º190	 i:7 	 global-step:3807	 l-p:0.05638670176267624
epoch£º190	 i:8 	 global-step:3808	 l-p:0.05664272978901863
epoch£º190	 i:9 	 global-step:3809	 l-p:0.056345753371715546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3372, 36.0654, 41.5919],
        [27.3372, 28.3065, 27.8361],
        [27.3372, 27.3387, 27.3372],
        [27.3372, 27.3372, 27.3372]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.05707326903939247 
model_pd.l_d.mean(): -6.502803444163874e-05 
model_pd.lagr.mean(): 0.05700824037194252 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6932], device='cuda:0')), ('power', tensor([-0.2497], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.05707326903939247
epoch£º191	 i:1 	 global-step:3821	 l-p:0.05639178678393364
epoch£º191	 i:2 	 global-step:3822	 l-p:0.057175908237695694
epoch£º191	 i:3 	 global-step:3823	 l-p:0.0567483976483345
epoch£º191	 i:4 	 global-step:3824	 l-p:0.0563296340405941
epoch£º191	 i:5 	 global-step:3825	 l-p:0.05652223527431488
epoch£º191	 i:6 	 global-step:3826	 l-p:0.05634603276848793
epoch£º191	 i:7 	 global-step:3827	 l-p:0.056393373757600784
epoch£º191	 i:8 	 global-step:3828	 l-p:0.0563267283141613
epoch£º191	 i:9 	 global-step:3829	 l-p:0.058056168258190155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3312, 27.7387, 27.4520],
        [27.3312, 27.6001, 27.3925],
        [27.3312, 35.4640, 40.2628],
        [27.3312, 27.8740, 27.5243]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.05640140548348427 
model_pd.l_d.mean(): -9.71574877439707e-07 
model_pd.lagr.mean(): 0.05640043318271637 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7904], device='cuda:0')), ('power', tensor([-0.5652], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.05640140548348427
epoch£º192	 i:1 	 global-step:3841	 l-p:0.05642041936516762
epoch£º192	 i:2 	 global-step:3842	 l-p:0.0565761923789978
epoch£º192	 i:3 	 global-step:3843	 l-p:0.05621809884905815
epoch£º192	 i:4 	 global-step:3844	 l-p:0.05781060457229614
epoch£º192	 i:5 	 global-step:3845	 l-p:0.05816758796572685
epoch£º192	 i:6 	 global-step:3846	 l-p:0.056815192103385925
epoch£º192	 i:7 	 global-step:3847	 l-p:0.05624324828386307
epoch£º192	 i:8 	 global-step:3848	 l-p:0.05631996691226959
epoch£º192	 i:9 	 global-step:3849	 l-p:0.05630107596516609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3971, 35.0834, 39.3428],
        [27.3971, 27.7748, 27.5036],
        [27.3971, 27.4579, 27.4027],
        [27.3971, 27.4010, 27.3972]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.05627771094441414 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05627771094441414 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8171], device='cuda:0')), ('power', tensor([-0.5989], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.05627771094441414
epoch£º193	 i:1 	 global-step:3861	 l-p:0.056165773421525955
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05652245879173279
epoch£º193	 i:3 	 global-step:3863	 l-p:0.05776747688651085
epoch£º193	 i:4 	 global-step:3864	 l-p:0.05635799467563629
epoch£º193	 i:5 	 global-step:3865	 l-p:0.056268833577632904
epoch£º193	 i:6 	 global-step:3866	 l-p:0.05684439465403557
epoch£º193	 i:7 	 global-step:3867	 l-p:0.056439802050590515
epoch£º193	 i:8 	 global-step:3868	 l-p:0.05812424421310425
epoch£º193	 i:9 	 global-step:3869	 l-p:0.05622091144323349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4901, 27.4901, 27.4901],
        [27.4901, 33.2868, 35.5048],
        [27.4901, 30.1796, 30.0379],
        [27.4901, 28.1755, 27.7720]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.056172050535678864 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056172050535678864 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8573], device='cuda:0')), ('power', tensor([-0.5682], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.056172050535678864
epoch£º194	 i:1 	 global-step:3881	 l-p:0.056732844561338425
epoch£º194	 i:2 	 global-step:3882	 l-p:0.05622785538434982
epoch£º194	 i:3 	 global-step:3883	 l-p:0.05680825933814049
epoch£º194	 i:4 	 global-step:3884	 l-p:0.05615469813346863
epoch£º194	 i:5 	 global-step:3885	 l-p:0.05638440325856209
epoch£º194	 i:6 	 global-step:3886	 l-p:0.05665336549282074
epoch£º194	 i:7 	 global-step:3887	 l-p:0.05799058452248573
epoch£º194	 i:8 	 global-step:3888	 l-p:0.05714729055762291
epoch£º194	 i:9 	 global-step:3889	 l-p:0.05638530105352402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5950, 28.8149, 28.3160],
        [27.5950, 33.0371, 34.9029],
        [27.5950, 27.6678, 27.6024],
        [27.5950, 32.2412, 33.4025]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.05629342794418335 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05629342794418335 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8162], device='cuda:0')), ('power', tensor([-0.3569], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.05629342794418335
epoch£º195	 i:1 	 global-step:3901	 l-p:0.05727262422442436
epoch£º195	 i:2 	 global-step:3902	 l-p:0.05630912259221077
epoch£º195	 i:3 	 global-step:3903	 l-p:0.05645863339304924
epoch£º195	 i:4 	 global-step:3904	 l-p:0.05668766796588898
epoch£º195	 i:5 	 global-step:3905	 l-p:0.05615772306919098
epoch£º195	 i:6 	 global-step:3906	 l-p:0.05612209066748619
epoch£º195	 i:7 	 global-step:3907	 l-p:0.05640588328242302
epoch£º195	 i:8 	 global-step:3908	 l-p:0.05610613152384758
epoch£º195	 i:9 	 global-step:3909	 l-p:0.05848865956068039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7036, 27.7038, 27.7036],
        [27.7036, 27.8815, 27.7346],
        [27.7036, 27.7115, 27.7038],
        [27.7036, 27.8391, 27.7236]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.056467313319444656 
model_pd.l_d.mean(): -8.721566473468556e-07 
model_pd.lagr.mean(): 0.056466441601514816 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.4663e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7497], device='cuda:0')), ('power', tensor([-0.0722], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.056467313319444656
epoch£º196	 i:1 	 global-step:3921	 l-p:0.056302573531866074
epoch£º196	 i:2 	 global-step:3922	 l-p:0.05624667555093765
epoch£º196	 i:3 	 global-step:3923	 l-p:0.057041775435209274
epoch£º196	 i:4 	 global-step:3924	 l-p:0.056369371712207794
epoch£º196	 i:5 	 global-step:3925	 l-p:0.05610230192542076
epoch£º196	 i:6 	 global-step:3926	 l-p:0.05707741156220436
epoch£º196	 i:7 	 global-step:3927	 l-p:0.056354958564043045
epoch£º196	 i:8 	 global-step:3928	 l-p:0.056208737194538116
epoch£º196	 i:9 	 global-step:3929	 l-p:0.05777660384774208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8138, 27.8280, 27.8144],
        [27.8138, 32.7720, 34.1759],
        [27.8138, 27.8138, 27.8138],
        [27.8138, 28.1930, 27.9199]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.057154249399900436 
model_pd.l_d.mean(): 3.567183455288614e-07 
model_pd.lagr.mean(): 0.05715460702776909 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.5590e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7535], device='cuda:0')), ('power', tensor([0.1403], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.057154249399900436
epoch£º197	 i:1 	 global-step:3941	 l-p:0.056171368807554245
epoch£º197	 i:2 	 global-step:3942	 l-p:0.05617506802082062
epoch£º197	 i:3 	 global-step:3943	 l-p:0.05679560825228691
epoch£º197	 i:4 	 global-step:3944	 l-p:0.05629350244998932
epoch£º197	 i:5 	 global-step:3945	 l-p:0.05605395510792732
epoch£º197	 i:6 	 global-step:3946	 l-p:0.05622003600001335
epoch£º197	 i:7 	 global-step:3947	 l-p:0.056520432233810425
epoch£º197	 i:8 	 global-step:3948	 l-p:0.0578787699341774
epoch£º197	 i:9 	 global-step:3949	 l-p:0.05633760988712311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9260, 28.4473, 28.1041],
        [27.9260, 27.9299, 27.9261],
        [27.9260, 27.9260, 27.9260],
        [27.9260, 27.9596, 27.9281]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.05616302788257599 
model_pd.l_d.mean(): -1.584059191372944e-06 
model_pd.lagr.mean(): 0.05616144463419914 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2946e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8472], device='cuda:0')), ('power', tensor([-0.0906], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.05616302788257599
epoch£º198	 i:1 	 global-step:3961	 l-p:0.056523244827985764
epoch£º198	 i:2 	 global-step:3962	 l-p:0.05698493868112564
epoch£º198	 i:3 	 global-step:3963	 l-p:0.056139733642339706
epoch£º198	 i:4 	 global-step:3964	 l-p:0.05618700012564659
epoch£º198	 i:5 	 global-step:3965	 l-p:0.05614272505044937
epoch£º198	 i:6 	 global-step:3966	 l-p:0.056322138756513596
epoch£º198	 i:7 	 global-step:3967	 l-p:0.05630514398217201
epoch£º198	 i:8 	 global-step:3968	 l-p:0.05627692490816116
epoch£º198	 i:9 	 global-step:3969	 l-p:0.05820218101143837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0322, 28.0659, 28.0343],
        [28.0322, 28.3426, 28.1083],
        [28.0322, 28.0323, 28.0322],
        [28.0322, 30.0019, 29.5753]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.05772484466433525 
model_pd.l_d.mean(): 2.6046691345982254e-05 
model_pd.lagr.mean(): 0.0577508918941021 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.2681e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7897], device='cuda:0')), ('power', tensor([0.3454], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.05772484466433525
epoch£º199	 i:1 	 global-step:3981	 l-p:0.05616552382707596
epoch£º199	 i:2 	 global-step:3982	 l-p:0.05635285750031471
epoch£º199	 i:3 	 global-step:3983	 l-p:0.056015219539403915
epoch£º199	 i:4 	 global-step:3984	 l-p:0.05608713999390602
epoch£º199	 i:5 	 global-step:3985	 l-p:0.056101832538843155
epoch£º199	 i:6 	 global-step:3986	 l-p:0.05617768317461014
epoch£º199	 i:7 	 global-step:3987	 l-p:0.05670742690563202
epoch£º199	 i:8 	 global-step:3988	 l-p:0.0566670261323452
epoch£º199	 i:9 	 global-step:3989	 l-p:0.05694517865777016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1276, 28.1276, 28.1276],
        [28.1276, 28.1457, 28.1284],
        [28.1276, 28.4783, 28.2203],
        [28.1276, 32.4327, 33.2685]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.056099049746990204 
model_pd.l_d.mean(): 2.4394967113039456e-05 
model_pd.lagr.mean(): 0.05612344294786453 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8597], device='cuda:0')), ('power', tensor([0.1324], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.056099049746990204
epoch£º200	 i:1 	 global-step:4001	 l-p:0.056012291461229324
epoch£º200	 i:2 	 global-step:4002	 l-p:0.05662668123841286
epoch£º200	 i:3 	 global-step:4003	 l-p:0.05621026083827019
epoch£º200	 i:4 	 global-step:4004	 l-p:0.056078340858221054
epoch£º200	 i:5 	 global-step:4005	 l-p:0.05610201135277748
epoch£º200	 i:6 	 global-step:4006	 l-p:0.05632668361067772
epoch£º200	 i:7 	 global-step:4007	 l-p:0.05615272745490074
epoch£º200	 i:8 	 global-step:4008	 l-p:0.05796097591519356
epoch£º200	 i:9 	 global-step:4009	 l-p:0.057178329676389694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1512, 28.1557, 28.1513],
        [28.1512, 33.8202, 35.8311],
        [28.1512, 28.1515, 28.1511],
        [28.1512, 29.9008, 29.4250]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.05732725188136101 
model_pd.l_d.mean(): 0.0001993158512050286 
model_pd.lagr.mean(): 0.057526566088199615 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7613], device='cuda:0')), ('power', tensor([0.6172], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.05732725188136101
epoch£º201	 i:1 	 global-step:4021	 l-p:0.056889381259679794
epoch£º201	 i:2 	 global-step:4022	 l-p:0.056072209030389786
epoch£º201	 i:3 	 global-step:4023	 l-p:0.05613074079155922
epoch£º201	 i:4 	 global-step:4024	 l-p:0.05606939271092415
epoch£º201	 i:5 	 global-step:4025	 l-p:0.05605316162109375
epoch£º201	 i:6 	 global-step:4026	 l-p:0.05619190260767937
epoch£º201	 i:7 	 global-step:4027	 l-p:0.05760326236486435
epoch£º201	 i:8 	 global-step:4028	 l-p:0.056271031498909
epoch£º201	 i:9 	 global-step:4029	 l-p:0.05608971044421196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1593, 28.1703, 28.1597],
        [28.1593, 28.1637, 28.1594],
        [28.1593, 29.8221, 29.3330],
        [28.1593, 28.5381, 28.2643]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.056279510259628296 
model_pd.l_d.mean(): 0.00010398967424407601 
model_pd.lagr.mean(): 0.05638350173830986 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8235], device='cuda:0')), ('power', tensor([0.2211], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.056279510259628296
epoch£º202	 i:1 	 global-step:4041	 l-p:0.0576760433614254
epoch£º202	 i:2 	 global-step:4042	 l-p:0.056421246379613876
epoch£º202	 i:3 	 global-step:4043	 l-p:0.056963518261909485
epoch£º202	 i:4 	 global-step:4044	 l-p:0.05678902938961983
epoch£º202	 i:5 	 global-step:4045	 l-p:0.056087128818035126
epoch£º202	 i:6 	 global-step:4046	 l-p:0.056056346744298935
epoch£º202	 i:7 	 global-step:4047	 l-p:0.0560169592499733
epoch£º202	 i:8 	 global-step:4048	 l-p:0.05632416158914566
epoch£º202	 i:9 	 global-step:4049	 l-p:0.05608538165688515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1467, 34.4853, 37.1553],
        [28.1467, 29.9174, 29.4452],
        [28.1467, 29.3995, 28.8898],
        [28.1467, 28.1577, 28.1470]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.05771138519048691 
model_pd.l_d.mean(): 0.00028127976111136377 
model_pd.lagr.mean(): 0.057992663234472275 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7869], device='cuda:0')), ('power', tensor([0.4558], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.05771138519048691
epoch£º203	 i:1 	 global-step:4061	 l-p:0.05664590746164322
epoch£º203	 i:2 	 global-step:4062	 l-p:0.05606408789753914
epoch£º203	 i:3 	 global-step:4063	 l-p:0.05610326677560806
epoch£º203	 i:4 	 global-step:4064	 l-p:0.0560200959444046
epoch£º203	 i:5 	 global-step:4065	 l-p:0.056616298854351044
epoch£º203	 i:6 	 global-step:4066	 l-p:0.05701654776930809
epoch£º203	 i:7 	 global-step:4067	 l-p:0.05620324984192848
epoch£º203	 i:8 	 global-step:4068	 l-p:0.056123070418834686
epoch£º203	 i:9 	 global-step:4069	 l-p:0.056262824684381485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1116, 34.0903, 36.4049],
        [28.1116, 28.1116, 28.1116],
        [28.1116, 34.0106, 36.2473],
        [28.1116, 30.0881, 29.6604]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.056122343987226486 
model_pd.l_d.mean(): 0.00013775615661870688 
model_pd.lagr.mean(): 0.05626010149717331 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8371], device='cuda:0')), ('power', tensor([0.1830], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.056122343987226486
epoch£º204	 i:1 	 global-step:4081	 l-p:0.05655281990766525
epoch£º204	 i:2 	 global-step:4082	 l-p:0.05635781213641167
epoch£º204	 i:3 	 global-step:4083	 l-p:0.056047916412353516
epoch£º204	 i:4 	 global-step:4084	 l-p:0.05614626780152321
epoch£º204	 i:5 	 global-step:4085	 l-p:0.05622060224413872
epoch£º204	 i:6 	 global-step:4086	 l-p:0.0577448345720768
epoch£º204	 i:7 	 global-step:4087	 l-p:0.05610939860343933
epoch£º204	 i:8 	 global-step:4088	 l-p:0.05698378384113312
epoch£º204	 i:9 	 global-step:4089	 l-p:0.05662979558110237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0555, 29.4712, 28.9634],
        [28.0555, 28.0555, 28.0555],
        [28.0555, 28.0638, 28.0557],
        [28.0555, 28.0555, 28.0555]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.05641287565231323 
model_pd.l_d.mean(): 0.00018678311607800424 
model_pd.lagr.mean(): 0.056599657982587814 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7849], device='cuda:0')), ('power', tensor([0.2157], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.05641287565231323
epoch£º205	 i:1 	 global-step:4101	 l-p:0.05611219257116318
epoch£º205	 i:2 	 global-step:4102	 l-p:0.058047711849212646
epoch£º205	 i:3 	 global-step:4103	 l-p:0.05609777197241783
epoch£º205	 i:4 	 global-step:4104	 l-p:0.05607154220342636
epoch£º205	 i:5 	 global-step:4105	 l-p:0.056140024214982986
epoch£º205	 i:6 	 global-step:4106	 l-p:0.05616409704089165
epoch£º205	 i:7 	 global-step:4107	 l-p:0.05611637607216835
epoch£º205	 i:8 	 global-step:4108	 l-p:0.057715147733688354
epoch£º205	 i:9 	 global-step:4109	 l-p:0.05622487887740135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9847, 28.8994, 28.4315],
        [27.9847, 37.1422, 43.0741],
        [27.9847, 34.2850, 36.9387],
        [27.9847, 27.9848, 27.9847]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.05617469176650047 
model_pd.l_d.mean(): 9.691687591839582e-05 
model_pd.lagr.mean(): 0.05627160891890526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8140], device='cuda:0')), ('power', tensor([0.1022], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.05617469176650047
epoch£º206	 i:1 	 global-step:4121	 l-p:0.056288495659828186
epoch£º206	 i:2 	 global-step:4122	 l-p:0.05636324733495712
epoch£º206	 i:3 	 global-step:4123	 l-p:0.05664444342255592
epoch£º206	 i:4 	 global-step:4124	 l-p:0.05675157532095909
epoch£º206	 i:5 	 global-step:4125	 l-p:0.05618688836693764
epoch£º206	 i:6 	 global-step:4126	 l-p:0.05604584887623787
epoch£º206	 i:7 	 global-step:4127	 l-p:0.05704410746693611
epoch£º206	 i:8 	 global-step:4128	 l-p:0.057658545672893524
epoch£º206	 i:9 	 global-step:4129	 l-p:0.05620066449046135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8989, 28.5083, 28.1292],
        [27.8989, 27.8989, 27.8989],
        [27.8989, 31.8266, 32.4019],
        [27.8989, 30.6365, 30.4956]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.056562311947345734 
model_pd.l_d.mean(): 1.8011180145549588e-05 
model_pd.lagr.mean(): 0.0565803237259388 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8166], device='cuda:0')), ('power', tensor([0.0181], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.056562311947345734
epoch£º207	 i:1 	 global-step:4141	 l-p:0.05614688619971275
epoch£º207	 i:2 	 global-step:4142	 l-p:0.05863769352436066
epoch£º207	 i:3 	 global-step:4143	 l-p:0.056147851049900055
epoch£º207	 i:4 	 global-step:4144	 l-p:0.056176356971263885
epoch£º207	 i:5 	 global-step:4145	 l-p:0.05636608600616455
epoch£º207	 i:6 	 global-step:4146	 l-p:0.0562685988843441
epoch£º207	 i:7 	 global-step:4147	 l-p:0.05634796991944313
epoch£º207	 i:8 	 global-step:4148	 l-p:0.05618585646152496
epoch£º207	 i:9 	 global-step:4149	 l-p:0.05677872523665428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8074, 27.8135, 27.8075],
        [27.8074, 33.6703, 35.9116],
        [27.8074, 30.1451, 29.8405],
        [27.8074, 27.8074, 27.8074]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.05610968545079231 
model_pd.l_d.mean(): -0.0002851421304512769 
model_pd.lagr.mean(): 0.05582454428076744 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8694], device='cuda:0')), ('power', tensor([-0.2869], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.05610968545079231
epoch£º208	 i:1 	 global-step:4161	 l-p:0.05717378854751587
epoch£º208	 i:2 	 global-step:4162	 l-p:0.05612586811184883
epoch£º208	 i:3 	 global-step:4163	 l-p:0.05645270273089409
epoch£º208	 i:4 	 global-step:4164	 l-p:0.057161372154951096
epoch£º208	 i:5 	 global-step:4165	 l-p:0.05631235986948013
epoch£º208	 i:6 	 global-step:4166	 l-p:0.056302737444639206
epoch£º208	 i:7 	 global-step:4167	 l-p:0.05634500831365585
epoch£º208	 i:8 	 global-step:4168	 l-p:0.056186493486166
epoch£º208	 i:9 	 global-step:4169	 l-p:0.057768795639276505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[27.7111, 32.2879, 33.3801],
        [27.7111, 37.0291, 43.2240],
        [27.7111, 28.9364, 28.4353],
        [27.7111, 31.4343, 31.8816]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): 0.056239936500787735 
model_pd.l_d.mean(): -0.00011758130858652294 
model_pd.lagr.mean(): 0.05612235516309738 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8072], device='cuda:0')), ('power', tensor([-0.1240], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:0.056239936500787735
epoch£º209	 i:1 	 global-step:4181	 l-p:0.05701640620827675
epoch£º209	 i:2 	 global-step:4182	 l-p:0.056312188506126404
epoch£º209	 i:3 	 global-step:4183	 l-p:0.056386157870292664
epoch£º209	 i:4 	 global-step:4184	 l-p:0.05611991882324219
epoch£º209	 i:5 	 global-step:4185	 l-p:0.05634848773479462
epoch£º209	 i:6 	 global-step:4186	 l-p:0.056672919541597366
epoch£º209	 i:7 	 global-step:4187	 l-p:0.05676780641078949
epoch£º209	 i:8 	 global-step:4188	 l-p:0.05804896354675293
epoch£º209	 i:9 	 global-step:4189	 l-p:0.056330714374780655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6202, 27.6202, 27.6202],
        [27.6202, 27.6202, 27.6202],
        [27.6202, 30.1443, 29.9221],
        [27.6202, 27.6202, 27.6202]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.056190375238657 
model_pd.l_d.mean(): -0.0003311714099254459 
model_pd.lagr.mean(): 0.05585920438170433 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8392], device='cuda:0')), ('power', tensor([-0.3865], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.056190375238657
epoch£º210	 i:1 	 global-step:4201	 l-p:0.05665057152509689
epoch£º210	 i:2 	 global-step:4202	 l-p:0.05739973112940788
epoch£º210	 i:3 	 global-step:4203	 l-p:0.05811160057783127
epoch£º210	 i:4 	 global-step:4204	 l-p:0.0562884658575058
epoch£º210	 i:5 	 global-step:4205	 l-p:0.056158438324928284
epoch£º210	 i:6 	 global-step:4206	 l-p:0.05628097429871559
epoch£º210	 i:7 	 global-step:4207	 l-p:0.056288182735443115
epoch£º210	 i:8 	 global-step:4208	 l-p:0.056923601776361465
epoch£º210	 i:9 	 global-step:4209	 l-p:0.05621355026960373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5422, 32.4047, 33.7550],
        [27.5422, 27.5430, 27.5422],
        [27.5422, 29.8563, 29.5547],
        [27.5422, 27.5425, 27.5422]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.05714116618037224 
model_pd.l_d.mean(): -0.00018427608301863074 
model_pd.lagr.mean(): 0.05695689097046852 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7913], device='cuda:0')), ('power', tensor([-0.2548], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.05714116618037224
epoch£º211	 i:1 	 global-step:4221	 l-p:0.05627456679940224
epoch£º211	 i:2 	 global-step:4222	 l-p:0.056561678647994995
epoch£º211	 i:3 	 global-step:4223	 l-p:0.05801527947187424
epoch£º211	 i:4 	 global-step:4224	 l-p:0.056324295699596405
epoch£º211	 i:5 	 global-step:4225	 l-p:0.05682322382926941
epoch£º211	 i:6 	 global-step:4226	 l-p:0.056316912174224854
epoch£º211	 i:7 	 global-step:4227	 l-p:0.05632490664720535
epoch£º211	 i:8 	 global-step:4228	 l-p:0.056709010154008865
epoch£º211	 i:9 	 global-step:4229	 l-p:0.05625075101852417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4795, 33.0519, 35.0549],
        [27.4795, 27.4902, 27.4799],
        [27.4795, 27.4795, 27.4795],
        [27.4795, 32.7969, 34.5609]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.05615993216633797 
model_pd.l_d.mean(): -0.0003425446921028197 
model_pd.lagr.mean(): 0.055817387998104095 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8665], device='cuda:0')), ('power', tensor([-0.6179], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.05615993216633797
epoch£º212	 i:1 	 global-step:4241	 l-p:0.056332435458898544
epoch£º212	 i:2 	 global-step:4242	 l-p:0.057994067668914795
epoch£º212	 i:3 	 global-step:4243	 l-p:0.05628679320216179
epoch£º212	 i:4 	 global-step:4244	 l-p:0.056452494114637375
epoch£º212	 i:5 	 global-step:4245	 l-p:0.05659296363592148
epoch£º212	 i:6 	 global-step:4246	 l-p:0.05663788318634033
epoch£º212	 i:7 	 global-step:4247	 l-p:0.05702749267220497
epoch£º212	 i:8 	 global-step:4248	 l-p:0.05720726028084755
epoch£º212	 i:9 	 global-step:4249	 l-p:0.05623525381088257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4385, 27.4884, 27.4426],
        [27.4385, 27.4385, 27.4385],
        [27.4385, 30.1285, 29.9898],
        [27.4385, 27.7801, 27.5288]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.05661735311150551 
model_pd.l_d.mean(): -0.00011023601837223396 
model_pd.lagr.mean(): 0.05650711804628372 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7405], device='cuda:0')), ('power', tensor([-0.3068], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.05661735311150551
epoch£º213	 i:1 	 global-step:4261	 l-p:0.059066493064165115
epoch£º213	 i:2 	 global-step:4262	 l-p:0.056378114968538284
epoch£º213	 i:3 	 global-step:4263	 l-p:0.056766804307699203
epoch£º213	 i:4 	 global-step:4264	 l-p:0.05624961107969284
epoch£º213	 i:5 	 global-step:4265	 l-p:0.056328658014535904
epoch£º213	 i:6 	 global-step:4266	 l-p:0.056294508278369904
epoch£º213	 i:7 	 global-step:4267	 l-p:0.05632634088397026
epoch£º213	 i:8 	 global-step:4268	 l-p:0.056231074035167694
epoch£º213	 i:9 	 global-step:4269	 l-p:0.056758515536785126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4252, 27.4363, 27.4256],
        [27.4252, 28.2385, 27.7989],
        [27.4252, 30.0157, 29.8320],
        [27.4252, 27.4252, 27.4252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.05627193674445152 
model_pd.l_d.mean(): -8.693119161762297e-05 
model_pd.lagr.mean(): 0.056185007095336914 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8275], device='cuda:0')), ('power', tensor([-0.5774], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.05627193674445152
epoch£º214	 i:1 	 global-step:4281	 l-p:0.05630471929907799
epoch£º214	 i:2 	 global-step:4282	 l-p:0.05679548904299736
epoch£º214	 i:3 	 global-step:4283	 l-p:0.05640234053134918
epoch£º214	 i:4 	 global-step:4284	 l-p:0.05732690542936325
epoch£º214	 i:5 	 global-step:4285	 l-p:0.05804121494293213
epoch£º214	 i:6 	 global-step:4286	 l-p:0.05646009370684624
epoch£º214	 i:7 	 global-step:4287	 l-p:0.05687880516052246
epoch£º214	 i:8 	 global-step:4288	 l-p:0.05622359737753868
epoch£º214	 i:9 	 global-step:4289	 l-p:0.056323885917663574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4392, 30.0310, 29.8472],
        [27.4392, 27.4578, 27.4400],
        [27.4392, 27.8484, 27.5604],
        [27.4392, 27.4405, 27.4392]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.05625787749886513 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05625787749886513 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8163], device='cuda:0')), ('power', tensor([-0.5371], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.05625787749886513
epoch£º215	 i:1 	 global-step:4301	 l-p:0.05632377043366432
epoch£º215	 i:2 	 global-step:4302	 l-p:0.0565057173371315
epoch£º215	 i:3 	 global-step:4303	 l-p:0.05660747364163399
epoch£º215	 i:4 	 global-step:4304	 l-p:0.05658954381942749
epoch£º215	 i:5 	 global-step:4305	 l-p:0.057911165058612823
epoch£º215	 i:6 	 global-step:4306	 l-p:0.05684635043144226
epoch£º215	 i:7 	 global-step:4307	 l-p:0.056310757994651794
epoch£º215	 i:8 	 global-step:4308	 l-p:0.05626419931650162
epoch£º215	 i:9 	 global-step:4309	 l-p:0.05732354149222374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4752, 27.5173, 27.4783],
        [27.4752, 31.8862, 32.8671],
        [27.4752, 27.9623, 27.6364],
        [27.4752, 27.4752, 27.4752]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.058053720742464066 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058053720742464066 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7705], device='cuda:0')), ('power', tensor([-0.2368], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.058053720742464066
epoch£º216	 i:1 	 global-step:4321	 l-p:0.056778669357299805
epoch£º216	 i:2 	 global-step:4322	 l-p:0.05717730522155762
epoch£º216	 i:3 	 global-step:4323	 l-p:0.05618032440543175
epoch£º216	 i:4 	 global-step:4324	 l-p:0.05704190954566002
epoch£º216	 i:5 	 global-step:4325	 l-p:0.05621415004134178
epoch£º216	 i:6 	 global-step:4326	 l-p:0.0562443807721138
epoch£º216	 i:7 	 global-step:4327	 l-p:0.05634119734168053
epoch£º216	 i:8 	 global-step:4328	 l-p:0.05629877373576164
epoch£º216	 i:9 	 global-step:4329	 l-p:0.05647944286465645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5234, 27.8664, 27.6141],
        [27.5234, 32.9203, 34.7527],
        [27.5234, 30.5109, 30.5183],
        [27.5234, 27.5235, 27.5234]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.05624872073531151 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05624872073531151 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8434], device='cuda:0')), ('power', tensor([-0.4948], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.05624872073531151
epoch£º217	 i:1 	 global-step:4341	 l-p:0.05719937011599541
epoch£º217	 i:2 	 global-step:4342	 l-p:0.05662650987505913
epoch£º217	 i:3 	 global-step:4343	 l-p:0.05720459297299385
epoch£º217	 i:4 	 global-step:4344	 l-p:0.056287411600351334
epoch£º217	 i:5 	 global-step:4345	 l-p:0.05794626101851463
epoch£º217	 i:6 	 global-step:4346	 l-p:0.05629328638315201
epoch£º217	 i:7 	 global-step:4347	 l-p:0.056206826120615005
epoch£º217	 i:8 	 global-step:4348	 l-p:0.056419868022203445
epoch£º217	 i:9 	 global-step:4349	 l-p:0.056211039423942566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5721, 27.7063, 27.5919],
        [27.5721, 27.5721, 27.5721],
        [27.5721, 27.5782, 27.5723],
        [27.5721, 27.5721, 27.5721]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.056216828525066376 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056216828525066376 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8399], device='cuda:0')), ('power', tensor([-0.4715], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.056216828525066376
epoch£º218	 i:1 	 global-step:4361	 l-p:0.05624178797006607
epoch£º218	 i:2 	 global-step:4362	 l-p:0.056518275290727615
epoch£º218	 i:3 	 global-step:4363	 l-p:0.05615156888961792
epoch£º218	 i:4 	 global-step:4364	 l-p:0.05713385343551636
epoch£º218	 i:5 	 global-step:4365	 l-p:0.05733002349734306
epoch£º218	 i:6 	 global-step:4366	 l-p:0.05618638917803764
epoch£º218	 i:7 	 global-step:4367	 l-p:0.058210305869579315
epoch£º218	 i:8 	 global-step:4368	 l-p:0.05633113160729408
epoch£º218	 i:9 	 global-step:4369	 l-p:0.05615665391087532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6210, 27.8191, 27.6580],
        [27.6210, 27.9954, 27.7254],
        [27.6210, 30.2413, 30.0610],
        [27.6210, 37.3412, 44.0788]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.05804246664047241 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05804246664047241 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7535], device='cuda:0')), ('power', tensor([-0.0317], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.05804246664047241
epoch£º219	 i:1 	 global-step:4381	 l-p:0.056367356330156326
epoch£º219	 i:2 	 global-step:4382	 l-p:0.05637584626674652
epoch£º219	 i:3 	 global-step:4383	 l-p:0.05659499019384384
epoch£º219	 i:4 	 global-step:4384	 l-p:0.0567520372569561
epoch£º219	 i:5 	 global-step:4385	 l-p:0.05626581236720085
epoch£º219	 i:6 	 global-step:4386	 l-p:0.05628317594528198
epoch£º219	 i:7 	 global-step:4387	 l-p:0.056355975568294525
epoch£º219	 i:8 	 global-step:4388	 l-p:0.05712294578552246
epoch£º219	 i:9 	 global-step:4389	 l-p:0.05615990608930588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6730, 27.6888, 27.6737],
        [27.6730, 29.0383, 28.5368],
        [27.6730, 27.6730, 27.6730],
        [27.6730, 28.0860, 27.7954]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.05628446489572525 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05628446489572525 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8220], device='cuda:0')), ('power', tensor([-0.3005], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.05628446489572525
epoch£º220	 i:1 	 global-step:4401	 l-p:0.056426748633384705
epoch£º220	 i:2 	 global-step:4402	 l-p:0.058179453015327454
epoch£º220	 i:3 	 global-step:4403	 l-p:0.056200455874204636
epoch£º220	 i:4 	 global-step:4404	 l-p:0.0562165230512619
epoch£º220	 i:5 	 global-step:4405	 l-p:0.05673287808895111
epoch£º220	 i:6 	 global-step:4406	 l-p:0.05620516091585159
epoch£º220	 i:7 	 global-step:4407	 l-p:0.056198958307504654
epoch£º220	 i:8 	 global-step:4408	 l-p:0.05718425661325455
epoch£º220	 i:9 	 global-step:4409	 l-p:0.056520726531744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7237, 33.3482, 35.3701],
        [27.7237, 31.7933, 32.4871],
        [27.7237, 29.8457, 29.4722],
        [27.7237, 27.7338, 27.7241]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.0563400536775589 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0563400536775589 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8172], device='cuda:0')), ('power', tensor([-0.1403], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.0563400536775589
epoch£º221	 i:1 	 global-step:4421	 l-p:0.057247962802648544
epoch£º221	 i:2 	 global-step:4422	 l-p:0.056401532143354416
epoch£º221	 i:3 	 global-step:4423	 l-p:0.0562962181866169
epoch£º221	 i:4 	 global-step:4424	 l-p:0.056213848292827606
epoch£º221	 i:5 	 global-step:4425	 l-p:0.05613383278250694
epoch£º221	 i:6 	 global-step:4426	 l-p:0.058353956788778305
epoch£º221	 i:7 	 global-step:4427	 l-p:0.056292999535799026
epoch£º221	 i:8 	 global-step:4428	 l-p:0.056529734283685684
epoch£º221	 i:9 	 global-step:4429	 l-p:0.05617441609501839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[27.7744, 37.6428, 44.5406],
        [27.7744, 34.0451, 36.6980],
        [27.7744, 37.3238, 43.8035],
        [27.7744, 33.6883, 35.9837]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.056434981524944305 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056434981524944305 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2975e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7484], device='cuda:0')), ('power', tensor([0.0259], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.056434981524944305
epoch£º222	 i:1 	 global-step:4441	 l-p:0.0561942458152771
epoch£º222	 i:2 	 global-step:4442	 l-p:0.056169137358665466
epoch£º222	 i:3 	 global-step:4443	 l-p:0.0561804361641407
epoch£º222	 i:4 	 global-step:4444	 l-p:0.05719360336661339
epoch£º222	 i:5 	 global-step:4445	 l-p:0.056465692818164825
epoch£º222	 i:6 	 global-step:4446	 l-p:0.0562937967479229
epoch£º222	 i:7 	 global-step:4447	 l-p:0.05636690929532051
epoch£º222	 i:8 	 global-step:4448	 l-p:0.056176651269197464
epoch£º222	 i:9 	 global-step:4449	 l-p:0.05834023654460907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8237, 27.8492, 27.8251],
        [27.8237, 30.0434, 29.6969],
        [27.8237, 31.0689, 31.2028],
        [27.8237, 28.2467, 27.9506]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.05611208453774452 
model_pd.l_d.mean(): -2.476160716469167e-06 
model_pd.lagr.mean(): 0.056109607219696045 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.1422e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8582], device='cuda:0')), ('power', tensor([-0.1694], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.05611208453774452
epoch£º223	 i:1 	 global-step:4461	 l-p:0.056262411177158356
epoch£º223	 i:2 	 global-step:4462	 l-p:0.05627141520380974
epoch£º223	 i:3 	 global-step:4463	 l-p:0.05627693608403206
epoch£º223	 i:4 	 global-step:4464	 l-p:0.056940969079732895
epoch£º223	 i:5 	 global-step:4465	 l-p:0.05617170035839081
epoch£º223	 i:6 	 global-step:4466	 l-p:0.0582859106361866
epoch£º223	 i:7 	 global-step:4467	 l-p:0.057018689811229706
epoch£º223	 i:8 	 global-step:4468	 l-p:0.056060705333948135
epoch£º223	 i:9 	 global-step:4469	 l-p:0.05625954642891884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8751, 27.8753, 27.8751],
        [27.8751, 27.8752, 27.8751],
        [27.8751, 28.7265, 28.2735],
        [27.8751, 27.9244, 27.8791]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.05618789419531822 
model_pd.l_d.mean(): -3.319879851915175e-06 
model_pd.lagr.mean(): 0.05618457496166229 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.7386e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8474], device='cuda:0')), ('power', tensor([-0.2066], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.05618789419531822
epoch£º224	 i:1 	 global-step:4481	 l-p:0.056208446621894836
epoch£º224	 i:2 	 global-step:4482	 l-p:0.05616357550024986
epoch£º224	 i:3 	 global-step:4483	 l-p:0.056994564831256866
epoch£º224	 i:4 	 global-step:4484	 l-p:0.05621253699064255
epoch£º224	 i:5 	 global-step:4485	 l-p:0.05620063841342926
epoch£º224	 i:6 	 global-step:4486	 l-p:0.05779265612363815
epoch£º224	 i:7 	 global-step:4487	 l-p:0.057168420404195786
epoch£º224	 i:8 	 global-step:4488	 l-p:0.05642569810152054
epoch£º224	 i:9 	 global-step:4489	 l-p:0.05614534392952919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9257, 27.9257, 27.9257],
        [27.9257, 27.9257, 27.9257],
        [27.9257, 27.9258, 27.9257],
        [27.9257, 30.3023, 30.0069]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.0565742664039135 
model_pd.l_d.mean(): 2.8004765226796735e-06 
model_pd.lagr.mean(): 0.05657706782221794 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.5596e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8093], device='cuda:0')), ('power', tensor([0.0662], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.0565742664039135
epoch£º225	 i:1 	 global-step:4501	 l-p:0.05718786269426346
epoch£º225	 i:2 	 global-step:4502	 l-p:0.05622042715549469
epoch£º225	 i:3 	 global-step:4503	 l-p:0.05619000270962715
epoch£º225	 i:4 	 global-step:4504	 l-p:0.05610853061079979
epoch£º225	 i:5 	 global-step:4505	 l-p:0.056161340326070786
epoch£º225	 i:6 	 global-step:4506	 l-p:0.056243933737277985
epoch£º225	 i:7 	 global-step:4507	 l-p:0.05770938843488693
epoch£º225	 i:8 	 global-step:4508	 l-p:0.056350093334913254
epoch£º225	 i:9 	 global-step:4509	 l-p:0.056599490344524384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9724, 30.7541, 30.6302],
        [27.9724, 34.2699, 36.9223],
        [27.9724, 29.4909, 28.9910],
        [27.9724, 27.9785, 27.9725]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.0561157688498497 
model_pd.l_d.mean(): -1.8318575030207285e-06 
model_pd.lagr.mean(): 0.05611393600702286 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.5687e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8570], device='cuda:0')), ('power', tensor([-0.0211], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.0561157688498497
epoch£º226	 i:1 	 global-step:4521	 l-p:0.05607027933001518
epoch£º226	 i:2 	 global-step:4522	 l-p:0.057771604508161545
epoch£º226	 i:3 	 global-step:4523	 l-p:0.056414272636175156
epoch£º226	 i:4 	 global-step:4524	 l-p:0.05616319552063942
epoch£º226	 i:5 	 global-step:4525	 l-p:0.05622190237045288
epoch£º226	 i:6 	 global-step:4526	 l-p:0.057038720697164536
epoch£º226	 i:7 	 global-step:4527	 l-p:0.05706553906202316
epoch£º226	 i:8 	 global-step:4528	 l-p:0.05616023391485214
epoch£º226	 i:9 	 global-step:4529	 l-p:0.056187376379966736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0139, 30.2510, 29.9025],
        [28.0139, 28.0952, 28.0226],
        [28.0139, 28.7389, 28.3191],
        [28.0139, 28.0233, 28.0142]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.05644304305315018 
model_pd.l_d.mean(): 2.3025970222079195e-05 
model_pd.lagr.mean(): 0.05646606907248497 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8316], device='cuda:0')), ('power', tensor([0.1505], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.05644304305315018
epoch£º227	 i:1 	 global-step:4541	 l-p:0.05607310310006142
epoch£º227	 i:2 	 global-step:4542	 l-p:0.056272853165864944
epoch£º227	 i:3 	 global-step:4543	 l-p:0.0566633865237236
epoch£º227	 i:4 	 global-step:4544	 l-p:0.05623921379446983
epoch£º227	 i:5 	 global-step:4545	 l-p:0.056163106113672256
epoch£º227	 i:6 	 global-step:4546	 l-p:0.057893723249435425
epoch£º227	 i:7 	 global-step:4547	 l-p:0.05623588338494301
epoch£º227	 i:8 	 global-step:4548	 l-p:0.05704430490732193
epoch£º227	 i:9 	 global-step:4549	 l-p:0.05606316030025482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0444, 28.0449, 28.0444],
        [28.0444, 28.0463, 28.0444],
        [28.0444, 28.0625, 28.0452],
        [28.0444, 28.9315, 28.4688]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.056106287986040115 
model_pd.l_d.mean(): 1.6064632291090675e-05 
model_pd.lagr.mean(): 0.05612235143780708 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8504], device='cuda:0')), ('power', tensor([0.0677], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.056106287986040115
epoch£º228	 i:1 	 global-step:4561	 l-p:0.05627359449863434
epoch£º228	 i:2 	 global-step:4562	 l-p:0.057004962116479874
epoch£º228	 i:3 	 global-step:4563	 l-p:0.05617479979991913
epoch£º228	 i:4 	 global-step:4564	 l-p:0.057620029896497726
epoch£º228	 i:5 	 global-step:4565	 l-p:0.05675152689218521
epoch£º228	 i:6 	 global-step:4566	 l-p:0.05601174384355545
epoch£º228	 i:7 	 global-step:4567	 l-p:0.05606745183467865
epoch£º228	 i:8 	 global-step:4568	 l-p:0.05660166218876839
epoch£º228	 i:9 	 global-step:4569	 l-p:0.05640202388167381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0637, 28.0808, 28.0644],
        [28.0637, 34.6236, 37.5333],
        [28.0637, 28.5877, 28.2427],
        [28.0637, 28.4465, 28.1708]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.057821206748485565 
model_pd.l_d.mean(): 0.00015476021508220583 
model_pd.lagr.mean(): 0.05797596648335457 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7486], device='cuda:0')), ('power', tensor([0.4631], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.057821206748485565
epoch£º229	 i:1 	 global-step:4581	 l-p:0.05634802207350731
epoch£º229	 i:2 	 global-step:4582	 l-p:0.05715567618608475
epoch£º229	 i:3 	 global-step:4583	 l-p:0.05693637579679489
epoch£º229	 i:4 	 global-step:4584	 l-p:0.05620456486940384
epoch£º229	 i:5 	 global-step:4585	 l-p:0.05604533851146698
epoch£º229	 i:6 	 global-step:4586	 l-p:0.05612596496939659
epoch£º229	 i:7 	 global-step:4587	 l-p:0.05610397830605507
epoch£º229	 i:8 	 global-step:4588	 l-p:0.0561177060008049
epoch£º229	 i:9 	 global-step:4589	 l-p:0.05611409246921539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[28.0678, 28.7464, 28.3413],
        [28.0678, 37.5866, 43.9622],
        [28.0678, 28.9254, 28.4691],
        [28.0678, 31.2799, 31.3784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.05709471553564072 
model_pd.l_d.mean(): 0.00016448550741188228 
model_pd.lagr.mean(): 0.057259202003479004 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7700], device='cuda:0')), ('power', tensor([0.3757], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.05709471553564072
epoch£º230	 i:1 	 global-step:4601	 l-p:0.05618094652891159
epoch£º230	 i:2 	 global-step:4602	 l-p:0.056120507419109344
epoch£º230	 i:3 	 global-step:4603	 l-p:0.05635515972971916
epoch£º230	 i:4 	 global-step:4604	 l-p:0.05670974776148796
epoch£º230	 i:5 	 global-step:4605	 l-p:0.05608495697379112
epoch£º230	 i:6 	 global-step:4606	 l-p:0.056226976215839386
epoch£º230	 i:7 	 global-step:4607	 l-p:0.05607067048549652
epoch£º230	 i:8 	 global-step:4608	 l-p:0.05641297996044159
epoch£º230	 i:9 	 global-step:4609	 l-p:0.0577356182038784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0508, 28.0510, 28.0508],
        [28.0508, 28.0711, 28.0518],
        [28.0508, 37.3383, 43.4211],
        [28.0508, 29.4085, 28.8993]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.05621304363012314 
model_pd.l_d.mean(): 9.592236892785877e-05 
model_pd.lagr.mean(): 0.056308966130018234 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8115], device='cuda:0')), ('power', tensor([0.1781], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.05621304363012314
epoch£º231	 i:1 	 global-step:4621	 l-p:0.05611713230609894
epoch£º231	 i:2 	 global-step:4622	 l-p:0.058438390493392944
epoch£º231	 i:3 	 global-step:4623	 l-p:0.05607236176729202
epoch£º231	 i:4 	 global-step:4624	 l-p:0.05599904805421829
epoch£º231	 i:5 	 global-step:4625	 l-p:0.056165702641010284
epoch£º231	 i:6 	 global-step:4626	 l-p:0.05660160258412361
epoch£º231	 i:7 	 global-step:4627	 l-p:0.05622658506035805
epoch£º231	 i:8 	 global-step:4628	 l-p:0.05704605579376221
epoch£º231	 i:9 	 global-step:4629	 l-p:0.056179411709308624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0215, 28.0274, 28.0216],
        [28.0215, 28.9417, 28.4723],
        [28.0215, 33.3169, 34.9968],
        [28.0215, 28.1504, 28.0398]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.056084759533405304 
model_pd.l_d.mean(): 1.8681676010601223e-05 
model_pd.lagr.mean(): 0.05610344186425209 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8562], device='cuda:0')), ('power', tensor([0.0297], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.056084759533405304
epoch£º232	 i:1 	 global-step:4641	 l-p:0.05619434267282486
epoch£º232	 i:2 	 global-step:4642	 l-p:0.05610933527350426
epoch£º232	 i:3 	 global-step:4643	 l-p:0.05619705095887184
epoch£º232	 i:4 	 global-step:4644	 l-p:0.056446317583322525
epoch£º232	 i:5 	 global-step:4645	 l-p:0.0565783828496933
epoch£º232	 i:6 	 global-step:4646	 l-p:0.05765896663069725
epoch£º232	 i:7 	 global-step:4647	 l-p:0.056290239095687866
epoch£º232	 i:8 	 global-step:4648	 l-p:0.057026226073503494
epoch£º232	 i:9 	 global-step:4649	 l-p:0.056597523391246796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9753, 29.1054, 28.6063],
        [27.9753, 28.6122, 28.2224],
        [27.9753, 32.2275, 33.0366],
        [27.9753, 36.4914, 41.6298]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.05625248700380325 
model_pd.l_d.mean(): 0.00011369158164598048 
model_pd.lagr.mean(): 0.05636617913842201 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8007], device='cuda:0')), ('power', tensor([0.1626], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.05625248700380325
epoch£º233	 i:1 	 global-step:4661	 l-p:0.05771707743406296
epoch£º233	 i:2 	 global-step:4662	 l-p:0.05645852908492088
epoch£º233	 i:3 	 global-step:4663	 l-p:0.05611494928598404
epoch£º233	 i:4 	 global-step:4664	 l-p:0.0561164990067482
epoch£º233	 i:5 	 global-step:4665	 l-p:0.05670623481273651
epoch£º233	 i:6 	 global-step:4666	 l-p:0.05633339285850525
epoch£º233	 i:7 	 global-step:4667	 l-p:0.056050579994916916
epoch£º233	 i:8 	 global-step:4668	 l-p:0.05704877898097038
epoch£º233	 i:9 	 global-step:4669	 l-p:0.05653349682688713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9185, 28.0782, 27.9445],
        [27.9185, 28.3237, 28.0365],
        [27.9185, 28.4806, 28.1201],
        [27.9185, 27.9210, 27.9186]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.056285180151462555 
model_pd.l_d.mean(): 4.4092423195252195e-05 
model_pd.lagr.mean(): 0.05632927268743515 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7953], device='cuda:0')), ('power', tensor([0.0591], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.056285180151462555
epoch£º234	 i:1 	 global-step:4681	 l-p:0.056297045201063156
epoch£º234	 i:2 	 global-step:4682	 l-p:0.056804437190294266
epoch£º234	 i:3 	 global-step:4683	 l-p:0.05623434856534004
epoch£º234	 i:4 	 global-step:4684	 l-p:0.05611853301525116
epoch£º234	 i:5 	 global-step:4685	 l-p:0.05705460533499718
epoch£º234	 i:6 	 global-step:4686	 l-p:0.05647503584623337
epoch£º234	 i:7 	 global-step:4687	 l-p:0.05625224858522415
epoch£º234	 i:8 	 global-step:4688	 l-p:0.057892076671123505
epoch£º234	 i:9 	 global-step:4689	 l-p:0.056126516312360764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8501, 35.4677, 39.5686],
        [27.8501, 27.8504, 27.8501],
        [27.8501, 28.0480, 27.8869],
        [27.8501, 27.8504, 27.8501]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.05620037764310837 
model_pd.l_d.mean(): -0.00012212898582220078 
model_pd.lagr.mean(): 0.05607824772596359 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8437], device='cuda:0')), ('power', tensor([-0.1605], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.05620037764310837
epoch£º235	 i:1 	 global-step:4701	 l-p:0.056203823536634445
epoch£º235	 i:2 	 global-step:4702	 l-p:0.05636981502175331
epoch£º235	 i:3 	 global-step:4703	 l-p:0.05659134313464165
epoch£º235	 i:4 	 global-step:4704	 l-p:0.05770557373762131
epoch£º235	 i:5 	 global-step:4705	 l-p:0.056201621890068054
epoch£º235	 i:6 	 global-step:4706	 l-p:0.057834021747112274
epoch£º235	 i:7 	 global-step:4707	 l-p:0.05629432573914528
epoch£º235	 i:8 	 global-step:4708	 l-p:0.05614768713712692
epoch£º235	 i:9 	 global-step:4709	 l-p:0.05620685592293739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7797, 31.2932, 31.5958],
        [27.7797, 27.8024, 27.7808],
        [27.7797, 30.4325, 30.2587],
        [27.7797, 28.6259, 28.1750]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.05620066449046135 
model_pd.l_d.mean(): -0.00012800347758457065 
model_pd.lagr.mean(): 0.0560726597905159 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8393], device='cuda:0')), ('power', tensor([-0.1725], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.05620066449046135
epoch£º236	 i:1 	 global-step:4721	 l-p:0.05612067133188248
epoch£º236	 i:2 	 global-step:4722	 l-p:0.05627647414803505
epoch£º236	 i:3 	 global-step:4723	 l-p:0.05775205418467522
epoch£º236	 i:4 	 global-step:4724	 l-p:0.05634282901883125
epoch£º236	 i:5 	 global-step:4725	 l-p:0.057383496314287186
epoch£º236	 i:6 	 global-step:4726	 l-p:0.056129615753889084
epoch£º236	 i:7 	 global-step:4727	 l-p:0.05668344348669052
epoch£º236	 i:8 	 global-step:4728	 l-p:0.056807540357112885
epoch£º236	 i:9 	 global-step:4729	 l-p:0.05628389120101929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7102, 35.9612, 40.8301],
        [27.7102, 29.6649, 29.2456],
        [27.7102, 27.7102, 27.7101],
        [27.7102, 30.3572, 30.1843]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.05675755441188812 
model_pd.l_d.mean(): -3.368626130395569e-05 
model_pd.lagr.mean(): 0.05672386661171913 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7791], device='cuda:0')), ('power', tensor([-0.0489], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.05675755441188812
epoch£º237	 i:1 	 global-step:4741	 l-p:0.0562899112701416
epoch£º237	 i:2 	 global-step:4742	 l-p:0.05622236058115959
epoch£º237	 i:3 	 global-step:4743	 l-p:0.056219156831502914
epoch£º237	 i:4 	 global-step:4744	 l-p:0.05644452944397926
epoch£º237	 i:5 	 global-step:4745	 l-p:0.05618075281381607
epoch£º237	 i:6 	 global-step:4746	 l-p:0.05710243433713913
epoch£º237	 i:7 	 global-step:4747	 l-p:0.05830712988972664
epoch£º237	 i:8 	 global-step:4748	 l-p:0.05628851801156998
epoch£º237	 i:9 	 global-step:4749	 l-p:0.05639231204986572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6464, 36.6882, 42.5447],
        [27.6464, 28.9833, 28.4818],
        [27.6464, 33.8054, 36.3628],
        [27.6464, 27.6464, 27.6464]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.05903902277350426 
model_pd.l_d.mean(): 0.00022309714404400438 
model_pd.lagr.mean(): 0.059262119233608246 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6492], device='cuda:0')), ('power', tensor([0.3704], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.05903902277350426
epoch£º238	 i:1 	 global-step:4761	 l-p:0.05665881186723709
epoch£º238	 i:2 	 global-step:4762	 l-p:0.05614164099097252
epoch£º238	 i:3 	 global-step:4763	 l-p:0.056374892592430115
epoch£º238	 i:4 	 global-step:4764	 l-p:0.056230656802654266
epoch£º238	 i:5 	 global-step:4765	 l-p:0.05619904398918152
epoch£º238	 i:6 	 global-step:4766	 l-p:0.05627240985631943
epoch£º238	 i:7 	 global-step:4767	 l-p:0.05627618357539177
epoch£º238	 i:8 	 global-step:4768	 l-p:0.05632295459508896
epoch£º238	 i:9 	 global-step:4769	 l-p:0.056854378432035446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[27.5975, 33.6575, 36.1220],
        [27.5975, 35.9932, 41.0585],
        [27.5975, 29.5384, 29.1195],
        [27.5975, 32.4703, 33.8235]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.056254491209983826 
model_pd.l_d.mean(): -0.00014263369666878134 
model_pd.lagr.mean(): 0.056111857295036316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8107], device='cuda:0')), ('power', tensor([-0.2920], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.056254491209983826
epoch£º239	 i:1 	 global-step:4781	 l-p:0.056455496698617935
epoch£º239	 i:2 	 global-step:4782	 l-p:0.056309327483177185
epoch£º239	 i:3 	 global-step:4783	 l-p:0.05622066557407379
epoch£º239	 i:4 	 global-step:4784	 l-p:0.05761384963989258
epoch£º239	 i:5 	 global-step:4785	 l-p:0.05639074370265007
epoch£º239	 i:6 	 global-step:4786	 l-p:0.05640522763133049
epoch£º239	 i:7 	 global-step:4787	 l-p:0.05615914240479469
epoch£º239	 i:8 	 global-step:4788	 l-p:0.056815870106220245
epoch£º239	 i:9 	 global-step:4789	 l-p:0.0579097606241703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5609, 32.4121, 33.7507],
        [27.5609, 28.7711, 28.2732],
        [27.5609, 29.3265, 28.8707],
        [27.5609, 27.5796, 27.5618]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): 0.05623399466276169 
model_pd.l_d.mean(): -0.00017055410717148334 
model_pd.lagr.mean(): 0.056063439697027206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8438], device='cuda:0')), ('power', tensor([-0.4840], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:0.05623399466276169
epoch£º240	 i:1 	 global-step:4801	 l-p:0.05676395446062088
epoch£º240	 i:2 	 global-step:4802	 l-p:0.05798982456326485
epoch£º240	 i:3 	 global-step:4803	 l-p:0.056276511400938034
epoch£º240	 i:4 	 global-step:4804	 l-p:0.05644272267818451
epoch£º240	 i:5 	 global-step:4805	 l-p:0.05645987391471863
epoch£º240	 i:6 	 global-step:4806	 l-p:0.056236010044813156
epoch£º240	 i:7 	 global-step:4807	 l-p:0.056222815066576004
epoch£º240	 i:8 	 global-step:4808	 l-p:0.057192131876945496
epoch£º240	 i:9 	 global-step:4809	 l-p:0.05679112300276756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5506, 28.9728, 28.4759],
        [27.5506, 27.5506, 27.5506],
        [27.5506, 33.7682, 36.3985],
        [27.5506, 35.7379, 40.5605]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.05627749487757683 
model_pd.l_d.mean(): -8.290931873489171e-05 
model_pd.lagr.mean(): 0.05619458481669426 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8197], device='cuda:0')), ('power', tensor([-0.4059], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.05627749487757683
epoch£º241	 i:1 	 global-step:4821	 l-p:0.05628145486116409
epoch£º241	 i:2 	 global-step:4822	 l-p:0.05803977698087692
epoch£º241	 i:3 	 global-step:4823	 l-p:0.056165263056755066
epoch£º241	 i:4 	 global-step:4824	 l-p:0.05624274164438248
epoch£º241	 i:5 	 global-step:4825	 l-p:0.05712774023413658
epoch£º241	 i:6 	 global-step:4826	 l-p:0.05620916187763214
epoch£º241	 i:7 	 global-step:4827	 l-p:0.05620735138654709
epoch£º241	 i:8 	 global-step:4828	 l-p:0.05740964412689209
epoch£º241	 i:9 	 global-step:4829	 l-p:0.056649547070264816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5638, 31.7781, 32.5959],
        [27.5638, 28.2294, 27.8320],
        [27.5638, 27.5690, 27.5639],
        [27.5638, 35.0993, 39.1558]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.05631235986948013 
model_pd.l_d.mean(): -1.9510232959873974e-05 
model_pd.lagr.mean(): 0.05629285052418709 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.8684e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8103], device='cuda:0')), ('power', tensor([-0.3479], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.05631235986948013
epoch£º242	 i:1 	 global-step:4841	 l-p:0.056330446153879166
epoch£º242	 i:2 	 global-step:4842	 l-p:0.05790671706199646
epoch£º242	 i:3 	 global-step:4843	 l-p:0.0571504682302475
epoch£º242	 i:4 	 global-step:4844	 l-p:0.05625718832015991
epoch£º242	 i:5 	 global-step:4845	 l-p:0.056626155972480774
epoch£º242	 i:6 	 global-step:4846	 l-p:0.05617126449942589
epoch£º242	 i:7 	 global-step:4847	 l-p:0.05618666112422943
epoch£º242	 i:8 	 global-step:4848	 l-p:0.057299885898828506
epoch£º242	 i:9 	 global-step:4849	 l-p:0.05629017576575279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6013, 28.5218, 28.0569],
        [27.6013, 27.6055, 27.6013],
        [27.6013, 27.6013, 27.6012],
        [27.6013, 34.6840, 38.2256]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.056559666991233826 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056559666991233826 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8156], device='cuda:0')), ('power', tensor([-0.2881], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.056559666991233826
epoch£º243	 i:1 	 global-step:4861	 l-p:0.056250132620334625
epoch£º243	 i:2 	 global-step:4862	 l-p:0.05677598714828491
epoch£º243	 i:3 	 global-step:4863	 l-p:0.0563751682639122
epoch£º243	 i:4 	 global-step:4864	 l-p:0.056304410099983215
epoch£º243	 i:5 	 global-step:4865	 l-p:0.057062529027462006
epoch£º243	 i:6 	 global-step:4866	 l-p:0.058316320180892944
epoch£º243	 i:7 	 global-step:4867	 l-p:0.05619444698095322
epoch£º243	 i:8 	 global-step:4868	 l-p:0.0562029667198658
epoch£º243	 i:9 	 global-step:4869	 l-p:0.05634262412786484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[27.6514, 28.3193, 27.9206],
        [27.6514, 32.0130, 32.9376],
        [27.6514, 28.7964, 28.3009],
        [27.6514, 34.3365, 37.4394]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.056220877915620804 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056220877915620804 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8237], device='cuda:0')), ('power', tensor([-0.2997], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.056220877915620804
epoch£º244	 i:1 	 global-step:4881	 l-p:0.056273650377988815
epoch£º244	 i:2 	 global-step:4882	 l-p:0.05613674968481064
epoch£º244	 i:3 	 global-step:4883	 l-p:0.05633870139718056
epoch£º244	 i:4 	 global-step:4884	 l-p:0.05629327520728111
epoch£º244	 i:5 	 global-step:4885	 l-p:0.05620551481842995
epoch£º244	 i:6 	 global-step:4886	 l-p:0.05754459276795387
epoch£º244	 i:7 	 global-step:4887	 l-p:0.056235987693071365
epoch£º244	 i:8 	 global-step:4888	 l-p:0.05872533842921257
epoch£º244	 i:9 	 global-step:4889	 l-p:0.05623145028948784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7050, 27.7052, 27.7050],
        [27.7050, 30.6013, 30.5496],
        [27.7050, 32.1489, 33.1338],
        [27.7050, 27.7050, 27.7050]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.05635225027799606 
model_pd.l_d.mean(): -1.1405486475268845e-06 
model_pd.lagr.mean(): 0.056351110339164734 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.1663e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7829], device='cuda:0')), ('power', tensor([-0.1309], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.05635225027799606
epoch£º245	 i:1 	 global-step:4901	 l-p:0.05621020495891571
epoch£º245	 i:2 	 global-step:4902	 l-p:0.05626487731933594
epoch£º245	 i:3 	 global-step:4903	 l-p:0.05640647932887077
epoch£º245	 i:4 	 global-step:4904	 l-p:0.05619661509990692
epoch£º245	 i:5 	 global-step:4905	 l-p:0.056647300720214844
epoch£º245	 i:6 	 global-step:4906	 l-p:0.05628093704581261
epoch£º245	 i:7 	 global-step:4907	 l-p:0.05623592436313629
epoch£º245	 i:8 	 global-step:4908	 l-p:0.058236122131347656
epoch£º245	 i:9 	 global-step:4909	 l-p:0.057195644825696945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7612, 33.5830, 35.7902],
        [27.7612, 27.7768, 27.7619],
        [27.7612, 29.0952, 28.5914],
        [27.7612, 27.7617, 27.7612]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.05666905269026756 
model_pd.l_d.mean(): 1.1284235768016515e-07 
model_pd.lagr.mean(): 0.05666916444897652 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4828e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7681], device='cuda:0')), ('power', tensor([0.0078], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.05666905269026756
epoch£º246	 i:1 	 global-step:4921	 l-p:0.05635668337345123
epoch£º246	 i:2 	 global-step:4922	 l-p:0.056183308362960815
epoch£º246	 i:3 	 global-step:4923	 l-p:0.056187570095062256
epoch£º246	 i:4 	 global-step:4924	 l-p:0.05668318271636963
epoch£º246	 i:5 	 global-step:4925	 l-p:0.057985976338386536
epoch£º246	 i:6 	 global-step:4926	 l-p:0.05632178485393524
epoch£º246	 i:7 	 global-step:4927	 l-p:0.05703505501151085
epoch£º246	 i:8 	 global-step:4928	 l-p:0.05624464526772499
epoch£º246	 i:9 	 global-step:4929	 l-p:0.056183330714702606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8208, 27.8208, 27.8207],
        [27.8208, 28.2598, 27.9556],
        [27.8208, 29.8219, 29.4106],
        [27.8208, 27.8209, 27.8208]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.056524429470300674 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056524429470300674 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8158], device='cuda:0')), ('power', tensor([-0.0646], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.056524429470300674
epoch£º247	 i:1 	 global-step:4941	 l-p:0.05616949498653412
epoch£º247	 i:2 	 global-step:4942	 l-p:0.05616697296500206
epoch£º247	 i:3 	 global-step:4943	 l-p:0.05784764140844345
epoch£º247	 i:4 	 global-step:4944	 l-p:0.05635496601462364
epoch£º247	 i:5 	 global-step:4945	 l-p:0.05613597854971886
epoch£º247	 i:6 	 global-step:4946	 l-p:0.05731683969497681
epoch£º247	 i:7 	 global-step:4947	 l-p:0.05610477924346924
epoch£º247	 i:8 	 global-step:4948	 l-p:0.056249573826789856
epoch£º247	 i:9 	 global-step:4949	 l-p:0.056792668998241425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8795, 27.8795, 27.8794],
        [27.8795, 27.8796, 27.8794],
        [27.8795, 28.3013, 28.0055],
        [27.8795, 27.8796, 27.8794]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.05612761899828911 
model_pd.l_d.mean(): -2.3297850475501036e-06 
model_pd.lagr.mean(): 0.05612529069185257 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.5332e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8441], device='cuda:0')), ('power', tensor([-0.1468], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.05612761899828911
epoch£º248	 i:1 	 global-step:4961	 l-p:0.056170616298913956
epoch£º248	 i:2 	 global-step:4962	 l-p:0.05608082562685013
epoch£º248	 i:3 	 global-step:4963	 l-p:0.05717608332633972
epoch£º248	 i:4 	 global-step:4964	 l-p:0.057094499468803406
epoch£º248	 i:5 	 global-step:4965	 l-p:0.05615627020597458
epoch£º248	 i:6 	 global-step:4966	 l-p:0.05609086900949478
epoch£º248	 i:7 	 global-step:4967	 l-p:0.056374773383140564
epoch£º248	 i:8 	 global-step:4968	 l-p:0.056239642202854156
epoch£º248	 i:9 	 global-step:4969	 l-p:0.05796263739466667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[27.9349, 33.4475, 35.3379],
        [27.9349, 33.8741, 36.1734],
        [27.9349, 29.7730, 29.3199],
        [27.9349, 36.2560, 41.1666]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.05609859526157379 
model_pd.l_d.mean(): -2.097975311698974e-06 
model_pd.lagr.mean(): 0.05609649792313576 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.2001e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8595], device='cuda:0')), ('power', tensor([-0.0473], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.05609859526157379
epoch£º249	 i:1 	 global-step:4981	 l-p:0.056180261075496674
epoch£º249	 i:2 	 global-step:4982	 l-p:0.05695214495062828
epoch£º249	 i:3 	 global-step:4983	 l-p:0.05604909732937813
epoch£º249	 i:4 	 global-step:4984	 l-p:0.05630577355623245
epoch£º249	 i:5 	 global-step:4985	 l-p:0.0562153235077858
epoch£º249	 i:6 	 global-step:4986	 l-p:0.05629057437181473
epoch£º249	 i:7 	 global-step:4987	 l-p:0.05657968297600746
epoch£º249	 i:8 	 global-step:4988	 l-p:0.058560118079185486
epoch£º249	 i:9 	 global-step:4989	 l-p:0.0560707226395607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9878, 31.2790, 31.4291],
        [27.9878, 27.9878, 27.9878],
        [27.9878, 28.6117, 28.2266],
        [27.9878, 27.9903, 27.9879]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.05659984424710274 
model_pd.l_d.mean(): 2.1746540369349532e-05 
model_pd.lagr.mean(): 0.05662159249186516 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7868], device='cuda:0')), ('power', tensor([0.2298], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.05659984424710274
epoch£º250	 i:1 	 global-step:5001	 l-p:0.05774957686662674
epoch£º250	 i:2 	 global-step:5002	 l-p:0.05622820556163788
epoch£º250	 i:3 	 global-step:5003	 l-p:0.05638652667403221
epoch£º250	 i:4 	 global-step:5004	 l-p:0.056299470365047455
epoch£º250	 i:5 	 global-step:5005	 l-p:0.05606704577803612
epoch£º250	 i:6 	 global-step:5006	 l-p:0.05614592507481575
epoch£º250	 i:7 	 global-step:5007	 l-p:0.05604805052280426
epoch£º250	 i:8 	 global-step:5008	 l-p:0.0575367733836174
epoch£º250	 i:9 	 global-step:5009	 l-p:0.05609188973903656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0347, 30.9674, 30.9152],
        [28.0347, 28.0414, 28.0349],
        [28.0347, 29.9201, 29.4739],
        [28.0347, 28.0347, 28.0346]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.05621464177966118 
model_pd.l_d.mean(): 2.552565638325177e-05 
model_pd.lagr.mean(): 0.05624016746878624 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8179], device='cuda:0')), ('power', tensor([0.1500], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.05621464177966118
epoch£º251	 i:1 	 global-step:5021	 l-p:0.056139182299375534
epoch£º251	 i:2 	 global-step:5022	 l-p:0.056996915489435196
epoch£º251	 i:3 	 global-step:5023	 l-p:0.05833565071225166
epoch£º251	 i:4 	 global-step:5024	 l-p:0.05616769567131996
epoch£º251	 i:5 	 global-step:5025	 l-p:0.056310128420591354
epoch£º251	 i:6 	 global-step:5026	 l-p:0.056149642914533615
epoch£º251	 i:7 	 global-step:5027	 l-p:0.05604926869273186
epoch£º251	 i:8 	 global-step:5028	 l-p:0.05622025951743126
epoch£º251	 i:9 	 global-step:5029	 l-p:0.05644331872463226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0674, 28.0795, 28.0679],
        [28.0674, 37.4739, 43.7052],
        [28.0674, 29.9400, 29.4899],
        [28.0674, 31.1174, 31.1252]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.056191034615039825 
model_pd.l_d.mean(): 5.773094017058611e-05 
model_pd.lagr.mean(): 0.05624876543879509 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8018], device='cuda:0')), ('power', tensor([0.2175], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.056191034615039825
epoch£º252	 i:1 	 global-step:5041	 l-p:0.05605242773890495
epoch£º252	 i:2 	 global-step:5042	 l-p:0.056218765676021576
epoch£º252	 i:3 	 global-step:5043	 l-p:0.056121960282325745
epoch£º252	 i:4 	 global-step:5044	 l-p:0.05608426779508591
epoch£º252	 i:5 	 global-step:5045	 l-p:0.05814288184046745
epoch£º252	 i:6 	 global-step:5046	 l-p:0.05614607036113739
epoch£º252	 i:7 	 global-step:5047	 l-p:0.05616755038499832
epoch£º252	 i:8 	 global-step:5048	 l-p:0.056830257177352905
epoch£º252	 i:9 	 global-step:5049	 l-p:0.056992702186107635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[28.0820, 37.3802, 43.4702],
        [28.0820, 30.1010, 29.6852],
        [28.0820, 29.9626, 29.5137],
        [28.0820, 33.7747, 35.8166]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.05609983205795288 
model_pd.l_d.mean(): 3.686510899569839e-05 
model_pd.lagr.mean(): 0.05613669753074646 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8591], device='cuda:0')), ('power', tensor([0.0990], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.05609983205795288
epoch£º253	 i:1 	 global-step:5061	 l-p:0.057008810341358185
epoch£º253	 i:2 	 global-step:5062	 l-p:0.05682651326060295
epoch£º253	 i:3 	 global-step:5063	 l-p:0.05632936954498291
epoch£º253	 i:4 	 global-step:5064	 l-p:0.05607587844133377
epoch£º253	 i:5 	 global-step:5065	 l-p:0.058134276419878006
epoch£º253	 i:6 	 global-step:5066	 l-p:0.05603368580341339
epoch£º253	 i:7 	 global-step:5067	 l-p:0.05617966875433922
epoch£º253	 i:8 	 global-step:5068	 l-p:0.05615566298365593
epoch£º253	 i:9 	 global-step:5069	 l-p:0.056080084294080734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0804, 28.6459, 28.2832],
        [28.0804, 29.4021, 28.8921],
        [28.0804, 28.0806, 28.0804],
        [28.0804, 28.0805, 28.0804]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.05620783567428589 
model_pd.l_d.mean(): 7.638178067281842e-05 
model_pd.lagr.mean(): 0.05628421902656555 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8241], device='cuda:0')), ('power', tensor([0.1579], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.05620783567428589
epoch£º254	 i:1 	 global-step:5081	 l-p:0.05607885122299194
epoch£º254	 i:2 	 global-step:5082	 l-p:0.05772868171334267
epoch£º254	 i:3 	 global-step:5083	 l-p:0.05630373954772949
epoch£º254	 i:4 	 global-step:5084	 l-p:0.05670849233865738
epoch£º254	 i:5 	 global-step:5085	 l-p:0.05759956315159798
epoch£º254	 i:6 	 global-step:5086	 l-p:0.05608563870191574
epoch£º254	 i:7 	 global-step:5087	 l-p:0.05609210208058357
epoch£º254	 i:8 	 global-step:5088	 l-p:0.056048277765512466
epoch£º254	 i:9 	 global-step:5089	 l-p:0.05610450729727745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[28.0569, 37.3465, 43.4308],
        [28.0569, 30.7390, 30.5640],
        [28.0569, 33.8075, 35.9078],
        [28.0569, 38.4464, 45.9746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.05775837600231171 
model_pd.l_d.mean(): 0.00025377716519869864 
model_pd.lagr.mean(): 0.05801215395331383 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7609], device='cuda:0')), ('power', tensor([0.4307], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.05775837600231171
epoch£º255	 i:1 	 global-step:5101	 l-p:0.056194476783275604
epoch£º255	 i:2 	 global-step:5102	 l-p:0.056309785693883896
epoch£º255	 i:3 	 global-step:5103	 l-p:0.05616098642349243
epoch£º255	 i:4 	 global-step:5104	 l-p:0.05708538740873337
epoch£º255	 i:5 	 global-step:5105	 l-p:0.056065633893013
epoch£º255	 i:6 	 global-step:5106	 l-p:0.0561918169260025
epoch£º255	 i:7 	 global-step:5107	 l-p:0.05646352469921112
epoch£º255	 i:8 	 global-step:5108	 l-p:0.05672987177968025
epoch£º255	 i:9 	 global-step:5109	 l-p:0.056095536798238754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0127, 29.1444, 28.6446],
        [28.0127, 28.2263, 28.0541],
        [28.0127, 28.0127, 28.0127],
        [28.0127, 33.2058, 34.7946]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.05610281229019165 
model_pd.l_d.mean(): 2.3093116396921687e-06 
model_pd.lagr.mean(): 0.0561051219701767 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8575], device='cuda:0')), ('power', tensor([0.0034], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.05610281229019165
epoch£º256	 i:1 	 global-step:5121	 l-p:0.05632347986102104
epoch£º256	 i:2 	 global-step:5122	 l-p:0.056015558540821075
epoch£º256	 i:3 	 global-step:5123	 l-p:0.056636981666088104
epoch£º256	 i:4 	 global-step:5124	 l-p:0.056498244404792786
epoch£º256	 i:5 	 global-step:5125	 l-p:0.05769823491573334
epoch£º256	 i:6 	 global-step:5126	 l-p:0.05623879283666611
epoch£º256	 i:7 	 global-step:5127	 l-p:0.056210629642009735
epoch£º256	 i:8 	 global-step:5128	 l-p:0.05734839290380478
epoch£º256	 i:9 	 global-step:5129	 l-p:0.05615862086415291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9502, 27.9515, 27.9502],
        [27.9502, 27.9502, 27.9502],
        [27.9502, 31.3225, 31.5241],
        [27.9502, 28.3596, 28.0701]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.05699005350470543 
model_pd.l_d.mean(): 9.88982428680174e-05 
model_pd.lagr.mean(): 0.057088952511548996 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8144], device='cuda:0')), ('power', tensor([0.1333], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.05699005350470543
epoch£º257	 i:1 	 global-step:5141	 l-p:0.056079164147377014
epoch£º257	 i:2 	 global-step:5142	 l-p:0.056693144142627716
epoch£º257	 i:3 	 global-step:5143	 l-p:0.05638865381479263
epoch£º257	 i:4 	 global-step:5144	 l-p:0.056157562881708145
epoch£º257	 i:5 	 global-step:5145	 l-p:0.05823913589119911
epoch£º257	 i:6 	 global-step:5146	 l-p:0.056201789528131485
epoch£º257	 i:7 	 global-step:5147	 l-p:0.05611153319478035
epoch£º257	 i:8 	 global-step:5148	 l-p:0.0562845915555954
epoch£º257	 i:9 	 global-step:5149	 l-p:0.056288860738277435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8770, 27.8946, 27.8778],
        [27.8770, 29.0029, 28.5056],
        [27.8770, 27.8770, 27.8770],
        [27.8770, 33.0984, 34.7282]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.05629812926054001 
model_pd.l_d.mean(): -4.196776899334509e-06 
model_pd.lagr.mean(): 0.05629393085837364 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8112], device='cuda:0')), ('power', tensor([-0.0054], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.05629812926054001
epoch£º258	 i:1 	 global-step:5161	 l-p:0.05698869749903679
epoch£º258	 i:2 	 global-step:5162	 l-p:0.05629739537835121
epoch£º258	 i:3 	 global-step:5163	 l-p:0.05608305335044861
epoch£º258	 i:4 	 global-step:5164	 l-p:0.0561843179166317
epoch£º258	 i:5 	 global-step:5165	 l-p:0.05608545243740082
epoch£º258	 i:6 	 global-step:5166	 l-p:0.058017365634441376
epoch£º258	 i:7 	 global-step:5167	 l-p:0.05631755664944649
epoch£º258	 i:8 	 global-step:5168	 l-p:0.056799933314323425
epoch£º258	 i:9 	 global-step:5169	 l-p:0.05661826580762863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7946, 32.9964, 34.6179],
        [27.7946, 27.7946, 27.7946],
        [27.7946, 27.7959, 27.7946],
        [27.7946, 29.6230, 29.1723]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.057908859103918076 
model_pd.l_d.mean(): 8.635826088720933e-05 
model_pd.lagr.mean(): 0.05799521878361702 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7712], device='cuda:0')), ('power', tensor([0.1130], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.057908859103918076
epoch£º259	 i:1 	 global-step:5181	 l-p:0.05624450743198395
epoch£º259	 i:2 	 global-step:5182	 l-p:0.05614664778113365
epoch£º259	 i:3 	 global-step:5183	 l-p:0.05626602843403816
epoch£º259	 i:4 	 global-step:5184	 l-p:0.05620822682976723
epoch£º259	 i:5 	 global-step:5185	 l-p:0.05714903771877289
epoch£º259	 i:6 	 global-step:5186	 l-p:0.05674886703491211
epoch£º259	 i:7 	 global-step:5187	 l-p:0.05622752383351326
epoch£º259	 i:8 	 global-step:5188	 l-p:0.056925103068351746
epoch£º259	 i:9 	 global-step:5189	 l-p:0.05611085891723633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7143, 34.3828, 37.4586],
        [27.7143, 32.0063, 32.8702],
        [27.7143, 27.7143, 27.7143],
        [27.7143, 29.7321, 29.3290]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.05629211291670799 
model_pd.l_d.mean(): -8.351841097464785e-05 
model_pd.lagr.mean(): 0.056208595633506775 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7996], device='cuda:0')), ('power', tensor([-0.1166], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.05629211291670799
epoch£º260	 i:1 	 global-step:5201	 l-p:0.058521125465631485
epoch£º260	 i:2 	 global-step:5202	 l-p:0.05613824725151062
epoch£º260	 i:3 	 global-step:5203	 l-p:0.0566461943089962
epoch£º260	 i:4 	 global-step:5204	 l-p:0.05673449486494064
epoch£º260	 i:5 	 global-step:5205	 l-p:0.05627588927745819
epoch£º260	 i:6 	 global-step:5206	 l-p:0.05610024929046631
epoch£º260	 i:7 	 global-step:5207	 l-p:0.05707232281565666
epoch£º260	 i:8 	 global-step:5208	 l-p:0.05626976490020752
epoch£º260	 i:9 	 global-step:5209	 l-p:0.05614016577601433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6380, 27.6381, 27.6380],
        [27.6380, 32.4277, 33.7054],
        [27.6380, 32.8089, 34.4207],
        [27.6380, 28.1039, 27.7873]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.05654400587081909 
model_pd.l_d.mean(): -0.00015070334484335035 
model_pd.lagr.mean(): 0.056393302977085114 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8208], device='cuda:0')), ('power', tensor([-0.2395], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.05654400587081909
epoch£º261	 i:1 	 global-step:5221	 l-p:0.056190211325883865
epoch£º261	 i:2 	 global-step:5222	 l-p:0.05660071223974228
epoch£º261	 i:3 	 global-step:5223	 l-p:0.05642164498567581
epoch£º261	 i:4 	 global-step:5224	 l-p:0.058133821934461594
epoch£º261	 i:5 	 global-step:5225	 l-p:0.05618663504719734
epoch£º261	 i:6 	 global-step:5226	 l-p:0.05622399225831032
epoch£º261	 i:7 	 global-step:5227	 l-p:0.05680592358112335
epoch£º261	 i:8 	 global-step:5228	 l-p:0.05718296021223068
epoch£º261	 i:9 	 global-step:5229	 l-p:0.056149065494537354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5731, 27.7307, 27.5987],
        [27.5731, 28.2002, 27.8163],
        [27.5731, 27.6223, 27.5770],
        [27.5731, 27.6139, 27.5760]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.05683296173810959 
model_pd.l_d.mean(): -0.00010080626088893041 
model_pd.lagr.mean(): 0.05673215538263321 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7755], device='cuda:0')), ('power', tensor([-0.1989], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.05683296173810959
epoch£º262	 i:1 	 global-step:5241	 l-p:0.05624121055006981
epoch£º262	 i:2 	 global-step:5242	 l-p:0.05614081397652626
epoch£º262	 i:3 	 global-step:5243	 l-p:0.05647857487201691
epoch£º262	 i:4 	 global-step:5244	 l-p:0.058177679777145386
epoch£º262	 i:5 	 global-step:5245	 l-p:0.05715096369385719
epoch£º262	 i:6 	 global-step:5246	 l-p:0.05633334070444107
epoch£º262	 i:7 	 global-step:5247	 l-p:0.056803446263074875
epoch£º262	 i:8 	 global-step:5248	 l-p:0.05622534081339836
epoch£º262	 i:9 	 global-step:5249	 l-p:0.05623304098844528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5313, 33.5830, 36.0481],
        [27.5313, 31.9459, 32.9242],
        [27.5313, 27.8689, 27.6197],
        [27.5313, 33.1068, 35.1062]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.05722943693399429 
model_pd.l_d.mean(): -7.54663415136747e-05 
model_pd.lagr.mean(): 0.05715397000312805 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7699], device='cuda:0')), ('power', tensor([-0.2112], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.05722943693399429
epoch£º263	 i:1 	 global-step:5261	 l-p:0.05630861595273018
epoch£º263	 i:2 	 global-step:5262	 l-p:0.05642608925700188
epoch£º263	 i:3 	 global-step:5263	 l-p:0.05630342662334442
epoch£º263	 i:4 	 global-step:5264	 l-p:0.056532859802246094
epoch£º263	 i:5 	 global-step:5265	 l-p:0.05627031996846199
epoch£º263	 i:6 	 global-step:5266	 l-p:0.05683654546737671
epoch£º263	 i:7 	 global-step:5267	 l-p:0.05794612690806389
epoch£º263	 i:8 	 global-step:5268	 l-p:0.05621294677257538
epoch£º263	 i:9 	 global-step:5269	 l-p:0.056653913110494614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5144, 27.6201, 27.5279],
        [27.5144, 33.7037, 36.3102],
        [27.5144, 35.6904, 40.5064],
        [27.5144, 32.3718, 33.7207]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.05624813959002495 
model_pd.l_d.mean(): -8.89333023224026e-05 
model_pd.lagr.mean(): 0.05615920573472977 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8194], device='cuda:0')), ('power', tensor([-0.4609], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.05624813959002495
epoch£º264	 i:1 	 global-step:5281	 l-p:0.056313931941986084
epoch£º264	 i:2 	 global-step:5282	 l-p:0.05632631108164787
epoch£º264	 i:3 	 global-step:5283	 l-p:0.05636603385210037
epoch£º264	 i:4 	 global-step:5284	 l-p:0.056322019547224045
epoch£º264	 i:5 	 global-step:5285	 l-p:0.05795106664299965
epoch£º264	 i:6 	 global-step:5286	 l-p:0.05685920640826225
epoch£º264	 i:7 	 global-step:5287	 l-p:0.05622179061174393
epoch£º264	 i:8 	 global-step:5288	 l-p:0.05712765455245972
epoch£º264	 i:9 	 global-step:5289	 l-p:0.05699444189667702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5284, 36.3206, 41.8879],
        [27.5284, 30.5102, 30.5141],
        [27.5284, 27.5285, 27.5284],
        [27.5284, 27.5309, 27.5285]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.056461580097675323 
model_pd.l_d.mean(): -7.231494237203151e-06 
model_pd.lagr.mean(): 0.05645434930920601 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.3142e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7606], device='cuda:0')), ('power', tensor([-0.2709], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.056461580097675323
epoch£º265	 i:1 	 global-step:5301	 l-p:0.05803252011537552
epoch£º265	 i:2 	 global-step:5302	 l-p:0.05632403865456581
epoch£º265	 i:3 	 global-step:5303	 l-p:0.05622788891196251
epoch£º265	 i:4 	 global-step:5304	 l-p:0.05649840086698532
epoch£º265	 i:5 	 global-step:5305	 l-p:0.05719935894012451
epoch£º265	 i:6 	 global-step:5306	 l-p:0.056300144642591476
epoch£º265	 i:7 	 global-step:5307	 l-p:0.05623907968401909
epoch£º265	 i:8 	 global-step:5308	 l-p:0.05623572692275047
epoch£º265	 i:9 	 global-step:5309	 l-p:0.05711658298969269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5742, 27.6305, 27.5791],
        [27.5742, 27.5850, 27.5746],
        [27.5742, 27.8633, 27.6427],
        [27.5742, 27.5784, 27.5743]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.05628085881471634 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05628085881471634 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8079], device='cuda:0')), ('power', tensor([-0.3758], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.05628085881471634
epoch£º266	 i:1 	 global-step:5321	 l-p:0.05732545256614685
epoch£º266	 i:2 	 global-step:5322	 l-p:0.056270960718393326
epoch£º266	 i:3 	 global-step:5323	 l-p:0.056558169424533844
epoch£º266	 i:4 	 global-step:5324	 l-p:0.056406620889902115
epoch£º266	 i:5 	 global-step:5325	 l-p:0.05634218081831932
epoch£º266	 i:6 	 global-step:5326	 l-p:0.058583199977874756
epoch£º266	 i:7 	 global-step:5327	 l-p:0.0561816580593586
epoch£º266	 i:8 	 global-step:5328	 l-p:0.056275490671396255
epoch£º266	 i:9 	 global-step:5329	 l-p:0.05623682960867882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6327, 27.6328, 27.6327],
        [27.6327, 29.8363, 29.4922],
        [27.6327, 27.6327, 27.6327],
        [27.6327, 27.8289, 27.6691]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.056371401995420456 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056371401995420456 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7796], device='cuda:0')), ('power', tensor([-0.1877], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.056371401995420456
epoch£º267	 i:1 	 global-step:5341	 l-p:0.05618785321712494
epoch£º267	 i:2 	 global-step:5342	 l-p:0.05793457850813866
epoch£º267	 i:3 	 global-step:5343	 l-p:0.05713636800646782
epoch£º267	 i:4 	 global-step:5344	 l-p:0.05678356811404228
epoch£º267	 i:5 	 global-step:5345	 l-p:0.056791041046381
epoch£º267	 i:6 	 global-step:5346	 l-p:0.056318700313568115
epoch£º267	 i:7 	 global-step:5347	 l-p:0.05629722401499748
epoch£º267	 i:8 	 global-step:5348	 l-p:0.05630340427160263
epoch£º267	 i:9 	 global-step:5349	 l-p:0.05614195764064789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6977, 27.7029, 27.6978],
        [27.6977, 31.4383, 31.8985],
        [27.6977, 27.9087, 27.7386],
        [27.6977, 27.6977, 27.6977]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.05627740919589996 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05627740919589996 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8026], device='cuda:0')), ('power', tensor([-0.2030], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.05627740919589996
epoch£º268	 i:1 	 global-step:5361	 l-p:0.05818567052483559
epoch£º268	 i:2 	 global-step:5362	 l-p:0.05672850459814072
epoch£º268	 i:3 	 global-step:5363	 l-p:0.05702729895710945
epoch£º268	 i:4 	 global-step:5364	 l-p:0.05647930130362511
epoch£º268	 i:5 	 global-step:5365	 l-p:0.05634215101599693
epoch£º268	 i:6 	 global-step:5366	 l-p:0.05634633079171181
epoch£º268	 i:7 	 global-step:5367	 l-p:0.05617479234933853
epoch£º268	 i:8 	 global-step:5368	 l-p:0.05631948262453079
epoch£º268	 i:9 	 global-step:5369	 l-p:0.05617496371269226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7642, 27.7755, 27.7646],
        [27.7642, 27.8520, 27.7742],
        [27.7642, 28.5069, 28.0836],
        [27.7642, 28.2325, 27.9143]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.05618049576878548 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05618049576878548 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8249], device='cuda:0')), ('power', tensor([-0.1348], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.05618049576878548
epoch£º269	 i:1 	 global-step:5381	 l-p:0.05610858276486397
epoch£º269	 i:2 	 global-step:5382	 l-p:0.0566343329846859
epoch£º269	 i:3 	 global-step:5383	 l-p:0.05633631721138954
epoch£º269	 i:4 	 global-step:5384	 l-p:0.056244973093271255
epoch£º269	 i:5 	 global-step:5385	 l-p:0.057032063603401184
epoch£º269	 i:6 	 global-step:5386	 l-p:0.056734759360551834
epoch£º269	 i:7 	 global-step:5387	 l-p:0.056275080889463425
epoch£º269	 i:8 	 global-step:5388	 l-p:0.057813338935375214
epoch£º269	 i:9 	 global-step:5389	 l-p:0.0564703643321991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8270, 27.9005, 27.8345],
        [27.8270, 32.1374, 33.0051],
        [27.8270, 27.8382, 27.8274],
        [27.8270, 30.4677, 30.2860]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.05614914372563362 
model_pd.l_d.mean(): -1.723966988720349e-06 
model_pd.lagr.mean(): 0.05614741891622543 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8446], device='cuda:0')), ('power', tensor([-0.1991], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.05614914372563362
epoch£º270	 i:1 	 global-step:5401	 l-p:0.05634729191660881
epoch£º270	 i:2 	 global-step:5402	 l-p:0.05657055228948593
epoch£º270	 i:3 	 global-step:5403	 l-p:0.05610229820013046
epoch£º270	 i:4 	 global-step:5404	 l-p:0.05611124262213707
epoch£º270	 i:5 	 global-step:5405	 l-p:0.05867559462785721
epoch£º270	 i:6 	 global-step:5406	 l-p:0.057000160217285156
epoch£º270	 i:7 	 global-step:5407	 l-p:0.05626481771469116
epoch£º270	 i:8 	 global-step:5408	 l-p:0.05620834603905678
epoch£º270	 i:9 	 global-step:5409	 l-p:0.05620213970541954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8930, 27.8930, 27.8930],
        [27.8930, 30.3118, 30.0339],
        [27.8930, 33.5373, 35.5569],
        [27.8930, 36.2098, 41.1231]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.05615752190351486 
model_pd.l_d.mean(): -1.416490135852655e-06 
model_pd.lagr.mean(): 0.05615610629320145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.3515e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8306], device='cuda:0')), ('power', tensor([-0.0540], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.05615752190351486
epoch£º271	 i:1 	 global-step:5421	 l-p:0.0561228021979332
epoch£º271	 i:2 	 global-step:5422	 l-p:0.056185778230428696
epoch£º271	 i:3 	 global-step:5423	 l-p:0.05844739452004433
epoch£º271	 i:4 	 global-step:5424	 l-p:0.05614814534783363
epoch£º271	 i:5 	 global-step:5425	 l-p:0.057130731642246246
epoch£º271	 i:6 	 global-step:5426	 l-p:0.056239478290081024
epoch£º271	 i:7 	 global-step:5427	 l-p:0.05659286305308342
epoch£º271	 i:8 	 global-step:5428	 l-p:0.05625468119978905
epoch£º271	 i:9 	 global-step:5429	 l-p:0.05614984780550003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9567, 28.1443, 27.9903],
        [27.9567, 34.3304, 37.0630],
        [27.9567, 28.3426, 28.0655],
        [27.9567, 28.0731, 27.9723]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.057708099484443665 
model_pd.l_d.mean(): 1.4378306332218926e-05 
model_pd.lagr.mean(): 0.05772247910499573 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.0578e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7869], device='cuda:0')), ('power', tensor([0.2469], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.057708099484443665
epoch£º272	 i:1 	 global-step:5441	 l-p:0.05677725747227669
epoch£º272	 i:2 	 global-step:5442	 l-p:0.05624540522694588
epoch£º272	 i:3 	 global-step:5443	 l-p:0.056504860520362854
epoch£º272	 i:4 	 global-step:5444	 l-p:0.05614346265792847
epoch£º272	 i:5 	 global-step:5445	 l-p:0.056192778050899506
epoch£º272	 i:6 	 global-step:5446	 l-p:0.056245334446430206
epoch£º272	 i:7 	 global-step:5447	 l-p:0.05630825087428093
epoch£º272	 i:8 	 global-step:5448	 l-p:0.05616937577724457
epoch£º272	 i:9 	 global-step:5449	 l-p:0.05694472789764404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0134, 29.9854, 29.5599],
        [28.0134, 28.0445, 28.0153],
        [28.0134, 28.4883, 28.1660],
        [28.0134, 28.1935, 28.0448]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.05744122341275215 
model_pd.l_d.mean(): 5.5123968195402995e-05 
model_pd.lagr.mean(): 0.05749634653329849 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7472], device='cuda:0')), ('power', tensor([0.4569], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.05744122341275215
epoch£º273	 i:1 	 global-step:5461	 l-p:0.05618787556886673
epoch£º273	 i:2 	 global-step:5462	 l-p:0.05611054226756096
epoch£º273	 i:3 	 global-step:5463	 l-p:0.05775919929146767
epoch£º273	 i:4 	 global-step:5464	 l-p:0.05619953200221062
epoch£º273	 i:5 	 global-step:5465	 l-p:0.056288719177246094
epoch£º273	 i:6 	 global-step:5466	 l-p:0.05668807402253151
epoch£º273	 i:7 	 global-step:5467	 l-p:0.05605284497141838
epoch£º273	 i:8 	 global-step:5468	 l-p:0.056049879640340805
epoch£º273	 i:9 	 global-step:5469	 l-p:0.056299932301044464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0584, 32.0099, 32.5887],
        [28.0584, 36.4181, 41.3516],
        [28.0584, 28.1000, 28.0614],
        [28.0584, 36.4038, 41.3201]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.056085795164108276 
model_pd.l_d.mean(): 6.555208074132679e-06 
model_pd.lagr.mean(): 0.05609235167503357 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8581], device='cuda:0')), ('power', tensor([0.0315], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.056085795164108276
epoch£º274	 i:1 	 global-step:5481	 l-p:0.05610741674900055
epoch£º274	 i:2 	 global-step:5482	 l-p:0.056111641228199005
epoch£º274	 i:3 	 global-step:5483	 l-p:0.056074026972055435
epoch£º274	 i:4 	 global-step:5484	 l-p:0.05817047506570816
epoch£º274	 i:5 	 global-step:5485	 l-p:0.05617782473564148
epoch£º274	 i:6 	 global-step:5486	 l-p:0.05627143755555153
epoch£º274	 i:7 	 global-step:5487	 l-p:0.0572214238345623
epoch£º274	 i:8 	 global-step:5488	 l-p:0.05631116405129433
epoch£º274	 i:9 	 global-step:5489	 l-p:0.05642654374241829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0863, 32.7285, 33.8366],
        [28.0863, 32.9579, 34.2577],
        [28.0863, 28.0863, 28.0863],
        [28.0863, 28.2668, 28.1178]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.056058235466480255 
model_pd.l_d.mean(): 8.19720298750326e-06 
model_pd.lagr.mean(): 0.05606643110513687 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8722], device='cuda:0')), ('power', tensor([0.0261], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.056058235466480255
epoch£º275	 i:1 	 global-step:5501	 l-p:0.05829441919922829
epoch£º275	 i:2 	 global-step:5502	 l-p:0.05613172799348831
epoch£º275	 i:3 	 global-step:5503	 l-p:0.05647695064544678
epoch£º275	 i:4 	 global-step:5504	 l-p:0.05611175298690796
epoch£º275	 i:5 	 global-step:5505	 l-p:0.056021612137556076
epoch£º275	 i:6 	 global-step:5506	 l-p:0.0562252402305603
epoch£º275	 i:7 	 global-step:5507	 l-p:0.0569525808095932
epoch£º275	 i:8 	 global-step:5508	 l-p:0.05642344430088997
epoch£º275	 i:9 	 global-step:5509	 l-p:0.056198280304670334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0963, 35.8672, 40.1014],
        [28.0963, 28.3036, 28.1357],
        [28.0963, 28.0965, 28.0963],
        [28.0963, 28.1168, 28.0973]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.056035980582237244 
model_pd.l_d.mean(): 1.142273595178267e-05 
model_pd.lagr.mean(): 0.05604740232229233 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8801], device='cuda:0')), ('power', tensor([0.0266], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.056035980582237244
epoch£º276	 i:1 	 global-step:5521	 l-p:0.05612744390964508
epoch£º276	 i:2 	 global-step:5522	 l-p:0.05898407846689224
epoch£º276	 i:3 	 global-step:5523	 l-p:0.056005433201789856
epoch£º276	 i:4 	 global-step:5524	 l-p:0.0562727190554142
epoch£º276	 i:5 	 global-step:5525	 l-p:0.05607285350561142
epoch£º276	 i:6 	 global-step:5526	 l-p:0.05687172710895538
epoch£º276	 i:7 	 global-step:5527	 l-p:0.056177251040935516
epoch£º276	 i:8 	 global-step:5528	 l-p:0.05627826601266861
epoch£º276	 i:9 	 global-step:5529	 l-p:0.056069158017635345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0822, 34.4860, 37.2316],
        [28.0822, 35.1928, 38.6880],
        [28.0822, 28.0847, 28.0823],
        [28.0822, 33.5396, 35.3607]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.0561806745827198 
model_pd.l_d.mean(): 0.00011724305659299716 
model_pd.lagr.mean(): 0.056297916918992996 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8213], device='cuda:0')), ('power', tensor([0.2149], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.0561806745827198
epoch£º277	 i:1 	 global-step:5541	 l-p:0.056111354380846024
epoch£º277	 i:2 	 global-step:5542	 l-p:0.057712435722351074
epoch£º277	 i:3 	 global-step:5543	 l-p:0.05605372413992882
epoch£º277	 i:4 	 global-step:5544	 l-p:0.05631854385137558
epoch£º277	 i:5 	 global-step:5545	 l-p:0.056229367852211
epoch£º277	 i:6 	 global-step:5546	 l-p:0.05621113255620003
epoch£º277	 i:7 	 global-step:5547	 l-p:0.05604437738656998
epoch£º277	 i:8 	 global-step:5548	 l-p:0.05710896477103233
epoch£º277	 i:9 	 global-step:5549	 l-p:0.05700765550136566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0418, 28.1352, 28.0527],
        [28.0418, 28.3916, 28.1343],
        [28.0418, 29.0793, 28.5899],
        [28.0418, 34.8260, 37.9752]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.05769103392958641 
model_pd.l_d.mean(): 0.00022210957831703126 
model_pd.lagr.mean(): 0.05791314318776131 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7974], device='cuda:0')), ('power', tensor([0.3426], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.05769103392958641
epoch£º278	 i:1 	 global-step:5561	 l-p:0.05636820197105408
epoch£º278	 i:2 	 global-step:5562	 l-p:0.056277964264154434
epoch£º278	 i:3 	 global-step:5563	 l-p:0.05660513788461685
epoch£º278	 i:4 	 global-step:5564	 l-p:0.05696924030780792
epoch£º278	 i:5 	 global-step:5565	 l-p:0.05657142400741577
epoch£º278	 i:6 	 global-step:5566	 l-p:0.05616648122668266
epoch£º278	 i:7 	 global-step:5567	 l-p:0.05611798167228699
epoch£º278	 i:8 	 global-step:5568	 l-p:0.05621127039194107
epoch£º278	 i:9 	 global-step:5569	 l-p:0.056139979511499405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9817, 29.5786, 29.0860],
        [27.9817, 28.4146, 28.1129],
        [27.9817, 34.3614, 37.0966],
        [27.9817, 29.3936, 28.8871]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.056146059185266495 
model_pd.l_d.mean(): -5.658329882862745e-06 
model_pd.lagr.mean(): 0.05614040046930313 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8407], device='cuda:0')), ('power', tensor([-0.0078], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.056146059185266495
epoch£º279	 i:1 	 global-step:5581	 l-p:0.05619192123413086
epoch£º279	 i:2 	 global-step:5582	 l-p:0.057797688990831375
epoch£º279	 i:3 	 global-step:5583	 l-p:0.05649194493889809
epoch£º279	 i:4 	 global-step:5584	 l-p:0.05618971213698387
epoch£º279	 i:5 	 global-step:5585	 l-p:0.05656036362051964
epoch£º279	 i:6 	 global-step:5586	 l-p:0.05608852580189705
epoch£º279	 i:7 	 global-step:5587	 l-p:0.05770982429385185
epoch£º279	 i:8 	 global-step:5588	 l-p:0.05611535534262657
epoch£º279	 i:9 	 global-step:5589	 l-p:0.056061744689941406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8991, 27.9103, 27.8995],
        [27.8991, 28.3212, 28.0253],
        [27.8991, 28.7416, 28.2905],
        [27.8991, 29.3549, 28.8522]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.056481312960386276 
model_pd.l_d.mean(): 5.427639848676336e-07 
model_pd.lagr.mean(): 0.05648185685276985 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8301], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.056481312960386276
epoch£º280	 i:1 	 global-step:5601	 l-p:0.056514352560043335
epoch£º280	 i:2 	 global-step:5602	 l-p:0.056212134659290314
epoch£º280	 i:3 	 global-step:5603	 l-p:0.05694573372602463
epoch£º280	 i:4 	 global-step:5604	 l-p:0.056731853634119034
epoch£º280	 i:5 	 global-step:5605	 l-p:0.05619669705629349
epoch£º280	 i:6 	 global-step:5606	 l-p:0.05613633990287781
epoch£º280	 i:7 	 global-step:5607	 l-p:0.05779613181948662
epoch£º280	 i:8 	 global-step:5608	 l-p:0.05633345618844032
epoch£º280	 i:9 	 global-step:5609	 l-p:0.056286294013261795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8057, 30.4444, 30.2627],
        [27.8057, 27.8118, 27.8059],
        [27.8057, 27.9610, 27.8306],
        [27.8057, 28.5956, 28.1587]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.05615714564919472 
model_pd.l_d.mean(): -0.00013543984096031636 
model_pd.lagr.mean(): 0.05602170526981354 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8410], device='cuda:0')), ('power', tensor([-0.1756], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.05615714564919472
epoch£º281	 i:1 	 global-step:5621	 l-p:0.05613063648343086
epoch£º281	 i:2 	 global-step:5622	 l-p:0.056375280022621155
epoch£º281	 i:3 	 global-step:5623	 l-p:0.05682604759931564
epoch£º281	 i:4 	 global-step:5624	 l-p:0.05785300210118294
epoch£º281	 i:5 	 global-step:5625	 l-p:0.056387271732091904
epoch£º281	 i:6 	 global-step:5626	 l-p:0.056151945143938065
epoch£º281	 i:7 	 global-step:5627	 l-p:0.05625195428729057
epoch£º281	 i:8 	 global-step:5628	 l-p:0.05765676498413086
epoch£º281	 i:9 	 global-step:5629	 l-p:0.05614648759365082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[27.7115, 28.5481, 28.1001],
        [27.7115, 30.7097, 30.7113],
        [27.7115, 28.6794, 28.2047],
        [27.7115, 35.5992, 40.0372]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.05733964964747429 
model_pd.l_d.mean(): 0.0001610726903891191 
model_pd.lagr.mean(): 0.057500723749399185 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7065], device='cuda:0')), ('power', tensor([0.2221], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.05733964964747429
epoch£º282	 i:1 	 global-step:5641	 l-p:0.05776403471827507
epoch£º282	 i:2 	 global-step:5642	 l-p:0.05615108460187912
epoch£º282	 i:3 	 global-step:5643	 l-p:0.05676580220460892
epoch£º282	 i:4 	 global-step:5644	 l-p:0.05677734315395355
epoch£º282	 i:5 	 global-step:5645	 l-p:0.05626041442155838
epoch£º282	 i:6 	 global-step:5646	 l-p:0.05624176934361458
epoch£º282	 i:7 	 global-step:5647	 l-p:0.05628048628568649
epoch£º282	 i:8 	 global-step:5648	 l-p:0.05635284259915352
epoch£º282	 i:9 	 global-step:5649	 l-p:0.05627160891890526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6260, 31.8507, 32.6707],
        [27.6260, 35.4882, 39.9117],
        [27.6260, 31.8504, 32.6702],
        [27.6260, 27.6753, 27.6300]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.05634792521595955 
model_pd.l_d.mean(): -0.00014601383008994162 
model_pd.lagr.mean(): 0.056201912462711334 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7890], device='cuda:0')), ('power', tensor([-0.2299], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.05634792521595955
epoch£º283	 i:1 	 global-step:5661	 l-p:0.05635815113782883
epoch£º283	 i:2 	 global-step:5662	 l-p:0.05623233690857887
epoch£º283	 i:3 	 global-step:5663	 l-p:0.05633868649601936
epoch£º283	 i:4 	 global-step:5664	 l-p:0.05636908486485481
epoch£º283	 i:5 	 global-step:5665	 l-p:0.057061851024627686
epoch£º283	 i:6 	 global-step:5666	 l-p:0.05843932554125786
epoch£º283	 i:7 	 global-step:5667	 l-p:0.05639982968568802
epoch£º283	 i:8 	 global-step:5668	 l-p:0.0561504103243351
epoch£º283	 i:9 	 global-step:5669	 l-p:0.056802503764629364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5519, 27.7093, 27.5774],
        [27.5519, 27.6377, 27.5615],
        [27.5519, 27.5558, 27.5519],
        [27.5519, 30.6687, 30.7460]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.05617426708340645 
model_pd.l_d.mean(): -0.00023653471725992858 
model_pd.lagr.mean(): 0.05593773350119591 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8487], device='cuda:0')), ('power', tensor([-0.4688], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.05617426708340645
epoch£º284	 i:1 	 global-step:5681	 l-p:0.05645914375782013
epoch£º284	 i:2 	 global-step:5682	 l-p:0.05632960796356201
epoch£º284	 i:3 	 global-step:5683	 l-p:0.05625975877046585
epoch£º284	 i:4 	 global-step:5684	 l-p:0.056227874010801315
epoch£º284	 i:5 	 global-step:5685	 l-p:0.057247113436460495
epoch£º284	 i:6 	 global-step:5686	 l-p:0.05621984228491783
epoch£º284	 i:7 	 global-step:5687	 l-p:0.05640047416090965
epoch£º284	 i:8 	 global-step:5688	 l-p:0.05838863179087639
epoch£º284	 i:9 	 global-step:5689	 l-p:0.05700256675481796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5036, 27.5036, 27.5036],
        [27.5036, 27.6300, 27.5215],
        [27.5036, 28.3409, 27.8947],
        [27.5036, 31.5392, 32.2270]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.056268200278282166 
model_pd.l_d.mean(): -0.0001584136625751853 
model_pd.lagr.mean(): 0.05610978603363037 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8217], device='cuda:0')), ('power', tensor([-0.4618], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.056268200278282166
epoch£º285	 i:1 	 global-step:5701	 l-p:0.05639825761318207
epoch£º285	 i:2 	 global-step:5702	 l-p:0.05808175355195999
epoch£º285	 i:3 	 global-step:5703	 l-p:0.056589528918266296
epoch£º285	 i:4 	 global-step:5704	 l-p:0.05618257075548172
epoch£º285	 i:5 	 global-step:5705	 l-p:0.05745583400130272
epoch£º285	 i:6 	 global-step:5706	 l-p:0.056861523538827896
epoch£º285	 i:7 	 global-step:5707	 l-p:0.056329596787691116
epoch£º285	 i:8 	 global-step:5708	 l-p:0.05638441815972328
epoch£º285	 i:9 	 global-step:5709	 l-p:0.056250594556331635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4928, 27.7217, 27.5397],
        [27.4928, 27.4931, 27.4928],
        [27.4928, 27.4928, 27.4928],
        [27.4928, 27.5212, 27.4944]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.056243646889925 
model_pd.l_d.mean(): -8.715193689567968e-05 
model_pd.lagr.mean(): 0.05615649372339249 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8283], device='cuda:0')), ('power', tensor([-0.5242], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.056243646889925
epoch£º286	 i:1 	 global-step:5721	 l-p:0.057138096541166306
epoch£º286	 i:2 	 global-step:5722	 l-p:0.05669661983847618
epoch£º286	 i:3 	 global-step:5723	 l-p:0.056292369961738586
epoch£º286	 i:4 	 global-step:5724	 l-p:0.05637659877538681
epoch£º286	 i:5 	 global-step:5725	 l-p:0.05705999210476875
epoch£º286	 i:6 	 global-step:5726	 l-p:0.058126144111156464
epoch£º286	 i:7 	 global-step:5727	 l-p:0.05629323050379753
epoch£º286	 i:8 	 global-step:5728	 l-p:0.056237854063510895
epoch£º286	 i:9 	 global-step:5729	 l-p:0.05632903426885605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5142, 31.2609, 31.7400],
        [27.5142, 32.0570, 33.1409],
        [27.5142, 28.8444, 28.3454],
        [27.5142, 27.8936, 27.6212]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.0562787763774395 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0562787763774395 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8125], device='cuda:0')), ('power', tensor([-0.4392], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.0562787763774395
epoch£º287	 i:1 	 global-step:5741	 l-p:0.05630170553922653
epoch£º287	 i:2 	 global-step:5742	 l-p:0.056206390261650085
epoch£º287	 i:3 	 global-step:5743	 l-p:0.05624617636203766
epoch£º287	 i:4 	 global-step:5744	 l-p:0.056240592151880264
epoch£º287	 i:5 	 global-step:5745	 l-p:0.05676063895225525
epoch£º287	 i:6 	 global-step:5746	 l-p:0.058121588081121445
epoch£º287	 i:7 	 global-step:5747	 l-p:0.05690431967377663
epoch£º287	 i:8 	 global-step:5748	 l-p:0.05718449130654335
epoch£º287	 i:9 	 global-step:5749	 l-p:0.05642365291714668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5654, 27.7753, 27.6061],
        [27.5654, 27.5654, 27.5654],
        [27.5654, 27.5812, 27.5660],
        [27.5654, 30.7789, 30.9113]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.0562230683863163 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0562230683863163 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8253], device='cuda:0')), ('power', tensor([-0.4082], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.0562230683863163
epoch£º288	 i:1 	 global-step:5761	 l-p:0.05614921450614929
epoch£º288	 i:2 	 global-step:5762	 l-p:0.05625610053539276
epoch£º288	 i:3 	 global-step:5763	 l-p:0.05789975821971893
epoch£º288	 i:4 	 global-step:5764	 l-p:0.05641215294599533
epoch£º288	 i:5 	 global-step:5765	 l-p:0.0562751479446888
epoch£º288	 i:6 	 global-step:5766	 l-p:0.05656183138489723
epoch£º288	 i:7 	 global-step:5767	 l-p:0.05626913905143738
epoch£º288	 i:8 	 global-step:5768	 l-p:0.056329064071178436
epoch£º288	 i:9 	 global-step:5769	 l-p:0.05809653177857399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6323, 32.8505, 34.5056],
        [27.6323, 27.6478, 27.6329],
        [27.6323, 27.6433, 27.6327],
        [27.6323, 27.8360, 27.6710]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.056927334517240524 
model_pd.l_d.mean(): -3.617154220592056e-07 
model_pd.lagr.mean(): 0.056926973164081573 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.7302e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7329], device='cuda:0')), ('power', tensor([-0.0412], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.056927334517240524
epoch£º289	 i:1 	 global-step:5781	 l-p:0.056343354284763336
epoch£º289	 i:2 	 global-step:5782	 l-p:0.05646077170968056
epoch£º289	 i:3 	 global-step:5783	 l-p:0.05611220747232437
epoch£º289	 i:4 	 global-step:5784	 l-p:0.056245822459459305
epoch£º289	 i:5 	 global-step:5785	 l-p:0.056207723915576935
epoch£º289	 i:6 	 global-step:5786	 l-p:0.057160500437021255
epoch£º289	 i:7 	 global-step:5787	 l-p:0.056709978729486465
epoch£º289	 i:8 	 global-step:5788	 l-p:0.056287746876478195
epoch£º289	 i:9 	 global-step:5789	 l-p:0.057785578072071075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7027, 27.7027, 27.7027],
        [27.7027, 27.7516, 27.7066],
        [27.7027, 27.7027, 27.7027],
        [27.7027, 29.0055, 28.5028]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.05620749294757843 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05620749294757843 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8413], device='cuda:0')), ('power', tensor([-0.3406], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.05620749294757843
epoch£º290	 i:1 	 global-step:5801	 l-p:0.05612488463521004
epoch£º290	 i:2 	 global-step:5802	 l-p:0.05625900253653526
epoch£º290	 i:3 	 global-step:5803	 l-p:0.05638350546360016
epoch£º290	 i:4 	 global-step:5804	 l-p:0.05679650232195854
epoch£º290	 i:5 	 global-step:5805	 l-p:0.05615164712071419
epoch£º290	 i:6 	 global-step:5806	 l-p:0.056289710104465485
epoch£º290	 i:7 	 global-step:5807	 l-p:0.05629626661539078
epoch£º290	 i:8 	 global-step:5808	 l-p:0.05834897235035896
epoch£º290	 i:9 	 global-step:5809	 l-p:0.057148560881614685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7752, 29.4141, 28.9319],
        [27.7752, 32.2311, 33.2187],
        [27.7752, 27.9912, 27.8177],
        [27.7752, 27.7756, 27.7752]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.056113388389348984 
model_pd.l_d.mean(): -5.369834980228916e-06 
model_pd.lagr.mean(): 0.0561080202460289 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.8065e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8703], device='cuda:0')), ('power', tensor([-0.3210], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.056113388389348984
epoch£º291	 i:1 	 global-step:5821	 l-p:0.056621454656124115
epoch£º291	 i:2 	 global-step:5822	 l-p:0.0562194399535656
epoch£º291	 i:3 	 global-step:5823	 l-p:0.0566847138106823
epoch£º291	 i:4 	 global-step:5824	 l-p:0.05616702511906624
epoch£º291	 i:5 	 global-step:5825	 l-p:0.057825520634651184
epoch£º291	 i:6 	 global-step:5826	 l-p:0.05720389261841774
epoch£º291	 i:7 	 global-step:5827	 l-p:0.05655200034379959
epoch£º291	 i:8 	 global-step:5828	 l-p:0.05622272193431854
epoch£º291	 i:9 	 global-step:5829	 l-p:0.05617125332355499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8515, 27.8516, 27.8515],
        [27.8515, 27.9336, 27.8604],
        [27.8515, 27.8574, 27.8516],
        [27.8515, 27.8516, 27.8515]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.0563679113984108 
model_pd.l_d.mean(): -3.3402199051124626e-07 
model_pd.lagr.mean(): 0.056367576122283936 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.2081e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8143], device='cuda:0')), ('power', tensor([-0.0406], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.0563679113984108
epoch£º292	 i:1 	 global-step:5841	 l-p:0.057833705097436905
epoch£º292	 i:2 	 global-step:5842	 l-p:0.056160714477300644
epoch£º292	 i:3 	 global-step:5843	 l-p:0.05660560354590416
epoch£º292	 i:4 	 global-step:5844	 l-p:0.056149158626794815
epoch£º292	 i:5 	 global-step:5845	 l-p:0.05624038726091385
epoch£º292	 i:6 	 global-step:5846	 l-p:0.056105468422174454
epoch£º292	 i:7 	 global-step:5847	 l-p:0.057765815407037735
epoch£º292	 i:8 	 global-step:5848	 l-p:0.05618901923298836
epoch£º292	 i:9 	 global-step:5849	 l-p:0.05612855777144432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[27.9268, 29.9361, 29.5231],
        [27.9268, 29.8888, 29.4638],
        [27.9268, 28.7778, 28.3244],
        [27.9268, 28.6018, 28.1989]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.05614378675818443 
model_pd.l_d.mean(): -2.0066468096047174e-06 
model_pd.lagr.mean(): 0.05614177882671356 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.9834e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8625], device='cuda:0')), ('power', tensor([-0.0836], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.05614378675818443
epoch£º293	 i:1 	 global-step:5861	 l-p:0.0567578487098217
epoch£º293	 i:2 	 global-step:5862	 l-p:0.05614425614476204
epoch£º293	 i:3 	 global-step:5863	 l-p:0.05772223323583603
epoch£º293	 i:4 	 global-step:5864	 l-p:0.0560886450111866
epoch£º293	 i:5 	 global-step:5865	 l-p:0.056204646825790405
epoch£º293	 i:6 	 global-step:5866	 l-p:0.057044897228479385
epoch£º293	 i:7 	 global-step:5867	 l-p:0.056783389300107956
epoch£º293	 i:8 	 global-step:5868	 l-p:0.056051548570394516
epoch£º293	 i:9 	 global-step:5869	 l-p:0.056371305137872696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[27.9970, 29.0328, 28.5442],
        [27.9970, 32.8523, 34.1477],
        [27.9970, 33.5304, 35.4326],
        [27.9970, 30.3397, 30.0286]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.05700935050845146 
model_pd.l_d.mean(): 1.6115665857796557e-05 
model_pd.lagr.mean(): 0.05702546611428261 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.4995e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8025], device='cuda:0')), ('power', tensor([0.2174], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.05700935050845146
epoch£º294	 i:1 	 global-step:5881	 l-p:0.056188229471445084
epoch£º294	 i:2 	 global-step:5882	 l-p:0.056533731520175934
epoch£º294	 i:3 	 global-step:5883	 l-p:0.05761931464076042
epoch£º294	 i:4 	 global-step:5884	 l-p:0.05632426217198372
epoch£º294	 i:5 	 global-step:5885	 l-p:0.056202393025159836
epoch£º294	 i:6 	 global-step:5886	 l-p:0.05615611746907234
epoch£º294	 i:7 	 global-step:5887	 l-p:0.056108467280864716
epoch£º294	 i:8 	 global-step:5888	 l-p:0.05625718832015991
epoch£º294	 i:9 	 global-step:5889	 l-p:0.056707561016082764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0580, 29.5821, 29.0806],
        [28.0580, 28.0614, 28.0581],
        [28.0580, 28.0810, 28.0592],
        [28.0580, 28.0582, 28.0580]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.05602700635790825 
model_pd.l_d.mean(): 2.482833224348724e-06 
model_pd.lagr.mean(): 0.056029487401247025 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8868], device='cuda:0')), ('power', tensor([0.0158], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.05602700635790825
epoch£º295	 i:1 	 global-step:5901	 l-p:0.05679266154766083
epoch£º295	 i:2 	 global-step:5902	 l-p:0.056338872760534286
epoch£º295	 i:3 	 global-step:5903	 l-p:0.056117791682481766
epoch£º295	 i:4 	 global-step:5904	 l-p:0.05623786896467209
epoch£º295	 i:5 	 global-step:5905	 l-p:0.056003399193286896
epoch£º295	 i:6 	 global-step:5906	 l-p:0.0566793791949749
epoch£º295	 i:7 	 global-step:5907	 l-p:0.05839353799819946
epoch£º295	 i:8 	 global-step:5908	 l-p:0.05619772896170616
epoch£º295	 i:9 	 global-step:5909	 l-p:0.056147489696741104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1000, 29.9039, 29.4391],
        [28.1000, 28.1962, 28.1115],
        [28.1000, 28.1113, 28.1004],
        [28.1000, 28.1000, 28.1000]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.05612015724182129 
model_pd.l_d.mean(): 2.962643156934064e-05 
model_pd.lagr.mean(): 0.05614978447556496 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8454], device='cuda:0')), ('power', tensor([0.1114], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.05612015724182129
epoch£º296	 i:1 	 global-step:5921	 l-p:0.056151848286390305
epoch£º296	 i:2 	 global-step:5922	 l-p:0.05619746446609497
epoch£º296	 i:3 	 global-step:5923	 l-p:0.05763502046465874
epoch£º296	 i:4 	 global-step:5924	 l-p:0.05707177147269249
epoch£º296	 i:5 	 global-step:5925	 l-p:0.05636410415172577
epoch£º296	 i:6 	 global-step:5926	 l-p:0.05660190433263779
epoch£º296	 i:7 	 global-step:5927	 l-p:0.05601004511117935
epoch£º296	 i:8 	 global-step:5928	 l-p:0.05658892169594765
epoch£º296	 i:9 	 global-step:5929	 l-p:0.05609390512108803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1224, 36.4878, 41.4160],
        [28.1224, 29.4749, 28.9642],
        [28.1224, 28.3347, 28.1634],
        [28.1224, 28.1337, 28.1228]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.056553345173597336 
model_pd.l_d.mean(): 0.00014300837938208133 
model_pd.lagr.mean(): 0.05669635534286499 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7836], device='cuda:0')), ('power', tensor([0.3655], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.056553345173597336
epoch£º297	 i:1 	 global-step:5941	 l-p:0.05621059238910675
epoch£º297	 i:2 	 global-step:5942	 l-p:0.05694780871272087
epoch£º297	 i:3 	 global-step:5943	 l-p:0.05824527144432068
epoch£º297	 i:4 	 global-step:5944	 l-p:0.0562424473464489
epoch£º297	 i:5 	 global-step:5945	 l-p:0.056112803518772125
epoch£º297	 i:6 	 global-step:5946	 l-p:0.05608908459544182
epoch£º297	 i:7 	 global-step:5947	 l-p:0.05601966381072998
epoch£º297	 i:8 	 global-step:5948	 l-p:0.056073691695928574
epoch£º297	 i:9 	 global-step:5949	 l-p:0.056308090686798096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1159, 28.1161, 28.1159],
        [28.1159, 28.7559, 28.3641],
        [28.1159, 28.1177, 28.1159],
        [28.1159, 37.8124, 44.4074]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.056489236652851105 
model_pd.l_d.mean(): 0.00013168113946449012 
model_pd.lagr.mean(): 0.056620918214321136 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8108], device='cuda:0')), ('power', tensor([0.2524], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.056489236652851105
epoch£º298	 i:1 	 global-step:5961	 l-p:0.05608591064810753
epoch£º298	 i:2 	 global-step:5962	 l-p:0.05765397846698761
epoch£º298	 i:3 	 global-step:5963	 l-p:0.05691635608673096
epoch£º298	 i:4 	 global-step:5964	 l-p:0.05627910792827606
epoch£º298	 i:5 	 global-step:5965	 l-p:0.05637120082974434
epoch£º298	 i:6 	 global-step:5966	 l-p:0.05607050657272339
epoch£º298	 i:7 	 global-step:5967	 l-p:0.05620477721095085
epoch£º298	 i:8 	 global-step:5968	 l-p:0.0561424158513546
epoch£º298	 i:9 	 global-step:5969	 l-p:0.05665140971541405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0779, 28.2348, 28.1030],
        [28.0779, 29.1214, 28.6307],
        [28.0779, 30.0967, 29.6809],
        [28.0779, 31.3155, 31.4280]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.05611734837293625 
model_pd.l_d.mean(): 6.843191658845171e-05 
model_pd.lagr.mean(): 0.056185781955718994 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8403], device='cuda:0')), ('power', tensor([0.1066], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.05611734837293625
epoch£º299	 i:1 	 global-step:5981	 l-p:0.056147508323192596
epoch£º299	 i:2 	 global-step:5982	 l-p:0.05619252473115921
epoch£º299	 i:3 	 global-step:5983	 l-p:0.057599104940891266
epoch£º299	 i:4 	 global-step:5984	 l-p:0.0560123510658741
epoch£º299	 i:5 	 global-step:5985	 l-p:0.056213364005088806
epoch£º299	 i:6 	 global-step:5986	 l-p:0.05605362355709076
epoch£º299	 i:7 	 global-step:5987	 l-p:0.056192412972450256
epoch£º299	 i:8 	 global-step:5988	 l-p:0.05817149952054024
epoch£º299	 i:9 	 global-step:5989	 l-p:0.056332044303417206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0110, 36.2708, 41.0936],
        [28.0110, 28.4632, 28.1518],
        [28.0110, 28.0110, 28.0110],
        [28.0110, 28.4443, 28.1423]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.056091826409101486 
model_pd.l_d.mean(): -8.331870958500076e-06 
model_pd.lagr.mean(): 0.05608349293470383 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8573], device='cuda:0')), ('power', tensor([-0.0113], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.056091826409101486
epoch£º300	 i:1 	 global-step:6001	 l-p:0.05877246707677841
epoch£º300	 i:2 	 global-step:6002	 l-p:0.05631707236170769
epoch£º300	 i:3 	 global-step:6003	 l-p:0.05613948404788971
epoch£º300	 i:4 	 global-step:6004	 l-p:0.05603756010532379
epoch£º300	 i:5 	 global-step:6005	 l-p:0.056273311376571655
epoch£º300	 i:6 	 global-step:6006	 l-p:0.056199558079242706
epoch£º300	 i:7 	 global-step:6007	 l-p:0.05626620352268219
epoch£º300	 i:8 	 global-step:6008	 l-p:0.05710462108254433
epoch£º300	 i:9 	 global-step:6009	 l-p:0.056052081286907196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9232, 28.8990, 28.4205],
        [27.9232, 27.9232, 27.9232],
        [27.9232, 31.4560, 31.7604],
        [27.9232, 27.9233, 27.9232]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.05771555379033089 
model_pd.l_d.mean(): 0.00020154552476014942 
model_pd.lagr.mean(): 0.05791709944605827 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7893], device='cuda:0')), ('power', tensor([0.2538], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.05771555379033089
epoch£º301	 i:1 	 global-step:6021	 l-p:0.05617387965321541
epoch£º301	 i:2 	 global-step:6022	 l-p:0.05710040032863617
epoch£º301	 i:3 	 global-step:6023	 l-p:0.05614858493208885
epoch£º301	 i:4 	 global-step:6024	 l-p:0.056692227721214294
epoch£º301	 i:5 	 global-step:6025	 l-p:0.056732214987277985
epoch£º301	 i:6 	 global-step:6026	 l-p:0.05614979565143585
epoch£º301	 i:7 	 global-step:6027	 l-p:0.0562751479446888
epoch£º301	 i:8 	 global-step:6028	 l-p:0.056222427636384964
epoch£º301	 i:9 	 global-step:6029	 l-p:0.056338485330343246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8180, 29.0401, 28.5373],
        [27.8180, 27.8339, 27.8186],
        [27.8180, 35.5153, 39.7134],
        [27.8180, 27.8321, 27.8185]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.05613832175731659 
model_pd.l_d.mean(): -0.0001635221706237644 
model_pd.lagr.mean(): 0.055974800139665604 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8580], device='cuda:0')), ('power', tensor([-0.2033], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.05613832175731659
epoch£º302	 i:1 	 global-step:6041	 l-p:0.05615907534956932
epoch£º302	 i:2 	 global-step:6042	 l-p:0.05630911886692047
epoch£º302	 i:3 	 global-step:6043	 l-p:0.057186998426914215
epoch£º302	 i:4 	 global-step:6044	 l-p:0.057742536067962646
epoch£º302	 i:5 	 global-step:6045	 l-p:0.05622754991054535
epoch£º302	 i:6 	 global-step:6046	 l-p:0.056363195180892944
epoch£º302	 i:7 	 global-step:6047	 l-p:0.05631115660071373
epoch£º302	 i:8 	 global-step:6048	 l-p:0.05713655799627304
epoch£º302	 i:9 	 global-step:6049	 l-p:0.05634989216923714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7045, 27.7046, 27.7045],
        [27.7045, 28.7955, 28.3042],
        [27.7045, 37.0204, 43.2139],
        [27.7045, 29.5665, 29.1257]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.057107992470264435 
model_pd.l_d.mean(): -1.6808678992674686e-05 
model_pd.lagr.mean(): 0.057091183960437775 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7684], device='cuda:0')), ('power', tensor([-0.0221], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.057107992470264435
epoch£º303	 i:1 	 global-step:6061	 l-p:0.05622519925236702
epoch£º303	 i:2 	 global-step:6062	 l-p:0.056227367371320724
epoch£º303	 i:3 	 global-step:6063	 l-p:0.05627526715397835
epoch£º303	 i:4 	 global-step:6064	 l-p:0.05622698739171028
epoch£º303	 i:5 	 global-step:6065	 l-p:0.05621637776494026
epoch£º303	 i:6 	 global-step:6066	 l-p:0.05644971877336502
epoch£º303	 i:7 	 global-step:6067	 l-p:0.05615866556763649
epoch£º303	 i:8 	 global-step:6068	 l-p:0.058012235909700394
epoch£º303	 i:9 	 global-step:6069	 l-p:0.05739660933613777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5977, 27.5976, 27.5977],
        [27.5977, 27.5976, 27.5976],
        [27.5977, 27.6128, 27.5983],
        [27.5977, 27.6028, 27.5978]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.058041762560606 
model_pd.l_d.mean(): -5.618087016046047e-05 
model_pd.lagr.mean(): 0.0579855814576149 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7765], device='cuda:0')), ('power', tensor([-0.0849], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.058041762560606
epoch£º304	 i:1 	 global-step:6081	 l-p:0.05679316446185112
epoch£º304	 i:2 	 global-step:6082	 l-p:0.05706321448087692
epoch£º304	 i:3 	 global-step:6083	 l-p:0.05639515817165375
epoch£º304	 i:4 	 global-step:6084	 l-p:0.05639875307679176
epoch£º304	 i:5 	 global-step:6085	 l-p:0.05684123560786247
epoch£º304	 i:6 	 global-step:6086	 l-p:0.05629054084420204
epoch£º304	 i:7 	 global-step:6087	 l-p:0.056365497410297394
epoch£º304	 i:8 	 global-step:6088	 l-p:0.056181132793426514
epoch£º304	 i:9 	 global-step:6089	 l-p:0.05619996041059494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5167, 29.4570, 29.0408],
        [27.5167, 27.5183, 27.5167],
        [27.5167, 35.5309, 40.1535],
        [27.5167, 28.3495, 27.9043]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.05635058134794235 
model_pd.l_d.mean(): -0.00019085564417764544 
model_pd.lagr.mean(): 0.05615972727537155 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7885], device='cuda:0')), ('power', tensor([-0.3697], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.05635058134794235
epoch£º305	 i:1 	 global-step:6101	 l-p:0.056133002042770386
epoch£º305	 i:2 	 global-step:6102	 l-p:0.05634082853794098
epoch£º305	 i:3 	 global-step:6103	 l-p:0.05624748021364212
epoch£º305	 i:4 	 global-step:6104	 l-p:0.056371983140707016
epoch£º305	 i:5 	 global-step:6105	 l-p:0.05903933569788933
epoch£º305	 i:6 	 global-step:6106	 l-p:0.056251268833875656
epoch£º305	 i:7 	 global-step:6107	 l-p:0.05689441040158272
epoch£º305	 i:8 	 global-step:6108	 l-p:0.056791458278894424
epoch£º305	 i:9 	 global-step:6109	 l-p:0.056419022381305695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4592, 27.4592, 27.4592],
        [27.4592, 35.2717, 39.6671],
        [27.4592, 27.4592, 27.4592],
        [27.4592, 30.2344, 30.1357]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.057353366166353226 
model_pd.l_d.mean(): -6.883220339659601e-05 
model_pd.lagr.mean(): 0.057284533977508545 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7313], device='cuda:0')), ('power', tensor([-0.2055], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.057353366166353226
epoch£º306	 i:1 	 global-step:6121	 l-p:0.05647159367799759
epoch£º306	 i:2 	 global-step:6122	 l-p:0.056327857077121735
epoch£º306	 i:3 	 global-step:6123	 l-p:0.056278541684150696
epoch£º306	 i:4 	 global-step:6124	 l-p:0.056749191135168076
epoch£º306	 i:5 	 global-step:6125	 l-p:0.056397464126348495
epoch£º306	 i:6 	 global-step:6126	 l-p:0.05626686289906502
epoch£º306	 i:7 	 global-step:6127	 l-p:0.05807417258620262
epoch£º306	 i:8 	 global-step:6128	 l-p:0.05638383701443672
epoch£º306	 i:9 	 global-step:6129	 l-p:0.05665941163897514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4430, 27.4430, 27.4430],
        [27.4430, 27.9949, 27.6409],
        [27.4430, 33.6111, 36.2059],
        [27.4430, 27.9295, 27.6039]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.05624844506382942 
model_pd.l_d.mean(): -6.826130265835673e-05 
model_pd.lagr.mean(): 0.056180182844400406 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8309], device='cuda:0')), ('power', tensor([-0.5067], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.05624844506382942
epoch£º307	 i:1 	 global-step:6141	 l-p:0.05669253319501877
epoch£º307	 i:2 	 global-step:6142	 l-p:0.056337807327508926
epoch£º307	 i:3 	 global-step:6143	 l-p:0.05809039995074272
epoch£º307	 i:4 	 global-step:6144	 l-p:0.056757502257823944
epoch£º307	 i:5 	 global-step:6145	 l-p:0.05634913221001625
epoch£º307	 i:6 	 global-step:6146	 l-p:0.05644902214407921
epoch£º307	 i:7 	 global-step:6147	 l-p:0.057471778243780136
epoch£º307	 i:8 	 global-step:6148	 l-p:0.05622320994734764
epoch£º307	 i:9 	 global-step:6149	 l-p:0.05632903054356575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4737, 27.6073, 27.4933],
        [27.4737, 27.6687, 27.5099],
        [27.4737, 28.1370, 27.7410],
        [27.4737, 30.1673, 30.0285]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.05616943538188934 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05616943538188934 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8672], device='cuda:0')), ('power', tensor([-0.6204], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.05616943538188934
epoch£º308	 i:1 	 global-step:6161	 l-p:0.05722959339618683
epoch£º308	 i:2 	 global-step:6162	 l-p:0.05639874190092087
epoch£º308	 i:3 	 global-step:6163	 l-p:0.0564921610057354
epoch£º308	 i:4 	 global-step:6164	 l-p:0.058000367134809494
epoch£º308	 i:5 	 global-step:6165	 l-p:0.05714963749051094
epoch£º308	 i:6 	 global-step:6166	 l-p:0.05647442862391472
epoch£º308	 i:7 	 global-step:6167	 l-p:0.05619971081614494
epoch£º308	 i:8 	 global-step:6168	 l-p:0.056230515241622925
epoch£º308	 i:9 	 global-step:6169	 l-p:0.05644742026925087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5377, 28.8691, 28.3697],
        [27.5377, 32.0839, 33.1683],
        [27.5377, 27.5410, 27.5378],
        [27.5377, 33.3451, 35.5673]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.05734405294060707 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05734405294060707 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7430], device='cuda:0')), ('power', tensor([-0.1599], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.05734405294060707
epoch£º309	 i:1 	 global-step:6181	 l-p:0.05628901347517967
epoch£º309	 i:2 	 global-step:6182	 l-p:0.056224092841148376
epoch£º309	 i:3 	 global-step:6183	 l-p:0.05636262893676758
epoch£º309	 i:4 	 global-step:6184	 l-p:0.05666852742433548
epoch£º309	 i:5 	 global-step:6185	 l-p:0.05627799406647682
epoch£º309	 i:6 	 global-step:6186	 l-p:0.058532312512397766
epoch£º309	 i:7 	 global-step:6187	 l-p:0.05625413358211517
epoch£º309	 i:8 	 global-step:6188	 l-p:0.05629197135567665
epoch£º309	 i:9 	 global-step:6189	 l-p:0.05631120130419731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6134, 27.7257, 27.6282],
        [27.6134, 27.6159, 27.6135],
        [27.6134, 32.0479, 33.0342],
        [27.6134, 34.4622, 37.7464]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.05615512654185295 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05615512654185295 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8542], device='cuda:0')), ('power', tensor([-0.3914], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.05615512654185295
epoch£º310	 i:1 	 global-step:6201	 l-p:0.056121427565813065
epoch£º310	 i:2 	 global-step:6202	 l-p:0.05633269250392914
epoch£º310	 i:3 	 global-step:6203	 l-p:0.05627985671162605
epoch£º310	 i:4 	 global-step:6204	 l-p:0.05659795552492142
epoch£º310	 i:5 	 global-step:6205	 l-p:0.05637785792350769
epoch£º310	 i:6 	 global-step:6206	 l-p:0.058021266013383865
epoch£º310	 i:7 	 global-step:6207	 l-p:0.05626789480447769
epoch£º310	 i:8 	 global-step:6208	 l-p:0.05798903852701187
epoch£º310	 i:9 	 global-step:6209	 l-p:0.05615019053220749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[27.6919, 29.7079, 29.3052],
        [27.6919, 29.6395, 29.2190],
        [27.6919, 28.9082, 28.4077],
        [27.6919, 30.1152, 29.8483]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.057936891913414 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057936891913414 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0888e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7700], device='cuda:0')), ('power', tensor([0.0218], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.057936891913414
epoch£º311	 i:1 	 global-step:6221	 l-p:0.056290216743946075
epoch£º311	 i:2 	 global-step:6222	 l-p:0.05631019547581673
epoch£º311	 i:3 	 global-step:6223	 l-p:0.05621562525629997
epoch£º311	 i:4 	 global-step:6224	 l-p:0.05631372332572937
epoch£º311	 i:5 	 global-step:6225	 l-p:0.056950006633996964
epoch£º311	 i:6 	 global-step:6226	 l-p:0.056699082255363464
epoch£º311	 i:7 	 global-step:6227	 l-p:0.05710062012076378
epoch£º311	 i:8 	 global-step:6228	 l-p:0.05610058829188347
epoch£º311	 i:9 	 global-step:6229	 l-p:0.0561286099255085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7769, 29.8580, 29.4704],
        [27.7769, 27.8694, 27.7877],
        [27.7769, 27.7769, 27.7769],
        [27.7769, 34.4939, 37.6118]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.05612152814865112 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05612152814865112 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8568], device='cuda:0')), ('power', tensor([-0.2495], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.05612152814865112
epoch£º312	 i:1 	 global-step:6241	 l-p:0.0561845637857914
epoch£º312	 i:2 	 global-step:6242	 l-p:0.056442201137542725
epoch£º312	 i:3 	 global-step:6243	 l-p:0.05632918328046799
epoch£º312	 i:4 	 global-step:6244	 l-p:0.05739337205886841
epoch£º312	 i:5 	 global-step:6245	 l-p:0.05612336844205856
epoch£º312	 i:6 	 global-step:6246	 l-p:0.0561661534011364
epoch£º312	 i:7 	 global-step:6247	 l-p:0.056339114904403687
epoch£º312	 i:8 	 global-step:6248	 l-p:0.05617806315422058
epoch£º312	 i:9 	 global-step:6249	 l-p:0.058482713997364044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8557, 28.3756, 28.0334],
        [27.8557, 35.5570, 39.7530],
        [27.8557, 29.9431, 29.5543],
        [27.8557, 28.0182, 27.8824]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.05612636357545853 
model_pd.l_d.mean(): -3.6435010315472027e-06 
model_pd.lagr.mean(): 0.05612272024154663 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4516e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8487], device='cuda:0')), ('power', tensor([-0.1613], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.05612636357545853
epoch£º313	 i:1 	 global-step:6261	 l-p:0.056173596531152725
epoch£º313	 i:2 	 global-step:6262	 l-p:0.05713758245110512
epoch£º313	 i:3 	 global-step:6263	 l-p:0.056349270045757294
epoch£º313	 i:4 	 global-step:6264	 l-p:0.05656172335147858
epoch£º313	 i:5 	 global-step:6265	 l-p:0.05605877935886383
epoch£º313	 i:6 	 global-step:6266	 l-p:0.056343983858823776
epoch£º313	 i:7 	 global-step:6267	 l-p:0.05625202879309654
epoch£º313	 i:8 	 global-step:6268	 l-p:0.056661080569028854
epoch£º313	 i:9 	 global-step:6269	 l-p:0.0578438825905323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9366, 27.9366, 27.9366],
        [27.9366, 28.1474, 27.9772],
        [27.9366, 28.7659, 28.3177],
        [27.9366, 29.0941, 28.5933]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.05627685785293579 
model_pd.l_d.mean(): 3.5548612231650623e-06 
model_pd.lagr.mean(): 0.056280411779880524 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.4601e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7781], device='cuda:0')), ('power', tensor([0.0885], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.05627685785293579
epoch£º314	 i:1 	 global-step:6281	 l-p:0.05604492127895355
epoch£º314	 i:2 	 global-step:6282	 l-p:0.05683501437306404
epoch£º314	 i:3 	 global-step:6283	 l-p:0.056499820202589035
epoch£º314	 i:4 	 global-step:6284	 l-p:0.05611532926559448
epoch£º314	 i:5 	 global-step:6285	 l-p:0.057042963802814484
epoch£º314	 i:6 	 global-step:6286	 l-p:0.05768454447388649
epoch£º314	 i:7 	 global-step:6287	 l-p:0.05620153248310089
epoch£º314	 i:8 	 global-step:6288	 l-p:0.05649375170469284
epoch£º314	 i:9 	 global-step:6289	 l-p:0.05607125163078308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0145, 32.3015, 33.1337],
        [28.0145, 29.8903, 29.4426],
        [28.0145, 28.0145, 28.0145],
        [28.0145, 34.2352, 36.8039]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.05626162514090538 
model_pd.l_d.mean(): 9.907796993502416e-06 
model_pd.lagr.mean(): 0.05627153441309929 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8320], device='cuda:0')), ('power', tensor([0.1023], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.05626162514090538
epoch£º315	 i:1 	 global-step:6301	 l-p:0.05821046605706215
epoch£º315	 i:2 	 global-step:6302	 l-p:0.05612726882100105
epoch£º315	 i:3 	 global-step:6303	 l-p:0.05622356757521629
epoch£º315	 i:4 	 global-step:6304	 l-p:0.05610146373510361
epoch£º315	 i:5 	 global-step:6305	 l-p:0.05607715994119644
epoch£º315	 i:6 	 global-step:6306	 l-p:0.057172782719135284
epoch£º315	 i:7 	 global-step:6307	 l-p:0.056133393198251724
epoch£º315	 i:8 	 global-step:6308	 l-p:0.05660955235362053
epoch£º315	 i:9 	 global-step:6309	 l-p:0.05612935125827789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0793, 28.2168, 28.0996],
        [28.0793, 28.0996, 28.0803],
        [28.0793, 30.4699, 30.1728],
        [28.0793, 28.6449, 28.2822]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.0560113899409771 
model_pd.l_d.mean(): -5.256408712739358e-06 
model_pd.lagr.mean(): 0.05600613355636597 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8950], device='cuda:0')), ('power', tensor([-0.0277], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.0560113899409771
epoch£º316	 i:1 	 global-step:6321	 l-p:0.05612699314951897
epoch£º316	 i:2 	 global-step:6322	 l-p:0.058105114847421646
epoch£º316	 i:3 	 global-step:6323	 l-p:0.056193992495536804
epoch£º316	 i:4 	 global-step:6324	 l-p:0.05611693114042282
epoch£º316	 i:5 	 global-step:6325	 l-p:0.05605683848261833
epoch£º316	 i:6 	 global-step:6326	 l-p:0.05602892115712166
epoch£º316	 i:7 	 global-step:6327	 l-p:0.05619664117693901
epoch£º316	 i:8 	 global-step:6328	 l-p:0.05604996532201767
epoch£º316	 i:9 	 global-step:6329	 l-p:0.05798468366265297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1186, 28.7589, 28.3670],
        [28.1186, 28.1323, 28.1191],
        [28.1186, 29.4708, 28.9602],
        [28.1186, 29.2680, 28.7651]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.05614802613854408 
model_pd.l_d.mean(): 6.020090222591534e-05 
model_pd.lagr.mean(): 0.05620822682976723 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8316], device='cuda:0')), ('power', tensor([0.1950], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.05614802613854408
epoch£º317	 i:1 	 global-step:6341	 l-p:0.056177929043769836
epoch£º317	 i:2 	 global-step:6342	 l-p:0.05612953007221222
epoch£º317	 i:3 	 global-step:6343	 l-p:0.05732401832938194
epoch£º317	 i:4 	 global-step:6344	 l-p:0.056160952895879745
epoch£º317	 i:5 	 global-step:6345	 l-p:0.056112952530384064
epoch£º317	 i:6 	 global-step:6346	 l-p:0.05783529579639435
epoch£º317	 i:7 	 global-step:6347	 l-p:0.05619492381811142
epoch£º317	 i:8 	 global-step:6348	 l-p:0.05599411204457283
epoch£º317	 i:9 	 global-step:6349	 l-p:0.05670931190252304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1327, 28.8096, 28.4047],
        [28.1327, 28.1352, 28.1328],
        [28.1327, 28.2775, 28.1547],
        [28.1327, 28.1411, 28.1330]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.05619331821799278 
model_pd.l_d.mean(): 0.00010430770635139197 
model_pd.lagr.mean(): 0.056297626346349716 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8290], device='cuda:0')), ('power', tensor([0.2362], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.05619331821799278
epoch£º318	 i:1 	 global-step:6361	 l-p:0.05603810027241707
epoch£º318	 i:2 	 global-step:6362	 l-p:0.05627550929784775
epoch£º318	 i:3 	 global-step:6363	 l-p:0.05853981897234917
epoch£º318	 i:4 	 global-step:6364	 l-p:0.05682555213570595
epoch£º318	 i:5 	 global-step:6365	 l-p:0.05619124695658684
epoch£º318	 i:6 	 global-step:6366	 l-p:0.05604766681790352
epoch£º318	 i:7 	 global-step:6367	 l-p:0.056467454880476
epoch£º318	 i:8 	 global-step:6368	 l-p:0.05607392638921738
epoch£º318	 i:9 	 global-step:6369	 l-p:0.05613331496715546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1147, 29.2801, 28.7759],
        [28.1147, 29.9179, 29.4525],
        [28.1147, 28.1147, 28.1147],
        [28.1147, 28.1147, 28.1147]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.05609148368239403 
model_pd.l_d.mean(): 9.195218444801867e-05 
model_pd.lagr.mean(): 0.05618343502283096 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8540], device='cuda:0')), ('power', tensor([0.1600], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.05609148368239403
epoch£º319	 i:1 	 global-step:6381	 l-p:0.05773741006851196
epoch£º319	 i:2 	 global-step:6382	 l-p:0.05626077577471733
epoch£º319	 i:3 	 global-step:6383	 l-p:0.056420546025037766
epoch£º319	 i:4 	 global-step:6384	 l-p:0.05605325475335121
epoch£º319	 i:5 	 global-step:6385	 l-p:0.05661986395716667
epoch£º319	 i:6 	 global-step:6386	 l-p:0.05607299506664276
epoch£º319	 i:7 	 global-step:6387	 l-p:0.056575626134872437
epoch£º319	 i:8 	 global-step:6388	 l-p:0.0570126511156559
epoch£º319	 i:9 	 global-step:6389	 l-p:0.05604638159275055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0594, 29.8050, 29.3311],
        [28.0594, 30.7132, 30.5254],
        [28.0594, 30.4934, 30.2138],
        [28.0594, 28.1167, 28.0644]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.0564117506146431 
model_pd.l_d.mean(): 9.502909233560786e-05 
model_pd.lagr.mean(): 0.05650677904486656 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8471], device='cuda:0')), ('power', tensor([0.1376], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.0564117506146431
epoch£º320	 i:1 	 global-step:6401	 l-p:0.0566246397793293
epoch£º320	 i:2 	 global-step:6402	 l-p:0.05661046504974365
epoch£º320	 i:3 	 global-step:6403	 l-p:0.05624925345182419
epoch£º320	 i:4 	 global-step:6404	 l-p:0.05613160878419876
epoch£º320	 i:5 	 global-step:6405	 l-p:0.056113146245479584
epoch£º320	 i:6 	 global-step:6406	 l-p:0.05604743957519531
epoch£º320	 i:7 	 global-step:6407	 l-p:0.056032389402389526
epoch£º320	 i:8 	 global-step:6408	 l-p:0.05875388905405998
epoch£º320	 i:9 	 global-step:6409	 l-p:0.05615140497684479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9697, 28.2958, 28.0523],
        [27.9697, 33.4975, 35.3977],
        [27.9697, 28.0509, 27.9785],
        [27.9697, 27.9736, 27.9698]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.05669976398348808 
model_pd.l_d.mean(): 0.00013557141937781125 
model_pd.lagr.mean(): 0.05683533474802971 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7902], device='cuda:0')), ('power', tensor([0.1758], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.05669976398348808
epoch£º321	 i:1 	 global-step:6421	 l-p:0.056178558617830276
epoch£º321	 i:2 	 global-step:6422	 l-p:0.056143686175346375
epoch£º321	 i:3 	 global-step:6423	 l-p:0.05611030012369156
epoch£º321	 i:4 	 global-step:6424	 l-p:0.05637003853917122
epoch£º321	 i:5 	 global-step:6425	 l-p:0.056334350258111954
epoch£º321	 i:6 	 global-step:6426	 l-p:0.05616585537791252
epoch£º321	 i:7 	 global-step:6427	 l-p:0.056580208241939545
epoch£º321	 i:8 	 global-step:6428	 l-p:0.05776933580636978
epoch£º321	 i:9 	 global-step:6429	 l-p:0.05709439143538475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8591, 37.4392, 43.9398],
        [27.8591, 27.8608, 27.8592],
        [27.8591, 27.8659, 27.8593],
        [27.8591, 27.8591, 27.8591]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.05614301189780235 
model_pd.l_d.mean(): -0.00011983061267528683 
model_pd.lagr.mean(): 0.05602318048477173 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8506], device='cuda:0')), ('power', tensor([-0.1492], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.05614301189780235
epoch£º322	 i:1 	 global-step:6441	 l-p:0.0560840368270874
epoch£º322	 i:2 	 global-step:6442	 l-p:0.05657755583524704
epoch£º322	 i:3 	 global-step:6443	 l-p:0.05614908039569855
epoch£º322	 i:4 	 global-step:6444	 l-p:0.056479115039110184
epoch£º322	 i:5 	 global-step:6445	 l-p:0.056374773383140564
epoch£º322	 i:6 	 global-step:6446	 l-p:0.05779970437288284
epoch£º322	 i:7 	 global-step:6447	 l-p:0.05712496116757393
epoch£º322	 i:8 	 global-step:6448	 l-p:0.056430883705616
epoch£º322	 i:9 	 global-step:6449	 l-p:0.05664502829313278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7409, 27.7411, 27.7409],
        [27.7409, 27.8214, 27.7495],
        [27.7409, 31.5369, 32.0321],
        [27.7409, 28.1236, 27.8488]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.056229062378406525 
model_pd.l_d.mean(): -0.0001462204963900149 
model_pd.lagr.mean(): 0.056082841008901596 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8166], device='cuda:0')), ('power', tensor([-0.1879], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.056229062378406525
epoch£º323	 i:1 	 global-step:6461	 l-p:0.05808119475841522
epoch£º323	 i:2 	 global-step:6462	 l-p:0.05725758522748947
epoch£º323	 i:3 	 global-step:6463	 l-p:0.05679483339190483
epoch£º323	 i:4 	 global-step:6464	 l-p:0.05615893751382828
epoch£º323	 i:5 	 global-step:6465	 l-p:0.05624151974916458
epoch£º323	 i:6 	 global-step:6466	 l-p:0.056154489517211914
epoch£º323	 i:7 	 global-step:6467	 l-p:0.056679680943489075
epoch£º323	 i:8 	 global-step:6468	 l-p:0.056295208632946014
epoch£º323	 i:9 	 global-step:6469	 l-p:0.05625636875629425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6266, 27.6268, 27.6266],
        [27.6266, 27.6268, 27.6266],
        [27.6266, 27.8030, 27.6573],
        [27.6266, 28.8113, 28.3135]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.05643899738788605 
model_pd.l_d.mean(): -7.314985123230144e-05 
model_pd.lagr.mean(): 0.05636584758758545 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7483], device='cuda:0')), ('power', tensor([-0.1050], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.05643899738788605
epoch£º324	 i:1 	 global-step:6481	 l-p:0.05792056396603584
epoch£º324	 i:2 	 global-step:6482	 l-p:0.05613768473267555
epoch£º324	 i:3 	 global-step:6483	 l-p:0.056390564888715744
epoch£º324	 i:4 	 global-step:6484	 l-p:0.056287731975317
epoch£º324	 i:5 	 global-step:6485	 l-p:0.05616702884435654
epoch£º324	 i:6 	 global-step:6486	 l-p:0.056582603603601456
epoch£º324	 i:7 	 global-step:6487	 l-p:0.0568641796708107
epoch£º324	 i:8 	 global-step:6488	 l-p:0.05724309757351875
epoch£º324	 i:9 	 global-step:6489	 l-p:0.05649399757385254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5232, 29.4559, 29.0376],
        [27.5232, 27.6998, 27.5540],
        [27.5232, 27.5256, 27.5232],
        [27.5232, 30.6704, 30.7670]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.05731779336929321 
model_pd.l_d.mean(): -0.00011335942690493539 
model_pd.lagr.mean(): 0.05720443278551102 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7737], device='cuda:0')), ('power', tensor([-0.2025], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.05731779336929321
epoch£º325	 i:1 	 global-step:6501	 l-p:0.05799447372555733
epoch£º325	 i:2 	 global-step:6502	 l-p:0.05669643357396126
epoch£º325	 i:3 	 global-step:6503	 l-p:0.05633765459060669
epoch£º325	 i:4 	 global-step:6504	 l-p:0.05646831542253494
epoch£º325	 i:5 	 global-step:6505	 l-p:0.05634043738245964
epoch£º325	 i:6 	 global-step:6506	 l-p:0.056341834366321564
epoch£º325	 i:7 	 global-step:6507	 l-p:0.05623255670070648
epoch£º325	 i:8 	 global-step:6508	 l-p:0.056804560124874115
epoch£º325	 i:9 	 global-step:6509	 l-p:0.0562758632004261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4529, 30.4261, 30.4299],
        [27.4529, 27.4683, 27.4535],
        [27.4529, 36.6300, 42.6998],
        [27.4529, 30.2274, 30.1287]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.05635387450456619 
model_pd.l_d.mean(): -0.00017654216208029538 
model_pd.lagr.mean(): 0.05617733299732208 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8016], device='cuda:0')), ('power', tensor([-0.4647], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.05635387450456619
epoch£º326	 i:1 	 global-step:6521	 l-p:0.05624980479478836
epoch£º326	 i:2 	 global-step:6522	 l-p:0.05678705871105194
epoch£º326	 i:3 	 global-step:6523	 l-p:0.056700199842453
epoch£º326	 i:4 	 global-step:6524	 l-p:0.05641902983188629
epoch£º326	 i:5 	 global-step:6525	 l-p:0.056350577622652054
epoch£º326	 i:6 	 global-step:6526	 l-p:0.05622627213597298
epoch£º326	 i:7 	 global-step:6527	 l-p:0.059127483516931534
epoch£º326	 i:8 	 global-step:6528	 l-p:0.056589994579553604
epoch£º326	 i:9 	 global-step:6529	 l-p:0.056213632225990295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4199, 36.7092, 42.9303],
        [27.4199, 27.4223, 27.4199],
        [27.4199, 28.2471, 27.8041],
        [27.4199, 27.4366, 27.4206]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.05645539239048958 
model_pd.l_d.mean(): -8.124704618239775e-05 
model_pd.lagr.mean(): 0.056374143809080124 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7920], device='cuda:0')), ('power', tensor([-0.4717], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.05645539239048958
epoch£º327	 i:1 	 global-step:6541	 l-p:0.056495364755392075
epoch£º327	 i:2 	 global-step:6542	 l-p:0.0563199520111084
epoch£º327	 i:3 	 global-step:6543	 l-p:0.05679977312684059
epoch£º327	 i:4 	 global-step:6544	 l-p:0.05798402801156044
epoch£º327	 i:5 	 global-step:6545	 l-p:0.056197796016931534
epoch£º327	 i:6 	 global-step:6546	 l-p:0.057772453874349594
epoch£º327	 i:7 	 global-step:6547	 l-p:0.05637426674365997
epoch£º327	 i:8 	 global-step:6548	 l-p:0.05631759390234947
epoch£º327	 i:9 	 global-step:6549	 l-p:0.05632292479276657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4436, 27.5578, 27.4589],
        [27.4436, 27.4857, 27.4467],
        [27.4436, 30.2886, 30.2257],
        [27.4436, 34.3848, 37.7961]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.05640019476413727 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05640019476413727 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7795], device='cuda:0')), ('power', tensor([-0.4153], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.05640019476413727
epoch£º328	 i:1 	 global-step:6561	 l-p:0.05624670907855034
epoch£º328	 i:2 	 global-step:6562	 l-p:0.05639856681227684
epoch£º328	 i:3 	 global-step:6563	 l-p:0.05627139285206795
epoch£º328	 i:4 	 global-step:6564	 l-p:0.056176695972681046
epoch£º328	 i:5 	 global-step:6565	 l-p:0.05933763459324837
epoch£º328	 i:6 	 global-step:6566	 l-p:0.05620556324720383
epoch£º328	 i:7 	 global-step:6567	 l-p:0.05631665140390396
epoch£º328	 i:8 	 global-step:6568	 l-p:0.05619269609451294
epoch£º328	 i:9 	 global-step:6569	 l-p:0.057340897619724274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5079, 27.5080, 27.5079],
        [27.5079, 32.2741, 33.5454],
        [27.5079, 27.5219, 27.5085],
        [27.5079, 28.1689, 27.7735]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.05731455236673355 
model_pd.l_d.mean(): -7.658644562980044e-08 
model_pd.lagr.mean(): 0.05731447413563728 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7353], device='cuda:0')), ('power', tensor([-0.1089], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.05731455236673355
epoch£º329	 i:1 	 global-step:6581	 l-p:0.05729438737034798
epoch£º329	 i:2 	 global-step:6582	 l-p:0.05620577558875084
epoch£º329	 i:3 	 global-step:6583	 l-p:0.05649295076727867
epoch£º329	 i:4 	 global-step:6584	 l-p:0.05639088526368141
epoch£º329	 i:5 	 global-step:6585	 l-p:0.05626585707068443
epoch£º329	 i:6 	 global-step:6586	 l-p:0.056173648685216904
epoch£º329	 i:7 	 global-step:6587	 l-p:0.05799908936023712
epoch£º329	 i:8 	 global-step:6588	 l-p:0.05613967776298523
epoch£º329	 i:9 	 global-step:6589	 l-p:0.05636560171842575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5919, 27.6038, 27.5923],
        [27.5919, 27.6097, 27.5927],
        [27.5919, 27.6019, 27.5922],
        [27.5919, 27.5919, 27.5919]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.05625096336007118 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05625096336007118 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8304], device='cuda:0')), ('power', tensor([-0.3889], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.05625096336007118
epoch£º330	 i:1 	 global-step:6601	 l-p:0.05708497017621994
epoch£º330	 i:2 	 global-step:6602	 l-p:0.05616065859794617
epoch£º330	 i:3 	 global-step:6603	 l-p:0.05639471113681793
epoch£º330	 i:4 	 global-step:6604	 l-p:0.05617324635386467
epoch£º330	 i:5 	 global-step:6605	 l-p:0.056326769292354584
epoch£º330	 i:6 	 global-step:6606	 l-p:0.056418854743242264
epoch£º330	 i:7 	 global-step:6607	 l-p:0.05629901587963104
epoch£º330	 i:8 	 global-step:6608	 l-p:0.05628447234630585
epoch£º330	 i:9 	 global-step:6609	 l-p:0.058952078223228455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6776, 30.2933, 30.1080],
        [27.6776, 36.6337, 42.3758],
        [27.6776, 27.7319, 27.6822],
        [27.6776, 35.8345, 40.5969]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.056289032101631165 
model_pd.l_d.mean(): -3.5780722100753337e-06 
model_pd.lagr.mean(): 0.05628545582294464 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.0053e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7987], device='cuda:0')), ('power', tensor([-0.2304], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.056289032101631165
epoch£º331	 i:1 	 global-step:6621	 l-p:0.05612131580710411
epoch£º331	 i:2 	 global-step:6622	 l-p:0.056933801621198654
epoch£º331	 i:3 	 global-step:6623	 l-p:0.057318080216646194
epoch£º331	 i:4 	 global-step:6624	 l-p:0.05682862177491188
epoch£º331	 i:5 	 global-step:6625	 l-p:0.056142158806324005
epoch£º331	 i:6 	 global-step:6626	 l-p:0.056079618632793427
epoch£º331	 i:7 	 global-step:6627	 l-p:0.05623558536171913
epoch£º331	 i:8 	 global-step:6628	 l-p:0.056290462613105774
epoch£º331	 i:9 	 global-step:6629	 l-p:0.05782582610845566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7684, 29.5202, 29.0558],
        [27.7684, 31.7899, 32.4441],
        [27.7684, 30.4036, 30.2223],
        [27.7684, 27.8151, 27.7720]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.05623000115156174 
model_pd.l_d.mean(): -3.0588708455070446e-08 
model_pd.lagr.mean(): 0.05622997134923935 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8104], device='cuda:0')), ('power', tensor([-0.1205], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.05623000115156174
epoch£º332	 i:1 	 global-step:6641	 l-p:0.05621674284338951
epoch£º332	 i:2 	 global-step:6642	 l-p:0.057685259729623795
epoch£º332	 i:3 	 global-step:6643	 l-p:0.0564374104142189
epoch£º332	 i:4 	 global-step:6644	 l-p:0.056101515889167786
epoch£º332	 i:5 	 global-step:6645	 l-p:0.056123074144124985
epoch£º332	 i:6 	 global-step:6646	 l-p:0.056136373430490494
epoch£º332	 i:7 	 global-step:6647	 l-p:0.05778513848781586
epoch£º332	 i:8 	 global-step:6648	 l-p:0.05682373046875
epoch£º332	 i:9 	 global-step:6649	 l-p:0.05623701959848404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8611, 27.8611, 27.8611],
        [27.8611, 32.6529, 33.9087],
        [27.8611, 27.8611, 27.8611],
        [27.8611, 27.8613, 27.8611]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.056223828345537186 
model_pd.l_d.mean(): -7.635465522071172e-07 
model_pd.lagr.mean(): 0.056223064661026 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.6417e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8337], device='cuda:0')), ('power', tensor([-0.0739], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.056223828345537186
epoch£º333	 i:1 	 global-step:6661	 l-p:0.05786875635385513
epoch£º333	 i:2 	 global-step:6662	 l-p:0.056157588958740234
epoch£º333	 i:3 	 global-step:6663	 l-p:0.05612901598215103
epoch£º333	 i:4 	 global-step:6664	 l-p:0.05621938034892082
epoch£º333	 i:5 	 global-step:6665	 l-p:0.05617595463991165
epoch£º333	 i:6 	 global-step:6666	 l-p:0.05661702901124954
epoch£º333	 i:7 	 global-step:6667	 l-p:0.056197166442871094
epoch£º333	 i:8 	 global-step:6668	 l-p:0.05676363408565521
epoch£º333	 i:9 	 global-step:6669	 l-p:0.057135436683893204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9528, 27.9589, 27.9529],
        [27.9528, 27.9956, 27.9559],
        [27.9528, 30.1984, 29.8552],
        [27.9528, 29.3055, 28.7981]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.05605439841747284 
model_pd.l_d.mean(): -3.785135731959599e-06 
model_pd.lagr.mean(): 0.0560506135225296 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.7445e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8809], device='cuda:0')), ('power', tensor([-0.1142], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.05605439841747284
epoch£º334	 i:1 	 global-step:6681	 l-p:0.056700076907873154
epoch£º334	 i:2 	 global-step:6682	 l-p:0.05622148513793945
epoch£º334	 i:3 	 global-step:6683	 l-p:0.056219432502985
epoch£º334	 i:4 	 global-step:6684	 l-p:0.056140925735235214
epoch£º334	 i:5 	 global-step:6685	 l-p:0.056526463478803635
epoch£º334	 i:6 	 global-step:6686	 l-p:0.05618195980787277
epoch£º334	 i:7 	 global-step:6687	 l-p:0.0564756765961647
epoch£º334	 i:8 	 global-step:6688	 l-p:0.05845401808619499
epoch£º334	 i:9 	 global-step:6689	 l-p:0.05622443929314613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0362, 28.8687, 28.4188],
        [28.0362, 36.9982, 42.6738],
        [28.0362, 28.0362, 28.0362],
        [28.0362, 37.5708, 43.9735]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.05667773261666298 
model_pd.l_d.mean(): 3.0056799005251378e-05 
model_pd.lagr.mean(): 0.05670778825879097 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7534], device='cuda:0')), ('power', tensor([0.3036], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.05667773261666298
epoch£º335	 i:1 	 global-step:6701	 l-p:0.05620966851711273
epoch£º335	 i:2 	 global-step:6702	 l-p:0.056185655295848846
epoch£º335	 i:3 	 global-step:6703	 l-p:0.05615749582648277
epoch£º335	 i:4 	 global-step:6704	 l-p:0.05623277649283409
epoch£º335	 i:5 	 global-step:6705	 l-p:0.05661803483963013
epoch£º335	 i:6 	 global-step:6706	 l-p:0.05614418908953667
epoch£º335	 i:7 	 global-step:6707	 l-p:0.057824764400720596
epoch£º335	 i:8 	 global-step:6708	 l-p:0.056012172251939774
epoch£º335	 i:9 	 global-step:6709	 l-p:0.05689693242311478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[28.1060, 28.8961, 28.4566],
        [28.1060, 29.5586, 29.0512],
        [28.1060, 33.0082, 34.3321],
        [28.1060, 29.0292, 28.5583]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.057626042515039444 
model_pd.l_d.mean(): 6.480311276391149e-05 
model_pd.lagr.mean(): 0.05769084393978119 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8170], device='cuda:0')), ('power', tensor([0.3186], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.057626042515039444
epoch£º336	 i:1 	 global-step:6721	 l-p:0.05636344477534294
epoch£º336	 i:2 	 global-step:6722	 l-p:0.05658695474267006
epoch£º336	 i:3 	 global-step:6723	 l-p:0.056334707885980606
epoch£º336	 i:4 	 global-step:6724	 l-p:0.056573789566755295
epoch£º336	 i:5 	 global-step:6725	 l-p:0.05608747899532318
epoch£º336	 i:6 	 global-step:6726	 l-p:0.05693991854786873
epoch£º336	 i:7 	 global-step:6727	 l-p:0.05604466423392296
epoch£º336	 i:8 	 global-step:6728	 l-p:0.05603976920247078
epoch£º336	 i:9 	 global-step:6729	 l-p:0.05618848651647568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1522, 38.5786, 46.1338],
        [28.1522, 28.7616, 28.3810],
        [28.1522, 31.4379, 31.5737],
        [28.1522, 35.1412, 38.4932]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.0581170916557312 
model_pd.l_d.mean(): 0.00019971818255726248 
model_pd.lagr.mean(): 0.05831680819392204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7672], device='cuda:0')), ('power', tensor([0.5911], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.0581170916557312
epoch£º337	 i:1 	 global-step:6741	 l-p:0.05626646801829338
epoch£º337	 i:2 	 global-step:6742	 l-p:0.056271299719810486
epoch£º337	 i:3 	 global-step:6743	 l-p:0.056079089641571045
epoch£º337	 i:4 	 global-step:6744	 l-p:0.056480422616004944
epoch£º337	 i:5 	 global-step:6745	 l-p:0.05616169795393944
epoch£º337	 i:6 	 global-step:6746	 l-p:0.05695569887757301
epoch£º337	 i:7 	 global-step:6747	 l-p:0.05627036094665527
epoch£º337	 i:8 	 global-step:6748	 l-p:0.05599421262741089
epoch£º337	 i:9 	 global-step:6749	 l-p:0.05609557405114174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1587, 29.2008, 28.7092],
        [28.1587, 28.5872, 28.2872],
        [28.1587, 28.2345, 28.1665],
        [28.1587, 28.1587, 28.1587]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.056170396506786346 
model_pd.l_d.mean(): 0.0001154994242824614 
model_pd.lagr.mean(): 0.05628589540719986 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8255], device='cuda:0')), ('power', tensor([0.2374], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.056170396506786346
epoch£º338	 i:1 	 global-step:6761	 l-p:0.056152477860450745
epoch£º338	 i:2 	 global-step:6762	 l-p:0.05606873333454132
epoch£º338	 i:3 	 global-step:6763	 l-p:0.05630316957831383
epoch£º338	 i:4 	 global-step:6764	 l-p:0.05622591823339462
epoch£º338	 i:5 	 global-step:6765	 l-p:0.0569385290145874
epoch£º338	 i:6 	 global-step:6766	 l-p:0.057581279426813126
epoch£º338	 i:7 	 global-step:6767	 l-p:0.056287191808223724
epoch£º338	 i:8 	 global-step:6768	 l-p:0.05607783794403076
epoch£º338	 i:9 	 global-step:6769	 l-p:0.05693422257900238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1208, 35.9073, 40.1550],
        [28.1208, 28.8230, 28.4097],
        [28.1208, 28.5649, 28.2572],
        [28.1208, 28.1208, 28.1208]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.0561235174536705 
model_pd.l_d.mean(): 0.00011155230458825827 
model_pd.lagr.mean(): 0.056235071271657944 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8384], device='cuda:0')), ('power', tensor([0.1778], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.0561235174536705
epoch£º339	 i:1 	 global-step:6781	 l-p:0.05610188841819763
epoch£º339	 i:2 	 global-step:6782	 l-p:0.056285932660102844
epoch£º339	 i:3 	 global-step:6783	 l-p:0.056792423129081726
epoch£º339	 i:4 	 global-step:6784	 l-p:0.05603957548737526
epoch£º339	 i:5 	 global-step:6785	 l-p:0.056520625948905945
epoch£º339	 i:6 	 global-step:6786	 l-p:0.05602629482746124
epoch£º339	 i:7 	 global-step:6787	 l-p:0.057805150747299194
epoch£º339	 i:8 	 global-step:6788	 l-p:0.05696464702486992
epoch£º339	 i:9 	 global-step:6789	 l-p:0.056252505630254745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0456, 34.2983, 36.8949],
        [28.0456, 28.0795, 28.0478],
        [28.0456, 28.0477, 28.0457],
        [28.0456, 29.0260, 28.5453]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.05632375553250313 
model_pd.l_d.mean(): 0.0001499343488831073 
model_pd.lagr.mean(): 0.05647369101643562 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7987], device='cuda:0')), ('power', tensor([0.2021], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.05632375553250313
epoch£º340	 i:1 	 global-step:6801	 l-p:0.056067876517772675
epoch£º340	 i:2 	 global-step:6802	 l-p:0.05707472562789917
epoch£º340	 i:3 	 global-step:6803	 l-p:0.05675358697772026
epoch£º340	 i:4 	 global-step:6804	 l-p:0.056120023131370544
epoch£º340	 i:5 	 global-step:6805	 l-p:0.056575268507003784
epoch£º340	 i:6 	 global-step:6806	 l-p:0.05620502308011055
epoch£º340	 i:7 	 global-step:6807	 l-p:0.057770270854234695
epoch£º340	 i:8 	 global-step:6808	 l-p:0.05605868250131607
epoch£º340	 i:9 	 global-step:6809	 l-p:0.056237298995256424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9377, 27.9489, 27.9381],
        [27.9377, 29.3473, 28.8416],
        [27.9377, 34.8710, 38.1960],
        [27.9377, 35.0101, 38.4864]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.05625148490071297 
model_pd.l_d.mean(): 7.386905781459063e-05 
model_pd.lagr.mean(): 0.05632535368204117 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8053], device='cuda:0')), ('power', tensor([0.0910], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.05625148490071297
epoch£º341	 i:1 	 global-step:6821	 l-p:0.056330785155296326
epoch£º341	 i:2 	 global-step:6822	 l-p:0.05660111457109451
epoch£º341	 i:3 	 global-step:6823	 l-p:0.05615345761179924
epoch£º341	 i:4 	 global-step:6824	 l-p:0.05620955303311348
epoch£º341	 i:5 	 global-step:6825	 l-p:0.056267112493515015
epoch£º341	 i:6 	 global-step:6826	 l-p:0.05755426362156868
epoch£º341	 i:7 	 global-step:6827	 l-p:0.05626174807548523
epoch£º341	 i:8 	 global-step:6828	 l-p:0.056142449378967285
epoch£º341	 i:9 	 global-step:6829	 l-p:0.057815179228782654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8023, 27.8023, 27.8023],
        [27.8023, 35.4882, 39.6757],
        [27.8023, 36.9738, 42.9618],
        [27.8023, 35.4952, 39.6908]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.05613139644265175 
model_pd.l_d.mean(): -0.00016570434672757983 
model_pd.lagr.mean(): 0.055965691804885864 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8528], device='cuda:0')), ('power', tensor([-0.2014], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.05613139644265175
epoch£º342	 i:1 	 global-step:6841	 l-p:0.05620253458619118
epoch£º342	 i:2 	 global-step:6842	 l-p:0.05621600151062012
epoch£º342	 i:3 	 global-step:6843	 l-p:0.05713105946779251
epoch£º342	 i:4 	 global-step:6844	 l-p:0.057000573724508286
epoch£º342	 i:5 	 global-step:6845	 l-p:0.056373290717601776
epoch£º342	 i:6 	 global-step:6846	 l-p:0.05665985122323036
epoch£º342	 i:7 	 global-step:6847	 l-p:0.056161146610975266
epoch£º342	 i:8 	 global-step:6848	 l-p:0.05803641676902771
epoch£º342	 i:9 	 global-step:6849	 l-p:0.0561106875538826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6629, 35.3154, 39.4888],
        [27.6629, 27.6629, 27.6629],
        [27.6629, 36.7105, 42.5709],
        [27.6629, 28.0022, 27.7517]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.05615685507655144 
model_pd.l_d.mean(): -0.0002884056302718818 
model_pd.lagr.mean(): 0.05586845055222511 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8515], device='cuda:0')), ('power', tensor([-0.3770], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.05615685507655144
epoch£º343	 i:1 	 global-step:6861	 l-p:0.05629601329565048
epoch£º343	 i:2 	 global-step:6862	 l-p:0.056176383048295975
epoch£º343	 i:3 	 global-step:6863	 l-p:0.05743749067187309
epoch£º343	 i:4 	 global-step:6864	 l-p:0.05620761588215828
epoch£º343	 i:5 	 global-step:6865	 l-p:0.05843193829059601
epoch£º343	 i:6 	 global-step:6866	 l-p:0.056332778185606
epoch£º343	 i:7 	 global-step:6867	 l-p:0.056331899017095566
epoch£º343	 i:8 	 global-step:6868	 l-p:0.05626516789197922
epoch£º343	 i:9 	 global-step:6869	 l-p:0.05681715905666351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[27.5360, 36.5402, 42.3723],
        [27.5360, 35.2968, 39.6180],
        [27.5360, 28.8305, 28.3310],
        [27.5360, 28.7532, 28.2554]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.05618894100189209 
model_pd.l_d.mean(): -0.00032544523128308356 
model_pd.lagr.mean(): 0.05586349591612816 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8461], device='cuda:0')), ('power', tensor([-0.5078], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.05618894100189209
epoch£º344	 i:1 	 global-step:6881	 l-p:0.0564265251159668
epoch£º344	 i:2 	 global-step:6882	 l-p:0.05722072347998619
epoch£º344	 i:3 	 global-step:6883	 l-p:0.056458692997694016
epoch£º344	 i:4 	 global-step:6884	 l-p:0.058085452765226364
epoch£º344	 i:5 	 global-step:6885	 l-p:0.05711904913187027
epoch£º344	 i:6 	 global-step:6886	 l-p:0.05642791464924812
epoch£º344	 i:7 	 global-step:6887	 l-p:0.05635005235671997
epoch£º344	 i:8 	 global-step:6888	 l-p:0.056177232414484024
epoch£º344	 i:9 	 global-step:6889	 l-p:0.05636914074420929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4387, 27.6592, 27.4829],
        [27.4387, 36.4931, 42.4094],
        [27.4387, 27.7262, 27.5068],
        [27.4387, 30.5757, 30.6719]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.05797920003533363 
model_pd.l_d.mean(): -0.00015621274360455573 
model_pd.lagr.mean(): 0.05782298743724823 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7914], device='cuda:0')), ('power', tensor([-0.3392], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.05797920003533363
epoch£º345	 i:1 	 global-step:6901	 l-p:0.05716773867607117
epoch£º345	 i:2 	 global-step:6902	 l-p:0.056364756077528
epoch£º345	 i:3 	 global-step:6903	 l-p:0.056178979575634
epoch£º345	 i:4 	 global-step:6904	 l-p:0.05637092515826225
epoch£º345	 i:5 	 global-step:6905	 l-p:0.05699240416288376
epoch£º345	 i:6 	 global-step:6906	 l-p:0.05668560788035393
epoch£º345	 i:7 	 global-step:6907	 l-p:0.05627929046750069
epoch£º345	 i:8 	 global-step:6908	 l-p:0.056695956736803055
epoch£º345	 i:9 	 global-step:6909	 l-p:0.05635688826441765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3867, 27.5125, 27.4046],
        [27.3867, 27.3880, 27.3867],
        [27.3867, 27.7893, 27.5049],
        [27.3867, 31.0642, 31.5058]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.05646369978785515 
model_pd.l_d.mean(): -0.0001011818676488474 
model_pd.lagr.mean(): 0.056362517178058624 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7500], device='cuda:0')), ('power', tensor([-0.4170], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.05646369978785515
epoch£º346	 i:1 	 global-step:6921	 l-p:0.05721449851989746
epoch£º346	 i:2 	 global-step:6922	 l-p:0.056239910423755646
epoch£º346	 i:3 	 global-step:6923	 l-p:0.057124506682157516
epoch£º346	 i:4 	 global-step:6924	 l-p:0.05862865224480629
epoch£º346	 i:5 	 global-step:6925	 l-p:0.05629979819059372
epoch£º346	 i:6 	 global-step:6926	 l-p:0.05630049109458923
epoch£º346	 i:7 	 global-step:6927	 l-p:0.05624057352542877
epoch£º346	 i:8 	 global-step:6928	 l-p:0.05633733794093132
epoch£º346	 i:9 	 global-step:6929	 l-p:0.05633426457643509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3896, 27.3896, 27.3896],
        [27.3896, 30.5815, 30.7130],
        [27.3896, 36.7954, 43.1738],
        [27.3896, 27.4409, 27.3938]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.05646173655986786 
model_pd.l_d.mean(): -4.474444722291082e-06 
model_pd.lagr.mean(): 0.056457262486219406 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7609], device='cuda:0')), ('power', tensor([-0.4375], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.05646173655986786
epoch£º347	 i:1 	 global-step:6941	 l-p:0.05681803077459335
epoch£º347	 i:2 	 global-step:6942	 l-p:0.056380175054073334
epoch£º347	 i:3 	 global-step:6943	 l-p:0.057209718972444534
epoch£º347	 i:4 	 global-step:6944	 l-p:0.05641438439488411
epoch£º347	 i:5 	 global-step:6945	 l-p:0.05629591643810272
epoch£º347	 i:6 	 global-step:6946	 l-p:0.05651472136378288
epoch£º347	 i:7 	 global-step:6947	 l-p:0.05618865042924881
epoch£º347	 i:8 	 global-step:6948	 l-p:0.058167509734630585
epoch£º347	 i:9 	 global-step:6949	 l-p:0.05663058161735535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4457, 28.1793, 27.7612],
        [27.4457, 27.8241, 27.5523],
        [27.4457, 28.1064, 27.7114],
        [27.4457, 27.4457, 27.4457]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.056335315108299255 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056335315108299255 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8141], device='cuda:0')), ('power', tensor([-0.5241], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.056335315108299255
epoch£º348	 i:1 	 global-step:6961	 l-p:0.05628243833780289
epoch£º348	 i:2 	 global-step:6962	 l-p:0.056215763092041016
epoch£º348	 i:3 	 global-step:6963	 l-p:0.056346356868743896
epoch£º348	 i:4 	 global-step:6964	 l-p:0.05656423419713974
epoch£º348	 i:5 	 global-step:6965	 l-p:0.05719120055437088
epoch£º348	 i:6 	 global-step:6966	 l-p:0.05634140223264694
epoch£º348	 i:7 	 global-step:6967	 l-p:0.05801760032773018
epoch£º348	 i:8 	 global-step:6968	 l-p:0.056488171219825745
epoch£º348	 i:9 	 global-step:6969	 l-p:0.057054098695516586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5312, 27.5312, 27.5312],
        [27.5312, 27.5467, 27.5319],
        [27.5312, 27.5505, 27.5321],
        [27.5312, 27.9044, 27.6353]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.05808297544717789 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05808297544717789 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7378], device='cuda:0')), ('power', tensor([-0.0788], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.05808297544717789
epoch£º349	 i:1 	 global-step:6981	 l-p:0.05640285462141037
epoch£º349	 i:2 	 global-step:6982	 l-p:0.056168120354413986
epoch£º349	 i:3 	 global-step:6983	 l-p:0.05782007426023483
epoch£º349	 i:4 	 global-step:6984	 l-p:0.056456308811903
epoch£º349	 i:5 	 global-step:6985	 l-p:0.05622464045882225
epoch£º349	 i:6 	 global-step:6986	 l-p:0.05619293078780174
epoch£º349	 i:7 	 global-step:6987	 l-p:0.05628778412938118
epoch£º349	 i:8 	 global-step:6988	 l-p:0.05620604753494263
epoch£º349	 i:9 	 global-step:6989	 l-p:0.05670908838510513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6336, 27.6530, 27.6345],
        [27.6336, 34.9556, 38.7580],
        [27.6336, 33.7424, 36.2507],
        [27.6336, 35.0276, 38.9112]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.0562455840408802 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0562455840408802 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8226], device='cuda:0')), ('power', tensor([-0.2808], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.0562455840408802
epoch£º350	 i:1 	 global-step:7001	 l-p:0.05623345077037811
epoch£º350	 i:2 	 global-step:7002	 l-p:0.05632862076163292
epoch£º350	 i:3 	 global-step:7003	 l-p:0.05768965557217598
epoch£º350	 i:4 	 global-step:7004	 l-p:0.056159503757953644
epoch£º350	 i:5 	 global-step:7005	 l-p:0.05629437789320946
epoch£º350	 i:6 	 global-step:7006	 l-p:0.056454408913850784
epoch£º350	 i:7 	 global-step:7007	 l-p:0.0566147156059742
epoch£º350	 i:8 	 global-step:7008	 l-p:0.05625594034790993
epoch£º350	 i:9 	 global-step:7009	 l-p:0.0579204261302948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7299, 28.3606, 27.9745],
        [27.7299, 28.6358, 28.1723],
        [27.7299, 31.9711, 32.7942],
        [27.7299, 32.1782, 33.1641]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.056287024170160294 
model_pd.l_d.mean(): -3.76386367406667e-07 
model_pd.lagr.mean(): 0.05628664791584015 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7948], device='cuda:0')), ('power', tensor([-0.1356], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.056287024170160294
epoch£º351	 i:1 	 global-step:7021	 l-p:0.05703604221343994
epoch£º351	 i:2 	 global-step:7022	 l-p:0.05620412155985832
epoch£º351	 i:3 	 global-step:7023	 l-p:0.056409623473882675
epoch£º351	 i:4 	 global-step:7024	 l-p:0.056164782494306564
epoch£º351	 i:5 	 global-step:7025	 l-p:0.056183286011219025
epoch£º351	 i:6 	 global-step:7026	 l-p:0.056240104138851166
epoch£º351	 i:7 	 global-step:7027	 l-p:0.05709225684404373
epoch£º351	 i:8 	 global-step:7028	 l-p:0.05615350976586342
epoch£º351	 i:9 	 global-step:7029	 l-p:0.05810823291540146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8297, 28.3899, 28.0307],
        [27.8297, 31.2470, 31.4848],
        [27.8297, 28.9536, 28.4573],
        [27.8297, 32.7309, 34.0835]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.056070879101753235 
model_pd.l_d.mean(): -4.929941496811807e-06 
model_pd.lagr.mean(): 0.05606595054268837 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.2640e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8870], device='cuda:0')), ('power', tensor([-0.2576], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.056070879101753235
epoch£º352	 i:1 	 global-step:7041	 l-p:0.05709295719861984
epoch£º352	 i:2 	 global-step:7042	 l-p:0.056342124938964844
epoch£º352	 i:3 	 global-step:7043	 l-p:0.056328728795051575
epoch£º352	 i:4 	 global-step:7044	 l-p:0.05619727820158005
epoch£º352	 i:5 	 global-step:7045	 l-p:0.05775168165564537
epoch£º352	 i:6 	 global-step:7046	 l-p:0.05628730729222298
epoch£º352	 i:7 	 global-step:7047	 l-p:0.05620042607188225
epoch£º352	 i:8 	 global-step:7048	 l-p:0.05712218955159187
epoch£º352	 i:9 	 global-step:7049	 l-p:0.056176431477069855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9325, 28.0282, 27.9439],
        [27.9325, 27.9650, 27.9346],
        [27.9325, 27.9614, 27.9342],
        [27.9325, 27.9326, 27.9325]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.05617120489478111 
model_pd.l_d.mean(): -1.1763571592382505e-06 
model_pd.lagr.mean(): 0.0561700277030468 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6685e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8385], device='cuda:0')), ('power', tensor([-0.0409], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.05617120489478111
epoch£º353	 i:1 	 global-step:7061	 l-p:0.05767187848687172
epoch£º353	 i:2 	 global-step:7062	 l-p:0.056730449199676514
epoch£º353	 i:3 	 global-step:7063	 l-p:0.05693576857447624
epoch£º353	 i:4 	 global-step:7064	 l-p:0.056102484464645386
epoch£º353	 i:5 	 global-step:7065	 l-p:0.05612919107079506
epoch£º353	 i:6 	 global-step:7066	 l-p:0.05682441592216492
epoch£º353	 i:7 	 global-step:7067	 l-p:0.05609237402677536
epoch£º353	 i:8 	 global-step:7068	 l-p:0.056264810264110565
epoch£º353	 i:9 	 global-step:7069	 l-p:0.056340597569942474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0298, 28.4988, 28.1792],
        [28.0298, 33.5313, 35.3997],
        [28.0298, 37.9333, 44.8188],
        [28.0298, 28.1232, 28.0407]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.05610263720154762 
model_pd.l_d.mean(): -2.4667642719578e-06 
model_pd.lagr.mean(): 0.05610017105937004 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.6839e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8794], device='cuda:0')), ('power', tensor([-0.0280], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.05610263720154762
epoch£º354	 i:1 	 global-step:7081	 l-p:0.057127129286527634
epoch£º354	 i:2 	 global-step:7082	 l-p:0.05625413730740547
epoch£º354	 i:3 	 global-step:7083	 l-p:0.05611215531826019
epoch£º354	 i:4 	 global-step:7084	 l-p:0.05615177005529404
epoch£º354	 i:5 	 global-step:7085	 l-p:0.05853256955742836
epoch£º354	 i:6 	 global-step:7086	 l-p:0.056141145527362823
epoch£º354	 i:7 	 global-step:7087	 l-p:0.056254271417856216
epoch£º354	 i:8 	 global-step:7088	 l-p:0.05615026503801346
epoch£º354	 i:9 	 global-step:7089	 l-p:0.05615144595503807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1082, 38.1008, 45.0860],
        [28.1082, 28.3852, 28.1713],
        [28.1082, 28.1579, 28.1122],
        [28.1082, 28.1251, 28.1089]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.05625729262828827 
model_pd.l_d.mean(): 3.395187741261907e-05 
model_pd.lagr.mean(): 0.05629124492406845 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8184], device='cuda:0')), ('power', tensor([0.1775], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.05625729262828827
epoch£º355	 i:1 	 global-step:7101	 l-p:0.0567786879837513
epoch£º355	 i:2 	 global-step:7102	 l-p:0.05617726221680641
epoch£º355	 i:3 	 global-step:7103	 l-p:0.05693596601486206
epoch£º355	 i:4 	 global-step:7104	 l-p:0.057586297392845154
epoch£º355	 i:5 	 global-step:7105	 l-p:0.056071117520332336
epoch£º355	 i:6 	 global-step:7106	 l-p:0.056074827909469604
epoch£º355	 i:7 	 global-step:7107	 l-p:0.05649666488170624
epoch£º355	 i:8 	 global-step:7108	 l-p:0.05617685988545418
epoch£º355	 i:9 	 global-step:7109	 l-p:0.056215982884168625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1588, 29.0880, 28.6153],
        [28.1588, 35.3918, 39.0092],
        [28.1588, 28.1590, 28.1588],
        [28.1588, 29.1434, 28.6606]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.056578416377305984 
model_pd.l_d.mean(): 0.00010027555254055187 
model_pd.lagr.mean(): 0.05667869374155998 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8299], device='cuda:0')), ('power', tensor([0.3065], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.056578416377305984
epoch£º356	 i:1 	 global-step:7121	 l-p:0.05601932480931282
epoch£º356	 i:2 	 global-step:7122	 l-p:0.05622785538434982
epoch£º356	 i:3 	 global-step:7123	 l-p:0.057301606982946396
epoch£º356	 i:4 	 global-step:7124	 l-p:0.05621436983346939
epoch£º356	 i:5 	 global-step:7125	 l-p:0.056089580059051514
epoch£º356	 i:6 	 global-step:7126	 l-p:0.05611015856266022
epoch£º356	 i:7 	 global-step:7127	 l-p:0.05635868012905121
epoch£º356	 i:8 	 global-step:7128	 l-p:0.0577392652630806
epoch£º356	 i:9 	 global-step:7129	 l-p:0.0560288280248642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1681, 37.8536, 44.4229],
        [28.1681, 28.1940, 28.1695],
        [28.1681, 28.3054, 28.1883],
        [28.1681, 29.6335, 29.1254]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.056216176599264145 
model_pd.l_d.mean(): 0.00015869890921749175 
model_pd.lagr.mean(): 0.05637487396597862 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7877], device='cuda:0')), ('power', tensor([0.3312], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.056216176599264145
epoch£º357	 i:1 	 global-step:7141	 l-p:0.056020624935626984
epoch£º357	 i:2 	 global-step:7142	 l-p:0.0561363622546196
epoch£º357	 i:3 	 global-step:7143	 l-p:0.05642055347561836
epoch£º357	 i:4 	 global-step:7144	 l-p:0.05620482191443443
epoch£º357	 i:5 	 global-step:7145	 l-p:0.05655747652053833
epoch£º357	 i:6 	 global-step:7146	 l-p:0.057551898062229156
epoch£º357	 i:7 	 global-step:7147	 l-p:0.05645124241709709
epoch£º357	 i:8 	 global-step:7148	 l-p:0.05608879402279854
epoch£º357	 i:9 	 global-step:7149	 l-p:0.05705748498439789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1324, 28.1438, 28.1328],
        [28.1324, 28.1325, 28.1324],
        [28.1324, 30.3917, 30.0457],
        [28.1324, 36.5241, 41.4819]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.05608572065830231 
model_pd.l_d.mean(): 8.074943616520613e-05 
model_pd.lagr.mean(): 0.05616647005081177 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8562], device='cuda:0')), ('power', tensor([0.1290], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.05608572065830231
epoch£º358	 i:1 	 global-step:7161	 l-p:0.05664193630218506
epoch£º358	 i:2 	 global-step:7162	 l-p:0.0575534962117672
epoch£º358	 i:3 	 global-step:7163	 l-p:0.056275445967912674
epoch£º358	 i:4 	 global-step:7164	 l-p:0.05608959496021271
epoch£º358	 i:5 	 global-step:7165	 l-p:0.057583998888731
epoch£º358	 i:6 	 global-step:7166	 l-p:0.05606016144156456
epoch£º358	 i:7 	 global-step:7167	 l-p:0.05615159496665001
epoch£º358	 i:8 	 global-step:7168	 l-p:0.056298527866601944
epoch£º358	 i:9 	 global-step:7169	 l-p:0.05612026900053024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0547, 28.4374, 28.1617],
        [28.0547, 28.0546, 28.0547],
        [28.0547, 29.9012, 29.4461],
        [28.0547, 28.0550, 28.0547]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.05621057376265526 
model_pd.l_d.mean(): 9.621703065931797e-05 
model_pd.lagr.mean(): 0.05630679056048393 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8270], device='cuda:0')), ('power', tensor([0.1290], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.05621057376265526
epoch£º359	 i:1 	 global-step:7181	 l-p:0.05610691010951996
epoch£º359	 i:2 	 global-step:7182	 l-p:0.05707496404647827
epoch£º359	 i:3 	 global-step:7183	 l-p:0.05760319530963898
epoch£º359	 i:4 	 global-step:7184	 l-p:0.05707462504506111
epoch£º359	 i:5 	 global-step:7185	 l-p:0.05615707114338875
epoch£º359	 i:6 	 global-step:7186	 l-p:0.056126467883586884
epoch£º359	 i:7 	 global-step:7187	 l-p:0.05623352527618408
epoch£º359	 i:8 	 global-step:7188	 l-p:0.056404780596494675
epoch£º359	 i:9 	 global-step:7189	 l-p:0.056170910596847534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9337, 29.3431, 28.8375],
        [27.9337, 29.3915, 28.8882],
        [27.9337, 28.9255, 28.4442],
        [27.9337, 27.9340, 27.9337]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.05627264082431793 
model_pd.l_d.mean(): 6.883159949211404e-05 
model_pd.lagr.mean(): 0.05634147301316261 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8036], device='cuda:0')), ('power', tensor([0.0842], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.05627264082431793
epoch£º360	 i:1 	 global-step:7201	 l-p:0.05624120682477951
epoch£º360	 i:2 	 global-step:7202	 l-p:0.056622665375471115
epoch£º360	 i:3 	 global-step:7203	 l-p:0.056119952350854874
epoch£º360	 i:4 	 global-step:7204	 l-p:0.057127103209495544
epoch£º360	 i:5 	 global-step:7205	 l-p:0.05629279837012291
epoch£º360	 i:6 	 global-step:7206	 l-p:0.05613701790571213
epoch£º360	 i:7 	 global-step:7207	 l-p:0.05830705910921097
epoch£º360	 i:8 	 global-step:7208	 l-p:0.05634034425020218
epoch£º360	 i:9 	 global-step:7209	 l-p:0.056151483207941055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7830, 27.7830, 27.7830],
        [27.7830, 27.7832, 27.7830],
        [27.7830, 27.7849, 27.7830],
        [27.7830, 27.8030, 27.7839]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.05615715682506561 
model_pd.l_d.mean(): -0.0001546690909890458 
model_pd.lagr.mean(): 0.05600248649716377 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8355], device='cuda:0')), ('power', tensor([-0.1880], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.05615715682506561
epoch£º361	 i:1 	 global-step:7221	 l-p:0.056209906935691833
epoch£º361	 i:2 	 global-step:7222	 l-p:0.056420858949422836
epoch£º361	 i:3 	 global-step:7223	 l-p:0.056164178997278214
epoch£º361	 i:4 	 global-step:7224	 l-p:0.05873391032218933
epoch£º361	 i:5 	 global-step:7225	 l-p:0.056198686361312866
epoch£º361	 i:6 	 global-step:7226	 l-p:0.05656050145626068
epoch£º361	 i:7 	 global-step:7227	 l-p:0.05620476230978966
epoch£º361	 i:8 	 global-step:7228	 l-p:0.05714646354317665
epoch£º361	 i:9 	 global-step:7229	 l-p:0.05630379915237427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6280, 29.1959, 28.7089],
        [27.6280, 27.6281, 27.6280],
        [27.6280, 27.6281, 27.6280],
        [27.6280, 34.5262, 37.8615]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.05627318099141121 
model_pd.l_d.mean(): -0.0002094403316732496 
model_pd.lagr.mean(): 0.05606374144554138 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8181], device='cuda:0')), ('power', tensor([-0.2785], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.05627318099141121
epoch£º362	 i:1 	 global-step:7241	 l-p:0.056182559579610825
epoch£º362	 i:2 	 global-step:7242	 l-p:0.0563151091337204
epoch£º362	 i:3 	 global-step:7243	 l-p:0.057890862226486206
epoch£º362	 i:4 	 global-step:7244	 l-p:0.05648213252425194
epoch£º362	 i:5 	 global-step:7245	 l-p:0.05624110996723175
epoch£º362	 i:6 	 global-step:7246	 l-p:0.05780213326215744
epoch£º362	 i:7 	 global-step:7247	 l-p:0.05656794458627701
epoch£º362	 i:8 	 global-step:7248	 l-p:0.056293971836566925
epoch£º362	 i:9 	 global-step:7249	 l-p:0.05653330683708191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4899, 28.3193, 27.8752],
        [27.4899, 27.4977, 27.4901],
        [27.4899, 27.5091, 27.4908],
        [27.4899, 29.4229, 29.0057]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.05621190741658211 
model_pd.l_d.mean(): -0.00032852901495061815 
model_pd.lagr.mean(): 0.05588337779045105 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8382], device='cuda:0')), ('power', tensor([-0.5402], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.05621190741658211
epoch£º363	 i:1 	 global-step:7261	 l-p:0.05823152884840965
epoch£º363	 i:2 	 global-step:7262	 l-p:0.05644969642162323
epoch£º363	 i:3 	 global-step:7263	 l-p:0.05644521489739418
epoch£º363	 i:4 	 global-step:7264	 l-p:0.05674624815583229
epoch£º363	 i:5 	 global-step:7265	 l-p:0.056428708136081696
epoch£º363	 i:6 	 global-step:7266	 l-p:0.05622974783182144
epoch£º363	 i:7 	 global-step:7267	 l-p:0.057237278670072556
epoch£º363	 i:8 	 global-step:7268	 l-p:0.056194718927145004
epoch£º363	 i:9 	 global-step:7269	 l-p:0.05680021271109581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3902, 37.5220, 44.8626],
        [27.3902, 28.2189, 27.7759],
        [27.3902, 27.3916, 27.3902],
        [27.3902, 29.5734, 29.2325]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.058033693581819534 
model_pd.l_d.mean(): -0.00012686001718975604 
model_pd.lagr.mean(): 0.05790683254599571 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7692], device='cuda:0')), ('power', tensor([-0.3135], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.058033693581819534
epoch£º364	 i:1 	 global-step:7281	 l-p:0.05719330534338951
epoch£º364	 i:2 	 global-step:7282	 l-p:0.056371573358774185
epoch£º364	 i:3 	 global-step:7283	 l-p:0.056924667209386826
epoch£º364	 i:4 	 global-step:7284	 l-p:0.05656195431947708
epoch£º364	 i:5 	 global-step:7285	 l-p:0.05622659623622894
epoch£º364	 i:6 	 global-step:7286	 l-p:0.05694415420293808
epoch£º364	 i:7 	 global-step:7287	 l-p:0.056253936141729355
epoch£º364	 i:8 	 global-step:7288	 l-p:0.05625160411000252
epoch£º364	 i:9 	 global-step:7289	 l-p:0.05646027624607086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3485, 27.3485, 27.3485],
        [27.3485, 29.2683, 28.8527],
        [27.3485, 28.7687, 28.2762],
        [27.3485, 27.4348, 27.3583]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.05624788999557495 
model_pd.l_d.mean(): -0.00011131093197036535 
model_pd.lagr.mean(): 0.05613657832145691 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8309], device='cuda:0')), ('power', tensor([-0.6752], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.05624788999557495
epoch£º365	 i:1 	 global-step:7301	 l-p:0.05634049326181412
epoch£º365	 i:2 	 global-step:7302	 l-p:0.056231964379549026
epoch£º365	 i:3 	 global-step:7303	 l-p:0.05845009535551071
epoch£º365	 i:4 	 global-step:7304	 l-p:0.05636380985379219
epoch£º365	 i:5 	 global-step:7305	 l-p:0.0562700480222702
epoch£º365	 i:6 	 global-step:7306	 l-p:0.05701318755745888
epoch£º365	 i:7 	 global-step:7307	 l-p:0.05673046037554741
epoch£º365	 i:8 	 global-step:7308	 l-p:0.05733795836567879
epoch£º365	 i:9 	 global-step:7309	 l-p:0.05630036070942879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3722, 27.4234, 27.3764],
        [27.3722, 36.4037, 42.3050],
        [27.3722, 37.0329, 43.7488],
        [27.3722, 27.4127, 27.3751]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.05747818201780319 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05747818201780319 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6868], device='cuda:0')), ('power', tensor([-0.1468], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.05747818201780319
epoch£º366	 i:1 	 global-step:7321	 l-p:0.056269124150276184
epoch£º366	 i:2 	 global-step:7322	 l-p:0.056223247200250626
epoch£º366	 i:3 	 global-step:7323	 l-p:0.058192308992147446
epoch£º366	 i:4 	 global-step:7324	 l-p:0.056351304054260254
epoch£º366	 i:5 	 global-step:7325	 l-p:0.05616138502955437
epoch£º366	 i:6 	 global-step:7326	 l-p:0.056329965591430664
epoch£º366	 i:7 	 global-step:7327	 l-p:0.05672784522175789
epoch£º366	 i:8 	 global-step:7328	 l-p:0.056204576045274734
epoch£º366	 i:9 	 global-step:7329	 l-p:0.05716998130083084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[27.4539, 36.6315, 42.7016],
        [27.4539, 29.2140, 28.7603],
        [27.4539, 28.6592, 28.1633],
        [27.4539, 31.0300, 31.3980]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.0572526715695858 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0572526715695858 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7592], device='cuda:0')), ('power', tensor([-0.2202], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.0572526715695858
epoch£º367	 i:1 	 global-step:7341	 l-p:0.056157998740673065
epoch£º367	 i:2 	 global-step:7342	 l-p:0.056360553950071335
epoch£º367	 i:3 	 global-step:7343	 l-p:0.0563197135925293
epoch£º367	 i:4 	 global-step:7344	 l-p:0.05630675330758095
epoch£º367	 i:5 	 global-step:7345	 l-p:0.05627908557653427
epoch£º367	 i:6 	 global-step:7346	 l-p:0.05819479003548622
epoch£º367	 i:7 	 global-step:7347	 l-p:0.05617902800440788
epoch£º367	 i:8 	 global-step:7348	 l-p:0.057428207248449326
epoch£º367	 i:9 	 global-step:7349	 l-p:0.05631560832262039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5535, 27.5535, 27.5535],
        [27.5535, 27.5535, 27.5535],
        [27.5535, 27.5867, 27.5557],
        [27.5535, 30.4334, 30.3820]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.05628415569663048 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05628415569663048 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7972], device='cuda:0')), ('power', tensor([-0.3670], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.05628415569663048
epoch£º368	 i:1 	 global-step:7361	 l-p:0.05643659457564354
epoch£º368	 i:2 	 global-step:7362	 l-p:0.05618276447057724
epoch£º368	 i:3 	 global-step:7363	 l-p:0.05628100410103798
epoch£º368	 i:4 	 global-step:7364	 l-p:0.05808519199490547
epoch£º368	 i:5 	 global-step:7365	 l-p:0.05631132051348686
epoch£º368	 i:6 	 global-step:7366	 l-p:0.05634458735585213
epoch£º368	 i:7 	 global-step:7367	 l-p:0.05617380887269974
epoch£º368	 i:8 	 global-step:7368	 l-p:0.05644480884075165
epoch£º368	 i:9 	 global-step:7369	 l-p:0.05789864435791969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6589, 31.7187, 32.4108],
        [27.6589, 27.6822, 27.6601],
        [27.6589, 27.6589, 27.6589],
        [27.6589, 36.9590, 43.1418]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.056438252329826355 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056438252329826355 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7453], device='cuda:0')), ('power', tensor([-0.0983], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.056438252329826355
epoch£º369	 i:1 	 global-step:7381	 l-p:0.05786533281207085
epoch£º369	 i:2 	 global-step:7382	 l-p:0.05618119239807129
epoch£º369	 i:3 	 global-step:7383	 l-p:0.05627656728029251
epoch£º369	 i:4 	 global-step:7384	 l-p:0.056279100477695465
epoch£º369	 i:5 	 global-step:7385	 l-p:0.056153859943151474
epoch£º369	 i:6 	 global-step:7386	 l-p:0.05690740421414375
epoch£º369	 i:7 	 global-step:7387	 l-p:0.057050883769989014
epoch£º369	 i:8 	 global-step:7388	 l-p:0.05652427673339844
epoch£º369	 i:9 	 global-step:7389	 l-p:0.0564291886985302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7734, 30.9168, 30.9949],
        [27.7734, 27.8057, 27.7754],
        [27.7734, 35.5707, 39.8921],
        [27.7734, 28.6809, 28.2166]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.05886932089924812 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05886932089924812 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.7614e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7268], device='cuda:0')), ('power', tensor([0.3523], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.05886932089924812
epoch£º370	 i:1 	 global-step:7401	 l-p:0.056242212653160095
epoch£º370	 i:2 	 global-step:7402	 l-p:0.05623563751578331
epoch£º370	 i:3 	 global-step:7403	 l-p:0.056827906519174576
epoch£º370	 i:4 	 global-step:7404	 l-p:0.05606722831726074
epoch£º370	 i:5 	 global-step:7405	 l-p:0.05612562224268913
epoch£º370	 i:6 	 global-step:7406	 l-p:0.056145284324884415
epoch£º370	 i:7 	 global-step:7407	 l-p:0.056670088320970535
epoch£º370	 i:8 	 global-step:7408	 l-p:0.05628688633441925
epoch£º370	 i:9 	 global-step:7409	 l-p:0.056281570345163345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8886, 27.8888, 27.8886],
        [27.8886, 27.9045, 27.8893],
        [27.8886, 37.2126, 43.3766],
        [27.8886, 28.0668, 27.9196]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): 0.056230928748846054 
model_pd.l_d.mean(): -1.79653412146763e-07 
model_pd.lagr.mean(): 0.05623074993491173 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.8083e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8087], device='cuda:0')), ('power', tensor([-0.0381], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:0.056230928748846054
epoch£º371	 i:1 	 global-step:7421	 l-p:0.05635703355073929
epoch£º371	 i:2 	 global-step:7422	 l-p:0.056219134479761124
epoch£º371	 i:3 	 global-step:7423	 l-p:0.056452155113220215
epoch£º371	 i:4 	 global-step:7424	 l-p:0.057757772505283356
epoch£º371	 i:5 	 global-step:7425	 l-p:0.05614153668284416
epoch£º371	 i:6 	 global-step:7426	 l-p:0.05625623092055321
epoch£º371	 i:7 	 global-step:7427	 l-p:0.05712123587727547
epoch£º371	 i:8 	 global-step:7428	 l-p:0.05661294236779213
epoch£º371	 i:9 	 global-step:7429	 l-p:0.05622800812125206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9966, 38.3291, 45.7948],
        [27.9966, 27.9968, 27.9966],
        [27.9966, 28.0134, 27.9974],
        [27.9966, 30.9254, 30.8734]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.05702731013298035 
model_pd.l_d.mean(): 1.19847836685949e-05 
model_pd.lagr.mean(): 0.0570392943918705 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.8945e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7865], device='cuda:0')), ('power', tensor([0.2612], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.05702731013298035
epoch£º372	 i:1 	 global-step:7441	 l-p:0.05618572235107422
epoch£º372	 i:2 	 global-step:7442	 l-p:0.0562533363699913
epoch£º372	 i:3 	 global-step:7443	 l-p:0.056180667132139206
epoch£º372	 i:4 	 global-step:7444	 l-p:0.05610783025622368
epoch£º372	 i:5 	 global-step:7445	 l-p:0.056470513343811035
epoch£º372	 i:6 	 global-step:7446	 l-p:0.05620098114013672
epoch£º372	 i:7 	 global-step:7447	 l-p:0.058261752128601074
epoch£º372	 i:8 	 global-step:7448	 l-p:0.055995531380176544
epoch£º372	 i:9 	 global-step:7449	 l-p:0.05636338144540787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0948, 37.4966, 43.7159],
        [28.0948, 28.3898, 28.1648],
        [28.0948, 28.0948, 28.0948],
        [28.0948, 31.6506, 31.9571]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.05716239660978317 
model_pd.l_d.mean(): 5.806110493722372e-05 
model_pd.lagr.mean(): 0.057220458984375 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7667], device='cuda:0')), ('power', tensor([0.4240], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.05716239660978317
epoch£º373	 i:1 	 global-step:7461	 l-p:0.05625423416495323
epoch£º373	 i:2 	 global-step:7462	 l-p:0.05653836205601692
epoch£º373	 i:3 	 global-step:7463	 l-p:0.05611520633101463
epoch£º373	 i:4 	 global-step:7464	 l-p:0.056242380291223526
epoch£º373	 i:5 	 global-step:7465	 l-p:0.056087300181388855
epoch£º373	 i:6 	 global-step:7466	 l-p:0.0575375035405159
epoch£º373	 i:7 	 global-step:7467	 l-p:0.05664084479212761
epoch£º373	 i:8 	 global-step:7468	 l-p:0.05599834769964218
epoch£º373	 i:9 	 global-step:7469	 l-p:0.05620219185948372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1680, 30.7452, 30.5185],
        [28.1680, 28.6395, 28.3183],
        [28.1680, 28.4966, 28.2513],
        [28.1680, 38.0899, 44.9681]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.05608944594860077 
model_pd.l_d.mean(): 5.23513117514085e-05 
model_pd.lagr.mean(): 0.056141797453165054 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8675], device='cuda:0')), ('power', tensor([0.1928], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.05608944594860077
epoch£º374	 i:1 	 global-step:7481	 l-p:0.05734844133257866
epoch£º374	 i:2 	 global-step:7482	 l-p:0.05624780431389809
epoch£º374	 i:3 	 global-step:7483	 l-p:0.05616435408592224
epoch£º374	 i:4 	 global-step:7484	 l-p:0.05606350302696228
epoch£º374	 i:5 	 global-step:7485	 l-p:0.05601493641734123
epoch£º374	 i:6 	 global-step:7486	 l-p:0.056214164942502975
epoch£º374	 i:7 	 global-step:7487	 l-p:0.05671966075897217
epoch£º374	 i:8 	 global-step:7488	 l-p:0.0561535619199276
epoch£º374	 i:9 	 global-step:7489	 l-p:0.057594649493694305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1958, 28.2457, 28.1998],
        [28.1958, 28.1958, 28.1958],
        [28.1958, 28.2552, 28.2011],
        [28.1958, 28.1971, 28.1958]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.05618123710155487 
model_pd.l_d.mean(): 8.752875146456063e-05 
model_pd.lagr.mean(): 0.05626876652240753 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8583], device='cuda:0')), ('power', tensor([0.2023], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.05618123710155487
epoch£º375	 i:1 	 global-step:7501	 l-p:0.056239861994981766
epoch£º375	 i:2 	 global-step:7502	 l-p:0.056025829166173935
epoch£º375	 i:3 	 global-step:7503	 l-p:0.0566338375210762
epoch£º375	 i:4 	 global-step:7504	 l-p:0.05604862421751022
epoch£º375	 i:5 	 global-step:7505	 l-p:0.05713551491498947
epoch£º375	 i:6 	 global-step:7506	 l-p:0.05606940761208534
epoch£º375	 i:7 	 global-step:7507	 l-p:0.056160733103752136
epoch£º375	 i:8 	 global-step:7508	 l-p:0.055999431759119034
epoch£º375	 i:9 	 global-step:7509	 l-p:0.058104246854782104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1738, 28.8517, 28.4462],
        [28.1738, 31.4622, 31.5981],
        [28.1738, 28.1752, 28.1738],
        [28.1738, 30.0539, 29.6020]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.05640983581542969 
model_pd.l_d.mean(): 0.00021246242977213115 
model_pd.lagr.mean(): 0.05662229657173157 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7867], device='cuda:0')), ('power', tensor([0.3562], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.05640983581542969
epoch£º376	 i:1 	 global-step:7521	 l-p:0.056651629507541656
epoch£º376	 i:2 	 global-step:7522	 l-p:0.05648276209831238
epoch£º376	 i:3 	 global-step:7523	 l-p:0.05613062158226967
epoch£º376	 i:4 	 global-step:7524	 l-p:0.056299228221178055
epoch£º376	 i:5 	 global-step:7525	 l-p:0.056874215602874756
epoch£º376	 i:6 	 global-step:7526	 l-p:0.056121826171875
epoch£º376	 i:7 	 global-step:7527	 l-p:0.057686880230903625
epoch£º376	 i:8 	 global-step:7528	 l-p:0.056045979261398315
epoch£º376	 i:9 	 global-step:7529	 l-p:0.05603218823671341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0993, 30.1709, 29.7685],
        [28.0993, 28.7538, 28.3569],
        [28.0993, 28.6241, 28.2786],
        [28.0993, 37.2964, 43.2542]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): 0.056235697120428085 
model_pd.l_d.mean(): 0.0001227554603246972 
model_pd.lagr.mean(): 0.056358452886343 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8379], device='cuda:0')), ('power', tensor([0.1663], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:0.056235697120428085
epoch£º377	 i:1 	 global-step:7541	 l-p:0.056107815355062485
epoch£º377	 i:2 	 global-step:7542	 l-p:0.058005064725875854
epoch£º377	 i:3 	 global-step:7543	 l-p:0.05618548393249512
epoch£º377	 i:4 	 global-step:7544	 l-p:0.05629292502999306
epoch£º377	 i:5 	 global-step:7545	 l-p:0.057628460228443146
epoch£º377	 i:6 	 global-step:7546	 l-p:0.05611056834459305
epoch£º377	 i:7 	 global-step:7547	 l-p:0.05607479810714722
epoch£º377	 i:8 	 global-step:7548	 l-p:0.056091517210006714
epoch£º377	 i:9 	 global-step:7549	 l-p:0.05629439279437065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9734, 30.8035, 30.7031],
        [27.9734, 27.9734, 27.9734],
        [27.9734, 29.0128, 28.5240],
        [27.9734, 28.4699, 28.1377]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.05630965530872345 
model_pd.l_d.mean(): 0.00012783860438503325 
model_pd.lagr.mean(): 0.05643749237060547 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7849], device='cuda:0')), ('power', tensor([0.1538], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.05630965530872345
epoch£º378	 i:1 	 global-step:7561	 l-p:0.0563032291829586
epoch£º378	 i:2 	 global-step:7562	 l-p:0.05803847685456276
epoch£º378	 i:3 	 global-step:7563	 l-p:0.05631007254123688
epoch£º378	 i:4 	 global-step:7564	 l-p:0.05621473118662834
epoch£º378	 i:5 	 global-step:7565	 l-p:0.05623677745461464
epoch£º378	 i:6 	 global-step:7566	 l-p:0.05632023513317108
epoch£º378	 i:7 	 global-step:7567	 l-p:0.05609539896249771
epoch£º378	 i:8 	 global-step:7568	 l-p:0.05668671801686287
epoch£º378	 i:9 	 global-step:7569	 l-p:0.056974347680807114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8065, 29.4473, 28.9646],
        [27.8065, 27.8065, 27.8065],
        [27.8065, 27.9344, 27.8246],
        [27.8065, 29.8552, 29.4572]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.05784231051802635 
model_pd.l_d.mean(): 0.00015478194109164178 
model_pd.lagr.mean(): 0.057997092604637146 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7684], device='cuda:0')), ('power', tensor([0.1815], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.05784231051802635
epoch£º379	 i:1 	 global-step:7581	 l-p:0.0563022680580616
epoch£º379	 i:2 	 global-step:7582	 l-p:0.05768396332859993
epoch£º379	 i:3 	 global-step:7583	 l-p:0.056204114109277725
epoch£º379	 i:4 	 global-step:7584	 l-p:0.05623786896467209
epoch£º379	 i:5 	 global-step:7585	 l-p:0.05644376575946808
epoch£º379	 i:6 	 global-step:7586	 l-p:0.05628877505660057
epoch£º379	 i:7 	 global-step:7587	 l-p:0.0562230683863163
epoch£º379	 i:8 	 global-step:7588	 l-p:0.05618169158697128
epoch£º379	 i:9 	 global-step:7589	 l-p:0.05660247802734375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6286, 32.7979, 34.4092],
        [27.6286, 27.8060, 27.6596],
        [27.6286, 27.6317, 27.6287],
        [27.6286, 27.6607, 27.6306]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.05631716549396515 
model_pd.l_d.mean(): -0.00024692181614227593 
model_pd.lagr.mean(): 0.0560702420771122 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8198], device='cuda:0')), ('power', tensor([-0.3126], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.05631716549396515
epoch£º380	 i:1 	 global-step:7601	 l-p:0.05619235709309578
epoch£º380	 i:2 	 global-step:7602	 l-p:0.05669942870736122
epoch£º380	 i:3 	 global-step:7603	 l-p:0.056181829422712326
epoch£º380	 i:4 	 global-step:7604	 l-p:0.057122111320495605
epoch£º380	 i:5 	 global-step:7605	 l-p:0.05630633234977722
epoch£º380	 i:6 	 global-step:7606	 l-p:0.05629399046301842
epoch£º380	 i:7 	 global-step:7607	 l-p:0.05716988444328308
epoch£º380	 i:8 	 global-step:7608	 l-p:0.05802956596016884
epoch£º380	 i:9 	 global-step:7609	 l-p:0.05634467303752899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4577, 27.4585, 27.4577],
        [27.4577, 35.0438, 39.1767],
        [27.4577, 28.0512, 27.6805],
        [27.4577, 28.8839, 28.3893]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.05640123412013054 
model_pd.l_d.mean(): -0.00021965846826788038 
model_pd.lagr.mean(): 0.05618157610297203 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7662], device='cuda:0')), ('power', tensor([-0.3439], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.05640123412013054
epoch£º381	 i:1 	 global-step:7621	 l-p:0.056585539132356644
epoch£º381	 i:2 	 global-step:7622	 l-p:0.056387100368738174
epoch£º381	 i:3 	 global-step:7623	 l-p:0.056518469005823135
epoch£º381	 i:4 	 global-step:7624	 l-p:0.05685994029045105
epoch£º381	 i:5 	 global-step:7625	 l-p:0.058079395443201065
epoch£º381	 i:6 	 global-step:7626	 l-p:0.05621575564146042
epoch£º381	 i:7 	 global-step:7627	 l-p:0.05659589543938637
epoch£º381	 i:8 	 global-step:7628	 l-p:0.05718391388654709
epoch£º381	 i:9 	 global-step:7629	 l-p:0.05630902945995331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3380, 28.3074, 27.8369],
        [27.3380, 27.3404, 27.3380],
        [27.3380, 27.3458, 27.3382],
        [27.3380, 28.7125, 28.2181]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.05624838545918465 
model_pd.l_d.mean(): -0.00030796759529039264 
model_pd.lagr.mean(): 0.0559404194355011 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8437], device='cuda:0')), ('power', tensor([-0.7439], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.05624838545918465
epoch£º382	 i:1 	 global-step:7641	 l-p:0.0564398467540741
epoch£º382	 i:2 	 global-step:7642	 l-p:0.05664999037981033
epoch£º382	 i:3 	 global-step:7643	 l-p:0.05624401941895485
epoch£º382	 i:4 	 global-step:7644	 l-p:0.056813448667526245
epoch£º382	 i:5 	 global-step:7645	 l-p:0.056503407657146454
epoch£º382	 i:6 	 global-step:7646	 l-p:0.056229643523693085
epoch£º382	 i:7 	 global-step:7647	 l-p:0.05642923340201378
epoch£º382	 i:8 	 global-step:7648	 l-p:0.058419860899448395
epoch£º382	 i:9 	 global-step:7649	 l-p:0.05747463181614876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2842, 27.3170, 27.2863],
        [27.2842, 29.7760, 29.5565],
        [27.2842, 27.2842, 27.2842],
        [27.2842, 27.8737, 27.5056]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.05644189566373825 
model_pd.l_d.mean(): -8.895560313249007e-05 
model_pd.lagr.mean(): 0.05635293945670128 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7830], device='cuda:0')), ('power', tensor([-0.6166], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.05644189566373825
epoch£º383	 i:1 	 global-step:7661	 l-p:0.05661582946777344
epoch£º383	 i:2 	 global-step:7662	 l-p:0.05623853951692581
epoch£º383	 i:3 	 global-step:7663	 l-p:0.05630652233958244
epoch£º383	 i:4 	 global-step:7664	 l-p:0.0564076229929924
epoch£º383	 i:5 	 global-step:7665	 l-p:0.05730399489402771
epoch£º383	 i:6 	 global-step:7666	 l-p:0.05643685162067413
epoch£º383	 i:7 	 global-step:7667	 l-p:0.056205518543720245
epoch£º383	 i:8 	 global-step:7668	 l-p:0.056717365980148315
epoch£º383	 i:9 	 global-step:7669	 l-p:0.058803874999284744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3181, 35.3187, 39.9617],
        [27.3181, 29.0690, 28.6177],
        [27.3181, 33.0730, 35.2725],
        [27.3181, 27.6902, 27.4222]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.05623749643564224 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05623749643564224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8357], device='cuda:0')), ('power', tensor([-0.6684], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.05623749643564224
epoch£º384	 i:1 	 global-step:7681	 l-p:0.056534212082624435
epoch£º384	 i:2 	 global-step:7682	 l-p:0.05632985010743141
epoch£º384	 i:3 	 global-step:7683	 l-p:0.05691150575876236
epoch£º384	 i:4 	 global-step:7684	 l-p:0.05810318887233734
epoch£º384	 i:5 	 global-step:7685	 l-p:0.05635193735361099
epoch£º384	 i:6 	 global-step:7686	 l-p:0.05642947927117348
epoch£º384	 i:7 	 global-step:7687	 l-p:0.05668417736887932
epoch£º384	 i:8 	 global-step:7688	 l-p:0.05654822289943695
epoch£º384	 i:9 	 global-step:7689	 l-p:0.05712909996509552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4144, 30.0306, 29.8591],
        [27.4144, 27.4362, 27.4155],
        [27.4144, 27.4147, 27.4144],
        [27.4144, 35.1566, 39.4778]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.05638526380062103 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05638526380062103 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7896], device='cuda:0')), ('power', tensor([-0.4645], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.05638526380062103
epoch£º385	 i:1 	 global-step:7701	 l-p:0.0564224049448967
epoch£º385	 i:2 	 global-step:7702	 l-p:0.05793258175253868
epoch£º385	 i:3 	 global-step:7703	 l-p:0.05655435472726822
epoch£º385	 i:4 	 global-step:7704	 l-p:0.05616600811481476
epoch£º385	 i:5 	 global-step:7705	 l-p:0.056686047464609146
epoch£º385	 i:6 	 global-step:7706	 l-p:0.0563570000231266
epoch£º385	 i:7 	 global-step:7707	 l-p:0.05728529766201973
epoch£º385	 i:8 	 global-step:7708	 l-p:0.05622347071766853
epoch£º385	 i:9 	 global-step:7709	 l-p:0.05689442902803421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5302, 28.1917, 27.7960],
        [27.5302, 33.0987, 35.0915],
        [27.5302, 27.5302, 27.5302],
        [27.5302, 27.9943, 27.6789]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.056354328989982605 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056354328989982605 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7756], device='cuda:0')), ('power', tensor([-0.3142], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.056354328989982605
epoch£º386	 i:1 	 global-step:7721	 l-p:0.056230638176202774
epoch£º386	 i:2 	 global-step:7722	 l-p:0.05617697536945343
epoch£º386	 i:3 	 global-step:7723	 l-p:0.05852188915014267
epoch£º386	 i:4 	 global-step:7724	 l-p:0.056192927062511444
epoch£º386	 i:5 	 global-step:7725	 l-p:0.05658764764666557
epoch£º386	 i:6 	 global-step:7726	 l-p:0.05752842128276825
epoch£º386	 i:7 	 global-step:7727	 l-p:0.056209664791822433
epoch£º386	 i:8 	 global-step:7728	 l-p:0.05626026913523674
epoch£º386	 i:9 	 global-step:7729	 l-p:0.056453637778759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6514, 27.9961, 27.7425],
        [27.6514, 35.0270, 38.8868],
        [27.6514, 27.6566, 27.6515],
        [27.6514, 28.3411, 27.9351]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.05618485435843468 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05618485435843468 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8478], device='cuda:0')), ('power', tensor([-0.3342], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.05618485435843468
epoch£º387	 i:1 	 global-step:7741	 l-p:0.05808612331748009
epoch£º387	 i:2 	 global-step:7742	 l-p:0.05629902333021164
epoch£º387	 i:3 	 global-step:7743	 l-p:0.05617363750934601
epoch£º387	 i:4 	 global-step:7744	 l-p:0.05620243772864342
epoch£º387	 i:5 	 global-step:7745	 l-p:0.05623788386583328
epoch£º387	 i:6 	 global-step:7746	 l-p:0.057238996028900146
epoch£º387	 i:7 	 global-step:7747	 l-p:0.05629481375217438
epoch£º387	 i:8 	 global-step:7748	 l-p:0.05620593950152397
epoch£º387	 i:9 	 global-step:7749	 l-p:0.05719396099448204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7734, 27.8070, 27.7756],
        [27.7734, 28.1298, 27.8694],
        [27.7734, 27.8146, 27.7764],
        [27.7734, 30.1814, 29.9047]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.05650036036968231 
model_pd.l_d.mean(): 3.6392683000485704e-07 
model_pd.lagr.mean(): 0.05650072544813156 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.5667e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7237], device='cuda:0')), ('power', tensor([0.0934], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.05650036036968231
epoch£º388	 i:1 	 global-step:7761	 l-p:0.056664902716875076
epoch£º388	 i:2 	 global-step:7762	 l-p:0.058265332132577896
epoch£º388	 i:3 	 global-step:7763	 l-p:0.056184787303209305
epoch£º388	 i:4 	 global-step:7764	 l-p:0.05621465668082237
epoch£º388	 i:5 	 global-step:7765	 l-p:0.05610915645956993
epoch£º388	 i:6 	 global-step:7766	 l-p:0.056266333907842636
epoch£º388	 i:7 	 global-step:7767	 l-p:0.05619868263602257
epoch£º388	 i:8 	 global-step:7768	 l-p:0.05634341016411781
epoch£º388	 i:9 	 global-step:7769	 l-p:0.056981075555086136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8956, 31.8659, 32.4719],
        [27.8956, 30.1229, 29.7760],
        [27.8956, 27.8977, 27.8957],
        [27.8956, 28.5468, 28.1523]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.05706106498837471 
model_pd.l_d.mean(): 3.805072879004001e-07 
model_pd.lagr.mean(): 0.05706144496798515 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.1592e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7749], device='cuda:0')), ('power', tensor([0.1922], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.05706106498837471
epoch£º389	 i:1 	 global-step:7781	 l-p:0.05713225528597832
epoch£º389	 i:2 	 global-step:7782	 l-p:0.0561210960149765
epoch£º389	 i:3 	 global-step:7783	 l-p:0.05782339349389076
epoch£º389	 i:4 	 global-step:7784	 l-p:0.05611560121178627
epoch£º389	 i:5 	 global-step:7785	 l-p:0.056210801005363464
epoch£º389	 i:6 	 global-step:7786	 l-p:0.0561654269695282
epoch£º389	 i:7 	 global-step:7787	 l-p:0.056451644748449326
epoch£º389	 i:8 	 global-step:7788	 l-p:0.056197624653577805
epoch£º389	 i:9 	 global-step:7789	 l-p:0.05607297644019127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0157, 28.0182, 28.0157],
        [28.0157, 29.4294, 28.9222],
        [28.0157, 36.3622, 41.2880],
        [28.0157, 28.0181, 28.0157]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.056092895567417145 
model_pd.l_d.mean(): 4.125159023260494e-07 
model_pd.lagr.mean(): 0.056093309074640274 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.8819e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8546], device='cuda:0')), ('power', tensor([0.0085], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.056092895567417145
epoch£º390	 i:1 	 global-step:7801	 l-p:0.056251876056194305
epoch£º390	 i:2 	 global-step:7802	 l-p:0.056218221783638
epoch£º390	 i:3 	 global-step:7803	 l-p:0.05675208568572998
epoch£º390	 i:4 	 global-step:7804	 l-p:0.056182537227869034
epoch£º390	 i:5 	 global-step:7805	 l-p:0.05629026144742966
epoch£º390	 i:6 	 global-step:7806	 l-p:0.05661541223526001
epoch£º390	 i:7 	 global-step:7807	 l-p:0.05755743756890297
epoch£º390	 i:8 	 global-step:7808	 l-p:0.056022584438323975
epoch£º390	 i:9 	 global-step:7809	 l-p:0.0569952093064785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1157, 28.2120, 28.1271],
        [28.1157, 30.8026, 30.6267],
        [28.1157, 30.3601, 30.0099],
        [28.1157, 28.1573, 28.1187]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.05606315657496452 
model_pd.l_d.mean(): 1.3308096640685108e-05 
model_pd.lagr.mean(): 0.05607646331191063 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8654], device='cuda:0')), ('power', tensor([0.0894], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.05606315657496452
epoch£º391	 i:1 	 global-step:7821	 l-p:0.05625198408961296
epoch£º391	 i:2 	 global-step:7822	 l-p:0.056422632187604904
epoch£º391	 i:3 	 global-step:7823	 l-p:0.05626567825675011
epoch£º391	 i:4 	 global-step:7824	 l-p:0.05606969818472862
epoch£º391	 i:5 	 global-step:7825	 l-p:0.05663306266069412
epoch£º391	 i:6 	 global-step:7826	 l-p:0.0560724101960659
epoch£º391	 i:7 	 global-step:7827	 l-p:0.05716557428240776
epoch£º391	 i:8 	 global-step:7828	 l-p:0.05613105744123459
epoch£º391	 i:9 	 global-step:7829	 l-p:0.05762643739581108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1892, 28.7758, 28.4040],
        [28.1892, 28.1916, 28.1892],
        [28.1892, 38.5507, 46.0093],
        [28.1892, 28.1892, 28.1892]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.05657266825437546 
model_pd.l_d.mean(): 9.159400360658765e-05 
model_pd.lagr.mean(): 0.05666426196694374 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8288], device='cuda:0')), ('power', tensor([0.3119], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.05657266825437546
epoch£º392	 i:1 	 global-step:7841	 l-p:0.056958697736263275
epoch£º392	 i:2 	 global-step:7842	 l-p:0.056032828986644745
epoch£º392	 i:3 	 global-step:7843	 l-p:0.057722076773643494
epoch£º392	 i:4 	 global-step:7844	 l-p:0.05610804259777069
epoch£º392	 i:5 	 global-step:7845	 l-p:0.0560615211725235
epoch£º392	 i:6 	 global-step:7846	 l-p:0.05608155205845833
epoch£º392	 i:7 	 global-step:7847	 l-p:0.056301966309547424
epoch£º392	 i:8 	 global-step:7848	 l-p:0.05649350956082344
epoch£º392	 i:9 	 global-step:7849	 l-p:0.05620727315545082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2218, 37.3625, 43.2235],
        [28.2218, 28.2218, 28.2218],
        [28.2218, 33.9528, 36.0134],
        [28.2218, 28.2219, 28.2218]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.05612201616168022 
model_pd.l_d.mean(): 0.00015275970508810133 
model_pd.lagr.mean(): 0.05627477541565895 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8285], device='cuda:0')), ('power', tensor([0.3272], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.05612201616168022
epoch£º393	 i:1 	 global-step:7861	 l-p:0.056676071137189865
epoch£º393	 i:2 	 global-step:7862	 l-p:0.05600352957844734
epoch£º393	 i:3 	 global-step:7863	 l-p:0.05812440812587738
epoch£º393	 i:4 	 global-step:7864	 l-p:0.05619898810982704
epoch£º393	 i:5 	 global-step:7865	 l-p:0.05615099146962166
epoch£º393	 i:6 	 global-step:7866	 l-p:0.05619020387530327
epoch£º393	 i:7 	 global-step:7867	 l-p:0.05598926544189453
epoch£º393	 i:8 	 global-step:7868	 l-p:0.057014986872673035
epoch£º393	 i:9 	 global-step:7869	 l-p:0.05605925992131233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1863, 28.8283, 28.4353],
        [28.1863, 28.1903, 28.1863],
        [28.1863, 30.2999, 29.9064],
        [28.1863, 37.8841, 44.4652]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.05608585104346275 
model_pd.l_d.mean(): 0.00012574493302963674 
model_pd.lagr.mean(): 0.05621159449219704 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8587], device='cuda:0')), ('power', tensor([0.1962], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.05608585104346275
epoch£º394	 i:1 	 global-step:7881	 l-p:0.05620962008833885
epoch£º394	 i:2 	 global-step:7882	 l-p:0.05609096959233284
epoch£º394	 i:3 	 global-step:7883	 l-p:0.056591153144836426
epoch£º394	 i:4 	 global-step:7884	 l-p:0.05613744258880615
epoch£º394	 i:5 	 global-step:7885	 l-p:0.05636904016137123
epoch£º394	 i:6 	 global-step:7886	 l-p:0.056048519909381866
epoch£º394	 i:7 	 global-step:7887	 l-p:0.05697105452418327
epoch£º394	 i:8 	 global-step:7888	 l-p:0.05660191550850868
epoch£º394	 i:9 	 global-step:7889	 l-p:0.05765199661254883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0810, 28.0968, 28.0817],
        [28.0810, 29.0075, 28.5362],
        [28.0810, 28.0810, 28.0810],
        [28.0810, 28.7071, 28.3206]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.056236207485198975 
model_pd.l_d.mean(): 0.00016206077998504043 
model_pd.lagr.mean(): 0.05639826878905296 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8023], device='cuda:0')), ('power', tensor([0.2073], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.056236207485198975
epoch£º395	 i:1 	 global-step:7901	 l-p:0.05633668601512909
epoch£º395	 i:2 	 global-step:7902	 l-p:0.05601339787244797
epoch£º395	 i:3 	 global-step:7903	 l-p:0.05603479593992233
epoch£º395	 i:4 	 global-step:7904	 l-p:0.05800841376185417
epoch£º395	 i:5 	 global-step:7905	 l-p:0.056667082011699677
epoch£º395	 i:6 	 global-step:7906	 l-p:0.05608158931136131
epoch£º395	 i:7 	 global-step:7907	 l-p:0.05622635409235954
epoch£º395	 i:8 	 global-step:7908	 l-p:0.05724909529089928
epoch£º395	 i:9 	 global-step:7909	 l-p:0.056282442063093185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9298, 33.5822, 35.6047],
        [27.9298, 34.1555, 36.7408],
        [27.9298, 28.1546, 27.9749],
        [27.9298, 28.0584, 27.9481]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.05603712797164917 
model_pd.l_d.mean(): -0.00014937084051780403 
model_pd.lagr.mean(): 0.05588775873184204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8965], device='cuda:0')), ('power', tensor([-0.1736], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.05603712797164917
epoch£º396	 i:1 	 global-step:7921	 l-p:0.05637602508068085
epoch£º396	 i:2 	 global-step:7922	 l-p:0.057806890457868576
epoch£º396	 i:3 	 global-step:7923	 l-p:0.05617152154445648
epoch£º396	 i:4 	 global-step:7924	 l-p:0.05630817636847496
epoch£º396	 i:5 	 global-step:7925	 l-p:0.05623199790716171
epoch£º396	 i:6 	 global-step:7926	 l-p:0.05704803392291069
epoch£º396	 i:7 	 global-step:7927	 l-p:0.056183021515607834
epoch£º396	 i:8 	 global-step:7928	 l-p:0.05616457015275955
epoch£º396	 i:9 	 global-step:7929	 l-p:0.05733238905668259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7430, 37.2814, 43.7538],
        [27.7430, 27.7996, 27.7479],
        [27.7430, 28.5831, 28.1340],
        [27.7430, 28.4851, 28.0621]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.0566164068877697 
model_pd.l_d.mean(): 6.94466070854105e-05 
model_pd.lagr.mean(): 0.056685853749513626 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7188], device='cuda:0')), ('power', tensor([0.0811], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.0566164068877697
epoch£º397	 i:1 	 global-step:7941	 l-p:0.058411672711372375
epoch£º397	 i:2 	 global-step:7942	 l-p:0.05621327459812164
epoch£º397	 i:3 	 global-step:7943	 l-p:0.0561818964779377
epoch£º397	 i:4 	 global-step:7944	 l-p:0.05628183111548424
epoch£º397	 i:5 	 global-step:7945	 l-p:0.05624324828386307
epoch£º397	 i:6 	 global-step:7946	 l-p:0.05738937109708786
epoch£º397	 i:7 	 global-step:7947	 l-p:0.056201234459877014
epoch£º397	 i:8 	 global-step:7948	 l-p:0.05616738274693489
epoch£º397	 i:9 	 global-step:7949	 l-p:0.05654601752758026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5490, 33.4023, 35.6680],
        [27.5490, 27.5490, 27.5489],
        [27.5490, 27.5644, 27.5496],
        [27.5490, 32.7508, 34.4007]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.05696908384561539 
model_pd.l_d.mean(): -9.101707837544382e-05 
model_pd.lagr.mean(): 0.056878067553043365 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7532], device='cuda:0')), ('power', tensor([-0.1201], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.05696908384561539
epoch£º398	 i:1 	 global-step:7961	 l-p:0.05822448432445526
epoch£º398	 i:2 	 global-step:7962	 l-p:0.05617199465632439
epoch£º398	 i:3 	 global-step:7963	 l-p:0.05621488392353058
epoch£º398	 i:4 	 global-step:7964	 l-p:0.05618207901716232
epoch£º398	 i:5 	 global-step:7965	 l-p:0.05649151653051376
epoch£º398	 i:6 	 global-step:7966	 l-p:0.05627730116248131
epoch£º398	 i:7 	 global-step:7967	 l-p:0.0563819482922554
epoch£º398	 i:8 	 global-step:7968	 l-p:0.05662822723388672
epoch£º398	 i:9 	 global-step:7969	 l-p:0.05733644217252731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3766, 27.3808, 27.3767],
        [27.3766, 28.1331, 27.7089],
        [27.3766, 31.6657, 32.5592],
        [27.3766, 29.7709, 29.5071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.05647796019911766 
model_pd.l_d.mean(): -0.0002703596546780318 
model_pd.lagr.mean(): 0.056207600980997086 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7783], device='cuda:0')), ('power', tensor([-0.4764], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.05647796019911766
epoch£º399	 i:1 	 global-step:7981	 l-p:0.05812811851501465
epoch£º399	 i:2 	 global-step:7982	 l-p:0.05666954070329666
epoch£º399	 i:3 	 global-step:7983	 l-p:0.0562579371035099
epoch£º399	 i:4 	 global-step:7984	 l-p:0.05784992873668671
epoch£º399	 i:5 	 global-step:7985	 l-p:0.05624379217624664
epoch£º399	 i:6 	 global-step:7986	 l-p:0.056394848972558975
epoch£º399	 i:7 	 global-step:7987	 l-p:0.05664880946278572
epoch£º399	 i:8 	 global-step:7988	 l-p:0.05643312260508537
epoch£º399	 i:9 	 global-step:7989	 l-p:0.05626445636153221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2688, 30.3519, 30.4283],
        [27.2688, 32.0661, 33.3896],
        [27.2688, 27.4515, 27.3015],
        [27.2688, 30.1642, 30.1377]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.05726712942123413 
model_pd.l_d.mean(): -0.00015158564201556146 
model_pd.lagr.mean(): 0.05711554363369942 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7781], device='cuda:0')), ('power', tensor([-0.4961], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.05726712942123413
epoch£º400	 i:1 	 global-step:8001	 l-p:0.05626228079199791
epoch£º400	 i:2 	 global-step:8002	 l-p:0.05678480118513107
epoch£º400	 i:3 	 global-step:8003	 l-p:0.056987836956977844
epoch£º400	 i:4 	 global-step:8004	 l-p:0.05827532708644867
epoch£º400	 i:5 	 global-step:8005	 l-p:0.056328315287828445
epoch£º400	 i:6 	 global-step:8006	 l-p:0.05640210211277008
epoch£º400	 i:7 	 global-step:8007	 l-p:0.05663485452532768
epoch£º400	 i:8 	 global-step:8008	 l-p:0.05627848207950592
epoch£º400	 i:9 	 global-step:8009	 l-p:0.05637226998806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2599, 32.5329, 34.2820],
        [27.2599, 27.2619, 27.2599],
        [27.2599, 31.9809, 33.2401],
        [27.2599, 33.8140, 36.8366]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.056671950966119766 
model_pd.l_d.mean(): -5.360829618439311e-06 
model_pd.lagr.mean(): 0.05666659027338028 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7112], device='cuda:0')), ('power', tensor([-0.3893], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.056671950966119766
epoch£º401	 i:1 	 global-step:8021	 l-p:0.05627752095460892
epoch£º401	 i:2 	 global-step:8022	 l-p:0.05637367069721222
epoch£º401	 i:3 	 global-step:8023	 l-p:0.05713006854057312
epoch£º401	 i:4 	 global-step:8024	 l-p:0.05630730465054512
epoch£º401	 i:5 	 global-step:8025	 l-p:0.056814078241586685
epoch£º401	 i:6 	 global-step:8026	 l-p:0.05622035264968872
epoch£º401	 i:7 	 global-step:8027	 l-p:0.05917675793170929
epoch£º401	 i:8 	 global-step:8028	 l-p:0.056281670928001404
epoch£º401	 i:9 	 global-step:8029	 l-p:0.05631411448121071
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2852, 27.2852, 27.2852],
        [27.2852, 27.2896, 27.2853],
        [27.2852, 28.0509, 27.6250],
        [27.2852, 27.2852, 27.2852]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.056284137070178986 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056284137070178986 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8309], device='cuda:0')), ('power', tensor([-0.7681], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.056284137070178986
epoch£º402	 i:1 	 global-step:8041	 l-p:0.058203909546136856
epoch£º402	 i:2 	 global-step:8042	 l-p:0.0576644204556942
epoch£º402	 i:3 	 global-step:8043	 l-p:0.05683135241270065
epoch£º402	 i:4 	 global-step:8044	 l-p:0.05627238005399704
epoch£º402	 i:5 	 global-step:8045	 l-p:0.05631336197257042
epoch£º402	 i:6 	 global-step:8046	 l-p:0.05633888021111488
epoch£º402	 i:7 	 global-step:8047	 l-p:0.056347813457250595
epoch£º402	 i:8 	 global-step:8048	 l-p:0.05681244656443596
epoch£º402	 i:9 	 global-step:8049	 l-p:0.05638190731406212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3321, 27.9403, 27.5648],
        [27.3321, 27.3324, 27.3320],
        [27.3321, 27.8749, 27.5252],
        [27.3321, 27.5074, 27.3626]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.05634476989507675 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05634476989507675 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8137], device='cuda:0')), ('power', tensor([-0.6640], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.05634476989507675
epoch£º403	 i:1 	 global-step:8061	 l-p:0.05634104460477829
epoch£º403	 i:2 	 global-step:8062	 l-p:0.056881148368120193
epoch£º403	 i:3 	 global-step:8063	 l-p:0.058726776391267776
epoch£º403	 i:4 	 global-step:8064	 l-p:0.056288838386535645
epoch£º403	 i:5 	 global-step:8065	 l-p:0.05632967874407768
epoch£º403	 i:6 	 global-step:8066	 l-p:0.05650500953197479
epoch£º403	 i:7 	 global-step:8067	 l-p:0.05630573257803917
epoch£º403	 i:8 	 global-step:8068	 l-p:0.05730513110756874
epoch£º403	 i:9 	 global-step:8069	 l-p:0.05625166743993759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3826, 32.1260, 33.3912],
        [27.3826, 35.1400, 39.4846],
        [27.3826, 27.7556, 27.4869],
        [27.3826, 27.4309, 27.3865]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.057339057326316833 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057339057326316833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7312], device='cuda:0')), ('power', tensor([-0.2372], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.057339057326316833
epoch£º404	 i:1 	 global-step:8081	 l-p:0.056755274534225464
epoch£º404	 i:2 	 global-step:8082	 l-p:0.05661165714263916
epoch£º404	 i:3 	 global-step:8083	 l-p:0.056347429752349854
epoch£º404	 i:4 	 global-step:8084	 l-p:0.058095239102840424
epoch£º404	 i:5 	 global-step:8085	 l-p:0.056239742785692215
epoch£º404	 i:6 	 global-step:8086	 l-p:0.056373629719018936
epoch£º404	 i:7 	 global-step:8087	 l-p:0.05687502771615982
epoch£º404	 i:8 	 global-step:8088	 l-p:0.05617809295654297
epoch£º404	 i:9 	 global-step:8089	 l-p:0.0562916025519371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4364, 30.2924, 30.2355],
        [27.4364, 27.4363, 27.4363],
        [27.4364, 27.4363, 27.4364],
        [27.4364, 29.1599, 28.7002]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.056316204369068146 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056316204369068146 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7976], device='cuda:0')), ('power', tensor([-0.4828], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.056316204369068146
epoch£º405	 i:1 	 global-step:8101	 l-p:0.05739274248480797
epoch£º405	 i:2 	 global-step:8102	 l-p:0.056381795555353165
epoch£º405	 i:3 	 global-step:8103	 l-p:0.05620120093226433
epoch£º405	 i:4 	 global-step:8104	 l-p:0.05737556144595146
epoch£º405	 i:5 	 global-step:8105	 l-p:0.05647043138742447
epoch£º405	 i:6 	 global-step:8106	 l-p:0.056284163147211075
epoch£º405	 i:7 	 global-step:8107	 l-p:0.05619311332702637
epoch£º405	 i:8 	 global-step:8108	 l-p:0.05625194311141968
epoch£º405	 i:9 	 global-step:8109	 l-p:0.05805107578635216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4883, 28.2687, 27.8370],
        [27.4883, 27.7079, 27.5322],
        [27.4883, 29.1945, 28.7304],
        [27.4883, 32.6781, 34.3242]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.05671650916337967 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05671650916337967 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7659], device='cuda:0')), ('power', tensor([-0.2820], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.05671650916337967
epoch£º406	 i:1 	 global-step:8121	 l-p:0.05634753033518791
epoch£º406	 i:2 	 global-step:8122	 l-p:0.057141754776239395
epoch£º406	 i:3 	 global-step:8123	 l-p:0.05629238113760948
epoch£º406	 i:4 	 global-step:8124	 l-p:0.056329526007175446
epoch£º406	 i:5 	 global-step:8125	 l-p:0.05625147745013237
epoch£º406	 i:6 	 global-step:8126	 l-p:0.05638807266950607
epoch£º406	 i:7 	 global-step:8127	 l-p:0.056336164474487305
epoch£º406	 i:8 	 global-step:8128	 l-p:0.056805867701768875
epoch£º406	 i:9 	 global-step:8129	 l-p:0.058131732046604156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5416, 27.6027, 27.5472],
        [27.5416, 27.5957, 27.5462],
        [27.5416, 31.9641, 32.9476],
        [27.5416, 30.5314, 30.5388]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.05622744560241699 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05622744560241699 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8339], device='cuda:0')), ('power', tensor([-0.4644], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.05622744560241699
epoch£º407	 i:1 	 global-step:8141	 l-p:0.05684371665120125
epoch£º407	 i:2 	 global-step:8142	 l-p:0.058108434081077576
epoch£º407	 i:3 	 global-step:8143	 l-p:0.05641670897603035
epoch£º407	 i:4 	 global-step:8144	 l-p:0.056196849793195724
epoch£º407	 i:5 	 global-step:8145	 l-p:0.056265972554683685
epoch£º407	 i:6 	 global-step:8146	 l-p:0.05683472007513046
epoch£º407	 i:7 	 global-step:8147	 l-p:0.05720602348446846
epoch£º407	 i:8 	 global-step:8148	 l-p:0.05627910792827606
epoch£º407	 i:9 	 global-step:8149	 l-p:0.056192588061094284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5983, 28.0042, 27.7175],
        [27.5983, 27.9420, 27.6891],
        [27.5983, 29.3326, 28.8701],
        [27.5983, 31.7904, 32.5879]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.05620063096284866 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05620063096284866 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8334], device='cuda:0')), ('power', tensor([-0.3493], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.05620063096284866
epoch£º408	 i:1 	 global-step:8161	 l-p:0.056712184101343155
epoch£º408	 i:2 	 global-step:8162	 l-p:0.05652252212166786
epoch£º408	 i:3 	 global-step:8163	 l-p:0.05677612125873566
epoch£º408	 i:4 	 global-step:8164	 l-p:0.05630556866526604
epoch£º408	 i:5 	 global-step:8165	 l-p:0.05618936941027641
epoch£º408	 i:6 	 global-step:8166	 l-p:0.05635863542556763
epoch£º408	 i:7 	 global-step:8167	 l-p:0.05629371479153633
epoch£º408	 i:8 	 global-step:8168	 l-p:0.057128388434648514
epoch£º408	 i:9 	 global-step:8169	 l-p:0.057891469448804855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6510, 27.8926, 27.7020],
        [27.6510, 27.9232, 27.7130],
        [27.6510, 27.6668, 27.6516],
        [27.6510, 27.6621, 27.6514]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.05623180419206619 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05623180419206619 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8477], device='cuda:0')), ('power', tensor([-0.3663], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.05623180419206619
epoch£º409	 i:1 	 global-step:8181	 l-p:0.056126661598682404
epoch£º409	 i:2 	 global-step:8182	 l-p:0.056188877671957016
epoch£º409	 i:3 	 global-step:8183	 l-p:0.05633607879281044
epoch£º409	 i:4 	 global-step:8184	 l-p:0.05618826672434807
epoch£º409	 i:5 	 global-step:8185	 l-p:0.05809350684285164
epoch£º409	 i:6 	 global-step:8186	 l-p:0.05623964965343475
epoch£º409	 i:7 	 global-step:8187	 l-p:0.05772458389401436
epoch£º409	 i:8 	 global-step:8188	 l-p:0.056733448058366776
epoch£º409	 i:9 	 global-step:8189	 l-p:0.056345343589782715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7062, 28.1197, 27.8288],
        [27.7062, 28.6306, 28.1638],
        [27.7062, 27.7645, 27.7114],
        [27.7062, 27.7062, 27.7062]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.05621761828660965 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05621761828660965 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8216], device='cuda:0')), ('power', tensor([-0.2410], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.05621761828660965
epoch£º410	 i:1 	 global-step:8201	 l-p:0.05630466341972351
epoch£º410	 i:2 	 global-step:8202	 l-p:0.05885198339819908
epoch£º410	 i:3 	 global-step:8203	 l-p:0.056239254772663116
epoch£º410	 i:4 	 global-step:8204	 l-p:0.05681706219911575
epoch£º410	 i:5 	 global-step:8205	 l-p:0.056156985461711884
epoch£º410	 i:6 	 global-step:8206	 l-p:0.05698215216398239
epoch£º410	 i:7 	 global-step:8207	 l-p:0.05616099759936333
epoch£º410	 i:8 	 global-step:8208	 l-p:0.05609199404716492
epoch£º410	 i:9 	 global-step:8209	 l-p:0.0562150739133358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7633, 27.7633, 27.7633],
        [27.7633, 27.7692, 27.7634],
        [27.7633, 27.7646, 27.7633],
        [27.7633, 27.7741, 27.7636]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.056240107864141464 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056240107864141464 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8061], device='cuda:0')), ('power', tensor([-0.1705], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.056240107864141464
epoch£º411	 i:1 	 global-step:8221	 l-p:0.056984223425388336
epoch£º411	 i:2 	 global-step:8222	 l-p:0.05612683296203613
epoch£º411	 i:3 	 global-step:8223	 l-p:0.056265752762556076
epoch£º411	 i:4 	 global-step:8224	 l-p:0.058149371296167374
epoch£º411	 i:5 	 global-step:8225	 l-p:0.05642678216099739
epoch£º411	 i:6 	 global-step:8226	 l-p:0.056573133915662766
epoch£º411	 i:7 	 global-step:8227	 l-p:0.05614085868000984
epoch£º411	 i:8 	 global-step:8228	 l-p:0.05687570571899414
epoch£º411	 i:9 	 global-step:8229	 l-p:0.05607122927904129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8179, 27.8747, 27.8229],
        [27.8179, 38.1153, 45.5765],
        [27.8179, 27.8179, 27.8179],
        [27.8179, 28.7269, 28.2619]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.05782755836844444 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05782755836844444 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.8768e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7695], device='cuda:0')), ('power', tensor([0.1575], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.05782755836844444
epoch£º412	 i:1 	 global-step:8241	 l-p:0.056161701679229736
epoch£º412	 i:2 	 global-step:8242	 l-p:0.0562707893550396
epoch£º412	 i:3 	 global-step:8243	 l-p:0.05624709650874138
epoch£º412	 i:4 	 global-step:8244	 l-p:0.05628387629985809
epoch£º412	 i:5 	 global-step:8245	 l-p:0.05625402182340622
epoch£º412	 i:6 	 global-step:8246	 l-p:0.056112755089998245
epoch£º412	 i:7 	 global-step:8247	 l-p:0.056784823536872864
epoch£º412	 i:8 	 global-step:8248	 l-p:0.05742192268371582
epoch£º412	 i:9 	 global-step:8249	 l-p:0.05631545931100845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8724, 28.5213, 28.1278],
        [27.8724, 27.8724, 27.8724],
        [27.8724, 37.7776, 44.7014],
        [27.8724, 27.8724, 27.8724]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.0564214326441288 
model_pd.l_d.mean(): 7.131969823603868e-07 
model_pd.lagr.mean(): 0.056422144174575806 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.0460e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7933], device='cuda:0')), ('power', tensor([0.0385], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.0564214326441288
epoch£º413	 i:1 	 global-step:8261	 l-p:0.05809513479471207
epoch£º413	 i:2 	 global-step:8262	 l-p:0.056138671934604645
epoch£º413	 i:3 	 global-step:8263	 l-p:0.056326210498809814
epoch£º413	 i:4 	 global-step:8264	 l-p:0.05632774159312248
epoch£º413	 i:5 	 global-step:8265	 l-p:0.05769030377268791
epoch£º413	 i:6 	 global-step:8266	 l-p:0.05606909468770027
epoch£º413	 i:7 	 global-step:8267	 l-p:0.0561215877532959
epoch£º413	 i:8 	 global-step:8268	 l-p:0.05616806820034981
epoch£º413	 i:9 	 global-step:8269	 l-p:0.056153543293476105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9259, 27.9595, 27.9281],
        [27.9259, 33.1023, 34.6860],
        [27.9259, 27.9259, 27.9259],
        [27.9259, 28.1997, 27.9881]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.05675515905022621 
model_pd.l_d.mean(): 6.7156752265873365e-06 
model_pd.lagr.mean(): 0.05676187574863434 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.7182e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7843], device='cuda:0')), ('power', tensor([0.1747], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.05675515905022621
epoch£º414	 i:1 	 global-step:8281	 l-p:0.05607995390892029
epoch£º414	 i:2 	 global-step:8282	 l-p:0.056248076260089874
epoch£º414	 i:3 	 global-step:8283	 l-p:0.05629415437579155
epoch£º414	 i:4 	 global-step:8284	 l-p:0.057422518730163574
epoch£º414	 i:5 	 global-step:8285	 l-p:0.05628429353237152
epoch£º414	 i:6 	 global-step:8286	 l-p:0.056095197796821594
epoch£º414	 i:7 	 global-step:8287	 l-p:0.05620599165558815
epoch£º414	 i:8 	 global-step:8288	 l-p:0.0576845146715641
epoch£º414	 i:9 	 global-step:8289	 l-p:0.056273479014635086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9729, 32.7850, 34.0463],
        [27.9729, 36.2212, 41.0373],
        [27.9729, 32.1350, 32.8757],
        [27.9729, 28.1169, 27.9948]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.05630121007561684 
model_pd.l_d.mean(): 1.6512656657141633e-05 
model_pd.lagr.mean(): 0.05631772428750992 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.2889e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7667], device='cuda:0')), ('power', tensor([0.1991], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.05630121007561684
epoch£º415	 i:1 	 global-step:8301	 l-p:0.05607522279024124
epoch£º415	 i:2 	 global-step:8302	 l-p:0.05672149360179901
epoch£º415	 i:3 	 global-step:8303	 l-p:0.056051887571811676
epoch£º415	 i:4 	 global-step:8304	 l-p:0.056131504476070404
epoch£º415	 i:5 	 global-step:8305	 l-p:0.057650525122880936
epoch£º415	 i:6 	 global-step:8306	 l-p:0.05629083514213562
epoch£º415	 i:7 	 global-step:8307	 l-p:0.05634862929582596
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05753764510154724
epoch£º415	 i:9 	 global-step:8309	 l-p:0.056093234568834305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0156, 28.3958, 28.1216],
        [28.0156, 28.0158, 28.0156],
        [28.0156, 32.6451, 33.7497],
        [28.0156, 28.0156, 28.0156]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.05609520897269249 
model_pd.l_d.mean(): 7.894899681559764e-06 
model_pd.lagr.mean(): 0.05610310286283493 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8555], device='cuda:0')), ('power', tensor([0.0528], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.05609520897269249
epoch£º416	 i:1 	 global-step:8321	 l-p:0.056206509470939636
epoch£º416	 i:2 	 global-step:8322	 l-p:0.056146252900362015
epoch£º416	 i:3 	 global-step:8323	 l-p:0.05606263875961304
epoch£º416	 i:4 	 global-step:8324	 l-p:0.058022353798151016
epoch£º416	 i:5 	 global-step:8325	 l-p:0.05736478790640831
epoch£º416	 i:6 	 global-step:8326	 l-p:0.05622323974967003
epoch£º416	 i:7 	 global-step:8327	 l-p:0.05657615885138512
epoch£º416	 i:8 	 global-step:8328	 l-p:0.056180715560913086
epoch£º416	 i:9 	 global-step:8329	 l-p:0.05620463192462921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0499, 28.1327, 28.0589],
        [28.0499, 28.2382, 28.0837],
        [28.0499, 28.7049, 28.3081],
        [28.0499, 28.0500, 28.0499]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.05615796893835068 
model_pd.l_d.mean(): 2.104591294482816e-05 
model_pd.lagr.mean(): 0.05617901310324669 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8304], device='cuda:0')), ('power', tensor([0.0893], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.05615796893835068
epoch£º417	 i:1 	 global-step:8341	 l-p:0.05664139613509178
epoch£º417	 i:2 	 global-step:8342	 l-p:0.05612530559301376
epoch£º417	 i:3 	 global-step:8343	 l-p:0.056139521300792694
epoch£º417	 i:4 	 global-step:8344	 l-p:0.05613243579864502
epoch£º417	 i:5 	 global-step:8345	 l-p:0.05608951300382614
epoch£º417	 i:6 	 global-step:8346	 l-p:0.05854051560163498
epoch£º417	 i:7 	 global-step:8347	 l-p:0.056151218712329865
epoch£º417	 i:8 	 global-step:8348	 l-p:0.056653935462236404
epoch£º417	 i:9 	 global-step:8349	 l-p:0.056362979114055634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0699, 28.0713, 28.0699],
        [28.0699, 28.0707, 28.0699],
        [28.0699, 33.9954, 36.2633],
        [28.0699, 29.3122, 28.8042]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): 0.056937966495752335 
model_pd.l_d.mean(): 0.00010565295087872073 
model_pd.lagr.mean(): 0.05704361945390701 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8098], device='cuda:0')), ('power', tensor([0.3152], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:0.056937966495752335
epoch£º418	 i:1 	 global-step:8361	 l-p:0.056020401418209076
epoch£º418	 i:2 	 global-step:8362	 l-p:0.05682022497057915
epoch£º418	 i:3 	 global-step:8363	 l-p:0.057614486664533615
epoch£º418	 i:4 	 global-step:8364	 l-p:0.056105922907590866
epoch£º418	 i:5 	 global-step:8365	 l-p:0.05659402906894684
epoch£º418	 i:6 	 global-step:8366	 l-p:0.05624022334814072
epoch£º418	 i:7 	 global-step:8367	 l-p:0.056239526718854904
epoch£º418	 i:8 	 global-step:8368	 l-p:0.056279256939888
epoch£º418	 i:9 	 global-step:8369	 l-p:0.056100063025951385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0755, 29.3097, 28.8020],
        [28.0755, 32.1997, 32.9031],
        [28.0755, 31.1265, 31.1343],
        [28.0755, 28.1643, 28.0855]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.056181736290454865 
model_pd.l_d.mean(): 8.959902334026992e-05 
model_pd.lagr.mean(): 0.05627133697271347 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8261], device='cuda:0')), ('power', tensor([0.2027], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.056181736290454865
epoch£º419	 i:1 	 global-step:8381	 l-p:0.0577479712665081
epoch£º419	 i:2 	 global-step:8382	 l-p:0.05602500960230827
epoch£º419	 i:3 	 global-step:8383	 l-p:0.05669749155640602
epoch£º419	 i:4 	 global-step:8384	 l-p:0.056128501892089844
epoch£º419	 i:5 	 global-step:8385	 l-p:0.05688481032848358
epoch£º419	 i:6 	 global-step:8386	 l-p:0.056305691599845886
epoch£º419	 i:7 	 global-step:8387	 l-p:0.05617598816752434
epoch£º419	 i:8 	 global-step:8388	 l-p:0.056516412645578384
epoch£º419	 i:9 	 global-step:8389	 l-p:0.05629773810505867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0607, 33.4967, 35.3005],
        [28.0607, 29.1036, 28.6132],
        [28.0607, 29.8234, 29.3523],
        [28.0607, 28.0647, 28.0608]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.05669992417097092 
model_pd.l_d.mean(): 0.00014283716154750437 
model_pd.lagr.mean(): 0.05684276297688484 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8012], device='cuda:0')), ('power', tensor([0.2611], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.05669992417097092
epoch£º420	 i:1 	 global-step:8401	 l-p:0.05609239637851715
epoch£º420	 i:2 	 global-step:8402	 l-p:0.05768134072422981
epoch£º420	 i:3 	 global-step:8403	 l-p:0.05643092095851898
epoch£º420	 i:4 	 global-step:8404	 l-p:0.05599565431475639
epoch£º420	 i:5 	 global-step:8405	 l-p:0.05651136115193367
epoch£º420	 i:6 	 global-step:8406	 l-p:0.05616668239235878
epoch£º420	 i:7 	 global-step:8407	 l-p:0.05717029795050621
epoch£º420	 i:8 	 global-step:8408	 l-p:0.05609599128365517
epoch£º420	 i:9 	 global-step:8409	 l-p:0.056187886744737625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0274, 30.0424, 29.6274],
        [28.0274, 37.1999, 43.1417],
        [28.0274, 28.4799, 28.1683],
        [28.0274, 28.0863, 28.0326]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.05619577318429947 
model_pd.l_d.mean(): 5.041802432970144e-05 
model_pd.lagr.mean(): 0.05624619126319885 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8167], device='cuda:0')), ('power', tensor([0.0787], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.05619577318429947
epoch£º421	 i:1 	 global-step:8421	 l-p:0.05660667642951012
epoch£º421	 i:2 	 global-step:8422	 l-p:0.05624110996723175
epoch£º421	 i:3 	 global-step:8423	 l-p:0.05689641460776329
epoch£º421	 i:4 	 global-step:8424	 l-p:0.05627085641026497
epoch£º421	 i:5 	 global-step:8425	 l-p:0.05695647746324539
epoch£º421	 i:6 	 global-step:8426	 l-p:0.05618748441338539
epoch£º421	 i:7 	 global-step:8427	 l-p:0.05608415603637695
epoch£º421	 i:8 	 global-step:8428	 l-p:0.057611893862485886
epoch£º421	 i:9 	 global-step:8429	 l-p:0.05611897259950638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9752, 28.1315, 28.0002],
        [27.9752, 27.9752, 27.9752],
        [27.9752, 27.9813, 27.9753],
        [27.9752, 31.7555, 32.2208]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.0562078095972538 
model_pd.l_d.mean(): 4.147259460296482e-05 
model_pd.lagr.mean(): 0.056249283254146576 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8105], device='cuda:0')), ('power', tensor([0.0581], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.0562078095972538
epoch£º422	 i:1 	 global-step:8441	 l-p:0.05606742575764656
epoch£º422	 i:2 	 global-step:8442	 l-p:0.05655729025602341
epoch£º422	 i:3 	 global-step:8443	 l-p:0.05629872530698776
epoch£º422	 i:4 	 global-step:8444	 l-p:0.05785658583045006
epoch£º422	 i:5 	 global-step:8445	 l-p:0.056172892451286316
epoch£º422	 i:6 	 global-step:8446	 l-p:0.0569610670208931
epoch£º422	 i:7 	 global-step:8447	 l-p:0.056157417595386505
epoch£º422	 i:8 	 global-step:8448	 l-p:0.05665202811360359
epoch£º422	 i:9 	 global-step:8449	 l-p:0.05641745403409004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9112, 27.9115, 27.9112],
        [27.9112, 32.3162, 33.2502],
        [27.9112, 28.0994, 27.9450],
        [27.9112, 30.8305, 30.7785]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.05630950629711151 
model_pd.l_d.mean(): 7.698903209529817e-05 
model_pd.lagr.mean(): 0.056386496871709824 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8159], device='cuda:0')), ('power', tensor([0.1016], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.05630950629711151
epoch£º423	 i:1 	 global-step:8461	 l-p:0.05610708147287369
epoch£º423	 i:2 	 global-step:8462	 l-p:0.05624332278966904
epoch£º423	 i:3 	 global-step:8463	 l-p:0.05614088103175163
epoch£º423	 i:4 	 global-step:8464	 l-p:0.05783679708838463
epoch£º423	 i:5 	 global-step:8465	 l-p:0.05763672664761543
epoch£º423	 i:6 	 global-step:8466	 l-p:0.05618540197610855
epoch£º423	 i:7 	 global-step:8467	 l-p:0.05619212985038757
epoch£º423	 i:8 	 global-step:8468	 l-p:0.05639764666557312
epoch£º423	 i:9 	 global-step:8469	 l-p:0.056514546275138855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8383, 28.9916, 28.4926],
        [27.8383, 27.8586, 27.8393],
        [27.8383, 34.1244, 36.7840],
        [27.8383, 27.8383, 27.8383]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.056263625621795654 
model_pd.l_d.mean(): 4.796469966095174e-06 
model_pd.lagr.mean(): 0.05626842379570007 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8013], device='cuda:0')), ('power', tensor([0.0062], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.056263625621795654
epoch£º424	 i:1 	 global-step:8481	 l-p:0.05788460746407509
epoch£º424	 i:2 	 global-step:8482	 l-p:0.05614255368709564
epoch£º424	 i:3 	 global-step:8483	 l-p:0.056602589786052704
epoch£º424	 i:4 	 global-step:8484	 l-p:0.05622264742851257
epoch£º424	 i:5 	 global-step:8485	 l-p:0.05627574026584625
epoch£º424	 i:6 	 global-step:8486	 l-p:0.05612249672412872
epoch£º424	 i:7 	 global-step:8487	 l-p:0.05625293776392937
epoch£º424	 i:8 	 global-step:8488	 l-p:0.056824568659067154
epoch£º424	 i:9 	 global-step:8489	 l-p:0.057205721735954285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7611, 28.7310, 28.2554],
        [27.7611, 29.8439, 29.4572],
        [27.7611, 33.1362, 34.9195],
        [27.7611, 27.7611, 27.7611]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.05777014046907425 
model_pd.l_d.mean(): 2.1798232410219498e-05 
model_pd.lagr.mean(): 0.05779193714261055 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8046], device='cuda:0')), ('power', tensor([0.0293], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.05777014046907425
epoch£º425	 i:1 	 global-step:8501	 l-p:0.05625851824879646
epoch£º425	 i:2 	 global-step:8502	 l-p:0.05621996894478798
epoch£º425	 i:3 	 global-step:8503	 l-p:0.05623060464859009
epoch£º425	 i:4 	 global-step:8504	 l-p:0.05640525743365288
epoch£º425	 i:5 	 global-step:8505	 l-p:0.05625014752149582
epoch£º425	 i:6 	 global-step:8506	 l-p:0.05677921324968338
epoch£º425	 i:7 	 global-step:8507	 l-p:0.05644172430038452
epoch£º425	 i:8 	 global-step:8508	 l-p:0.056610092520713806
epoch£º425	 i:9 	 global-step:8509	 l-p:0.057074420154094696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6869, 27.7672, 27.6955],
        [27.6869, 27.6927, 27.6870],
        [27.6869, 27.7095, 27.6880],
        [27.6869, 34.7289, 38.2118]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): 0.05792335793375969 
model_pd.l_d.mean(): 6.767433660570532e-05 
model_pd.lagr.mean(): 0.05799103155732155 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7458], device='cuda:0')), ('power', tensor([0.0996], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:0.05792335793375969
epoch£º426	 i:1 	 global-step:8521	 l-p:0.05617373436689377
epoch£º426	 i:2 	 global-step:8522	 l-p:0.05626553297042847
epoch£º426	 i:3 	 global-step:8523	 l-p:0.05616740882396698
epoch£º426	 i:4 	 global-step:8524	 l-p:0.05746961385011673
epoch£º426	 i:5 	 global-step:8525	 l-p:0.056493062525987625
epoch£º426	 i:6 	 global-step:8526	 l-p:0.05629945546388626
epoch£º426	 i:7 	 global-step:8527	 l-p:0.05688051879405975
epoch£º426	 i:8 	 global-step:8528	 l-p:0.056229885667562485
epoch£º426	 i:9 	 global-step:8529	 l-p:0.05636104568839073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6222, 27.6765, 27.6268],
        [27.6222, 27.6455, 27.6234],
        [27.6222, 28.2666, 27.8762],
        [27.6222, 31.1150, 31.4158]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.05708575248718262 
model_pd.l_d.mean(): -0.00013376631250139326 
model_pd.lagr.mean(): 0.05695198476314545 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8094], device='cuda:0')), ('power', tensor([-0.2298], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.05708575248718262
epoch£º427	 i:1 	 global-step:8541	 l-p:0.05630879104137421
epoch£º427	 i:2 	 global-step:8542	 l-p:0.05631617456674576
epoch£º427	 i:3 	 global-step:8543	 l-p:0.05805249139666557
epoch£º427	 i:4 	 global-step:8544	 l-p:0.05702115222811699
epoch£º427	 i:5 	 global-step:8545	 l-p:0.05634447932243347
epoch£º427	 i:6 	 global-step:8546	 l-p:0.056186478585004807
epoch£º427	 i:7 	 global-step:8547	 l-p:0.05619148910045624
epoch£º427	 i:8 	 global-step:8548	 l-p:0.056603867560625076
epoch£º427	 i:9 	 global-step:8549	 l-p:0.05635024979710579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[27.5714, 35.6027, 40.2352],
        [27.5714, 29.6364, 29.2517],
        [27.5714, 34.5468, 37.9750],
        [27.5714, 29.9838, 29.7181]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.056230172514915466 
model_pd.l_d.mean(): -0.00018525400082580745 
model_pd.lagr.mean(): 0.056044917553663254 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8272], device='cuda:0')), ('power', tensor([-0.4069], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.056230172514915466
epoch£º428	 i:1 	 global-step:8561	 l-p:0.05644427239894867
epoch£º428	 i:2 	 global-step:8562	 l-p:0.056183822453022
epoch£º428	 i:3 	 global-step:8563	 l-p:0.057883042842149734
epoch£º428	 i:4 	 global-step:8564	 l-p:0.056470759212970734
epoch£º428	 i:5 	 global-step:8565	 l-p:0.05636785924434662
epoch£º428	 i:6 	 global-step:8566	 l-p:0.05671360716223717
epoch£º428	 i:7 	 global-step:8567	 l-p:0.05620846524834633
epoch£º428	 i:8 	 global-step:8568	 l-p:0.056743744760751724
epoch£º428	 i:9 	 global-step:8569	 l-p:0.05736376717686653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5397, 30.3954, 30.3323],
        [27.5397, 27.5404, 27.5397],
        [27.5397, 33.3131, 35.5017],
        [27.5397, 37.2625, 44.0217]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.05664629861712456 
model_pd.l_d.mean(): -3.972119156969711e-05 
model_pd.lagr.mean(): 0.05660657584667206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7187], device='cuda:0')), ('power', tensor([-0.1292], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.05664629861712456
epoch£º429	 i:1 	 global-step:8581	 l-p:0.05776778981089592
epoch£º429	 i:2 	 global-step:8582	 l-p:0.05675923824310303
epoch£º429	 i:3 	 global-step:8583	 l-p:0.0561392679810524
epoch£º429	 i:4 	 global-step:8584	 l-p:0.05636190250515938
epoch£º429	 i:5 	 global-step:8585	 l-p:0.056168332695961
epoch£º429	 i:6 	 global-step:8586	 l-p:0.05631399154663086
epoch£º429	 i:7 	 global-step:8587	 l-p:0.05795622617006302
epoch£º429	 i:8 	 global-step:8588	 l-p:0.05625467747449875
epoch£º429	 i:9 	 global-step:8589	 l-p:0.05630534142255783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5340, 27.8771, 27.6247],
        [27.5340, 37.2226, 43.9383],
        [27.5340, 28.4413, 27.9797],
        [27.5340, 27.5450, 27.5344]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.056636471301317215 
model_pd.l_d.mean(): -1.7380258213961497e-05 
model_pd.lagr.mean(): 0.05661909282207489 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7079], device='cuda:0')), ('power', tensor([-0.1161], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.056636471301317215
epoch£º430	 i:1 	 global-step:8601	 l-p:0.05660380423069
epoch£º430	 i:2 	 global-step:8602	 l-p:0.0561804473400116
epoch£º430	 i:3 	 global-step:8603	 l-p:0.05625438690185547
epoch£º430	 i:4 	 global-step:8604	 l-p:0.05625355616211891
epoch£º430	 i:5 	 global-step:8605	 l-p:0.056387994438409805
epoch£º430	 i:6 	 global-step:8606	 l-p:0.05793214961886406
epoch£º430	 i:7 	 global-step:8607	 l-p:0.05731751397252083
epoch£º430	 i:8 	 global-step:8608	 l-p:0.05627496540546417
epoch£º430	 i:9 	 global-step:8609	 l-p:0.05681348592042923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5542, 32.7089, 34.3156],
        [27.5542, 37.3350, 44.1677],
        [27.5542, 27.6662, 27.5689],
        [27.5542, 30.1857, 30.0138]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.05639725178480148 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05639725178480148 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7780], device='cuda:0')), ('power', tensor([-0.3052], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.05639725178480148
epoch£º431	 i:1 	 global-step:8621	 l-p:0.05720549076795578
epoch£º431	 i:2 	 global-step:8622	 l-p:0.05662359669804573
epoch£º431	 i:3 	 global-step:8623	 l-p:0.0562964603304863
epoch£º431	 i:4 	 global-step:8624	 l-p:0.05621064081788063
epoch£º431	 i:5 	 global-step:8625	 l-p:0.0563318133354187
epoch£º431	 i:6 	 global-step:8626	 l-p:0.05627472698688507
epoch£º431	 i:7 	 global-step:8627	 l-p:0.058628734201192856
epoch£º431	 i:8 	 global-step:8628	 l-p:0.05629901587963104
epoch£º431	 i:9 	 global-step:8629	 l-p:0.05627763643860817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6000, 36.8794, 43.0486],
        [27.6000, 27.6005, 27.6000],
        [27.6000, 27.7772, 27.6309],
        [27.6000, 27.9630, 27.6993]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.05635903775691986 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05635903775691986 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7893], device='cuda:0')), ('power', tensor([-0.2592], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.05635903775691986
epoch£º432	 i:1 	 global-step:8641	 l-p:0.056125983595848083
epoch£º432	 i:2 	 global-step:8642	 l-p:0.05641632899641991
epoch£º432	 i:3 	 global-step:8643	 l-p:0.05707036703824997
epoch£º432	 i:4 	 global-step:8644	 l-p:0.05675042048096657
epoch£º432	 i:5 	 global-step:8645	 l-p:0.056449174880981445
epoch£º432	 i:6 	 global-step:8646	 l-p:0.05622187629342079
epoch£º432	 i:7 	 global-step:8647	 l-p:0.05672168731689453
epoch£º432	 i:8 	 global-step:8648	 l-p:0.05621842294931412
epoch£º432	 i:9 	 global-step:8649	 l-p:0.05803937464952469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6546, 29.6433, 29.2345],
        [27.6546, 33.4841, 35.7125],
        [27.6546, 27.6546, 27.6546],
        [27.6546, 27.9256, 27.7162]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.05636895075440407 
model_pd.l_d.mean(): -7.680111480112828e-07 
model_pd.lagr.mean(): 0.056368183344602585 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7869], device='cuda:0')), ('power', tensor([-0.1924], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.05636895075440407
epoch£º433	 i:1 	 global-step:8661	 l-p:0.0562351793050766
epoch£º433	 i:2 	 global-step:8662	 l-p:0.05628375709056854
epoch£º433	 i:3 	 global-step:8663	 l-p:0.0562005378305912
epoch£º433	 i:4 	 global-step:8664	 l-p:0.05634887516498566
epoch£º433	 i:5 	 global-step:8665	 l-p:0.05693654716014862
epoch£º433	 i:6 	 global-step:8666	 l-p:0.05766468867659569
epoch£º433	 i:7 	 global-step:8667	 l-p:0.057786379009485245
epoch£º433	 i:8 	 global-step:8668	 l-p:0.05626232177019119
epoch£º433	 i:9 	 global-step:8669	 l-p:0.05610189214348793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[27.7147, 30.8085, 30.8624],
        [27.7147, 34.0308, 36.7385],
        [27.7147, 29.0465, 28.5435],
        [27.7147, 29.6610, 29.2394]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.05610937625169754 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05610937625169754 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8734], device='cuda:0')), ('power', tensor([-0.3156], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.05610937625169754
epoch£º434	 i:1 	 global-step:8681	 l-p:0.056156814098358154
epoch£º434	 i:2 	 global-step:8682	 l-p:0.05791104584932327
epoch£º434	 i:3 	 global-step:8683	 l-p:0.0563691109418869
epoch£º434	 i:4 	 global-step:8684	 l-p:0.056189920753240585
epoch£º434	 i:5 	 global-step:8685	 l-p:0.05666666477918625
epoch£º434	 i:6 	 global-step:8686	 l-p:0.05693049356341362
epoch£º434	 i:7 	 global-step:8687	 l-p:0.056417617946863174
epoch£º434	 i:8 	 global-step:8688	 l-p:0.057040609419345856
epoch£º434	 i:9 	 global-step:8689	 l-p:0.05620557814836502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7772, 27.7784, 27.7772],
        [27.7772, 27.7771, 27.7771],
        [27.7772, 27.7776, 27.7772],
        [27.7772, 36.9487, 42.9420]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.05612703040242195 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05612703040242195 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8594], device='cuda:0')), ('power', tensor([-0.2826], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.05612703040242195
epoch£º435	 i:1 	 global-step:8701	 l-p:0.056180886924266815
epoch£º435	 i:2 	 global-step:8702	 l-p:0.05641644820570946
epoch£º435	 i:3 	 global-step:8703	 l-p:0.05627790838479996
epoch£º435	 i:4 	 global-step:8704	 l-p:0.056380145251750946
epoch£º435	 i:5 	 global-step:8705	 l-p:0.05658895522356033
epoch£º435	 i:6 	 global-step:8706	 l-p:0.05706595256924629
epoch£º435	 i:7 	 global-step:8707	 l-p:0.056225065141916275
epoch£º435	 i:8 	 global-step:8708	 l-p:0.056231915950775146
epoch£º435	 i:9 	 global-step:8709	 l-p:0.058295100927352905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8363, 36.8722, 42.6815],
        [27.8363, 34.9181, 38.4209],
        [27.8363, 27.9071, 27.8433],
        [27.8363, 32.7387, 34.0917]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.05617264285683632 
model_pd.l_d.mean(): -9.107705523092591e-07 
model_pd.lagr.mean(): 0.056171733886003494 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2750e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8193], device='cuda:0')), ('power', tensor([-0.0582], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.05617264285683632
epoch£º436	 i:1 	 global-step:8721	 l-p:0.05612808093428612
epoch£º436	 i:2 	 global-step:8722	 l-p:0.05646397918462753
epoch£º436	 i:3 	 global-step:8723	 l-p:0.05778459459543228
epoch£º436	 i:4 	 global-step:8724	 l-p:0.056413743644952774
epoch£º436	 i:5 	 global-step:8725	 l-p:0.05624966323375702
epoch£º436	 i:6 	 global-step:8726	 l-p:0.05646088346838951
epoch£º436	 i:7 	 global-step:8727	 l-p:0.056132860481739044
epoch£º436	 i:8 	 global-step:8728	 l-p:0.0571233369410038
epoch£º436	 i:9 	 global-step:8729	 l-p:0.05667637661099434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8986, 27.9323, 27.9007],
        [27.8986, 38.2271, 45.7111],
        [27.8986, 33.8235, 36.1134],
        [27.8986, 34.1169, 36.6993]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.05771418288350105 
model_pd.l_d.mean(): 2.7482401492306963e-06 
model_pd.lagr.mean(): 0.05771693214774132 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6548e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8034], device='cuda:0')), ('power', tensor([0.1409], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.05771418288350105
epoch£º437	 i:1 	 global-step:8741	 l-p:0.05632344260811806
epoch£º437	 i:2 	 global-step:8742	 l-p:0.05633936822414398
epoch£º437	 i:3 	 global-step:8743	 l-p:0.05625607445836067
epoch£º437	 i:4 	 global-step:8744	 l-p:0.05614634230732918
epoch£º437	 i:5 	 global-step:8745	 l-p:0.05733850225806236
epoch£º437	 i:6 	 global-step:8746	 l-p:0.05605674535036087
epoch£º437	 i:7 	 global-step:8747	 l-p:0.05609270930290222
epoch£º437	 i:8 	 global-step:8748	 l-p:0.056515276432037354
epoch£º437	 i:9 	 global-step:8749	 l-p:0.05663519352674484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9589, 29.9241, 29.4989],
        [27.9589, 32.2087, 33.0174],
        [27.9589, 28.2330, 28.0211],
        [27.9589, 27.9589, 27.9589]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.05772193893790245 
model_pd.l_d.mean(): 1.0511582331673708e-05 
model_pd.lagr.mean(): 0.05773245170712471 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.3677e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8075], device='cuda:0')), ('power', tensor([0.1949], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.05772193893790245
epoch£º438	 i:1 	 global-step:8761	 l-p:0.05618681758642197
epoch£º438	 i:2 	 global-step:8762	 l-p:0.05706586688756943
epoch£º438	 i:3 	 global-step:8763	 l-p:0.056337855756282806
epoch£º438	 i:4 	 global-step:8764	 l-p:0.05625559762120247
epoch£º438	 i:5 	 global-step:8765	 l-p:0.05609235540032387
epoch£º438	 i:6 	 global-step:8766	 l-p:0.05623988062143326
epoch£º438	 i:7 	 global-step:8767	 l-p:0.05613706633448601
epoch£º438	 i:8 	 global-step:8768	 l-p:0.05702214688062668
epoch£º438	 i:9 	 global-step:8769	 l-p:0.056178875267505646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0126, 30.8471, 30.7465],
        [28.0126, 31.2424, 31.3546],
        [28.0126, 31.3071, 31.4574],
        [28.0126, 28.0146, 28.0127]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.05606287345290184 
model_pd.l_d.mean(): -5.888287887501065e-06 
model_pd.lagr.mean(): 0.05605698376893997 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8724], device='cuda:0')), ('power', tensor([-0.0505], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.05606287345290184
epoch£º439	 i:1 	 global-step:8781	 l-p:0.057461898773908615
epoch£º439	 i:2 	 global-step:8782	 l-p:0.056048955768346786
epoch£º439	 i:3 	 global-step:8783	 l-p:0.05629093945026398
epoch£º439	 i:4 	 global-step:8784	 l-p:0.05626054108142853
epoch£º439	 i:5 	 global-step:8785	 l-p:0.05620324984192848
epoch£º439	 i:6 	 global-step:8786	 l-p:0.056177448481321335
epoch£º439	 i:7 	 global-step:8787	 l-p:0.056613195687532425
epoch£º439	 i:8 	 global-step:8788	 l-p:0.057872600853443146
epoch£º439	 i:9 	 global-step:8789	 l-p:0.056087616831064224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0539, 28.3290, 28.1164],
        [28.0539, 28.2657, 28.0947],
        [28.0539, 29.4666, 28.9586],
        [28.0539, 33.7409, 35.7808]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.05764345824718475 
model_pd.l_d.mean(): 6.413809023797512e-05 
model_pd.lagr.mean(): 0.05770759657025337 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7980], device='cuda:0')), ('power', tensor([0.3163], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.05764345824718475
epoch£º440	 i:1 	 global-step:8801	 l-p:0.05710854008793831
epoch£º440	 i:2 	 global-step:8802	 l-p:0.056608896702528
epoch£º440	 i:3 	 global-step:8803	 l-p:0.05638378858566284
epoch£º440	 i:4 	 global-step:8804	 l-p:0.05602160468697548
epoch£º440	 i:5 	 global-step:8805	 l-p:0.056156329810619354
epoch£º440	 i:6 	 global-step:8806	 l-p:0.0563218779861927
epoch£º440	 i:7 	 global-step:8807	 l-p:0.056453555822372437
epoch£º440	 i:8 	 global-step:8808	 l-p:0.05601932480931282
epoch£º440	 i:9 	 global-step:8809	 l-p:0.056253962218761444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0842, 28.1557, 28.0912],
        [28.0842, 33.7705, 35.8060],
        [28.0842, 28.0909, 28.0843],
        [28.0842, 30.6036, 30.3569]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.05685041472315788 
model_pd.l_d.mean(): 0.00012907321797683835 
model_pd.lagr.mean(): 0.05697948858141899 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7604], device='cuda:0')), ('power', tensor([0.4198], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.05685041472315788
epoch£º441	 i:1 	 global-step:8821	 l-p:0.05700875446200371
epoch£º441	 i:2 	 global-step:8822	 l-p:0.056213635951280594
epoch£º441	 i:3 	 global-step:8823	 l-p:0.05632404610514641
epoch£º441	 i:4 	 global-step:8824	 l-p:0.0561562143266201
epoch£º441	 i:5 	 global-step:8825	 l-p:0.05604353919625282
epoch£º441	 i:6 	 global-step:8826	 l-p:0.05605955794453621
epoch£º441	 i:7 	 global-step:8827	 l-p:0.05627508834004402
epoch£º441	 i:8 	 global-step:8828	 l-p:0.05605127289891243
epoch£º441	 i:9 	 global-step:8829	 l-p:0.05792409926652908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0900, 28.1030, 28.0904],
        [28.0900, 35.7766, 39.9151],
        [28.0900, 32.2704, 33.0145],
        [28.0900, 28.2917, 28.1277]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.05624833330512047 
model_pd.l_d.mean(): 6.260220106923953e-05 
model_pd.lagr.mean(): 0.05631093680858612 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8435], device='cuda:0')), ('power', tensor([0.1486], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.05624833330512047
epoch£º442	 i:1 	 global-step:8841	 l-p:0.056001339107751846
epoch£º442	 i:2 	 global-step:8842	 l-p:0.056128863245248795
epoch£º442	 i:3 	 global-step:8843	 l-p:0.056737422943115234
epoch£º442	 i:4 	 global-step:8844	 l-p:0.05612746253609657
epoch£º442	 i:5 	 global-step:8845	 l-p:0.0562264509499073
epoch£º442	 i:6 	 global-step:8846	 l-p:0.05661414563655853
epoch£º442	 i:7 	 global-step:8847	 l-p:0.0584353543817997
epoch£º442	 i:8 	 global-step:8848	 l-p:0.05614978075027466
epoch£º442	 i:9 	 global-step:8849	 l-p:0.05624832957983017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0776, 28.5092, 28.2079],
        [28.0776, 28.0835, 28.0777],
        [28.0776, 28.0860, 28.0778],
        [28.0776, 28.7036, 28.3171]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.056448034942150116 
model_pd.l_d.mean(): 9.538239100947976e-05 
model_pd.lagr.mean(): 0.056543417274951935 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8338], device='cuda:0')), ('power', tensor([0.1787], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.056448034942150116
epoch£º443	 i:1 	 global-step:8861	 l-p:0.056097812950611115
epoch£º443	 i:2 	 global-step:8862	 l-p:0.056451208889484406
epoch£º443	 i:3 	 global-step:8863	 l-p:0.05765579640865326
epoch£º443	 i:4 	 global-step:8864	 l-p:0.056148093193769455
epoch£º443	 i:5 	 global-step:8865	 l-p:0.05613204836845398
epoch£º443	 i:6 	 global-step:8866	 l-p:0.056141555309295654
epoch£º443	 i:7 	 global-step:8867	 l-p:0.056213609874248505
epoch£º443	 i:8 	 global-step:8868	 l-p:0.05703475698828697
epoch£º443	 i:9 	 global-step:8869	 l-p:0.05665743723511696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0452, 33.5889, 35.4947],
        [28.0452, 28.6522, 28.2732],
        [28.0452, 29.8912, 29.4362],
        [28.0452, 32.9864, 34.3503]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 0.05777280777692795 
model_pd.l_d.mean(): 0.0002773783926386386 
model_pd.lagr.mean(): 0.058050185441970825 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7646], device='cuda:0')), ('power', tensor([0.4361], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:0.05777280777692795
epoch£º444	 i:1 	 global-step:8881	 l-p:0.05606789514422417
epoch£º444	 i:2 	 global-step:8882	 l-p:0.056067273020744324
epoch£º444	 i:3 	 global-step:8883	 l-p:0.056129805743694305
epoch£º444	 i:4 	 global-step:8884	 l-p:0.056684594601392746
epoch£º444	 i:5 	 global-step:8885	 l-p:0.05614437535405159
epoch£º444	 i:6 	 global-step:8886	 l-p:0.05629248544573784
epoch£º444	 i:7 	 global-step:8887	 l-p:0.05661694332957268
epoch£º444	 i:8 	 global-step:8888	 l-p:0.05704062432050705
epoch£º444	 i:9 	 global-step:8889	 l-p:0.056287992745637894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9925, 27.9958, 27.9925],
        [27.9925, 30.3468, 30.0401],
        [27.9925, 28.0857, 28.0034],
        [27.9925, 29.1234, 28.6239]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): 0.05611211806535721 
model_pd.l_d.mean(): 1.2695725672529079e-05 
model_pd.lagr.mean(): 0.056124813854694366 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8496], device='cuda:0')), ('power', tensor([0.0177], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:0.05611211806535721
epoch£º445	 i:1 	 global-step:8901	 l-p:0.056290123611688614
epoch£º445	 i:2 	 global-step:8902	 l-p:0.056835465133190155
epoch£º445	 i:3 	 global-step:8903	 l-p:0.05716264620423317
epoch£º445	 i:4 	 global-step:8904	 l-p:0.05610867217183113
epoch£º445	 i:5 	 global-step:8905	 l-p:0.05615460127592087
epoch£º445	 i:6 	 global-step:8906	 l-p:0.05772988125681877
epoch£º445	 i:7 	 global-step:8907	 l-p:0.05611807852983475
epoch£º445	 i:8 	 global-step:8908	 l-p:0.05621963366866112
epoch£º445	 i:9 	 global-step:8909	 l-p:0.05657343566417694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[27.9207, 30.2567, 29.9465],
        [27.9207, 36.2235, 41.1147],
        [27.9207, 31.1393, 31.2510],
        [27.9207, 36.4197, 41.5479]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.05626830831170082 
model_pd.l_d.mean(): 3.67513312085066e-05 
model_pd.lagr.mean(): 0.05630505830049515 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8070], device='cuda:0')), ('power', tensor([0.0478], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.05626830831170082
epoch£º446	 i:1 	 global-step:8921	 l-p:0.056569259613752365
epoch£º446	 i:2 	 global-step:8922	 l-p:0.056265056133270264
epoch£º446	 i:3 	 global-step:8923	 l-p:0.05616098269820213
epoch£º446	 i:4 	 global-step:8924	 l-p:0.05776713415980339
epoch£º446	 i:5 	 global-step:8925	 l-p:0.056348055601119995
epoch£º446	 i:6 	 global-step:8926	 l-p:0.056054770946502686
epoch£º446	 i:7 	 global-step:8927	 l-p:0.05778145790100098
epoch£º446	 i:8 	 global-step:8928	 l-p:0.05618990957736969
epoch£º446	 i:9 	 global-step:8929	 l-p:0.0561438612639904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8371, 27.8783, 27.8401],
        [27.8371, 28.2604, 27.9640],
        [27.8371, 30.4971, 30.3235],
        [27.8371, 30.5685, 30.4279]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.056225765496492386 
model_pd.l_d.mean(): -5.927994425292127e-05 
model_pd.lagr.mean(): 0.05616648495197296 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8193], device='cuda:0')), ('power', tensor([-0.0758], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.056225765496492386
epoch£º447	 i:1 	 global-step:8941	 l-p:0.05665576830506325
epoch£º447	 i:2 	 global-step:8942	 l-p:0.05672967806458473
epoch£º447	 i:3 	 global-step:8943	 l-p:0.05614236742258072
epoch£º447	 i:4 	 global-step:8944	 l-p:0.05621035397052765
epoch£º447	 i:5 	 global-step:8945	 l-p:0.05796089395880699
epoch£º447	 i:6 	 global-step:8946	 l-p:0.05619942396879196
epoch£º447	 i:7 	 global-step:8947	 l-p:0.05649486929178238
epoch£º447	 i:8 	 global-step:8948	 l-p:0.05703539401292801
epoch£º447	 i:9 	 global-step:8949	 l-p:0.056171052157878876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7483, 36.9099, 42.8967],
        [27.7483, 35.5383, 39.8557],
        [27.7483, 28.0394, 27.8173],
        [27.7483, 27.7489, 27.7483]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.056482162326574326 
model_pd.l_d.mean(): -2.6761088520288467e-05 
model_pd.lagr.mean(): 0.056455399841070175 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7745], device='cuda:0')), ('power', tensor([-0.0355], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.056482162326574326
epoch£º448	 i:1 	 global-step:8961	 l-p:0.056430384516716
epoch£º448	 i:2 	 global-step:8962	 l-p:0.057810552418231964
epoch£º448	 i:3 	 global-step:8963	 l-p:0.0566084124147892
epoch£º448	 i:4 	 global-step:8964	 l-p:0.05706039443612099
epoch£º448	 i:5 	 global-step:8965	 l-p:0.05639025196433067
epoch£º448	 i:6 	 global-step:8966	 l-p:0.05612662807106972
epoch£º448	 i:7 	 global-step:8967	 l-p:0.056779440492391586
epoch£º448	 i:8 	 global-step:8968	 l-p:0.05626998469233513
epoch£º448	 i:9 	 global-step:8969	 l-p:0.05613575503230095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6636, 27.6804, 27.6643],
        [27.6636, 29.1065, 28.6082],
        [27.6636, 28.0912, 27.7932],
        [27.6636, 28.2202, 27.8632]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.05615827813744545 
model_pd.l_d.mean(): -0.0002247039374196902 
model_pd.lagr.mean(): 0.055933572351932526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8452], device='cuda:0')), ('power', tensor([-0.3298], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.05615827813744545
epoch£º449	 i:1 	 global-step:8981	 l-p:0.056171711534261703
epoch£º449	 i:2 	 global-step:8982	 l-p:0.05614723265171051
epoch£º449	 i:3 	 global-step:8983	 l-p:0.05639640986919403
epoch£º449	 i:4 	 global-step:8984	 l-p:0.05716647952795029
epoch£º449	 i:5 	 global-step:8985	 l-p:0.05634218081831932
epoch£º449	 i:6 	 global-step:8986	 l-p:0.05722792446613312
epoch£º449	 i:7 	 global-step:8987	 l-p:0.05797938257455826
epoch£º449	 i:8 	 global-step:8988	 l-p:0.05652522295713425
epoch£º449	 i:9 	 global-step:8989	 l-p:0.05626662075519562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5873, 27.5905, 27.5873],
        [27.5873, 27.5885, 27.5873],
        [27.5873, 27.6126, 27.5886],
        [27.5873, 34.2241, 37.2852]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.05619955062866211 
model_pd.l_d.mean(): -0.0002112676011165604 
model_pd.lagr.mean(): 0.05598828196525574 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8369], device='cuda:0')), ('power', tensor([-0.3714], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.05619955062866211
epoch£º450	 i:1 	 global-step:9001	 l-p:0.05617990344762802
epoch£º450	 i:2 	 global-step:9002	 l-p:0.056635502725839615
epoch£º450	 i:3 	 global-step:9003	 l-p:0.05724781006574631
epoch£º450	 i:4 	 global-step:9004	 l-p:0.056265249848365784
epoch£º450	 i:5 	 global-step:9005	 l-p:0.0567772276699543
epoch£º450	 i:6 	 global-step:9006	 l-p:0.058116666972637177
epoch£º450	 i:7 	 global-step:9007	 l-p:0.056141093373298645
epoch£º450	 i:8 	 global-step:9008	 l-p:0.05634619668126106
epoch£º450	 i:9 	 global-step:9009	 l-p:0.05668332800269127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5334, 27.7498, 27.5762],
        [27.5334, 28.1756, 27.7865],
        [27.5334, 27.5340, 27.5334],
        [27.5334, 27.5353, 27.5334]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.05746692419052124 
model_pd.l_d.mean(): -2.888443304982502e-06 
model_pd.lagr.mean(): 0.05746403709053993 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7086], device='cuda:0')), ('power', tensor([-0.0068], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.05746692419052124
epoch£º451	 i:1 	 global-step:9021	 l-p:0.0563262403011322
epoch£º451	 i:2 	 global-step:9022	 l-p:0.058048639446496964
epoch£º451	 i:3 	 global-step:9023	 l-p:0.05619559809565544
epoch£º451	 i:4 	 global-step:9024	 l-p:0.05623418837785721
epoch£º451	 i:5 	 global-step:9025	 l-p:0.05635296553373337
epoch£º451	 i:6 	 global-step:9026	 l-p:0.057200413197278976
epoch£º451	 i:7 	 global-step:9027	 l-p:0.05626680329442024
epoch£º451	 i:8 	 global-step:9028	 l-p:0.05618522688746452
epoch£º451	 i:9 	 global-step:9029	 l-p:0.05644313618540764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5068, 28.3609, 27.9108],
        [27.5068, 27.5174, 27.5071],
        [27.5068, 27.5286, 27.5079],
        [27.5068, 27.5068, 27.5067]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.05641207471489906 
model_pd.l_d.mean(): -8.747481479076669e-05 
model_pd.lagr.mean(): 0.05632460117340088 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7775], device='cuda:0')), ('power', tensor([-0.3384], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.05641207471489906
epoch£º452	 i:1 	 global-step:9041	 l-p:0.05620083212852478
epoch£º452	 i:2 	 global-step:9042	 l-p:0.05714103579521179
epoch£º452	 i:3 	 global-step:9043	 l-p:0.05617300048470497
epoch£º452	 i:4 	 global-step:9044	 l-p:0.05675477534532547
epoch£º452	 i:5 	 global-step:9045	 l-p:0.05639425292611122
epoch£º452	 i:6 	 global-step:9046	 l-p:0.058939699083566666
epoch£º452	 i:7 	 global-step:9047	 l-p:0.05623002350330353
epoch£º452	 i:8 	 global-step:9048	 l-p:0.05621734634041786
epoch£º452	 i:9 	 global-step:9049	 l-p:0.05631539225578308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5073, 27.5183, 27.5077],
        [27.5073, 27.5073, 27.5073],
        [27.5073, 27.5240, 27.5080],
        [27.5073, 27.9098, 27.6251]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.05621108412742615 
model_pd.l_d.mean(): -4.077531411894597e-05 
model_pd.lagr.mean(): 0.05617030709981918 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.1579e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8357], device='cuda:0')), ('power', tensor([-0.4772], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.05621108412742615
epoch£º453	 i:1 	 global-step:9061	 l-p:0.05801702290773392
epoch£º453	 i:2 	 global-step:9062	 l-p:0.05721501633524895
epoch£º453	 i:3 	 global-step:9063	 l-p:0.05734347179532051
epoch£º453	 i:4 	 global-step:9064	 l-p:0.05625636875629425
epoch£º453	 i:5 	 global-step:9065	 l-p:0.05631311610341072
epoch£º453	 i:6 	 global-step:9066	 l-p:0.05648532882332802
epoch£º453	 i:7 	 global-step:9067	 l-p:0.05657939240336418
epoch£º453	 i:8 	 global-step:9068	 l-p:0.056144941598176956
epoch£º453	 i:9 	 global-step:9069	 l-p:0.05615837126970291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5443, 28.6690, 28.1768],
        [27.5443, 32.8753, 34.6438],
        [27.5443, 27.5668, 27.5455],
        [27.5443, 27.6502, 27.5578]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.05634095519781113 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05634095519781113 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8197], device='cuda:0')), ('power', tensor([-0.4431], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.05634095519781113
epoch£º454	 i:1 	 global-step:9081	 l-p:0.05683223530650139
epoch£º454	 i:2 	 global-step:9082	 l-p:0.05708134174346924
epoch£º454	 i:3 	 global-step:9083	 l-p:0.05620366334915161
epoch£º454	 i:4 	 global-step:9084	 l-p:0.05790211632847786
epoch£º454	 i:5 	 global-step:9085	 l-p:0.05635838210582733
epoch£º454	 i:6 	 global-step:9086	 l-p:0.05631360784173012
epoch£º454	 i:7 	 global-step:9087	 l-p:0.05684326961636543
epoch£º454	 i:8 	 global-step:9088	 l-p:0.05634678155183792
epoch£º454	 i:9 	 global-step:9089	 l-p:0.05634534731507301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6003, 29.9479, 29.6560],
        [27.6003, 27.6042, 27.6004],
        [27.6003, 27.6016, 27.6003],
        [27.6003, 27.6003, 27.6003]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.056350596249103546 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056350596249103546 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7939], device='cuda:0')), ('power', tensor([-0.2735], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.056350596249103546
epoch£º455	 i:1 	 global-step:9101	 l-p:0.05807279422879219
epoch£º455	 i:2 	 global-step:9102	 l-p:0.056435585021972656
epoch£º455	 i:3 	 global-step:9103	 l-p:0.056189507246017456
epoch£º455	 i:4 	 global-step:9104	 l-p:0.05625717341899872
epoch£º455	 i:5 	 global-step:9105	 l-p:0.05626349151134491
epoch£º455	 i:6 	 global-step:9106	 l-p:0.056133415549993515
epoch£º455	 i:7 	 global-step:9107	 l-p:0.056315794587135315
epoch£º455	 i:8 	 global-step:9108	 l-p:0.05801958963274956
epoch£º455	 i:9 	 global-step:9109	 l-p:0.05632712319493294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6633, 27.6633, 27.6633],
        [27.6633, 28.5079, 28.0585],
        [27.6633, 27.6673, 27.6634],
        [27.6633, 27.6853, 27.6644]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.05629340931773186 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05629340931773186 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8156], device='cuda:0')), ('power', tensor([-0.2913], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.05629340931773186
epoch£º456	 i:1 	 global-step:9121	 l-p:0.057831596583127975
epoch£º456	 i:2 	 global-step:9122	 l-p:0.05715218931436539
epoch£º456	 i:3 	 global-step:9123	 l-p:0.05687263235449791
epoch£º456	 i:4 	 global-step:9124	 l-p:0.05668709799647331
epoch£º456	 i:5 	 global-step:9125	 l-p:0.05616559833288193
epoch£º456	 i:6 	 global-step:9126	 l-p:0.05650866776704788
epoch£º456	 i:7 	 global-step:9127	 l-p:0.05616116523742676
epoch£º456	 i:8 	 global-step:9128	 l-p:0.05631795898079872
epoch£º456	 i:9 	 global-step:9129	 l-p:0.05617208778858185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7331, 33.8652, 36.3832],
        [27.7331, 28.7020, 28.2269],
        [27.7331, 31.9753, 32.7989],
        [27.7331, 27.7331, 27.7331]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.0561177060008049 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0561177060008049 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8645], device='cuda:0')), ('power', tensor([-0.2820], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.0561177060008049
epoch£º457	 i:1 	 global-step:9141	 l-p:0.056710127741098404
epoch£º457	 i:2 	 global-step:9142	 l-p:0.056277234107255936
epoch£º457	 i:3 	 global-step:9143	 l-p:0.05623622238636017
epoch£º457	 i:4 	 global-step:9144	 l-p:0.05626271292567253
epoch£º457	 i:5 	 global-step:9145	 l-p:0.0567520335316658
epoch£º457	 i:6 	 global-step:9146	 l-p:0.05713927745819092
epoch£º457	 i:7 	 global-step:9147	 l-p:0.0562877394258976
epoch£º457	 i:8 	 global-step:9148	 l-p:0.056090906262397766
epoch£º457	 i:9 	 global-step:9149	 l-p:0.058049626648426056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7979, 27.7984, 27.7979],
        [27.7979, 28.2628, 27.9460],
        [27.7979, 27.7979, 27.7979],
        [27.7979, 27.7986, 27.7979]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.05612696707248688 
model_pd.l_d.mean(): -2.373598135818611e-06 
model_pd.lagr.mean(): 0.05612459406256676 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.0977e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8496], device='cuda:0')), ('power', tensor([-0.2072], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.05612696707248688
epoch£º458	 i:1 	 global-step:9161	 l-p:0.056683458387851715
epoch£º458	 i:2 	 global-step:9162	 l-p:0.056386783719062805
epoch£º458	 i:3 	 global-step:9163	 l-p:0.05609352886676788
epoch£º458	 i:4 	 global-step:9164	 l-p:0.05653131380677223
epoch£º458	 i:5 	 global-step:9165	 l-p:0.05638521909713745
epoch£º458	 i:6 	 global-step:9166	 l-p:0.05618166923522949
epoch£º458	 i:7 	 global-step:9167	 l-p:0.05784020200371742
epoch£º458	 i:8 	 global-step:9168	 l-p:0.05625500902533531
epoch£º458	 i:9 	 global-step:9169	 l-p:0.057229384779930115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8660, 27.8660, 27.8660],
        [27.8660, 33.2795, 35.0859],
        [27.8660, 38.0639, 45.3796],
        [27.8660, 27.8662, 27.8660]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.056464068591594696 
model_pd.l_d.mean(): -1.4590987120755017e-06 
model_pd.lagr.mean(): 0.0564626082777977 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.9710e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8397], device='cuda:0')), ('power', tensor([-0.0637], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.056464068591594696
epoch£º459	 i:1 	 global-step:9181	 l-p:0.05613020807504654
epoch£º459	 i:2 	 global-step:9182	 l-p:0.05616233870387077
epoch£º459	 i:3 	 global-step:9183	 l-p:0.056408319622278214
epoch£º459	 i:4 	 global-step:9184	 l-p:0.057163067162036896
epoch£º459	 i:5 	 global-step:9185	 l-p:0.057788483798503876
epoch£º459	 i:6 	 global-step:9186	 l-p:0.05609497055411339
epoch£º459	 i:7 	 global-step:9187	 l-p:0.0567004531621933
epoch£º459	 i:8 	 global-step:9188	 l-p:0.056256115436553955
epoch£º459	 i:9 	 global-step:9189	 l-p:0.05633597820997238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9345, 28.0156, 27.9432],
        [27.9345, 30.2837, 29.9777],
        [27.9345, 37.7706, 44.5890],
        [27.9345, 32.3435, 33.2783]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.05788803845643997 
model_pd.l_d.mean(): 1.5874273231020197e-05 
model_pd.lagr.mean(): 0.05790391191840172 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.1282e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7355], device='cuda:0')), ('power', tensor([0.3719], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.05788803845643997
epoch£º460	 i:1 	 global-step:9201	 l-p:0.056119244545698166
epoch£º460	 i:2 	 global-step:9202	 l-p:0.05612102895975113
epoch£º460	 i:3 	 global-step:9203	 l-p:0.05620511993765831
epoch£º460	 i:4 	 global-step:9204	 l-p:0.05680055171251297
epoch£º460	 i:5 	 global-step:9205	 l-p:0.056043412536382675
epoch£º460	 i:6 	 global-step:9206	 l-p:0.057049017399549484
epoch£º460	 i:7 	 global-step:9207	 l-p:0.05636610463261604
epoch£º460	 i:8 	 global-step:9208	 l-p:0.056537747383117676
epoch£º460	 i:9 	 global-step:9209	 l-p:0.05617045611143112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9981, 28.0138, 27.9987],
        [27.9981, 30.2340, 29.8857],
        [27.9981, 28.4950, 28.1625],
        [27.9981, 33.2401, 34.8745]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.05809129402041435 
model_pd.l_d.mean(): 4.317048660595901e-05 
model_pd.lagr.mean(): 0.05813446268439293 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7438], device='cuda:0')), ('power', tensor([0.4519], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.05809129402041435
epoch£º461	 i:1 	 global-step:9221	 l-p:0.056220922619104385
epoch£º461	 i:2 	 global-step:9222	 l-p:0.056058257818222046
epoch£º461	 i:3 	 global-step:9223	 l-p:0.056670911610126495
epoch£º461	 i:4 	 global-step:9224	 l-p:0.05695156008005142
epoch£º461	 i:5 	 global-step:9225	 l-p:0.05628453940153122
epoch£º461	 i:6 	 global-step:9226	 l-p:0.05609734728932381
epoch£º461	 i:7 	 global-step:9227	 l-p:0.05607650801539421
epoch£º461	 i:8 	 global-step:9228	 l-p:0.05628557875752449
epoch£º461	 i:9 	 global-step:9229	 l-p:0.05638015642762184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0510, 31.4492, 31.6595],
        [28.0510, 30.8412, 30.7170],
        [28.0510, 28.4850, 28.1826],
        [28.0510, 28.4778, 28.1790]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.05614697188138962 
model_pd.l_d.mean(): 2.10012512980029e-05 
model_pd.lagr.mean(): 0.05616797134280205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8383], device='cuda:0')), ('power', tensor([0.1183], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.05614697188138962
epoch£º462	 i:1 	 global-step:9241	 l-p:0.056920621544122696
epoch£º462	 i:2 	 global-step:9242	 l-p:0.056187376379966736
epoch£º462	 i:3 	 global-step:9243	 l-p:0.05616217479109764
epoch£º462	 i:4 	 global-step:9244	 l-p:0.056200385093688965
epoch£º462	 i:5 	 global-step:9245	 l-p:0.056561630219221115
epoch£º462	 i:6 	 global-step:9246	 l-p:0.057705219835042953
epoch£º462	 i:7 	 global-step:9247	 l-p:0.05658517777919769
epoch£º462	 i:8 	 global-step:9248	 l-p:0.05624360963702202
epoch£º462	 i:9 	 global-step:9249	 l-p:0.0562569834291935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0859, 28.0980, 28.0864],
        [28.0859, 28.1795, 28.0969],
        [28.0859, 28.1688, 28.0949],
        [28.0859, 28.0871, 28.0859]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.05763813853263855 
model_pd.l_d.mean(): 0.00017922102415468544 
model_pd.lagr.mean(): 0.057817358523607254 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7060], device='cuda:0')), ('power', tensor([0.6373], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.05763813853263855
epoch£º463	 i:1 	 global-step:9261	 l-p:0.05638386681675911
epoch£º463	 i:2 	 global-step:9262	 l-p:0.056369293481111526
epoch£º463	 i:3 	 global-step:9263	 l-p:0.056034669280052185
epoch£º463	 i:4 	 global-step:9264	 l-p:0.05611173063516617
epoch£º463	 i:5 	 global-step:9265	 l-p:0.057554926723241806
epoch£º463	 i:6 	 global-step:9266	 l-p:0.05641794204711914
epoch£º463	 i:7 	 global-step:9267	 l-p:0.0560946986079216
epoch£º463	 i:8 	 global-step:9268	 l-p:0.05614662170410156
epoch£º463	 i:9 	 global-step:9269	 l-p:0.056136321276426315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1013, 28.1304, 28.1030],
        [28.1013, 33.1138, 34.5333],
        [28.1013, 34.2586, 36.7516],
        [28.1013, 28.1013, 28.1013]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.056092314422130585 
model_pd.l_d.mean(): 4.4358766899676993e-05 
model_pd.lagr.mean(): 0.05613667145371437 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8510], device='cuda:0')), ('power', tensor([0.1114], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.056092314422130585
epoch£º464	 i:1 	 global-step:9281	 l-p:0.05615251511335373
epoch£º464	 i:2 	 global-step:9282	 l-p:0.056201107800006866
epoch£º464	 i:3 	 global-step:9283	 l-p:0.05617726221680641
epoch£º464	 i:4 	 global-step:9284	 l-p:0.056049738079309464
epoch£º464	 i:5 	 global-step:9285	 l-p:0.057148199528455734
epoch£º464	 i:6 	 global-step:9286	 l-p:0.05650255084037781
epoch£º464	 i:7 	 global-step:9287	 l-p:0.05617633834481239
epoch£º464	 i:8 	 global-step:9288	 l-p:0.056111983954906464
epoch£º464	 i:9 	 global-step:9289	 l-p:0.058270301669836044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0890, 28.0890, 28.0890],
        [28.0890, 28.1003, 28.0894],
        [28.0890, 28.2776, 28.1228],
        [28.0890, 29.9703, 29.5213]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.05621741712093353 
model_pd.l_d.mean(): 0.00012287004210520536 
model_pd.lagr.mean(): 0.0563402883708477 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8033], device='cuda:0')), ('power', tensor([0.2379], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.05621741712093353
epoch£º465	 i:1 	 global-step:9301	 l-p:0.05621806159615517
epoch£º465	 i:2 	 global-step:9302	 l-p:0.05603186413645744
epoch£º465	 i:3 	 global-step:9303	 l-p:0.05624372512102127
epoch£º465	 i:4 	 global-step:9304	 l-p:0.05601508915424347
epoch£º465	 i:5 	 global-step:9305	 l-p:0.05623633787035942
epoch£º465	 i:6 	 global-step:9306	 l-p:0.05612165480852127
epoch£º465	 i:7 	 global-step:9307	 l-p:0.057590894401073456
epoch£º465	 i:8 	 global-step:9308	 l-p:0.056408852338790894
epoch£º465	 i:9 	 global-step:9309	 l-p:0.05786910280585289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0557, 29.4411, 28.9323],
        [28.0557, 29.9345, 29.4861],
        [28.0557, 28.1924, 28.0758],
        [28.0557, 32.1211, 32.7828]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.057812001556158066 
model_pd.l_d.mean(): 0.0002807931159622967 
model_pd.lagr.mean(): 0.05809279531240463 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7738], device='cuda:0')), ('power', tensor([0.4499], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.057812001556158066
epoch£º466	 i:1 	 global-step:9321	 l-p:0.05657271668314934
epoch£º466	 i:2 	 global-step:9322	 l-p:0.057187557220458984
epoch£º466	 i:3 	 global-step:9323	 l-p:0.056037407368421555
epoch£º466	 i:4 	 global-step:9324	 l-p:0.05611834302544594
epoch£º466	 i:5 	 global-step:9325	 l-p:0.05624206364154816
epoch£º466	 i:6 	 global-step:9326	 l-p:0.05604276433587074
epoch£º466	 i:7 	 global-step:9327	 l-p:0.05602794513106346
epoch£º466	 i:8 	 global-step:9328	 l-p:0.05669696256518364
epoch£º466	 i:9 	 global-step:9329	 l-p:0.05632595717906952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0031, 28.0035, 28.0031],
        [28.0031, 33.6071, 35.5747],
        [28.0031, 28.2209, 28.0459],
        [28.0031, 29.6563, 29.1700]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.05621470510959625 
model_pd.l_d.mean(): 6.200112693477422e-05 
model_pd.lagr.mean(): 0.056276705116033554 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8268], device='cuda:0')), ('power', tensor([0.0871], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.05621470510959625
epoch£º467	 i:1 	 global-step:9341	 l-p:0.05653960257768631
epoch£º467	 i:2 	 global-step:9342	 l-p:0.05648679658770561
epoch£º467	 i:3 	 global-step:9343	 l-p:0.056733567267656326
epoch£º467	 i:4 	 global-step:9344	 l-p:0.05709988623857498
epoch£º467	 i:5 	 global-step:9345	 l-p:0.05775358900427818
epoch£º467	 i:6 	 global-step:9346	 l-p:0.056044723838567734
epoch£º467	 i:7 	 global-step:9347	 l-p:0.05608166754245758
epoch£º467	 i:8 	 global-step:9348	 l-p:0.056179266422986984
epoch£º467	 i:9 	 global-step:9349	 l-p:0.05614243075251579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9240, 28.2720, 28.0159],
        [27.9240, 33.5752, 35.5973],
        [27.9240, 28.5742, 28.1799],
        [27.9240, 37.5516, 44.0997]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.056144170463085175 
model_pd.l_d.mean(): -1.57880404003663e-05 
model_pd.lagr.mean(): 0.05612838268280029 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8436], device='cuda:0')), ('power', tensor([-0.0206], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.056144170463085175
epoch£º468	 i:1 	 global-step:9361	 l-p:0.05680762976408005
epoch£º468	 i:2 	 global-step:9362	 l-p:0.056754644960165024
epoch£º468	 i:3 	 global-step:9363	 l-p:0.05772816017270088
epoch£º468	 i:4 	 global-step:9364	 l-p:0.05633019655942917
epoch£º468	 i:5 	 global-step:9365	 l-p:0.05626514181494713
epoch£º468	 i:6 	 global-step:9366	 l-p:0.056063517928123474
epoch£º468	 i:7 	 global-step:9367	 l-p:0.056983426213264465
epoch£º468	 i:8 	 global-step:9368	 l-p:0.05617624148726463
epoch£º468	 i:9 	 global-step:9369	 l-p:0.05629248544573784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8298, 28.2952, 27.9781],
        [27.8298, 27.8297, 27.8297],
        [27.8298, 27.8525, 27.8309],
        [27.8298, 27.8298, 27.8298]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.05621607229113579 
model_pd.l_d.mean(): -0.00014732715499121696 
model_pd.lagr.mean(): 0.056068744510412216 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8279], device='cuda:0')), ('power', tensor([-0.1890], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.05621607229113579
epoch£º469	 i:1 	 global-step:9381	 l-p:0.05614307150244713
epoch£º469	 i:2 	 global-step:9382	 l-p:0.056815557181835175
epoch£º469	 i:3 	 global-step:9383	 l-p:0.056326329708099365
epoch£º469	 i:4 	 global-step:9384	 l-p:0.0562104769051075
epoch£º469	 i:5 	 global-step:9385	 l-p:0.056643590331077576
epoch£º469	 i:6 	 global-step:9386	 l-p:0.05704984441399574
epoch£º469	 i:7 	 global-step:9387	 l-p:0.05625235289335251
epoch£º469	 i:8 	 global-step:9388	 l-p:0.057849157601594925
epoch£º469	 i:9 	 global-step:9389	 l-p:0.05637192726135254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7286, 27.7305, 27.7286],
        [27.7286, 29.1606, 28.6604],
        [27.7286, 28.3462, 27.9649],
        [27.7286, 27.7298, 27.7286]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.0562024787068367 
model_pd.l_d.mean(): -0.000207392469746992 
model_pd.lagr.mean(): 0.05599508807063103 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8322], device='cuda:0')), ('power', tensor([-0.2788], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.0562024787068367
epoch£º470	 i:1 	 global-step:9401	 l-p:0.05612039566040039
epoch£º470	 i:2 	 global-step:9402	 l-p:0.05668705701828003
epoch£º470	 i:3 	 global-step:9403	 l-p:0.056628093123435974
epoch£º470	 i:4 	 global-step:9404	 l-p:0.056354448199272156
epoch£º470	 i:5 	 global-step:9405	 l-p:0.05747224763035774
epoch£º470	 i:6 	 global-step:9406	 l-p:0.056161921471357346
epoch£º470	 i:7 	 global-step:9407	 l-p:0.05788910761475563
epoch£º470	 i:8 	 global-step:9408	 l-p:0.05624611675739288
epoch£º470	 i:9 	 global-step:9409	 l-p:0.05642698332667351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6362, 27.6362, 27.6362],
        [27.6362, 27.6362, 27.6362],
        [27.6362, 27.6362, 27.6362],
        [27.6362, 27.6362, 27.6362]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.0563817098736763 
model_pd.l_d.mean(): -0.0001372663100482896 
model_pd.lagr.mean(): 0.056244444102048874 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7767], device='cuda:0')), ('power', tensor([-0.2080], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.0563817098736763
epoch£º471	 i:1 	 global-step:9421	 l-p:0.0562124066054821
epoch£º471	 i:2 	 global-step:9422	 l-p:0.056414708495140076
epoch£º471	 i:3 	 global-step:9423	 l-p:0.05611895024776459
epoch£º471	 i:4 	 global-step:9424	 l-p:0.05803053081035614
epoch£º471	 i:5 	 global-step:9425	 l-p:0.05640305578708649
epoch£º471	 i:6 	 global-step:9426	 l-p:0.05750944837927818
epoch£º471	 i:7 	 global-step:9427	 l-p:0.056212395429611206
epoch£º471	 i:8 	 global-step:9428	 l-p:0.05642540007829666
epoch£º471	 i:9 	 global-step:9429	 l-p:0.056755729019641876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5596, 30.3456, 30.2466],
        [27.5596, 27.7445, 27.5927],
        [27.5596, 34.9330, 38.8057],
        [27.5596, 27.5596, 27.5596]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.0562298521399498 
model_pd.l_d.mean(): -0.00022574022295884788 
model_pd.lagr.mean(): 0.0560041107237339 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8328], device='cuda:0')), ('power', tensor([-0.4227], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.0562298521399498
epoch£º472	 i:1 	 global-step:9441	 l-p:0.057150740176439285
epoch£º472	 i:2 	 global-step:9442	 l-p:0.05634224787354469
epoch£º472	 i:3 	 global-step:9443	 l-p:0.05639967694878578
epoch£º472	 i:4 	 global-step:9444	 l-p:0.056484173983335495
epoch£º472	 i:5 	 global-step:9445	 l-p:0.056408002972602844
epoch£º472	 i:6 	 global-step:9446	 l-p:0.057980846613645554
epoch£º472	 i:7 	 global-step:9447	 l-p:0.05621401593089104
epoch£º472	 i:8 	 global-step:9448	 l-p:0.05715109035372734
epoch£º472	 i:9 	 global-step:9449	 l-p:0.05632218345999718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5064, 29.4838, 29.0773],
        [27.5064, 27.5066, 27.5064],
        [27.5064, 27.5067, 27.5064],
        [27.5064, 27.7017, 27.5427]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.05652817711234093 
model_pd.l_d.mean(): -6.334387580864131e-05 
model_pd.lagr.mean(): 0.05646483227610588 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7358], device='cuda:0')), ('power', tensor([-0.1687], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.05652817711234093
epoch£º473	 i:1 	 global-step:9461	 l-p:0.05755243077874184
epoch£º473	 i:2 	 global-step:9462	 l-p:0.05618530139327049
epoch£º473	 i:3 	 global-step:9463	 l-p:0.056298255920410156
epoch£º473	 i:4 	 global-step:9464	 l-p:0.05680212751030922
epoch£º473	 i:5 	 global-step:9465	 l-p:0.05813566595315933
epoch£º473	 i:6 	 global-step:9466	 l-p:0.05652156099677086
epoch£º473	 i:7 	 global-step:9467	 l-p:0.05632578209042549
epoch£º473	 i:8 	 global-step:9468	 l-p:0.05619543790817261
epoch£º473	 i:9 	 global-step:9469	 l-p:0.056263744831085205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4854, 27.5004, 27.4860],
        [27.4854, 27.4854, 27.4854],
        [27.4854, 27.4854, 27.4854],
        [27.4854, 28.1843, 27.7765]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.056371115148067474 
model_pd.l_d.mean(): -7.518754136981443e-05 
model_pd.lagr.mean(): 0.05629592761397362 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7908], device='cuda:0')), ('power', tensor([-0.3805], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.056371115148067474
epoch£º474	 i:1 	 global-step:9481	 l-p:0.0572514645755291
epoch£º474	 i:2 	 global-step:9482	 l-p:0.056436870247125626
epoch£º474	 i:3 	 global-step:9483	 l-p:0.05639415979385376
epoch£º474	 i:4 	 global-step:9484	 l-p:0.0579829216003418
epoch£º474	 i:5 	 global-step:9485	 l-p:0.056790292263031006
epoch£º474	 i:6 	 global-step:9486	 l-p:0.05672864615917206
epoch£º474	 i:7 	 global-step:9487	 l-p:0.056249625980854034
epoch£º474	 i:8 	 global-step:9488	 l-p:0.056196391582489014
epoch£º474	 i:9 	 global-step:9489	 l-p:0.05642750486731529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5001, 28.4603, 27.9894],
        [27.5001, 28.9340, 28.4388],
        [27.5001, 27.5001, 27.5001],
        [27.5001, 27.5001, 27.5001]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.05642746016383171 
model_pd.l_d.mean(): -5.268074801278999e-06 
model_pd.lagr.mean(): 0.056422192603349686 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.8688e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7735], device='cuda:0')), ('power', tensor([-0.3064], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.05642746016383171
epoch£º475	 i:1 	 global-step:9501	 l-p:0.056843724101781845
epoch£º475	 i:2 	 global-step:9502	 l-p:0.05660054460167885
epoch£º475	 i:3 	 global-step:9503	 l-p:0.058031950145959854
epoch£º475	 i:4 	 global-step:9504	 l-p:0.057255033403635025
epoch£º475	 i:5 	 global-step:9505	 l-p:0.056198909878730774
epoch£º475	 i:6 	 global-step:9506	 l-p:0.056154340505599976
epoch£º475	 i:7 	 global-step:9507	 l-p:0.05636028200387955
epoch£º475	 i:8 	 global-step:9508	 l-p:0.0563434399664402
epoch£º475	 i:9 	 global-step:9509	 l-p:0.056511495262384415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5506, 35.0827, 39.1375],
        [27.5506, 27.5507, 27.5506],
        [27.5506, 34.5563, 38.0212],
        [27.5506, 28.1048, 27.7494]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.056272849440574646 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056272849440574646 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8146], device='cuda:0')), ('power', tensor([-0.4123], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.056272849440574646
epoch£º476	 i:1 	 global-step:9521	 l-p:0.057089995592832565
epoch£º476	 i:2 	 global-step:9522	 l-p:0.05627630278468132
epoch£º476	 i:3 	 global-step:9523	 l-p:0.0561370924115181
epoch£º476	 i:4 	 global-step:9524	 l-p:0.05714431032538414
epoch£º476	 i:5 	 global-step:9525	 l-p:0.05623364821076393
epoch£º476	 i:6 	 global-step:9526	 l-p:0.056914348155260086
epoch£º476	 i:7 	 global-step:9527	 l-p:0.05622038245201111
epoch£º476	 i:8 	 global-step:9528	 l-p:0.05616600438952446
epoch£º476	 i:9 	 global-step:9529	 l-p:0.05806687846779823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6145, 27.6212, 27.6147],
        [27.6145, 32.8295, 34.4836],
        [27.6145, 28.0726, 27.7598],
        [27.6145, 29.5565, 29.1372]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.05721678584814072 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05721678584814072 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7554], device='cuda:0')), ('power', tensor([-0.0877], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.05721678584814072
epoch£º477	 i:1 	 global-step:9541	 l-p:0.05618356168270111
epoch£º477	 i:2 	 global-step:9542	 l-p:0.05660666897892952
epoch£º477	 i:3 	 global-step:9543	 l-p:0.05610988661646843
epoch£º477	 i:4 	 global-step:9544	 l-p:0.056898631155490875
epoch£º477	 i:5 	 global-step:9545	 l-p:0.05617695674300194
epoch£º477	 i:6 	 global-step:9546	 l-p:0.058171503245830536
epoch£º477	 i:7 	 global-step:9547	 l-p:0.05618749558925629
epoch£º477	 i:8 	 global-step:9548	 l-p:0.05657793954014778
epoch£º477	 i:9 	 global-step:9549	 l-p:0.05617501214146614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6892, 27.6892, 27.6892],
        [27.6892, 35.9200, 40.7685],
        [27.6892, 27.6892, 27.6892],
        [27.6892, 30.6960, 30.7035]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.058075178414583206 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058075178414583206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.7223e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7309], device='cuda:0')), ('power', tensor([0.1544], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.058075178414583206
epoch£º478	 i:1 	 global-step:9561	 l-p:0.05655452609062195
epoch£º478	 i:2 	 global-step:9562	 l-p:0.05627463385462761
epoch£º478	 i:3 	 global-step:9563	 l-p:0.05678827688097954
epoch£º478	 i:4 	 global-step:9564	 l-p:0.05620305985212326
epoch£º478	 i:5 	 global-step:9565	 l-p:0.05711746960878372
epoch£º478	 i:6 	 global-step:9566	 l-p:0.05633217841386795
epoch£º478	 i:7 	 global-step:9567	 l-p:0.056224524974823
epoch£º478	 i:8 	 global-step:9568	 l-p:0.056244101375341415
epoch£º478	 i:9 	 global-step:9569	 l-p:0.056253500282764435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7674, 35.9523, 40.7313],
        [27.7674, 27.9026, 27.7873],
        [27.7674, 30.7721, 30.7738],
        [27.7674, 28.5915, 28.1460]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.056451015174388885 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056451015174388885 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7714], device='cuda:0')), ('power', tensor([-0.0446], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.056451015174388885
epoch£º479	 i:1 	 global-step:9581	 l-p:0.056251876056194305
epoch£º479	 i:2 	 global-step:9582	 l-p:0.056224048137664795
epoch£º479	 i:3 	 global-step:9583	 l-p:0.05620040372014046
epoch£º479	 i:4 	 global-step:9584	 l-p:0.056112464517354965
epoch£º479	 i:5 	 global-step:9585	 l-p:0.057230234146118164
epoch£º479	 i:6 	 global-step:9586	 l-p:0.05698185786604881
epoch£º479	 i:7 	 global-step:9587	 l-p:0.057839591056108475
epoch£º479	 i:8 	 global-step:9588	 l-p:0.05624452978372574
epoch£º479	 i:9 	 global-step:9589	 l-p:0.05627016723155975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8408, 27.8412, 27.8408],
        [27.8408, 29.2316, 28.7273],
        [27.8408, 27.8412, 27.8408],
        [27.8408, 28.5103, 28.1098]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.0578145869076252 
model_pd.l_d.mean(): 1.23884296954202e-06 
model_pd.lagr.mean(): 0.057815827429294586 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.5896e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7952], device='cuda:0')), ('power', tensor([0.1368], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.0578145869076252
epoch£º480	 i:1 	 global-step:9601	 l-p:0.0562974214553833
epoch£º480	 i:2 	 global-step:9602	 l-p:0.056558530777692795
epoch£º480	 i:3 	 global-step:9603	 l-p:0.0562879852950573
epoch£º480	 i:4 	 global-step:9604	 l-p:0.057014692574739456
epoch£º480	 i:5 	 global-step:9605	 l-p:0.05618144944310188
epoch£º480	 i:6 	 global-step:9606	 l-p:0.05686201527714729
epoch£º480	 i:7 	 global-step:9607	 l-p:0.05620727315545082
epoch£º480	 i:8 	 global-step:9608	 l-p:0.05617663264274597
epoch£º480	 i:9 	 global-step:9609	 l-p:0.05618123337626457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9183, 28.2438, 28.0008],
        [27.9183, 27.9184, 27.9183],
        [27.9183, 27.9503, 27.9203],
        [27.9183, 27.9185, 27.9183]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.05607728287577629 
model_pd.l_d.mean(): -1.695405217105872e-06 
model_pd.lagr.mean(): 0.05607558786869049 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4434e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8708], device='cuda:0')), ('power', tensor([-0.0896], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.05607728287577629
epoch£º481	 i:1 	 global-step:9621	 l-p:0.05618629232048988
epoch£º481	 i:2 	 global-step:9622	 l-p:0.056153953075408936
epoch£º481	 i:3 	 global-step:9623	 l-p:0.0565437488257885
epoch£º481	 i:4 	 global-step:9624	 l-p:0.056615181267261505
epoch£º481	 i:5 	 global-step:9625	 l-p:0.05620047450065613
epoch£º481	 i:6 	 global-step:9626	 l-p:0.05630508437752724
epoch£º481	 i:7 	 global-step:9627	 l-p:0.056409310549497604
epoch£º481	 i:8 	 global-step:9628	 l-p:0.05621454119682312
epoch£º481	 i:9 	 global-step:9629	 l-p:0.058620378375053406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9872, 28.1807, 28.0225],
        [27.9872, 28.4840, 28.1516],
        [27.9872, 30.6444, 30.4618],
        [27.9872, 33.2271, 34.8608]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.056052736937999725 
model_pd.l_d.mean(): -4.584975158650195e-06 
model_pd.lagr.mean(): 0.05604815110564232 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.0811e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8778], device='cuda:0')), ('power', tensor([-0.0712], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.056052736937999725
epoch£º482	 i:1 	 global-step:9641	 l-p:0.05615401640534401
epoch£º482	 i:2 	 global-step:9642	 l-p:0.0561123825609684
epoch£º482	 i:3 	 global-step:9643	 l-p:0.056650079786777496
epoch£º482	 i:4 	 global-step:9644	 l-p:0.05781934782862663
epoch£º482	 i:5 	 global-step:9645	 l-p:0.05628534033894539
epoch£º482	 i:6 	 global-step:9646	 l-p:0.05677132308483124
epoch£º482	 i:7 	 global-step:9647	 l-p:0.05613768473267555
epoch£º482	 i:8 	 global-step:9648	 l-p:0.056083373725414276
epoch£º482	 i:9 	 global-step:9649	 l-p:0.05705450847744942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0546, 28.0546, 28.0546],
        [28.0546, 33.3113, 34.9523],
        [28.0546, 28.0577, 28.0546],
        [28.0546, 28.0546, 28.0546]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): 0.0572979710996151 
model_pd.l_d.mean(): 6.80381417623721e-05 
model_pd.lagr.mean(): 0.057366009801626205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7328], device='cuda:0')), ('power', tensor([0.4732], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:0.0572979710996151
epoch£º483	 i:1 	 global-step:9661	 l-p:0.058288753032684326
epoch£º483	 i:2 	 global-step:9662	 l-p:0.05603307485580444
epoch£º483	 i:3 	 global-step:9663	 l-p:0.056210730224847794
epoch£º483	 i:4 	 global-step:9664	 l-p:0.05607317388057709
epoch£º483	 i:5 	 global-step:9665	 l-p:0.056217554956674576
epoch£º483	 i:6 	 global-step:9666	 l-p:0.05617295205593109
epoch£º483	 i:7 	 global-step:9667	 l-p:0.056436166167259216
epoch£º483	 i:8 	 global-step:9668	 l-p:0.056134190410375595
epoch£º483	 i:9 	 global-step:9669	 l-p:0.056078195571899414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[28.1056, 34.6864, 37.6119],
        [28.1056, 30.6768, 30.4506],
        [28.1056, 31.6629, 31.9695],
        [28.1056, 34.9063, 38.0634]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.05637180060148239 
model_pd.l_d.mean(): 9.34327399590984e-05 
model_pd.lagr.mean(): 0.056465234607458115 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7546], device='cuda:0')), ('power', tensor([0.3682], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.05637180060148239
epoch£º484	 i:1 	 global-step:9681	 l-p:0.05753489211201668
epoch£º484	 i:2 	 global-step:9682	 l-p:0.05702778697013855
epoch£º484	 i:3 	 global-step:9683	 l-p:0.05664984509348869
epoch£º484	 i:4 	 global-step:9684	 l-p:0.05613655969500542
epoch£º484	 i:5 	 global-step:9685	 l-p:0.05661115050315857
epoch£º484	 i:6 	 global-step:9686	 l-p:0.056114234030246735
epoch£º484	 i:7 	 global-step:9687	 l-p:0.056231383234262466
epoch£º484	 i:8 	 global-step:9688	 l-p:0.056037917733192444
epoch£º484	 i:9 	 global-step:9689	 l-p:0.05610250309109688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1286, 28.4239, 28.1986],
        [28.1286, 29.1697, 28.6786],
        [28.1286, 28.1286, 28.1286],
        [28.1286, 28.1286, 28.1286]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.05597630888223648 
model_pd.l_d.mean(): -7.577314136142377e-06 
model_pd.lagr.mean(): 0.05596873164176941 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9156], device='cuda:0')), ('power', tensor([-0.0198], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.05597630888223648
epoch£º485	 i:1 	 global-step:9701	 l-p:0.05766839534044266
epoch£º485	 i:2 	 global-step:9702	 l-p:0.05616423487663269
epoch£º485	 i:3 	 global-step:9703	 l-p:0.05633886530995369
epoch£º485	 i:4 	 global-step:9704	 l-p:0.056498993188142776
epoch£º485	 i:5 	 global-step:9705	 l-p:0.056136779487133026
epoch£º485	 i:6 	 global-step:9706	 l-p:0.05599982663989067
epoch£º485	 i:7 	 global-step:9707	 l-p:0.05760926008224487
epoch£º485	 i:8 	 global-step:9708	 l-p:0.05620786175131798
epoch£º485	 i:9 	 global-step:9709	 l-p:0.05618787184357643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1204, 31.9019, 32.3566],
        [28.1204, 29.8870, 29.4149],
        [28.1204, 34.5336, 37.2833],
        [28.1204, 28.7349, 28.3526]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.0562659427523613 
model_pd.l_d.mean(): 0.00015735565102659166 
model_pd.lagr.mean(): 0.05642329901456833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7944], device='cuda:0')), ('power', tensor([0.3054], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.0562659427523613
epoch£º486	 i:1 	 global-step:9721	 l-p:0.05603085830807686
epoch£º486	 i:2 	 global-step:9722	 l-p:0.056766919791698456
epoch£º486	 i:3 	 global-step:9723	 l-p:0.056019265204668045
epoch£º486	 i:4 	 global-step:9724	 l-p:0.05629492178559303
epoch£º486	 i:5 	 global-step:9725	 l-p:0.05622132122516632
epoch£º486	 i:6 	 global-step:9726	 l-p:0.05600931867957115
epoch£º486	 i:7 	 global-step:9727	 l-p:0.05738456919789314
epoch£º486	 i:8 	 global-step:9728	 l-p:0.05623190850019455
epoch£º486	 i:9 	 global-step:9729	 l-p:0.05763799697160721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0812, 29.8540, 29.3841],
        [28.0812, 28.0823, 28.0812],
        [28.0812, 28.0812, 28.0812],
        [28.0812, 32.9130, 34.1796]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.05628833547234535 
model_pd.l_d.mean(): 0.0001319101866101846 
model_pd.lagr.mean(): 0.05642024427652359 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8155], device='cuda:0')), ('power', tensor([0.2071], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.05628833547234535
epoch£º487	 i:1 	 global-step:9741	 l-p:0.05609248951077461
epoch£º487	 i:2 	 global-step:9742	 l-p:0.05625944957137108
epoch£º487	 i:3 	 global-step:9743	 l-p:0.0565471313893795
epoch£º487	 i:4 	 global-step:9744	 l-p:0.05761335417628288
epoch£º487	 i:5 	 global-step:9745	 l-p:0.05620632693171501
epoch£º487	 i:6 	 global-step:9746	 l-p:0.05606645345687866
epoch£º487	 i:7 	 global-step:9747	 l-p:0.05664730817079544
epoch£º487	 i:8 	 global-step:9748	 l-p:0.0562044195830822
epoch£º487	 i:9 	 global-step:9749	 l-p:0.0570894330739975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0172, 28.0255, 28.0174],
        [28.0172, 31.3123, 31.4626],
        [28.0172, 28.2052, 28.0509],
        [28.0172, 35.7726, 40.0027]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.05616896599531174 
model_pd.l_d.mean(): 7.371297397185117e-05 
model_pd.lagr.mean(): 0.0562426783144474 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8264], device='cuda:0')), ('power', tensor([0.1004], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.05616896599531174
epoch£º488	 i:1 	 global-step:9761	 l-p:0.056203536689281464
epoch£º488	 i:2 	 global-step:9762	 l-p:0.05610547959804535
epoch£º488	 i:3 	 global-step:9763	 l-p:0.05814826861023903
epoch£º488	 i:4 	 global-step:9764	 l-p:0.05635906010866165
epoch£º488	 i:5 	 global-step:9765	 l-p:0.05657206475734711
epoch£º488	 i:6 	 global-step:9766	 l-p:0.05705508962273598
epoch£º488	 i:7 	 global-step:9767	 l-p:0.056193072348833084
epoch£º488	 i:8 	 global-step:9768	 l-p:0.05626566335558891
epoch£º488	 i:9 	 global-step:9769	 l-p:0.056173160672187805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9286, 28.2217, 27.9981],
        [27.9286, 27.9311, 27.9287],
        [27.9286, 28.2542, 28.0111],
        [27.9286, 27.9286, 27.9286]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.056120507419109344 
model_pd.l_d.mean(): -8.728990360395983e-05 
model_pd.lagr.mean(): 0.056033216416835785 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8572], device='cuda:0')), ('power', tensor([-0.1099], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.056120507419109344
epoch£º489	 i:1 	 global-step:9781	 l-p:0.05620275437831879
epoch£º489	 i:2 	 global-step:9782	 l-p:0.05632342770695686
epoch£º489	 i:3 	 global-step:9783	 l-p:0.05622651055455208
epoch£º489	 i:4 	 global-step:9784	 l-p:0.056696996092796326
epoch£º489	 i:5 	 global-step:9785	 l-p:0.0570632740855217
epoch£º489	 i:6 	 global-step:9786	 l-p:0.05825939029455185
epoch£º489	 i:7 	 global-step:9787	 l-p:0.05626819655299187
epoch£º489	 i:8 	 global-step:9788	 l-p:0.05620899051427841
epoch£º489	 i:9 	 global-step:9789	 l-p:0.056194715201854706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8204, 27.8204, 27.8204],
        [27.8204, 27.8237, 27.8205],
        [27.8204, 27.8257, 27.8206],
        [27.8204, 27.8204, 27.8204]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.05610871687531471 
model_pd.l_d.mean(): -0.00018139205349143595 
model_pd.lagr.mean(): 0.055927325040102005 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8633], device='cuda:0')), ('power', tensor([-0.2251], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.05610871687531471
epoch£º490	 i:1 	 global-step:9801	 l-p:0.05620666220784187
epoch£º490	 i:2 	 global-step:9802	 l-p:0.05636311322450638
epoch£º490	 i:3 	 global-step:9803	 l-p:0.05632903799414635
epoch£º490	 i:4 	 global-step:9804	 l-p:0.05775218829512596
epoch£º490	 i:5 	 global-step:9805	 l-p:0.05613543465733528
epoch£º490	 i:6 	 global-step:9806	 l-p:0.05646389350295067
epoch£º490	 i:7 	 global-step:9807	 l-p:0.056576136499643326
epoch£º490	 i:8 	 global-step:9808	 l-p:0.056168489158153534
epoch£º490	 i:9 	 global-step:9809	 l-p:0.05782078579068184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7061, 35.8258, 40.5386],
        [27.7061, 29.2864, 28.7989],
        [27.7061, 28.0586, 27.8005],
        [27.7061, 33.8795, 36.4430]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.057836197316646576 
model_pd.l_d.mean(): -8.136532414937392e-06 
model_pd.lagr.mean(): 0.057828061282634735 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7859], device='cuda:0')), ('power', tensor([-0.0107], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.057836197316646576
epoch£º491	 i:1 	 global-step:9821	 l-p:0.05613257363438606
epoch£º491	 i:2 	 global-step:9822	 l-p:0.0562320277094841
epoch£º491	 i:3 	 global-step:9823	 l-p:0.05661638081073761
epoch£º491	 i:4 	 global-step:9824	 l-p:0.056203216314315796
epoch£º491	 i:5 	 global-step:9825	 l-p:0.05631902441382408
epoch£º491	 i:6 	 global-step:9826	 l-p:0.05684984102845192
epoch£º491	 i:7 	 global-step:9827	 l-p:0.0564686618745327
epoch£º491	 i:8 	 global-step:9828	 l-p:0.05725574865937233
epoch£º491	 i:9 	 global-step:9829	 l-p:0.056345079094171524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6018, 27.6019, 27.6018],
        [27.6018, 30.6824, 30.7360],
        [27.6018, 27.6582, 27.6067],
        [27.6018, 27.6427, 27.6048]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.056177880614995956 
model_pd.l_d.mean(): -0.00023657677229493856 
model_pd.lagr.mean(): 0.055941302329301834 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8398], device='cuda:0')), ('power', tensor([-0.3552], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.056177880614995956
epoch£º492	 i:1 	 global-step:9841	 l-p:0.05637370049953461
epoch£º492	 i:2 	 global-step:9842	 l-p:0.057015255093574524
epoch£º492	 i:3 	 global-step:9843	 l-p:0.05630527436733246
epoch£º492	 i:4 	 global-step:9844	 l-p:0.05629519000649452
epoch£º492	 i:5 	 global-step:9845	 l-p:0.056673090904951096
epoch£º492	 i:6 	 global-step:9846	 l-p:0.05787735804915428
epoch£º492	 i:7 	 global-step:9847	 l-p:0.05616355687379837
epoch£º492	 i:8 	 global-step:9848	 l-p:0.05726858973503113
epoch£º492	 i:9 	 global-step:9849	 l-p:0.05645301565527916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5126, 37.6587, 44.9891],
        [27.5126, 27.5144, 27.5126],
        [27.5126, 28.8995, 28.4019],
        [27.5126, 36.5091, 42.3363]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.05913098156452179 
model_pd.l_d.mean(): 0.00011426976561779156 
model_pd.lagr.mean(): 0.05924525111913681 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6356], device='cuda:0')), ('power', tensor([0.2197], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.05913098156452179
epoch£º493	 i:1 	 global-step:9861	 l-p:0.05632268264889717
epoch£º493	 i:2 	 global-step:9862	 l-p:0.05615118891000748
epoch£º493	 i:3 	 global-step:9863	 l-p:0.056509606540203094
epoch£º493	 i:4 	 global-step:9864	 l-p:0.05635848268866539
epoch£º493	 i:5 	 global-step:9865	 l-p:0.05629505217075348
epoch£º493	 i:6 	 global-step:9866	 l-p:0.05691611021757126
epoch£º493	 i:7 	 global-step:9867	 l-p:0.05657584220170975
epoch£º493	 i:8 	 global-step:9868	 l-p:0.056228864938020706
epoch£º493	 i:9 	 global-step:9869	 l-p:0.056332577019929886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4578, 30.5214, 30.5746],
        [27.4578, 37.1185, 43.8147],
        [27.4578, 28.4725, 27.9938],
        [27.4578, 27.5967, 27.4787]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.05794508382678032 
model_pd.l_d.mean(): 2.2851845642435364e-05 
model_pd.lagr.mean(): 0.057967934757471085 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6806], device='cuda:0')), ('power', tensor([0.0675], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.05794508382678032
epoch£º494	 i:1 	 global-step:9881	 l-p:0.05650227889418602
epoch£º494	 i:2 	 global-step:9882	 l-p:0.05623418837785721
epoch£º494	 i:3 	 global-step:9883	 l-p:0.056702110916376114
epoch£º494	 i:4 	 global-step:9884	 l-p:0.05616308003664017
epoch£º494	 i:5 	 global-step:9885	 l-p:0.05633656308054924
epoch£º494	 i:6 	 global-step:9886	 l-p:0.05638883635401726
epoch£º494	 i:7 	 global-step:9887	 l-p:0.05636858940124512
epoch£º494	 i:8 	 global-step:9888	 l-p:0.05809866636991501
epoch£º494	 i:9 	 global-step:9889	 l-p:0.05623427778482437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4362, 28.7259, 28.2282],
        [27.4362, 27.4400, 27.4363],
        [27.4362, 34.7176, 38.5072],
        [27.4362, 27.5217, 27.4458]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.05633242428302765 
model_pd.l_d.mean(): -6.937787111382931e-05 
model_pd.lagr.mean(): 0.05626304820179939 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8116], device='cuda:0')), ('power', tensor([-0.5080], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.05633242428302765
epoch£º495	 i:1 	 global-step:9901	 l-p:0.05657041445374489
epoch£º495	 i:2 	 global-step:9902	 l-p:0.05663229525089264
epoch£º495	 i:3 	 global-step:9903	 l-p:0.05734691396355629
epoch£º495	 i:4 	 global-step:9904	 l-p:0.05638490244746208
epoch£º495	 i:5 	 global-step:9905	 l-p:0.05803024023771286
epoch£º495	 i:6 	 global-step:9906	 l-p:0.056221164762973785
epoch£º495	 i:7 	 global-step:9907	 l-p:0.056834448128938675
epoch£º495	 i:8 	 global-step:9908	 l-p:0.05623587593436241
epoch£º495	 i:9 	 global-step:9909	 l-p:0.05638698488473892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4646, 27.4647, 27.4646],
        [27.4646, 28.0892, 27.7069],
        [27.4646, 27.4648, 27.4646],
        [27.4646, 34.7539, 38.5477]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.05626553297042847 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05626553297042847 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8204], device='cuda:0')), ('power', tensor([-0.5157], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.05626553297042847
epoch£º496	 i:1 	 global-step:9921	 l-p:0.05847154185175896
epoch£º496	 i:2 	 global-step:9922	 l-p:0.05626944452524185
epoch£º496	 i:3 	 global-step:9923	 l-p:0.05718398839235306
epoch£º496	 i:4 	 global-step:9924	 l-p:0.05622822791337967
epoch£º496	 i:5 	 global-step:9925	 l-p:0.05617006495594978
epoch£º496	 i:6 	 global-step:9926	 l-p:0.05708344280719757
epoch£º496	 i:7 	 global-step:9927	 l-p:0.0564761608839035
epoch£º496	 i:8 	 global-step:9928	 l-p:0.05644691362977028
epoch£º496	 i:9 	 global-step:9929	 l-p:0.056227341294288635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5302, 30.3132, 30.2143],
        [27.5302, 27.5302, 27.5302],
        [27.5302, 27.5304, 27.5302],
        [27.5302, 27.5412, 27.5306]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.056326549500226974 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056326549500226974 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7971], device='cuda:0')), ('power', tensor([-0.3392], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.056326549500226974
epoch£º497	 i:1 	 global-step:9941	 l-p:0.05630640685558319
epoch£º497	 i:2 	 global-step:9942	 l-p:0.05618484318256378
epoch£º497	 i:3 	 global-step:9943	 l-p:0.05639362335205078
epoch£º497	 i:4 	 global-step:9944	 l-p:0.05622595548629761
epoch£º497	 i:5 	 global-step:9945	 l-p:0.05642080307006836
epoch£º497	 i:6 	 global-step:9946	 l-p:0.05623583495616913
epoch£º497	 i:7 	 global-step:9947	 l-p:0.05670086666941643
epoch£º497	 i:8 	 global-step:9948	 l-p:0.05675823986530304
epoch£º497	 i:9 	 global-step:9949	 l-p:0.05901395529508591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6017, 28.0162, 27.7250],
        [27.6017, 35.6894, 40.3834],
        [27.6017, 33.6442, 36.0903],
        [27.6017, 27.6553, 27.6062]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.05630115419626236 
model_pd.l_d.mean(): -4.169880412518978e-06 
model_pd.lagr.mean(): 0.05629698559641838 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8062], device='cuda:0')), ('power', tensor([-0.3065], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.05630115419626236
epoch£º498	 i:1 	 global-step:9961	 l-p:0.05627317726612091
epoch£º498	 i:2 	 global-step:9962	 l-p:0.05636521056294441
epoch£º498	 i:3 	 global-step:9963	 l-p:0.0562288798391819
epoch£º498	 i:4 	 global-step:9964	 l-p:0.056195616722106934
epoch£º498	 i:5 	 global-step:9965	 l-p:0.05714554339647293
epoch£º498	 i:6 	 global-step:9966	 l-p:0.05627784505486488
epoch£º498	 i:7 	 global-step:9967	 l-p:0.056537702679634094
epoch£º498	 i:8 	 global-step:9968	 l-p:0.05859360471367836
epoch£º498	 i:9 	 global-step:9969	 l-p:0.05640263482928276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6837, 27.6889, 27.6838],
        [27.6837, 35.5635, 39.9971],
        [27.6837, 27.6837, 27.6837],
        [27.6837, 27.6837, 27.6837]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.05610958859324455 
model_pd.l_d.mean(): -1.3475668083628989e-06 
model_pd.lagr.mean(): 0.05610824003815651 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8762], device='cuda:0')), ('power', tensor([-0.4214], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.05610958859324455
epoch£º499	 i:1 	 global-step:9981	 l-p:0.05746772512793541
epoch£º499	 i:2 	 global-step:9982	 l-p:0.05616580694913864
epoch£º499	 i:3 	 global-step:9983	 l-p:0.05661384388804436
epoch£º499	 i:4 	 global-step:9984	 l-p:0.056148942559957504
epoch£º499	 i:5 	 global-step:9985	 l-p:0.0567827969789505
epoch£º499	 i:6 	 global-step:9986	 l-p:0.0562780499458313
epoch£º499	 i:7 	 global-step:9987	 l-p:0.05620042607188225
epoch£º499	 i:8 	 global-step:9988	 l-p:0.05646653100848198
epoch£º499	 i:9 	 global-step:9989	 l-p:0.057820819318294525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7680, 28.6753, 28.2112],
        [27.7680, 30.5286, 30.4056],
        [27.7680, 28.1852, 27.8922],
        [27.7680, 33.2542, 35.1400]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.05611295625567436 
model_pd.l_d.mean(): -7.801408514751529e-07 
model_pd.lagr.mean(): 0.056112177670001984 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8648], device='cuda:0')), ('power', tensor([-0.2325], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.05611295625567436
epoch£º500	 i:1 	 global-step:10001	 l-p:0.056146278977394104
epoch£º500	 i:2 	 global-step:10002	 l-p:0.05618519335985184
epoch£º500	 i:3 	 global-step:10003	 l-p:0.05701111629605293
epoch£º500	 i:4 	 global-step:10004	 l-p:0.05619462952017784
epoch£º500	 i:5 	 global-step:10005	 l-p:0.0567459873855114
epoch£º500	 i:6 	 global-step:10006	 l-p:0.05862967297434807
epoch£º500	 i:7 	 global-step:10007	 l-p:0.05629073083400726
epoch£º500	 i:8 	 global-step:10008	 l-p:0.0562407560646534
epoch£º500	 i:9 	 global-step:10009	 l-p:0.056230440735816956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8535, 29.0774, 28.5739],
        [27.8535, 30.7435, 30.6799],
        [27.8535, 31.1283, 31.2776],
        [27.8535, 28.0726, 27.8968]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.0563993863761425 
model_pd.l_d.mean(): 1.987600910524634e-07 
model_pd.lagr.mean(): 0.05639958381652832 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.3360e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7915], device='cuda:0')), ('power', tensor([0.0245], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.0563993863761425
epoch£º501	 i:1 	 global-step:10021	 l-p:0.05623531714081764
epoch£º501	 i:2 	 global-step:10022	 l-p:0.05610261484980583
epoch£º501	 i:3 	 global-step:10023	 l-p:0.057879798114299774
epoch£º501	 i:4 	 global-step:10024	 l-p:0.05605585128068924
epoch£º501	 i:5 	 global-step:10025	 l-p:0.057141002267599106
epoch£º501	 i:6 	 global-step:10026	 l-p:0.056200187653303146
epoch£º501	 i:7 	 global-step:10027	 l-p:0.056313905864953995
epoch£º501	 i:8 	 global-step:10028	 l-p:0.05617808550596237
epoch£º501	 i:9 	 global-step:10029	 l-p:0.0570150651037693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9380, 30.1689, 29.8214],
        [27.9380, 27.9380, 27.9380],
        [27.9380, 35.8178, 40.2057],
        [27.9380, 27.9396, 27.9380]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.05663631856441498 
model_pd.l_d.mean(): 4.242972863721661e-06 
model_pd.lagr.mean(): 0.056640561670064926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.4992e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7725], device='cuda:0')), ('power', tensor([0.1561], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.05663631856441498
epoch£º502	 i:1 	 global-step:10041	 l-p:0.05620214343070984
epoch£º502	 i:2 	 global-step:10042	 l-p:0.05607505515217781
epoch£º502	 i:3 	 global-step:10043	 l-p:0.056722335517406464
epoch£º502	 i:4 	 global-step:10044	 l-p:0.0563056543469429
epoch£º502	 i:5 	 global-step:10045	 l-p:0.05630669370293617
epoch£º502	 i:6 	 global-step:10046	 l-p:0.05851517617702484
epoch£º502	 i:7 	 global-step:10047	 l-p:0.056097038090229034
epoch£º502	 i:8 	 global-step:10048	 l-p:0.05624283850193024
epoch£º502	 i:9 	 global-step:10049	 l-p:0.0561576746404171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0169, 28.0173, 28.0169],
        [28.0169, 35.7735, 40.0049],
        [28.0169, 28.0168, 28.0168],
        [28.0169, 28.2051, 28.0506]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.05656222999095917 
model_pd.l_d.mean(): 1.446725582354702e-05 
model_pd.lagr.mean(): 0.056576699018478394 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.3253e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7968], device='cuda:0')), ('power', tensor([0.1708], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.05656222999095917
epoch£º503	 i:1 	 global-step:10061	 l-p:0.05616740137338638
epoch£º503	 i:2 	 global-step:10062	 l-p:0.05607786029577255
epoch£º503	 i:3 	 global-step:10063	 l-p:0.056184906512498856
epoch£º503	 i:4 	 global-step:10064	 l-p:0.05619743838906288
epoch£º503	 i:5 	 global-step:10065	 l-p:0.05672356113791466
epoch£º503	 i:6 	 global-step:10066	 l-p:0.05711859464645386
epoch£º503	 i:7 	 global-step:10067	 l-p:0.056265976279973984
epoch£º503	 i:8 	 global-step:10068	 l-p:0.05607493594288826
epoch£º503	 i:9 	 global-step:10069	 l-p:0.057653311640024185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0814, 28.1629, 28.0901],
        [28.0814, 30.9974, 30.9339],
        [28.0814, 28.3270, 28.1333],
        [28.0814, 29.2868, 28.7804]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.056141313165426254 
model_pd.l_d.mean(): 2.8904472856083885e-05 
model_pd.lagr.mean(): 0.05617021769285202 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8235], device='cuda:0')), ('power', tensor([0.1622], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.056141313165426254
epoch£º504	 i:1 	 global-step:10081	 l-p:0.05676976591348648
epoch£º504	 i:2 	 global-step:10082	 l-p:0.05614688619971275
epoch£º504	 i:3 	 global-step:10083	 l-p:0.05604304373264313
epoch£º504	 i:4 	 global-step:10084	 l-p:0.05607189983129501
epoch£º504	 i:5 	 global-step:10085	 l-p:0.05666154623031616
epoch£º504	 i:6 	 global-step:10086	 l-p:0.056024931371212006
epoch£º504	 i:7 	 global-step:10087	 l-p:0.05615842714905739
epoch£º504	 i:8 	 global-step:10088	 l-p:0.057582270354032516
epoch£º504	 i:9 	 global-step:10089	 l-p:0.057253770530223846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1265, 29.6547, 29.1518],
        [28.1265, 28.5807, 28.2680],
        [28.1265, 28.1577, 28.1284],
        [28.1265, 28.8067, 28.4007]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.056086886674165726 
model_pd.l_d.mean(): 4.3714226194424555e-05 
model_pd.lagr.mean(): 0.05613059923052788 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8532], device='cuda:0')), ('power', tensor([0.1460], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.056086886674165726
epoch£º505	 i:1 	 global-step:10101	 l-p:0.05624869838356972
epoch£º505	 i:2 	 global-step:10102	 l-p:0.0560702420771122
epoch£º505	 i:3 	 global-step:10103	 l-p:0.05600874125957489
epoch£º505	 i:4 	 global-step:10104	 l-p:0.056204669177532196
epoch£º505	 i:5 	 global-step:10105	 l-p:0.05621907114982605
epoch£º505	 i:6 	 global-step:10106	 l-p:0.05649985373020172
epoch£º505	 i:7 	 global-step:10107	 l-p:0.05757385119795799
epoch£º505	 i:8 	 global-step:10108	 l-p:0.0562121719121933
epoch£º505	 i:9 	 global-step:10109	 l-p:0.057631127536296844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1441, 28.2259, 28.1529],
        [28.1441, 38.3074, 45.5111],
        [28.1441, 37.2846, 43.1617],
        [28.1441, 31.3900, 31.5028]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.05639483407139778 
model_pd.l_d.mean(): 0.00015556224389001727 
model_pd.lagr.mean(): 0.056550394743680954 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7752], device='cuda:0')), ('power', tensor([0.3560], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.05639483407139778
epoch£º506	 i:1 	 global-step:10121	 l-p:0.056007660925388336
epoch£º506	 i:2 	 global-step:10122	 l-p:0.05663732439279556
epoch£º506	 i:3 	 global-step:10123	 l-p:0.05601317808032036
epoch£º506	 i:4 	 global-step:10124	 l-p:0.056134819984436035
epoch£º506	 i:5 	 global-step:10125	 l-p:0.056951675564050674
epoch£º506	 i:6 	 global-step:10126	 l-p:0.05652565881609917
epoch£º506	 i:7 	 global-step:10127	 l-p:0.056430500000715256
epoch£º506	 i:8 	 global-step:10128	 l-p:0.05756332725286484
epoch£º506	 i:9 	 global-step:10129	 l-p:0.05608989670872688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1291, 28.5376, 28.2481],
        [28.1291, 30.0215, 29.5737],
        [28.1291, 30.5698, 30.2895],
        [28.1291, 37.8308, 44.4295]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.05613671988248825 
model_pd.l_d.mean(): 0.00012698411592282355 
model_pd.lagr.mean(): 0.05626370385289192 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8337], device='cuda:0')), ('power', tensor([0.2203], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.05613671988248825
epoch£º507	 i:1 	 global-step:10141	 l-p:0.05655713379383087
epoch£º507	 i:2 	 global-step:10142	 l-p:0.05701051652431488
epoch£º507	 i:3 	 global-step:10143	 l-p:0.056042175740003586
epoch£º507	 i:4 	 global-step:10144	 l-p:0.057685162872076035
epoch£º507	 i:5 	 global-step:10145	 l-p:0.056048180907964706
epoch£º507	 i:6 	 global-step:10146	 l-p:0.056150201708078384
epoch£º507	 i:7 	 global-step:10147	 l-p:0.05606497824192047
epoch£º507	 i:8 	 global-step:10148	 l-p:0.05702172964811325
epoch£º507	 i:9 	 global-step:10149	 l-p:0.056125495582818985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0773, 28.2668, 28.1114],
        [28.0773, 28.0927, 28.0780],
        [28.0773, 28.0813, 28.0774],
        [28.0773, 32.5906, 33.5947]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.056721143424510956 
model_pd.l_d.mean(): 0.00024901714641600847 
model_pd.lagr.mean(): 0.05697016045451164 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7792], device='cuda:0')), ('power', tensor([0.3556], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.056721143424510956
epoch£º508	 i:1 	 global-step:10161	 l-p:0.0561787411570549
epoch£º508	 i:2 	 global-step:10162	 l-p:0.05758349224925041
epoch£º508	 i:3 	 global-step:10163	 l-p:0.05614163354039192
epoch£º508	 i:4 	 global-step:10164	 l-p:0.0560198538005352
epoch£º508	 i:5 	 global-step:10165	 l-p:0.056084029376506805
epoch£º508	 i:6 	 global-step:10166	 l-p:0.05618643760681152
epoch£º508	 i:7 	 global-step:10167	 l-p:0.05671738460659981
epoch£º508	 i:8 	 global-step:10168	 l-p:0.056925732642412186
epoch£º508	 i:9 	 global-step:10169	 l-p:0.0564945787191391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9891, 34.7274, 37.8358],
        [27.9891, 27.9922, 27.9891],
        [27.9891, 38.2342, 45.5839],
        [27.9891, 28.3383, 28.0814]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.05650503560900688 
model_pd.l_d.mean(): 0.00013396535359788686 
model_pd.lagr.mean(): 0.05663900077342987 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8065], device='cuda:0')), ('power', tensor([0.1695], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.05650503560900688
epoch£º509	 i:1 	 global-step:10181	 l-p:0.058523163199424744
epoch£º509	 i:2 	 global-step:10182	 l-p:0.056179437786340714
epoch£º509	 i:3 	 global-step:10183	 l-p:0.05626532807946205
epoch£º509	 i:4 	 global-step:10184	 l-p:0.056170813739299774
epoch£º509	 i:5 	 global-step:10185	 l-p:0.056237440556287766
epoch£º509	 i:6 	 global-step:10186	 l-p:0.056742310523986816
epoch£º509	 i:7 	 global-step:10187	 l-p:0.05609876662492752
epoch£º509	 i:8 	 global-step:10188	 l-p:0.05634687468409538
epoch£º509	 i:9 	 global-step:10189	 l-p:0.05628092214465141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8755, 27.8849, 27.8758],
        [27.8755, 33.7229, 35.9400],
        [27.8755, 34.0641, 36.6195],
        [27.8755, 32.4502, 33.5239]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.0585760660469532 
model_pd.l_d.mean(): 0.00020747818052768707 
model_pd.lagr.mean(): 0.05878354609012604 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7804], device='cuda:0')), ('power', tensor([0.2495], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.0585760660469532
epoch£º510	 i:1 	 global-step:10201	 l-p:0.05613860860466957
epoch£º510	 i:2 	 global-step:10202	 l-p:0.056231457740068436
epoch£º510	 i:3 	 global-step:10203	 l-p:0.05630197376012802
epoch£º510	 i:4 	 global-step:10204	 l-p:0.05665135756134987
epoch£º510	 i:5 	 global-step:10205	 l-p:0.05658361315727234
epoch£º510	 i:6 	 global-step:10206	 l-p:0.056210435926914215
epoch£º510	 i:7 	 global-step:10207	 l-p:0.05631716921925545
epoch£º510	 i:8 	 global-step:10208	 l-p:0.05648646131157875
epoch£º510	 i:9 	 global-step:10209	 l-p:0.05623089522123337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7456, 28.4633, 28.0477],
        [27.7456, 33.1346, 34.9327],
        [27.7456, 28.0175, 27.8074],
        [27.7456, 27.7457, 27.7456]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.05635423958301544 
model_pd.l_d.mean(): -4.0655260818311945e-05 
model_pd.lagr.mean(): 0.05631358548998833 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7883], device='cuda:0')), ('power', tensor([-0.0500], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.05635423958301544
epoch£º511	 i:1 	 global-step:10221	 l-p:0.056285493075847626
epoch£º511	 i:2 	 global-step:10222	 l-p:0.05673905834555626
epoch£º511	 i:3 	 global-step:10223	 l-p:0.05669338256120682
epoch£º511	 i:4 	 global-step:10224	 l-p:0.056181058287620544
epoch£º511	 i:5 	 global-step:10225	 l-p:0.056213121861219406
epoch£º511	 i:6 	 global-step:10226	 l-p:0.05617879331111908
epoch£º511	 i:7 	 global-step:10227	 l-p:0.059075724333524704
epoch£º511	 i:8 	 global-step:10228	 l-p:0.05631713196635246
epoch£º511	 i:9 	 global-step:10229	 l-p:0.05616234987974167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6118, 27.6931, 27.6206],
        [27.6118, 27.9560, 27.7028],
        [27.6118, 32.7814, 34.3948],
        [27.6118, 28.8144, 28.3160]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.05624670907855034 
model_pd.l_d.mean(): -0.00027267346740700305 
model_pd.lagr.mean(): 0.05597403645515442 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8418], device='cuda:0')), ('power', tensor([-0.3743], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.05624670907855034
epoch£º512	 i:1 	 global-step:10241	 l-p:0.056828346103429794
epoch£º512	 i:2 	 global-step:10242	 l-p:0.056312959641218185
epoch£º512	 i:3 	 global-step:10243	 l-p:0.057481784373521805
epoch£º512	 i:4 	 global-step:10244	 l-p:0.057996708899736404
epoch£º512	 i:5 	 global-step:10245	 l-p:0.056286171078681946
epoch£º512	 i:6 	 global-step:10246	 l-p:0.05637853592634201
epoch£º512	 i:7 	 global-step:10247	 l-p:0.05638425797224045
epoch£º512	 i:8 	 global-step:10248	 l-p:0.05617650970816612
epoch£º512	 i:9 	 global-step:10249	 l-p:0.05649393051862717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5010, 29.2668, 28.8127],
        [27.5010, 35.2691, 39.6048],
        [27.5010, 28.9932, 28.5021],
        [27.5010, 27.5150, 27.5016]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.05628751590847969 
model_pd.l_d.mean(): -0.0002467007434461266 
model_pd.lagr.mean(): 0.05604081600904465 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8041], device='cuda:0')), ('power', tensor([-0.4233], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.05628751590847969
epoch£º513	 i:1 	 global-step:10261	 l-p:0.05652223527431488
epoch£º513	 i:2 	 global-step:10262	 l-p:0.05623999610543251
epoch£º513	 i:3 	 global-step:10263	 l-p:0.056203391402959824
epoch£º513	 i:4 	 global-step:10264	 l-p:0.0581376887857914
epoch£º513	 i:5 	 global-step:10265	 l-p:0.05638935789465904
epoch£º513	 i:6 	 global-step:10266	 l-p:0.05632968619465828
epoch£º513	 i:7 	 global-step:10267	 l-p:0.056706078350543976
epoch£º513	 i:8 	 global-step:10268	 l-p:0.05680961161851883
epoch£º513	 i:9 	 global-step:10269	 l-p:0.05730188265442848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4203, 36.6006, 42.6813],
        [27.4203, 35.0028, 39.1379],
        [27.4203, 28.7092, 28.2119],
        [27.4203, 27.4216, 27.4203]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.05632733553647995 
model_pd.l_d.mean(): -0.00021178970928303897 
model_pd.lagr.mean(): 0.05611554533243179 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8012], device='cuda:0')), ('power', tensor([-0.5448], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.05632733553647995
epoch£º514	 i:1 	 global-step:10281	 l-p:0.056233424693346024
epoch£º514	 i:2 	 global-step:10282	 l-p:0.05620136111974716
epoch£º514	 i:3 	 global-step:10283	 l-p:0.05630916357040405
epoch£º514	 i:4 	 global-step:10284	 l-p:0.05872678756713867
epoch£º514	 i:5 	 global-step:10285	 l-p:0.057357873767614365
epoch£º514	 i:6 	 global-step:10286	 l-p:0.05662832036614418
epoch£º514	 i:7 	 global-step:10287	 l-p:0.056646641343832016
epoch£º514	 i:8 	 global-step:10288	 l-p:0.05643070489168167
epoch£º514	 i:9 	 global-step:10289	 l-p:0.05625762417912483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3905, 34.7166, 38.5643],
        [27.3905, 34.3180, 37.7226],
        [27.3905, 27.8447, 27.5345],
        [27.3905, 31.5776, 32.3903]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.058134131133556366 
model_pd.l_d.mean(): -3.1354498787550256e-05 
model_pd.lagr.mean(): 0.05810277536511421 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7293], device='cuda:0')), ('power', tensor([-0.1891], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.058134131133556366
epoch£º515	 i:1 	 global-step:10301	 l-p:0.05722343176603317
epoch£º515	 i:2 	 global-step:10302	 l-p:0.05692344531416893
epoch£º515	 i:3 	 global-step:10303	 l-p:0.05625913664698601
epoch£º515	 i:4 	 global-step:10304	 l-p:0.056200627237558365
epoch£º515	 i:5 	 global-step:10305	 l-p:0.05690007656812668
epoch£º515	 i:6 	 global-step:10306	 l-p:0.05667015537619591
epoch£º515	 i:7 	 global-step:10307	 l-p:0.0562560148537159
epoch£º515	 i:8 	 global-step:10308	 l-p:0.05632466822862625
epoch£º515	 i:9 	 global-step:10309	 l-p:0.056240007281303406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4167, 27.4799, 27.4226],
        [27.4167, 28.8459, 28.3524],
        [27.4167, 34.7268, 38.5522],
        [27.4167, 27.5391, 27.4337]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.05650005117058754 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05650005117058754 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7642], device='cuda:0')), ('power', tensor([-0.3574], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.05650005117058754
epoch£º516	 i:1 	 global-step:10321	 l-p:0.056354645639657974
epoch£º516	 i:2 	 global-step:10322	 l-p:0.05633942037820816
epoch£º516	 i:3 	 global-step:10323	 l-p:0.05674782022833824
epoch£º516	 i:4 	 global-step:10324	 l-p:0.0563822016119957
epoch£º516	 i:5 	 global-step:10325	 l-p:0.05721224471926689
epoch£º516	 i:6 	 global-step:10326	 l-p:0.05646068602800369
epoch£º516	 i:7 	 global-step:10327	 l-p:0.05628292262554169
epoch£º516	 i:8 	 global-step:10328	 l-p:0.05626685172319412
epoch£º516	 i:9 	 global-step:10329	 l-p:0.05842742696404457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4771, 27.5262, 27.4811],
        [27.4771, 30.3960, 30.3694],
        [27.4771, 30.1009, 29.9295],
        [27.4771, 27.4773, 27.4771]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.05663203075528145 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05663203075528145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7929], device='cuda:0')), ('power', tensor([-0.3762], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.05663203075528145
epoch£º517	 i:1 	 global-step:10341	 l-p:0.05638178810477257
epoch£º517	 i:2 	 global-step:10342	 l-p:0.05638390779495239
epoch£º517	 i:3 	 global-step:10343	 l-p:0.05622259899973869
epoch£º517	 i:4 	 global-step:10344	 l-p:0.056222543120384216
epoch£º517	 i:5 	 global-step:10345	 l-p:0.05635562911629677
epoch£º517	 i:6 	 global-step:10346	 l-p:0.056857556104660034
epoch£º517	 i:7 	 global-step:10347	 l-p:0.056341465562582016
epoch£º517	 i:8 	 global-step:10348	 l-p:0.0580885149538517
epoch£º517	 i:9 	 global-step:10349	 l-p:0.057244766503572464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[27.5591, 33.6740, 36.1987],
        [27.5591, 31.2803, 31.7381],
        [27.5591, 28.3833, 27.9398],
        [27.5591, 29.4029, 28.9627]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.056303903460502625 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056303903460502625 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7949], device='cuda:0')), ('power', tensor([-0.3234], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.056303903460502625
epoch£º518	 i:1 	 global-step:10361	 l-p:0.05710513889789581
epoch£º518	 i:2 	 global-step:10362	 l-p:0.05651559680700302
epoch£º518	 i:3 	 global-step:10363	 l-p:0.057933323085308075
epoch£º518	 i:4 	 global-step:10364	 l-p:0.05626792460680008
epoch£º518	 i:5 	 global-step:10365	 l-p:0.05625588819384575
epoch£º518	 i:6 	 global-step:10366	 l-p:0.056363362818956375
epoch£º518	 i:7 	 global-step:10367	 l-p:0.056646738201379776
epoch£º518	 i:8 	 global-step:10368	 l-p:0.05627204477787018
epoch£º518	 i:9 	 global-step:10369	 l-p:0.05679653212428093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6536, 27.6721, 27.6545],
        [27.6536, 37.4186, 44.2073],
        [27.6536, 27.6738, 27.6546],
        [27.6536, 28.2971, 27.9068]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.05626755952835083 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05626755952835083 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8225], device='cuda:0')), ('power', tensor([-0.2970], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.05626755952835083
epoch£º519	 i:1 	 global-step:10381	 l-p:0.05614566057920456
epoch£º519	 i:2 	 global-step:10382	 l-p:0.05634663626551628
epoch£º519	 i:3 	 global-step:10383	 l-p:0.05617094784975052
epoch£º519	 i:4 	 global-step:10384	 l-p:0.056263282895088196
epoch£º519	 i:5 	 global-step:10385	 l-p:0.059439487755298615
epoch£º519	 i:6 	 global-step:10386	 l-p:0.056191686540842056
epoch£º519	 i:7 	 global-step:10387	 l-p:0.0565820187330246
epoch£º519	 i:8 	 global-step:10388	 l-p:0.05645831301808357
epoch£º519	 i:9 	 global-step:10389	 l-p:0.05628317594528198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7457, 27.7457, 27.7457],
        [27.7457, 36.7254, 42.4827],
        [27.7457, 27.7458, 27.7457],
        [27.7457, 27.7953, 27.7497]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.05778219923377037 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05778219923377037 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8062], device='cuda:0')), ('power', tensor([-0.0248], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.05778219923377037
epoch£º520	 i:1 	 global-step:10401	 l-p:0.05686561390757561
epoch£º520	 i:2 	 global-step:10402	 l-p:0.05621286854147911
epoch£º520	 i:3 	 global-step:10403	 l-p:0.05643998086452484
epoch£º520	 i:4 	 global-step:10404	 l-p:0.057162828743457794
epoch£º520	 i:5 	 global-step:10405	 l-p:0.05615626648068428
epoch£º520	 i:6 	 global-step:10406	 l-p:0.056689128279685974
epoch£º520	 i:7 	 global-step:10407	 l-p:0.05617271363735199
epoch£º520	 i:8 	 global-step:10408	 l-p:0.05622830614447594
epoch£º520	 i:9 	 global-step:10409	 l-p:0.056152552366256714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8418, 30.0765, 29.7342],
        [27.8418, 28.0518, 27.8823],
        [27.8418, 37.4399, 43.9678],
        [27.8418, 29.5713, 29.1009]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.05613548308610916 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05613548308610916 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8573], device='cuda:0')), ('power', tensor([-0.1805], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.05613548308610916
epoch£º521	 i:1 	 global-step:10421	 l-p:0.05639577656984329
epoch£º521	 i:2 	 global-step:10422	 l-p:0.05616176500916481
epoch£º521	 i:3 	 global-step:10423	 l-p:0.05645934119820595
epoch£º521	 i:4 	 global-step:10424	 l-p:0.056328337639570236
epoch£º521	 i:5 	 global-step:10425	 l-p:0.057537827640771866
epoch£º521	 i:6 	 global-step:10426	 l-p:0.05647895112633705
epoch£º521	 i:7 	 global-step:10427	 l-p:0.05770568177103996
epoch£º521	 i:8 	 global-step:10428	 l-p:0.05611496791243553
epoch£º521	 i:9 	 global-step:10429	 l-p:0.05622845143079758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9313, 27.9549, 27.9325],
        [27.9313, 27.9314, 27.9313],
        [27.9313, 27.9884, 27.9363],
        [27.9313, 27.9958, 27.9374]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.056104887276887894 
model_pd.l_d.mean(): -1.477061687182868e-06 
model_pd.lagr.mean(): 0.0561034120619297 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.5169e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8552], device='cuda:0')), ('power', tensor([-0.0531], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.056104887276887894
epoch£º522	 i:1 	 global-step:10441	 l-p:0.056627869606018066
epoch£º522	 i:2 	 global-step:10442	 l-p:0.056158874183893204
epoch£º522	 i:3 	 global-step:10443	 l-p:0.056337326765060425
epoch£º522	 i:4 	 global-step:10444	 l-p:0.05625630170106888
epoch£º522	 i:5 	 global-step:10445	 l-p:0.05762165039777756
epoch£º522	 i:6 	 global-step:10446	 l-p:0.056228842586278915
epoch£º522	 i:7 	 global-step:10447	 l-p:0.05694301798939705
epoch£º522	 i:8 	 global-step:10448	 l-p:0.056766051799058914
epoch£º522	 i:9 	 global-step:10449	 l-p:0.0562242791056633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0177, 28.1466, 28.0360],
        [28.0177, 28.0177, 28.0177],
        [28.0177, 37.6792, 44.2505],
        [28.0177, 28.0188, 28.0177]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.05765258148312569 
model_pd.l_d.mean(): 4.848744356422685e-05 
model_pd.lagr.mean(): 0.05770106986165047 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7032], device='cuda:0')), ('power', tensor([0.5796], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.05765258148312569
epoch£º523	 i:1 	 global-step:10461	 l-p:0.05611799657344818
epoch£º523	 i:2 	 global-step:10462	 l-p:0.057697851210832596
epoch£º523	 i:3 	 global-step:10463	 l-p:0.056138474494218826
epoch£º523	 i:4 	 global-step:10464	 l-p:0.05608586221933365
epoch£º523	 i:5 	 global-step:10465	 l-p:0.05617668852210045
epoch£º523	 i:6 	 global-step:10466	 l-p:0.05615880340337753
epoch£º523	 i:7 	 global-step:10467	 l-p:0.056175850331783295
epoch£º523	 i:8 	 global-step:10468	 l-p:0.05611288547515869
epoch£º523	 i:9 	 global-step:10469	 l-p:0.05670813098549843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0925, 28.1682, 28.1003],
        [28.0925, 35.5917, 39.5167],
        [28.0925, 28.0944, 28.0926],
        [28.0925, 28.1641, 28.0996]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.05623311921954155 
model_pd.l_d.mean(): 3.4538254112703726e-05 
model_pd.lagr.mean(): 0.05626765638589859 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8331], device='cuda:0')), ('power', tensor([0.1913], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.05623311921954155
epoch£º524	 i:1 	 global-step:10481	 l-p:0.05608196184039116
epoch£º524	 i:2 	 global-step:10482	 l-p:0.056072261184453964
epoch£º524	 i:3 	 global-step:10483	 l-p:0.05613336339592934
epoch£º524	 i:4 	 global-step:10484	 l-p:0.05657503008842468
epoch£º524	 i:5 	 global-step:10485	 l-p:0.05774915590882301
epoch£º524	 i:6 	 global-step:10486	 l-p:0.05745260417461395
epoch£º524	 i:7 	 global-step:10487	 l-p:0.05620932951569557
epoch£º524	 i:8 	 global-step:10488	 l-p:0.056220099329948425
epoch£º524	 i:9 	 global-step:10489	 l-p:0.05609500780701637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1405, 28.1552, 28.1411],
        [28.1405, 28.1406, 28.1405],
        [28.1405, 28.1608, 28.1414],
        [28.1405, 30.1221, 29.6946]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.05665194243192673 
model_pd.l_d.mean(): 0.00011405317491153255 
model_pd.lagr.mean(): 0.05676599591970444 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8112], device='cuda:0')), ('power', tensor([0.3708], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.05665194243192673
epoch£º525	 i:1 	 global-step:10501	 l-p:0.056028954684734344
epoch£º525	 i:2 	 global-step:10502	 l-p:0.056222543120384216
epoch£º525	 i:3 	 global-step:10503	 l-p:0.0562485009431839
epoch£º525	 i:4 	 global-step:10504	 l-p:0.05692753568291664
epoch£º525	 i:5 	 global-step:10505	 l-p:0.056280553340911865
epoch£º525	 i:6 	 global-step:10506	 l-p:0.05605645105242729
epoch£º525	 i:7 	 global-step:10507	 l-p:0.057737797498703
epoch£º525	 i:8 	 global-step:10508	 l-p:0.05641670152544975
epoch£º525	 i:9 	 global-step:10509	 l-p:0.05614635720849037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1554, 28.2299, 28.1630],
        [28.1554, 29.6109, 29.1025],
        [28.1554, 33.8642, 35.9120],
        [28.1554, 34.5771, 37.3305]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.05606125667691231 
model_pd.l_d.mean(): 9.018240234581754e-05 
model_pd.lagr.mean(): 0.056151438504457474 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8569], device='cuda:0')), ('power', tensor([0.1997], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.05606125667691231
epoch£º526	 i:1 	 global-step:10521	 l-p:0.05613525211811066
epoch£º526	 i:2 	 global-step:10522	 l-p:0.05670636147260666
epoch£º526	 i:3 	 global-step:10523	 l-p:0.05759827420115471
epoch£º526	 i:4 	 global-step:10524	 l-p:0.05607230216264725
epoch£º526	 i:5 	 global-step:10525	 l-p:0.057523589581251144
epoch£º526	 i:6 	 global-step:10526	 l-p:0.05607810989022255
epoch£º526	 i:7 	 global-step:10527	 l-p:0.056041501462459564
epoch£º526	 i:8 	 global-step:10528	 l-p:0.05637132376432419
epoch£º526	 i:9 	 global-step:10529	 l-p:0.056134529411792755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1321, 28.1321, 28.1321],
        [28.1321, 30.9794, 30.8785],
        [28.1321, 28.1321, 28.1321],
        [28.1321, 28.1361, 28.1322]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.05612652748823166 
model_pd.l_d.mean(): 9.003449667943642e-05 
model_pd.lagr.mean(): 0.056216560304164886 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8513], device='cuda:0')), ('power', tensor([0.1514], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.05612652748823166
epoch£º527	 i:1 	 global-step:10541	 l-p:0.056992415338754654
epoch£º527	 i:2 	 global-step:10542	 l-p:0.05603044852614403
epoch£º527	 i:3 	 global-step:10543	 l-p:0.056105587631464005
epoch£º527	 i:4 	 global-step:10544	 l-p:0.057096097618341446
epoch£º527	 i:5 	 global-step:10545	 l-p:0.056447990238666534
epoch£º527	 i:6 	 global-step:10546	 l-p:0.05609370023012161
epoch£º527	 i:7 	 global-step:10547	 l-p:0.05617455020546913
epoch£º527	 i:8 	 global-step:10548	 l-p:0.057702697813510895
epoch£º527	 i:9 	 global-step:10549	 l-p:0.056091830134391785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0643, 28.4247, 28.1613],
        [28.0643, 28.1073, 28.0674],
        [28.0643, 35.8335, 40.0711],
        [28.0643, 35.8347, 40.0737]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.05650851130485535 
model_pd.l_d.mean(): 0.00021198387548793107 
model_pd.lagr.mean(): 0.05672049522399902 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7875], device='cuda:0')), ('power', tensor([0.2959], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:0.05650851130485535
epoch£º528	 i:1 	 global-step:10561	 l-p:0.05627249553799629
epoch£º528	 i:2 	 global-step:10562	 l-p:0.05614908039569855
epoch£º528	 i:3 	 global-step:10563	 l-p:0.056426361203193665
epoch£º528	 i:4 	 global-step:10564	 l-p:0.05770111456513405
epoch£º528	 i:5 	 global-step:10565	 l-p:0.05660202354192734
epoch£º528	 i:6 	 global-step:10566	 l-p:0.056281786412000656
epoch£º528	 i:7 	 global-step:10567	 l-p:0.056183766573667526
epoch£º528	 i:8 	 global-step:10568	 l-p:0.056923702359199524
epoch£º528	 i:9 	 global-step:10569	 l-p:0.05606849491596222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[27.9615, 30.9996, 31.0074],
        [27.9615, 32.3751, 33.3109],
        [27.9615, 28.6734, 28.2581],
        [27.9615, 35.0770, 38.5965]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.056106653064489365 
model_pd.l_d.mean(): -4.230714694131166e-06 
model_pd.lagr.mean(): 0.05610242113471031 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8487], device='cuda:0')), ('power', tensor([-0.0053], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.056106653064489365
epoch£º529	 i:1 	 global-step:10581	 l-p:0.05796155706048012
epoch£º529	 i:2 	 global-step:10582	 l-p:0.0565025694668293
epoch£º529	 i:3 	 global-step:10583	 l-p:0.05624774843454361
epoch£º529	 i:4 	 global-step:10584	 l-p:0.056621253490448
epoch£º529	 i:5 	 global-step:10585	 l-p:0.056201763451099396
epoch£º529	 i:6 	 global-step:10586	 l-p:0.05630113556981087
epoch£º529	 i:7 	 global-step:10587	 l-p:0.05615449696779251
epoch£º529	 i:8 	 global-step:10588	 l-p:0.057119064033031464
epoch£º529	 i:9 	 global-step:10589	 l-p:0.05625179037451744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8322, 27.8322, 27.8322],
        [27.8322, 30.0661, 29.7239],
        [27.8322, 27.8322, 27.8322],
        [27.8322, 33.9469, 36.4338]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.05625615641474724 
model_pd.l_d.mean(): -5.006330684409477e-05 
model_pd.lagr.mean(): 0.05620609223842621 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8143], device='cuda:0')), ('power', tensor([-0.0610], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.05625615641474724
epoch£º530	 i:1 	 global-step:10601	 l-p:0.05626533553004265
epoch£º530	 i:2 	 global-step:10602	 l-p:0.056994158774614334
epoch£º530	 i:3 	 global-step:10603	 l-p:0.05630384385585785
epoch£º530	 i:4 	 global-step:10604	 l-p:0.056124381721019745
epoch£º530	 i:5 	 global-step:10605	 l-p:0.05642983317375183
epoch£º530	 i:6 	 global-step:10606	 l-p:0.05670652166008949
epoch£º530	 i:7 	 global-step:10607	 l-p:0.05657695233821869
epoch£º530	 i:8 	 global-step:10608	 l-p:0.05800395831465721
epoch£º530	 i:9 	 global-step:10609	 l-p:0.05627303570508957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6886, 27.7208, 27.6906],
        [27.6886, 27.6886, 27.6886],
        [27.6886, 27.6886, 27.6886],
        [27.6886, 27.6886, 27.6886]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.05788055434823036 
model_pd.l_d.mean(): -3.301541437394917e-05 
model_pd.lagr.mean(): 0.057847537100315094 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7922], device='cuda:0')), ('power', tensor([-0.0425], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.05788055434823036
epoch£º531	 i:1 	 global-step:10621	 l-p:0.056283291429281235
epoch£º531	 i:2 	 global-step:10622	 l-p:0.05676756799221039
epoch£º531	 i:3 	 global-step:10623	 l-p:0.05632249638438225
epoch£º531	 i:4 	 global-step:10624	 l-p:0.05730685219168663
epoch£º531	 i:5 	 global-step:10625	 l-p:0.05623656138777733
epoch£º531	 i:6 	 global-step:10626	 l-p:0.056234776973724365
epoch£º531	 i:7 	 global-step:10627	 l-p:0.05619382858276367
epoch£º531	 i:8 	 global-step:10628	 l-p:0.05641268566250801
epoch£º531	 i:9 	 global-step:10629	 l-p:0.0567028634250164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5585, 32.3342, 33.6082],
        [27.5585, 27.9288, 27.6612],
        [27.5585, 33.0692, 35.0037],
        [27.5585, 27.5759, 27.5593]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.05792714282870293 
model_pd.l_d.mean(): -8.778019400779158e-05 
model_pd.lagr.mean(): 0.05783936381340027 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7801], device='cuda:0')), ('power', tensor([-0.1318], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.05792714282870293
epoch£º532	 i:1 	 global-step:10641	 l-p:0.0576966367661953
epoch£º532	 i:2 	 global-step:10642	 l-p:0.05619945749640465
epoch£º532	 i:3 	 global-step:10643	 l-p:0.056234102696180344
epoch£º532	 i:4 	 global-step:10644	 l-p:0.05623001977801323
epoch£º532	 i:5 	 global-step:10645	 l-p:0.05648234859108925
epoch£º532	 i:6 	 global-step:10646	 l-p:0.05700144171714783
epoch£º532	 i:7 	 global-step:10647	 l-p:0.05632352828979492
epoch£º532	 i:8 	 global-step:10648	 l-p:0.05636954307556152
epoch£º532	 i:9 	 global-step:10649	 l-p:0.056271836161613464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4518, 34.7718, 38.6024],
        [27.4518, 27.4718, 27.4527],
        [27.4518, 27.6270, 27.4822],
        [27.4518, 29.2811, 28.8412]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.05717121809720993 
model_pd.l_d.mean(): -0.00015912644448690116 
model_pd.lagr.mean(): 0.05701209232211113 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7960], device='cuda:0')), ('power', tensor([-0.3211], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.05717121809720993
epoch£º533	 i:1 	 global-step:10661	 l-p:0.05630755424499512
epoch£º533	 i:2 	 global-step:10662	 l-p:0.05617646127939224
epoch£º533	 i:3 	 global-step:10663	 l-p:0.05630401521921158
epoch£º533	 i:4 	 global-step:10664	 l-p:0.057094186544418335
epoch£º533	 i:5 	 global-step:10665	 l-p:0.0566009022295475
epoch£º533	 i:6 	 global-step:10666	 l-p:0.058225858956575394
epoch£º533	 i:7 	 global-step:10667	 l-p:0.056553907692432404
epoch£º533	 i:8 	 global-step:10668	 l-p:0.056360382586717606
epoch£º533	 i:9 	 global-step:10669	 l-p:0.0562739260494709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3849, 29.1028, 28.6435],
        [27.3849, 30.9518, 31.3188],
        [27.3849, 27.4362, 27.3892],
        [27.3849, 29.6737, 29.3696]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.05648233741521835 
model_pd.l_d.mean(): -0.00011173888924531639 
model_pd.lagr.mean(): 0.05637059733271599 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7594], device='cuda:0')), ('power', tensor([-0.3993], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.05648233741521835
epoch£º534	 i:1 	 global-step:10681	 l-p:0.05627979710698128
epoch£º534	 i:2 	 global-step:10682	 l-p:0.05718996748328209
epoch£º534	 i:3 	 global-step:10683	 l-p:0.056225962936878204
epoch£º534	 i:4 	 global-step:10684	 l-p:0.056976545602083206
epoch£º534	 i:5 	 global-step:10685	 l-p:0.05633944272994995
epoch£º534	 i:6 	 global-step:10686	 l-p:0.05648455023765564
epoch£º534	 i:7 	 global-step:10687	 l-p:0.05813097208738327
epoch£º534	 i:8 	 global-step:10688	 l-p:0.05622896924614906
epoch£º534	 i:9 	 global-step:10689	 l-p:0.056869715452194214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3766, 27.3779, 27.3766],
        [27.3766, 27.5173, 27.3980],
        [27.3766, 27.3768, 27.3765],
        [27.3766, 27.7173, 27.4666]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.05640524625778198 
model_pd.l_d.mean(): -2.4982995455502532e-05 
model_pd.lagr.mean(): 0.0563802644610405 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.5387e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7936], device='cuda:0')), ('power', tensor([-0.5696], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.05640524625778198
epoch£º535	 i:1 	 global-step:10701	 l-p:0.05635274946689606
epoch£º535	 i:2 	 global-step:10702	 l-p:0.058209117501974106
epoch£º535	 i:3 	 global-step:10703	 l-p:0.05623137205839157
epoch£º535	 i:4 	 global-step:10704	 l-p:0.05618547275662422
epoch£º535	 i:5 	 global-step:10705	 l-p:0.05692658945918083
epoch£º535	 i:6 	 global-step:10706	 l-p:0.05645240843296051
epoch£º535	 i:7 	 global-step:10707	 l-p:0.0568310022354126
epoch£º535	 i:8 	 global-step:10708	 l-p:0.057167891412973404
epoch£º535	 i:9 	 global-step:10709	 l-p:0.05637102574110031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4328, 27.5025, 27.4397],
        [27.4328, 27.4657, 27.4349],
        [27.4328, 27.4960, 27.4387],
        [27.4328, 33.4365, 35.8668]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.05722653120756149 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05722653120756149 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7707], device='cuda:0')), ('power', tensor([-0.2680], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.05722653120756149
epoch£º536	 i:1 	 global-step:10721	 l-p:0.05635411664843559
epoch£º536	 i:2 	 global-step:10722	 l-p:0.056286491453647614
epoch£º536	 i:3 	 global-step:10723	 l-p:0.05622108653187752
epoch£º536	 i:4 	 global-step:10724	 l-p:0.05645110830664635
epoch£º536	 i:5 	 global-step:10725	 l-p:0.056882862001657486
epoch£º536	 i:6 	 global-step:10726	 l-p:0.0563558004796505
epoch£º536	 i:7 	 global-step:10727	 l-p:0.05838299170136452
epoch£º536	 i:8 	 global-step:10728	 l-p:0.056234996765851974
epoch£º536	 i:9 	 global-step:10729	 l-p:0.056491464376449585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5178, 27.5180, 27.5178],
        [27.5178, 29.4961, 29.0894],
        [27.5178, 29.3667, 28.9290],
        [27.5178, 27.5177, 27.5177]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.05618811398744583 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05618811398744583 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8465], device='cuda:0')), ('power', tensor([-0.5712], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.05618811398744583
epoch£º537	 i:1 	 global-step:10741	 l-p:0.05711228400468826
epoch£º537	 i:2 	 global-step:10742	 l-p:0.056527841836214066
epoch£º537	 i:3 	 global-step:10743	 l-p:0.05634070187807083
epoch£º537	 i:4 	 global-step:10744	 l-p:0.056556519120931625
epoch£º537	 i:5 	 global-step:10745	 l-p:0.05696021020412445
epoch£º537	 i:6 	 global-step:10746	 l-p:0.05639492720365524
epoch£º537	 i:7 	 global-step:10747	 l-p:0.05631905794143677
epoch£º537	 i:8 	 global-step:10748	 l-p:0.0579020157456398
epoch£º537	 i:9 	 global-step:10749	 l-p:0.05628093704581261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6137, 27.8550, 27.6647],
        [27.6137, 27.6168, 27.6138],
        [27.6137, 27.6140, 27.6137],
        [27.6137, 28.0378, 27.7417]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.05620504170656204 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05620504170656204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8358], device='cuda:0')), ('power', tensor([-0.3840], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.05620504170656204
epoch£º538	 i:1 	 global-step:10761	 l-p:0.0562780424952507
epoch£º538	 i:2 	 global-step:10762	 l-p:0.05618873983621597
epoch£º538	 i:3 	 global-step:10763	 l-p:0.05629228428006172
epoch£º538	 i:4 	 global-step:10764	 l-p:0.05685272067785263
epoch£º538	 i:5 	 global-step:10765	 l-p:0.05634894222021103
epoch£º538	 i:6 	 global-step:10766	 l-p:0.05631294474005699
epoch£º538	 i:7 	 global-step:10767	 l-p:0.05615755915641785
epoch£º538	 i:8 	 global-step:10768	 l-p:0.05881061404943466
epoch£º538	 i:9 	 global-step:10769	 l-p:0.05680694431066513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7117, 28.5340, 28.0895],
        [27.7117, 27.7118, 27.7116],
        [27.7117, 27.7980, 27.7213],
        [27.7117, 27.7121, 27.7117]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.0562044121325016 
model_pd.l_d.mean(): -4.352883024694165e-06 
model_pd.lagr.mean(): 0.056200060993433 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.8467e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8199], device='cuda:0')), ('power', tensor([-0.2423], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.0562044121325016
epoch£º539	 i:1 	 global-step:10781	 l-p:0.05618229880928993
epoch£º539	 i:2 	 global-step:10782	 l-p:0.0563487708568573
epoch£º539	 i:3 	 global-step:10783	 l-p:0.05618654936552048
epoch£º539	 i:4 	 global-step:10784	 l-p:0.05724314972758293
epoch£º539	 i:5 	 global-step:10785	 l-p:0.05640522018074989
epoch£º539	 i:6 	 global-step:10786	 l-p:0.05628371983766556
epoch£º539	 i:7 	 global-step:10787	 l-p:0.05789662152528763
epoch£º539	 i:8 	 global-step:10788	 l-p:0.05666451156139374
epoch£º539	 i:9 	 global-step:10789	 l-p:0.05652310699224472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8137, 31.0580, 31.1919],
        [27.8137, 27.8136, 27.8136],
        [27.8137, 29.0358, 28.5330],
        [27.8137, 33.9241, 36.4093]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.05673183873295784 
model_pd.l_d.mean(): 6.647519512625877e-07 
model_pd.lagr.mean(): 0.056732501834630966 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.2476e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7312], device='cuda:0')), ('power', tensor([0.1724], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.05673183873295784
epoch£º540	 i:1 	 global-step:10801	 l-p:0.05626684054732323
epoch£º540	 i:2 	 global-step:10802	 l-p:0.05773722380399704
epoch£º540	 i:3 	 global-step:10803	 l-p:0.056144244968891144
epoch£º540	 i:4 	 global-step:10804	 l-p:0.05622310936450958
epoch£º540	 i:5 	 global-step:10805	 l-p:0.0561012364923954
epoch£º540	 i:6 	 global-step:10806	 l-p:0.057042818516492844
epoch£º540	 i:7 	 global-step:10807	 l-p:0.0567297637462616
epoch£º540	 i:8 	 global-step:10808	 l-p:0.05650795251131058
epoch£º540	 i:9 	 global-step:10809	 l-p:0.056139931082725525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9182, 28.1919, 27.9804],
        [27.9182, 28.1289, 27.9588],
        [27.9182, 36.9821, 42.8096],
        [27.9182, 27.9182, 27.9182]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.05781075358390808 
model_pd.l_d.mean(): 3.5076930089417147e-06 
model_pd.lagr.mean(): 0.05781426280736923 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.7609e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7787], device='cuda:0')), ('power', tensor([0.1982], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.05781075358390808
epoch£º541	 i:1 	 global-step:10821	 l-p:0.056272074580192566
epoch£º541	 i:2 	 global-step:10822	 l-p:0.056085214018821716
epoch£º541	 i:3 	 global-step:10823	 l-p:0.05754521116614342
epoch£º541	 i:4 	 global-step:10824	 l-p:0.05616317689418793
epoch£º541	 i:5 	 global-step:10825	 l-p:0.056267060339450836
epoch£º541	 i:6 	 global-step:10826	 l-p:0.05629409849643707
epoch£º541	 i:7 	 global-step:10827	 l-p:0.05662595480680466
epoch£º541	 i:8 	 global-step:10828	 l-p:0.056108903139829636
epoch£º541	 i:9 	 global-step:10829	 l-p:0.056136250495910645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0173, 28.0173, 28.0173],
        [28.0173, 30.9264, 30.8630],
        [28.0173, 38.3920, 45.9097],
        [28.0173, 31.6710, 32.0474]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.05810031294822693 
model_pd.l_d.mean(): 3.288033985882066e-05 
model_pd.lagr.mean(): 0.058133192360401154 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.3984e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7393], device='cuda:0')), ('power', tensor([0.4648], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.05810031294822693
epoch£º542	 i:1 	 global-step:10841	 l-p:0.056111544370651245
epoch£º542	 i:2 	 global-step:10842	 l-p:0.056580688804388046
epoch£º542	 i:3 	 global-step:10843	 l-p:0.05617991462349892
epoch£º542	 i:4 	 global-step:10844	 l-p:0.05711646378040314
epoch£º542	 i:5 	 global-step:10845	 l-p:0.05613012984395027
epoch£º542	 i:6 	 global-step:10846	 l-p:0.056021980941295624
epoch£º542	 i:7 	 global-step:10847	 l-p:0.05634566769003868
epoch£º542	 i:8 	 global-step:10848	 l-p:0.05622893199324608
epoch£º542	 i:9 	 global-step:10849	 l-p:0.05620350316166878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0996, 31.6562, 31.9628],
        [28.0996, 38.0831, 45.0582],
        [28.0996, 28.7791, 28.3735],
        [28.0996, 28.1076, 28.0999]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.056332219392061234 
model_pd.l_d.mean(): 4.069253918714821e-05 
model_pd.lagr.mean(): 0.05637291073799133 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8176], device='cuda:0')), ('power', tensor([0.2405], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.056332219392061234
epoch£º543	 i:1 	 global-step:10861	 l-p:0.05608446151018143
epoch£º543	 i:2 	 global-step:10862	 l-p:0.05675765499472618
epoch£º543	 i:3 	 global-step:10863	 l-p:0.056459274142980576
epoch£º543	 i:4 	 global-step:10864	 l-p:0.0570656917989254
epoch£º543	 i:5 	 global-step:10865	 l-p:0.056100282818078995
epoch£º543	 i:6 	 global-step:10866	 l-p:0.05609826371073723
epoch£º543	 i:7 	 global-step:10867	 l-p:0.05608651041984558
epoch£º543	 i:8 	 global-step:10868	 l-p:0.05777977034449577
epoch£º543	 i:9 	 global-step:10869	 l-p:0.05602746829390526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1511, 28.1579, 28.1513],
        [28.1511, 32.1599, 32.7720],
        [28.1511, 28.1511, 28.1511],
        [28.1511, 36.5402, 41.4913]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.05612735450267792 
model_pd.l_d.mean(): 4.783508484251797e-05 
model_pd.lagr.mean(): 0.05617519095540047 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8589], device='cuda:0')), ('power', tensor([0.1590], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.05612735450267792
epoch£º544	 i:1 	 global-step:10881	 l-p:0.057501766830682755
epoch£º544	 i:2 	 global-step:10882	 l-p:0.05677179992198944
epoch£º544	 i:3 	 global-step:10883	 l-p:0.05629827454686165
epoch£º544	 i:4 	 global-step:10884	 l-p:0.0560736246407032
epoch£º544	 i:5 	 global-step:10885	 l-p:0.05601031333208084
epoch£º544	 i:6 	 global-step:10886	 l-p:0.05629061162471771
epoch£º544	 i:7 	 global-step:10887	 l-p:0.05704076588153839
epoch£º544	 i:8 	 global-step:10888	 l-p:0.056449044495821
epoch£º544	 i:9 	 global-step:10889	 l-p:0.056113678961992264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1712, 28.2476, 28.1791],
        [28.1712, 28.5228, 28.2642],
        [28.1712, 29.6422, 29.1344],
        [28.1712, 29.5795, 29.0689]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.05603580176830292 
model_pd.l_d.mean(): 7.809075032128021e-05 
model_pd.lagr.mean(): 0.056113891303539276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8726], device='cuda:0')), ('power', tensor([0.1729], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.05603580176830292
epoch£º545	 i:1 	 global-step:10901	 l-p:0.056537650525569916
epoch£º545	 i:2 	 global-step:10902	 l-p:0.058999545872211456
epoch£º545	 i:3 	 global-step:10903	 l-p:0.05621292069554329
epoch£º545	 i:4 	 global-step:10904	 l-p:0.056007809937000275
epoch£º545	 i:5 	 global-step:10905	 l-p:0.05618961527943611
epoch£º545	 i:6 	 global-step:10906	 l-p:0.05619121342897415
epoch£º545	 i:7 	 global-step:10907	 l-p:0.05610911175608635
epoch£º545	 i:8 	 global-step:10908	 l-p:0.05621710792183876
epoch£º545	 i:9 	 global-step:10909	 l-p:0.05617117881774902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1456, 28.3348, 28.1796],
        [28.1456, 28.3256, 28.1769],
        [28.1456, 28.1465, 28.1456],
        [28.1456, 30.9459, 30.8213]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.05750712379813194 
model_pd.l_d.mean(): 0.000169709135661833 
model_pd.lagr.mean(): 0.057676833122968674 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8547], device='cuda:0')), ('power', tensor([0.2816], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.05750712379813194
epoch£º546	 i:1 	 global-step:10921	 l-p:0.05609282851219177
epoch£º546	 i:2 	 global-step:10922	 l-p:0.056803420186042786
epoch£º546	 i:3 	 global-step:10923	 l-p:0.056445762515068054
epoch£º546	 i:4 	 global-step:10924	 l-p:0.056417301297187805
epoch£º546	 i:5 	 global-step:10925	 l-p:0.05615062639117241
epoch£º546	 i:6 	 global-step:10926	 l-p:0.05610654875636101
epoch£º546	 i:7 	 global-step:10927	 l-p:0.05605895072221756
epoch£º546	 i:8 	 global-step:10928	 l-p:0.057048603892326355
epoch£º546	 i:9 	 global-step:10929	 l-p:0.0561828650534153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0707, 29.0521, 28.5709],
        [28.0707, 37.3352, 43.3845],
        [28.0707, 28.0767, 28.0709],
        [28.0707, 28.0707, 28.0707]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.05613013729453087 
model_pd.l_d.mean(): 7.780056330375373e-05 
model_pd.lagr.mean(): 0.05620793625712395 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8417], device='cuda:0')), ('power', tensor([0.1065], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.05613013729453087
epoch£º547	 i:1 	 global-step:10941	 l-p:0.05699945613741875
epoch£º547	 i:2 	 global-step:10942	 l-p:0.05677468329668045
epoch£º547	 i:3 	 global-step:10943	 l-p:0.05778469890356064
epoch£º547	 i:4 	 global-step:10944	 l-p:0.05622866377234459
epoch£º547	 i:5 	 global-step:10945	 l-p:0.056236106902360916
epoch£º547	 i:6 	 global-step:10946	 l-p:0.05650036036968231
epoch£º547	 i:7 	 global-step:10947	 l-p:0.056099146604537964
epoch£º547	 i:8 	 global-step:10948	 l-p:0.056301020085811615
epoch£º547	 i:9 	 global-step:10949	 l-p:0.05604817345738411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9539, 27.9552, 27.9539],
        [27.9539, 28.7906, 28.3403],
        [27.9539, 28.1774, 27.9986],
        [27.9539, 28.3023, 28.0460]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.05705589801073074 
model_pd.l_d.mean(): 0.00019268316100351512 
model_pd.lagr.mean(): 0.05724858120083809 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7734], device='cuda:0')), ('power', tensor([0.2375], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.05705589801073074
epoch£º548	 i:1 	 global-step:10961	 l-p:0.0560859851539135
epoch£º548	 i:2 	 global-step:10962	 l-p:0.05625484511256218
epoch£º548	 i:3 	 global-step:10963	 l-p:0.056299686431884766
epoch£º548	 i:4 	 global-step:10964	 l-p:0.05633245408535004
epoch£º548	 i:5 	 global-step:10965	 l-p:0.05658622458577156
epoch£º548	 i:6 	 global-step:10966	 l-p:0.058317989110946655
epoch£º548	 i:7 	 global-step:10967	 l-p:0.05614668130874634
epoch£º548	 i:8 	 global-step:10968	 l-p:0.05622755363583565
epoch£º548	 i:9 	 global-step:10969	 l-p:0.05622592940926552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8051, 35.9554, 40.6860],
        [27.8051, 27.8359, 27.8070],
        [27.8051, 27.8542, 27.8091],
        [27.8051, 29.6344, 29.1835]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.05621373653411865 
model_pd.l_d.mean(): -0.0001526247797301039 
model_pd.lagr.mean(): 0.05606111139059067 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8389], device='cuda:0')), ('power', tensor([-0.1846], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.05621373653411865
epoch£º549	 i:1 	 global-step:10981	 l-p:0.05678720027208328
epoch£º549	 i:2 	 global-step:10982	 l-p:0.05625956878066063
epoch£º549	 i:3 	 global-step:10983	 l-p:0.05615592002868652
epoch£º549	 i:4 	 global-step:10984	 l-p:0.05637751892209053
epoch£º549	 i:5 	 global-step:10985	 l-p:0.05688462778925896
epoch£º549	 i:6 	 global-step:10986	 l-p:0.05623771622776985
epoch£º549	 i:7 	 global-step:10987	 l-p:0.056096356362104416
epoch£º549	 i:8 	 global-step:10988	 l-p:0.05621948093175888
epoch£º549	 i:9 	 global-step:10989	 l-p:0.05882878601551056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6443, 27.8780, 27.6927],
        [27.6443, 30.5344, 30.4830],
        [27.6443, 29.8506, 29.5068],
        [27.6443, 28.0624, 27.7693]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.058359287679195404 
model_pd.l_d.mean(): 0.00011571253708098084 
model_pd.lagr.mean(): 0.05847499892115593 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7257], device='cuda:0')), ('power', tensor([0.1511], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.058359287679195404
epoch£º550	 i:1 	 global-step:11001	 l-p:0.05636467784643173
epoch£º550	 i:2 	 global-step:11002	 l-p:0.057622041553258896
epoch£º550	 i:3 	 global-step:11003	 l-p:0.05622211471199989
epoch£º550	 i:4 	 global-step:11004	 l-p:0.05645894259214401
epoch£º550	 i:5 	 global-step:11005	 l-p:0.056317128241062164
epoch£º550	 i:6 	 global-step:11006	 l-p:0.056359127163887024
epoch£º550	 i:7 	 global-step:11007	 l-p:0.05617431551218033
epoch£º550	 i:8 	 global-step:11008	 l-p:0.05617016181349754
epoch£º550	 i:9 	 global-step:11009	 l-p:0.05643536522984505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5072, 28.6303, 28.1388],
        [27.5072, 33.5470, 36.0032],
        [27.5072, 37.2182, 43.9693],
        [27.5072, 27.5072, 27.5072]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.058632008731365204 
model_pd.l_d.mean(): 4.054515375173651e-05 
model_pd.lagr.mean(): 0.05867255479097366 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6953], device='cuda:0')), ('power', tensor([0.0642], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.058632008731365204
epoch£º551	 i:1 	 global-step:11021	 l-p:0.05716046318411827
epoch£º551	 i:2 	 global-step:11022	 l-p:0.05621045082807541
epoch£º551	 i:3 	 global-step:11023	 l-p:0.05635937675833702
epoch£º551	 i:4 	 global-step:11024	 l-p:0.05657796189188957
epoch£º551	 i:5 	 global-step:11025	 l-p:0.05618903413414955
epoch£º551	 i:6 	 global-step:11026	 l-p:0.05635938420891762
epoch£º551	 i:7 	 global-step:11027	 l-p:0.05628960579633713
epoch£º551	 i:8 	 global-step:11028	 l-p:0.05689636990427971
epoch£º551	 i:9 	 global-step:11029	 l-p:0.05623454973101616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3986, 30.3087, 30.2822],
        [27.3986, 27.3987, 27.3986],
        [27.3986, 27.3986, 27.3986],
        [27.3986, 27.3986, 27.3986]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.05655747279524803 
model_pd.l_d.mean(): -0.00019007307128049433 
model_pd.lagr.mean(): 0.05636740103363991 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7608], device='cuda:0')), ('power', tensor([-0.4367], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.05655747279524803
epoch£º552	 i:1 	 global-step:11041	 l-p:0.05724747106432915
epoch£º552	 i:2 	 global-step:11042	 l-p:0.056378286331892014
epoch£º552	 i:3 	 global-step:11043	 l-p:0.05624013766646385
epoch£º552	 i:4 	 global-step:11044	 l-p:0.05622264742851257
epoch£º552	 i:5 	 global-step:11045	 l-p:0.05645257979631424
epoch£º552	 i:6 	 global-step:11046	 l-p:0.05690556392073631
epoch£º552	 i:7 	 global-step:11047	 l-p:0.05673053488135338
epoch£º552	 i:8 	 global-step:11048	 l-p:0.05816826596856117
epoch£º552	 i:9 	 global-step:11049	 l-p:0.0563419908285141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3390, 27.3390, 27.3390],
        [27.3390, 30.2949, 30.2964],
        [27.3390, 36.7326, 43.1061],
        [27.3390, 27.6573, 27.4196]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.05639275535941124 
model_pd.l_d.mean(): -9.534820128465071e-05 
model_pd.lagr.mean(): 0.05629740655422211 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7768], device='cuda:0')), ('power', tensor([-0.4900], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.05639275535941124
epoch£º553	 i:1 	 global-step:11061	 l-p:0.0563964918255806
epoch£º553	 i:2 	 global-step:11062	 l-p:0.0580732636153698
epoch£º553	 i:3 	 global-step:11063	 l-p:0.05631854757666588
epoch£º553	 i:4 	 global-step:11064	 l-p:0.05636812001466751
epoch£º553	 i:5 	 global-step:11065	 l-p:0.05641935393214226
epoch£º553	 i:6 	 global-step:11066	 l-p:0.05618155375123024
epoch£º553	 i:7 	 global-step:11067	 l-p:0.058058079332113266
epoch£º553	 i:8 	 global-step:11068	 l-p:0.056906115263700485
epoch£º553	 i:9 	 global-step:11069	 l-p:0.056213054805994034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3546, 27.4930, 27.3754],
        [27.3546, 27.3597, 27.3547],
        [27.3546, 27.5301, 27.3852],
        [27.3546, 31.6674, 32.5814]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.056816596537828445 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056816596537828445 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7956], device='cuda:0')), ('power', tensor([-0.5284], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.056816596537828445
epoch£º554	 i:1 	 global-step:11081	 l-p:0.056445930153131485
epoch£º554	 i:2 	 global-step:11082	 l-p:0.056298308074474335
epoch£º554	 i:3 	 global-step:11083	 l-p:0.056502435356378555
epoch£º554	 i:4 	 global-step:11084	 l-p:0.056316688656806946
epoch£º554	 i:5 	 global-step:11085	 l-p:0.056331608444452286
epoch£º554	 i:6 	 global-step:11086	 l-p:0.056498583406209946
epoch£º554	 i:7 	 global-step:11087	 l-p:0.056353434920310974
epoch£º554	 i:8 	 global-step:11088	 l-p:0.057159196585416794
epoch£º554	 i:9 	 global-step:11089	 l-p:0.05844053998589516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4255, 27.4257, 27.4255],
        [27.4255, 28.0651, 27.6776],
        [27.4255, 27.5063, 27.4343],
        [27.4255, 28.2530, 27.8099]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.056347090750932693 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056347090750932693 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8086], device='cuda:0')), ('power', tensor([-0.5240], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.056347090750932693
epoch£º555	 i:1 	 global-step:11101	 l-p:0.05676328390836716
epoch£º555	 i:2 	 global-step:11102	 l-p:0.05920606479048729
epoch£º555	 i:3 	 global-step:11103	 l-p:0.05641033127903938
epoch£º555	 i:4 	 global-step:11104	 l-p:0.05633438006043434
epoch£º555	 i:5 	 global-step:11105	 l-p:0.05664410442113876
epoch£º555	 i:6 	 global-step:11106	 l-p:0.05618145689368248
epoch£º555	 i:7 	 global-step:11107	 l-p:0.056435976177453995
epoch£º555	 i:8 	 global-step:11108	 l-p:0.05633357912302017
epoch£º555	 i:9 	 global-step:11109	 l-p:0.05624087154865265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5318, 29.9991, 29.7573],
        [27.5318, 27.5318, 27.5318],
        [27.5318, 32.9691, 34.8379],
        [27.5318, 27.5426, 27.5322]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.05622211471199989 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05622211471199989 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8235], device='cuda:0')), ('power', tensor([-0.4291], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.05622211471199989
epoch£º556	 i:1 	 global-step:11121	 l-p:0.05621188133955002
epoch£º556	 i:2 	 global-step:11122	 l-p:0.05634821951389313
epoch£º556	 i:3 	 global-step:11123	 l-p:0.05722103640437126
epoch£º556	 i:4 	 global-step:11124	 l-p:0.05614989250898361
epoch£º556	 i:5 	 global-step:11125	 l-p:0.05613767355680466
epoch£º556	 i:6 	 global-step:11126	 l-p:0.05828321725130081
epoch£º556	 i:7 	 global-step:11127	 l-p:0.05625665932893753
epoch£º556	 i:8 	 global-step:11128	 l-p:0.05733605846762657
epoch£º556	 i:9 	 global-step:11129	 l-p:0.05635549873113632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6367, 27.7004, 27.6426],
        [27.6367, 31.2381, 31.6089],
        [27.6367, 28.0987, 27.7839],
        [27.6367, 33.1974, 35.1696]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.05642472207546234 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05642472207546234 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7769], device='cuda:0')), ('power', tensor([-0.1366], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.05642472207546234
epoch£º557	 i:1 	 global-step:11141	 l-p:0.058356884866952896
epoch£º557	 i:2 	 global-step:11142	 l-p:0.056596651673316956
epoch£º557	 i:3 	 global-step:11143	 l-p:0.056338388472795486
epoch£º557	 i:4 	 global-step:11144	 l-p:0.05622737109661102
epoch£º557	 i:5 	 global-step:11145	 l-p:0.05705239251255989
epoch£º557	 i:6 	 global-step:11146	 l-p:0.056144408881664276
epoch£º557	 i:7 	 global-step:11147	 l-p:0.05649647116661072
epoch£º557	 i:8 	 global-step:11148	 l-p:0.05620145797729492
epoch£º557	 i:9 	 global-step:11149	 l-p:0.05634912848472595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7510, 30.0840, 29.7800],
        [27.7510, 32.9672, 34.6067],
        [27.7510, 27.9481, 27.7876],
        [27.7510, 27.7921, 27.7540]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.056704577058553696 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056704577058553696 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8037], device='cuda:0')), ('power', tensor([-0.0743], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.056704577058553696
epoch£º558	 i:1 	 global-step:11161	 l-p:0.05624896287918091
epoch£º558	 i:2 	 global-step:11162	 l-p:0.058200787752866745
epoch£º558	 i:3 	 global-step:11163	 l-p:0.05706353485584259
epoch£º558	 i:4 	 global-step:11164	 l-p:0.056302811950445175
epoch£º558	 i:5 	 global-step:11165	 l-p:0.056205298751592636
epoch£º558	 i:6 	 global-step:11166	 l-p:0.05616125464439392
epoch£º558	 i:7 	 global-step:11167	 l-p:0.05647803097963333
epoch£º558	 i:8 	 global-step:11168	 l-p:0.0562421940267086
epoch£º558	 i:9 	 global-step:11169	 l-p:0.056215040385723114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8634, 32.2332, 33.1438],
        [27.8634, 27.9190, 27.8682],
        [27.8634, 27.8636, 27.8634],
        [27.8634, 27.8673, 27.8635]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.05637554079294205 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05637554079294205 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.0691e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7687], device='cuda:0')), ('power', tensor([0.1214], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.05637554079294205
epoch£º559	 i:1 	 global-step:11181	 l-p:0.057572584599256516
epoch£º559	 i:2 	 global-step:11182	 l-p:0.05658872798085213
epoch£º559	 i:3 	 global-step:11183	 l-p:0.05611807107925415
epoch£º559	 i:4 	 global-step:11184	 l-p:0.05615020543336868
epoch£º559	 i:5 	 global-step:11185	 l-p:0.05640009790658951
epoch£º559	 i:6 	 global-step:11186	 l-p:0.056059617549180984
epoch£º559	 i:7 	 global-step:11187	 l-p:0.057746026664972305
epoch£º559	 i:8 	 global-step:11188	 l-p:0.05630854517221451
epoch£º559	 i:9 	 global-step:11189	 l-p:0.056139953434467316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[27.9700, 38.2477, 45.6458],
        [27.9700, 31.0940, 31.1486],
        [27.9700, 38.2926, 45.7512],
        [27.9700, 33.4904, 35.3836]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.057586561888456345 
model_pd.l_d.mean(): 1.068072106136242e-05 
model_pd.lagr.mean(): 0.057597242295742035 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.6790e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7455], device='cuda:0')), ('power', tensor([0.3950], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.057586561888456345
epoch£º560	 i:1 	 global-step:11201	 l-p:0.05622386932373047
epoch£º560	 i:2 	 global-step:11202	 l-p:0.056219492107629776
epoch£º560	 i:3 	 global-step:11203	 l-p:0.05627622455358505
epoch£º560	 i:4 	 global-step:11204	 l-p:0.05632401257753372
epoch£º560	 i:5 	 global-step:11205	 l-p:0.056141447275877
epoch£º560	 i:6 	 global-step:11206	 l-p:0.05613648146390915
epoch£º560	 i:7 	 global-step:11207	 l-p:0.05602283403277397
epoch£º560	 i:8 	 global-step:11208	 l-p:0.056052837520837784
epoch£º560	 i:9 	 global-step:11209	 l-p:0.058143433183431625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0676, 28.0676, 28.0676],
        [28.0676, 30.9599, 30.8851],
        [28.0676, 28.1249, 28.0726],
        [28.0676, 33.7506, 35.7849]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.05694391950964928 
model_pd.l_d.mean(): 2.742892138485331e-05 
model_pd.lagr.mean(): 0.05697134882211685 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8161], device='cuda:0')), ('power', tensor([0.2607], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.05694391950964928
epoch£º561	 i:1 	 global-step:11221	 l-p:0.05803266540169716
epoch£º561	 i:2 	 global-step:11222	 l-p:0.05609060823917389
epoch£º561	 i:3 	 global-step:11223	 l-p:0.056365083903074265
epoch£º561	 i:4 	 global-step:11224	 l-p:0.056760262697935104
epoch£º561	 i:5 	 global-step:11225	 l-p:0.05609537661075592
epoch£º561	 i:6 	 global-step:11226	 l-p:0.05609092116355896
epoch£º561	 i:7 	 global-step:11227	 l-p:0.05605415999889374
epoch£º561	 i:8 	 global-step:11228	 l-p:0.056178443133831024
epoch£º561	 i:9 	 global-step:11229	 l-p:0.0562482625246048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1512, 28.4274, 28.2140],
        [28.1512, 28.1544, 28.1513],
        [28.1512, 28.1516, 28.1512],
        [28.1512, 32.2874, 32.9929]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.05657422915101051 
model_pd.l_d.mean(): 6.20878126937896e-05 
model_pd.lagr.mean(): 0.05663631856441498 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8265], device='cuda:0')), ('power', tensor([0.2709], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.05657422915101051
epoch£º562	 i:1 	 global-step:11241	 l-p:0.056028880178928375
epoch£º562	 i:2 	 global-step:11242	 l-p:0.05619502067565918
epoch£º562	 i:3 	 global-step:11243	 l-p:0.05687765032052994
epoch£º562	 i:4 	 global-step:11244	 l-p:0.05763964354991913
epoch£º562	 i:5 	 global-step:11245	 l-p:0.0560583621263504
epoch£º562	 i:6 	 global-step:11246	 l-p:0.05634326487779617
epoch£º562	 i:7 	 global-step:11247	 l-p:0.056491509079933167
epoch£º562	 i:8 	 global-step:11248	 l-p:0.056234411895275116
epoch£º562	 i:9 	 global-step:11249	 l-p:0.05620449781417847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1942, 28.2375, 28.1974],
        [28.1942, 29.1247, 28.6514],
        [28.1942, 28.7657, 28.4000],
        [28.1942, 28.4213, 28.2398]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.056079648435115814 
model_pd.l_d.mean(): 9.186407987726852e-05 
model_pd.lagr.mean(): 0.056171514093875885 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8533], device='cuda:0')), ('power', tensor([0.2388], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.056079648435115814
epoch£º563	 i:1 	 global-step:11261	 l-p:0.056010082364082336
epoch£º563	 i:2 	 global-step:11262	 l-p:0.05612777918577194
epoch£º563	 i:3 	 global-step:11263	 l-p:0.05602230876684189
epoch£º563	 i:4 	 global-step:11264	 l-p:0.058293893933296204
epoch£º563	 i:5 	 global-step:11265	 l-p:0.057400114834308624
epoch£º563	 i:6 	 global-step:11266	 l-p:0.05606558546423912
epoch£º563	 i:7 	 global-step:11267	 l-p:0.05614731088280678
epoch£º563	 i:8 	 global-step:11268	 l-p:0.0560477040708065
epoch£º563	 i:9 	 global-step:11269	 l-p:0.05638411268591881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[28.1892, 31.9807, 32.4365],
        [28.1892, 33.7551, 35.6640],
        [28.1892, 30.5612, 30.2523],
        [28.1892, 29.3006, 28.8001]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.05693122372031212 
model_pd.l_d.mean(): 0.00021344595006667078 
model_pd.lagr.mean(): 0.05714466795325279 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8248], device='cuda:0')), ('power', tensor([0.3873], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.05693122372031212
epoch£º564	 i:1 	 global-step:11281	 l-p:0.05665303021669388
epoch£º564	 i:2 	 global-step:11282	 l-p:0.05608693137764931
epoch£º564	 i:3 	 global-step:11283	 l-p:0.05647385120391846
epoch£º564	 i:4 	 global-step:11284	 l-p:0.056233473122119904
epoch£º564	 i:5 	 global-step:11285	 l-p:0.056192055344581604
epoch£º564	 i:6 	 global-step:11286	 l-p:0.05622313544154167
epoch£º564	 i:7 	 global-step:11287	 l-p:0.05607370287179947
epoch£º564	 i:8 	 global-step:11288	 l-p:0.05612478032708168
epoch£º564	 i:9 	 global-step:11289	 l-p:0.057681597769260406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1259, 28.1550, 28.1276],
        [28.1259, 29.0499, 28.5786],
        [28.1259, 37.9954, 44.8142],
        [28.1259, 38.0326, 44.9004]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.0562458299100399 
model_pd.l_d.mean(): 0.00018479235586710274 
model_pd.lagr.mean(): 0.056430622935295105 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8059], device='cuda:0')), ('power', tensor([0.2629], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.0562458299100399
epoch£º565	 i:1 	 global-step:11301	 l-p:0.05602738633751869
epoch£º565	 i:2 	 global-step:11302	 l-p:0.05688934028148651
epoch£º565	 i:3 	 global-step:11303	 l-p:0.05614180490374565
epoch£º565	 i:4 	 global-step:11304	 l-p:0.0560879223048687
epoch£º565	 i:5 	 global-step:11305	 l-p:0.05632394179701805
epoch£º565	 i:6 	 global-step:11306	 l-p:0.056638482958078384
epoch£º565	 i:7 	 global-step:11307	 l-p:0.058007173240184784
epoch£º565	 i:8 	 global-step:11308	 l-p:0.05641569197177887
epoch£º565	 i:9 	 global-step:11309	 l-p:0.05616973713040352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0127, 28.0940, 28.0214],
        [28.0127, 28.0221, 28.0130],
        [28.0127, 28.8836, 28.4247],
        [28.0127, 28.3694, 28.1082]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.056212279945611954 
model_pd.l_d.mean(): 7.185516733443365e-05 
model_pd.lagr.mean(): 0.05628413334488869 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8194], device='cuda:0')), ('power', tensor([0.0885], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.056212279945611954
epoch£º566	 i:1 	 global-step:11321	 l-p:0.0561734102666378
epoch£º566	 i:2 	 global-step:11322	 l-p:0.056084051728248596
epoch£º566	 i:3 	 global-step:11323	 l-p:0.056972503662109375
epoch£º566	 i:4 	 global-step:11324	 l-p:0.05649228021502495
epoch£º566	 i:5 	 global-step:11325	 l-p:0.05619967356324196
epoch£º566	 i:6 	 global-step:11326	 l-p:0.05620471015572548
epoch£º566	 i:7 	 global-step:11327	 l-p:0.058347806334495544
epoch£º566	 i:8 	 global-step:11328	 l-p:0.05616159364581108
epoch£º566	 i:9 	 global-step:11329	 l-p:0.056517355144023895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[27.8597, 37.7542, 44.6668],
        [27.8597, 37.3054, 43.6320],
        [27.8597, 29.0918, 28.5878],
        [27.8597, 35.9792, 40.6629]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.05810404568910599 
model_pd.l_d.mean(): 0.00030380673706531525 
model_pd.lagr.mean(): 0.058407850563526154 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7184], device='cuda:0')), ('power', tensor([0.3551], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.05810404568910599
epoch£º567	 i:1 	 global-step:11341	 l-p:0.05624265968799591
epoch£º567	 i:2 	 global-step:11342	 l-p:0.05621879920363426
epoch£º567	 i:3 	 global-step:11343	 l-p:0.056159839034080505
epoch£º567	 i:4 	 global-step:11344	 l-p:0.056754615157842636
epoch£º567	 i:5 	 global-step:11345	 l-p:0.05702383443713188
epoch£º567	 i:6 	 global-step:11346	 l-p:0.05651910603046417
epoch£º567	 i:7 	 global-step:11347	 l-p:0.056133951991796494
epoch£º567	 i:8 	 global-step:11348	 l-p:0.05624332278966904
epoch£º567	 i:9 	 global-step:11349	 l-p:0.05643506720662117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6893, 30.6962, 30.7037],
        [27.6893, 27.7436, 27.6939],
        [27.6893, 34.6039, 37.9473],
        [27.6893, 30.9438, 31.0921]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.05640512704849243 
model_pd.l_d.mean(): -9.968785161618143e-05 
model_pd.lagr.mean(): 0.05630543828010559 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7919], device='cuda:0')), ('power', tensor([-0.1215], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.05640512704849243
epoch£º568	 i:1 	 global-step:11361	 l-p:0.05620180442929268
epoch£º568	 i:2 	 global-step:11362	 l-p:0.05803225561976433
epoch£º568	 i:3 	 global-step:11363	 l-p:0.05616411194205284
epoch£º568	 i:4 	 global-step:11364	 l-p:0.05622778832912445
epoch£º568	 i:5 	 global-step:11365	 l-p:0.056294336915016174
epoch£º568	 i:6 	 global-step:11366	 l-p:0.05626094713807106
epoch£º568	 i:7 	 global-step:11367	 l-p:0.05690637603402138
epoch£º568	 i:8 	 global-step:11368	 l-p:0.05752662569284439
epoch£º568	 i:9 	 global-step:11369	 l-p:0.056405678391456604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5180, 27.5181, 27.5180],
        [27.5180, 27.7276, 27.5586],
        [27.5180, 37.1647, 43.8287],
        [27.5180, 27.5180, 27.5180]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.056843146681785583 
model_pd.l_d.mean(): -0.00016429106472060084 
model_pd.lagr.mean(): 0.056678853929042816 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7685], device='cuda:0')), ('power', tensor([-0.2347], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.056843146681785583
epoch£º569	 i:1 	 global-step:11381	 l-p:0.05721733346581459
epoch£º569	 i:2 	 global-step:11382	 l-p:0.0563538484275341
epoch£º569	 i:3 	 global-step:11383	 l-p:0.056292448192834854
epoch£º569	 i:4 	 global-step:11384	 l-p:0.05639258399605751
epoch£º569	 i:5 	 global-step:11385	 l-p:0.05817575007677078
epoch£º569	 i:6 	 global-step:11386	 l-p:0.0562170147895813
epoch£º569	 i:7 	 global-step:11387	 l-p:0.05627545714378357
epoch£º569	 i:8 	 global-step:11388	 l-p:0.056268010288476944
epoch£º569	 i:9 	 global-step:11389	 l-p:0.056907784193754196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3808, 27.4416, 27.3864],
        [27.3808, 27.3808, 27.3808],
        [27.3808, 30.1479, 30.0495],
        [27.3808, 35.0734, 39.3427]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.05651668459177017 
model_pd.l_d.mean(): -0.00025623932015150785 
model_pd.lagr.mean(): 0.05626044422388077 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7772], device='cuda:0')), ('power', tensor([-0.5107], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.05651668459177017
epoch£º570	 i:1 	 global-step:11401	 l-p:0.05622769147157669
epoch£º570	 i:2 	 global-step:11402	 l-p:0.05621880292892456
epoch£º570	 i:3 	 global-step:11403	 l-p:0.056710485368967056
epoch£º570	 i:4 	 global-step:11404	 l-p:0.05630764365196228
epoch£º570	 i:5 	 global-step:11405	 l-p:0.056316327303647995
epoch£º570	 i:6 	 global-step:11406	 l-p:0.05721867457032204
epoch£º570	 i:7 	 global-step:11407	 l-p:0.05848564952611923
epoch£º570	 i:8 	 global-step:11408	 l-p:0.05701213702559471
epoch£º570	 i:9 	 global-step:11409	 l-p:0.05633658915758133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2997, 29.2160, 28.8012],
        [27.2997, 27.2999, 27.2997],
        [27.2997, 27.4934, 27.3357],
        [27.2997, 27.3299, 27.3015]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.05814758315682411 
model_pd.l_d.mean(): -9.854100790107623e-05 
model_pd.lagr.mean(): 0.0580490417778492 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7613], device='cuda:0')), ('power', tensor([-0.3989], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.05814758315682411
epoch£º571	 i:1 	 global-step:11421	 l-p:0.05685152858495712
epoch£º571	 i:2 	 global-step:11422	 l-p:0.05756145343184471
epoch£º571	 i:3 	 global-step:11423	 l-p:0.05638165771961212
epoch£º571	 i:4 	 global-step:11424	 l-p:0.05644864961504936
epoch£º571	 i:5 	 global-step:11425	 l-p:0.05634723976254463
epoch£º571	 i:6 	 global-step:11426	 l-p:0.05634837970137596
epoch£º571	 i:7 	 global-step:11427	 l-p:0.05620817095041275
epoch£º571	 i:8 	 global-step:11428	 l-p:0.056311335414648056
epoch£º571	 i:9 	 global-step:11429	 l-p:0.05686309188604355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3057, 27.3108, 27.3058],
        [27.3057, 37.3285, 44.5421],
        [27.3057, 36.6873, 43.0527],
        [27.3057, 27.3796, 27.3133]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.05696084722876549 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05696084722876549 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7479], device='cuda:0')), ('power', tensor([-0.4135], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.05696084722876549
epoch£º572	 i:1 	 global-step:11441	 l-p:0.05645347014069557
epoch£º572	 i:2 	 global-step:11442	 l-p:0.05639459192752838
epoch£º572	 i:3 	 global-step:11443	 l-p:0.05628247931599617
epoch£º572	 i:4 	 global-step:11444	 l-p:0.05734066665172577
epoch£º572	 i:5 	 global-step:11445	 l-p:0.05802950635552406
epoch£º572	 i:6 	 global-step:11446	 l-p:0.056247249245643616
epoch£º572	 i:7 	 global-step:11447	 l-p:0.05644218251109123
epoch£º572	 i:8 	 global-step:11448	 l-p:0.05637581646442413
epoch£º572	 i:9 	 global-step:11449	 l-p:0.05682516098022461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3766, 27.7735, 27.4921],
        [27.3766, 28.9081, 28.4235],
        [27.3766, 27.9740, 27.6023],
        [27.3766, 27.3779, 27.3766]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.0580216646194458 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0580216646194458 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7885], device='cuda:0')), ('power', tensor([-0.3911], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.0580216646194458
epoch£º573	 i:1 	 global-step:11461	 l-p:0.05730186402797699
epoch£º573	 i:2 	 global-step:11462	 l-p:0.0563008189201355
epoch£º573	 i:3 	 global-step:11463	 l-p:0.05638280138373375
epoch£º573	 i:4 	 global-step:11464	 l-p:0.056352175772190094
epoch£º573	 i:5 	 global-step:11465	 l-p:0.05707157775759697
epoch£º573	 i:6 	 global-step:11466	 l-p:0.056617621332407
epoch£º573	 i:7 	 global-step:11467	 l-p:0.05614858493208885
epoch£º573	 i:8 	 global-step:11468	 l-p:0.05642170459032059
epoch£º573	 i:9 	 global-step:11469	 l-p:0.05644240230321884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4850, 30.0918, 29.9123],
        [27.4850, 33.3180, 35.5720],
        [27.4850, 27.4850, 27.4850],
        [27.4850, 27.5069, 27.4861]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.05640481039881706 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05640481039881706 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8103], device='cuda:0')), ('power', tensor([-0.4306], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.05640481039881706
epoch£º574	 i:1 	 global-step:11481	 l-p:0.05626506730914116
epoch£º574	 i:2 	 global-step:11482	 l-p:0.056689005345106125
epoch£º574	 i:3 	 global-step:11483	 l-p:0.05691114440560341
epoch£º574	 i:4 	 global-step:11484	 l-p:0.0573541633784771
epoch£º574	 i:5 	 global-step:11485	 l-p:0.05624842643737793
epoch£º574	 i:6 	 global-step:11486	 l-p:0.05799693614244461
epoch£º574	 i:7 	 global-step:11487	 l-p:0.05621976777911186
epoch£º574	 i:8 	 global-step:11488	 l-p:0.056229159235954285
epoch£º574	 i:9 	 global-step:11489	 l-p:0.056355323642492294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5982, 27.6685, 27.6052],
        [27.5982, 29.0324, 28.5351],
        [27.5982, 27.5983, 27.5982],
        [27.5982, 27.6783, 27.6069]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.056277453899383545 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056277453899383545 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7971], device='cuda:0')), ('power', tensor([-0.2701], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.056277453899383545
epoch£º575	 i:1 	 global-step:11501	 l-p:0.05670660734176636
epoch£º575	 i:2 	 global-step:11502	 l-p:0.056167569011449814
epoch£º575	 i:3 	 global-step:11503	 l-p:0.05731479078531265
epoch£º575	 i:4 	 global-step:11504	 l-p:0.056384265422821045
epoch£º575	 i:5 	 global-step:11505	 l-p:0.056095123291015625
epoch£º575	 i:6 	 global-step:11506	 l-p:0.05612988397479057
epoch£º575	 i:7 	 global-step:11507	 l-p:0.058314718306064606
epoch£º575	 i:8 	 global-step:11508	 l-p:0.05627160519361496
epoch£º575	 i:9 	 global-step:11509	 l-p:0.05662525072693825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7154, 27.7170, 27.7154],
        [27.7154, 27.7348, 27.7163],
        [27.7154, 36.9992, 43.1489],
        [27.7154, 32.1616, 33.1471]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.05628695338964462 
model_pd.l_d.mean(): -6.176698548188142e-07 
model_pd.lagr.mean(): 0.05628633499145508 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8039], device='cuda:0')), ('power', tensor([-0.1840], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.05628695338964462
epoch£º576	 i:1 	 global-step:11521	 l-p:0.05614669993519783
epoch£º576	 i:2 	 global-step:11522	 l-p:0.05649615079164505
epoch£º576	 i:3 	 global-step:11523	 l-p:0.05622091144323349
epoch£º576	 i:4 	 global-step:11524	 l-p:0.057689689099788666
epoch£º576	 i:5 	 global-step:11525	 l-p:0.056293632835149765
epoch£º576	 i:6 	 global-step:11526	 l-p:0.05615277215838432
epoch£º576	 i:7 	 global-step:11527	 l-p:0.05795937031507492
epoch£º576	 i:8 	 global-step:11528	 l-p:0.056348055601119995
epoch£º576	 i:9 	 global-step:11529	 l-p:0.05631077662110329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8347, 28.6828, 28.2309],
        [27.8347, 27.8347, 27.8347],
        [27.8347, 33.9571, 36.4513],
        [27.8347, 27.8347, 27.8347]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.05613474175333977 
model_pd.l_d.mean(): -1.4612672316616226e-07 
model_pd.lagr.mean(): 0.05613459646701813 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8467], device='cuda:0')), ('power', tensor([-0.1437], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.05613474175333977
epoch£º577	 i:1 	 global-step:11541	 l-p:0.05717797204852104
epoch£º577	 i:2 	 global-step:11542	 l-p:0.05612699314951897
epoch£º577	 i:3 	 global-step:11543	 l-p:0.056365661323070526
epoch£º577	 i:4 	 global-step:11544	 l-p:0.056241557002067566
epoch£º577	 i:5 	 global-step:11545	 l-p:0.057055745273828506
epoch£º577	 i:6 	 global-step:11546	 l-p:0.05631576478481293
epoch£º577	 i:7 	 global-step:11547	 l-p:0.05612419918179512
epoch£º577	 i:8 	 global-step:11548	 l-p:0.057812612503767014
epoch£º577	 i:9 	 global-step:11549	 l-p:0.056168489158153534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9530, 27.9530, 27.9530],
        [27.9530, 33.8968, 36.1980],
        [27.9530, 29.7173, 29.2496],
        [27.9530, 31.1517, 31.2497]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.05617864802479744 
model_pd.l_d.mean(): -8.873119128338658e-08 
model_pd.lagr.mean(): 0.056178558617830276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.2397e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8444], device='cuda:0')), ('power', tensor([-0.0039], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.05617864802479744
epoch£º578	 i:1 	 global-step:11561	 l-p:0.05606023222208023
epoch£º578	 i:2 	 global-step:11562	 l-p:0.05912543460726738
epoch£º578	 i:3 	 global-step:11563	 l-p:0.05631101503968239
epoch£º578	 i:4 	 global-step:11564	 l-p:0.056081321090459824
epoch£º578	 i:5 	 global-step:11565	 l-p:0.05614345520734787
epoch£º578	 i:6 	 global-step:11566	 l-p:0.05631605535745621
epoch£º578	 i:7 	 global-step:11567	 l-p:0.05665186420083046
epoch£º578	 i:8 	 global-step:11568	 l-p:0.0561249777674675
epoch£º578	 i:9 	 global-step:11569	 l-p:0.056179869920015335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0693, 38.0479, 45.0235],
        [28.0693, 28.6381, 28.2741],
        [28.0693, 31.1198, 31.1277],
        [28.0693, 28.4523, 28.1765]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.05617261677980423 
model_pd.l_d.mean(): 1.2133508789702319e-05 
model_pd.lagr.mean(): 0.05618475005030632 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8406], device='cuda:0')), ('power', tensor([0.1258], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.05617261677980423
epoch£º579	 i:1 	 global-step:11581	 l-p:0.05614197999238968
epoch£º579	 i:2 	 global-step:11582	 l-p:0.05776555463671684
epoch£º579	 i:3 	 global-step:11583	 l-p:0.05608909949660301
epoch£º579	 i:4 	 global-step:11584	 l-p:0.05605851858854294
epoch£º579	 i:5 	 global-step:11585	 l-p:0.056209687143564224
epoch£º579	 i:6 	 global-step:11586	 l-p:0.05687084421515465
epoch£º579	 i:7 	 global-step:11587	 l-p:0.05674448981881142
epoch£º579	 i:8 	 global-step:11588	 l-p:0.0565795823931694
epoch£º579	 i:9 	 global-step:11589	 l-p:0.05620357394218445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1596, 28.2856, 28.1772],
        [28.1596, 28.1923, 28.1617],
        [28.1596, 32.8144, 33.9251],
        [28.1596, 28.3239, 28.1866]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.05599798262119293 
model_pd.l_d.mean(): 1.9003357010660693e-05 
model_pd.lagr.mean(): 0.056016985327005386 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8963], device='cuda:0')), ('power', tensor([0.0856], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.05599798262119293
epoch£º580	 i:1 	 global-step:11601	 l-p:0.05603102594614029
epoch£º580	 i:2 	 global-step:11602	 l-p:0.05608587712049484
epoch£º580	 i:3 	 global-step:11603	 l-p:0.056190505623817444
epoch£º580	 i:4 	 global-step:11604	 l-p:0.05708704888820648
epoch£º580	 i:5 	 global-step:11605	 l-p:0.056159038096666336
epoch£º580	 i:6 	 global-step:11606	 l-p:0.057636719197034836
epoch£º580	 i:7 	 global-step:11607	 l-p:0.05662642791867256
epoch£º580	 i:8 	 global-step:11608	 l-p:0.05663501098752022
epoch£º580	 i:9 	 global-step:11609	 l-p:0.05615617707371712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2105, 28.2106, 28.2105],
        [28.2105, 35.5812, 39.3424],
        [28.2105, 31.1408, 31.0770],
        [28.2105, 29.0039, 28.5626]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.05616031959652901 
model_pd.l_d.mean(): 0.00012120757310185581 
model_pd.lagr.mean(): 0.056281525641679764 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8178], device='cuda:0')), ('power', tensor([0.3164], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.05616031959652901
epoch£º581	 i:1 	 global-step:11621	 l-p:0.0562765933573246
epoch£º581	 i:2 	 global-step:11622	 l-p:0.056064195930957794
epoch£º581	 i:3 	 global-step:11623	 l-p:0.05650625005364418
epoch£º581	 i:4 	 global-step:11624	 l-p:0.0569734089076519
epoch£º581	 i:5 	 global-step:11625	 l-p:0.056045178323984146
epoch£º581	 i:6 	 global-step:11626	 l-p:0.05609797313809395
epoch£º581	 i:7 	 global-step:11627	 l-p:0.05757707357406616
epoch£º581	 i:8 	 global-step:11628	 l-p:0.056554701179265976
epoch£º581	 i:9 	 global-step:11629	 l-p:0.05626837909221649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2071, 28.5536, 28.2978],
        [28.2071, 37.9371, 44.5552],
        [28.2071, 28.2091, 28.2072],
        [28.2071, 28.2698, 28.2129]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.05804525315761566 
model_pd.l_d.mean(): 0.00038057006895542145 
model_pd.lagr.mean(): 0.058425821363925934 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7493], device='cuda:0')), ('power', tensor([0.6816], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.05804525315761566
epoch£º582	 i:1 	 global-step:11641	 l-p:0.056679029017686844
epoch£º582	 i:2 	 global-step:11642	 l-p:0.05620218813419342
epoch£º582	 i:3 	 global-step:11643	 l-p:0.05605467036366463
epoch£º582	 i:4 	 global-step:11644	 l-p:0.05608104541897774
epoch£º582	 i:5 	 global-step:11645	 l-p:0.056157197803258896
epoch£º582	 i:6 	 global-step:11646	 l-p:0.05702725425362587
epoch£º582	 i:7 	 global-step:11647	 l-p:0.056282758712768555
epoch£º582	 i:8 	 global-step:11648	 l-p:0.05608230456709862
epoch£º582	 i:9 	 global-step:11649	 l-p:0.05598985031247139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1428, 28.3618, 28.1858],
        [28.1428, 34.7227, 37.6415],
        [28.1428, 34.9201, 38.0466],
        [28.1428, 31.5400, 31.7433]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.056121062487363815 
model_pd.l_d.mean(): 0.00017136857786681503 
model_pd.lagr.mean(): 0.05629242956638336 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8329], device='cuda:0')), ('power', tensor([0.2381], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.056121062487363815
epoch£º583	 i:1 	 global-step:11661	 l-p:0.056881822645664215
epoch£º583	 i:2 	 global-step:11662	 l-p:0.057274896651506424
epoch£º583	 i:3 	 global-step:11663	 l-p:0.05615806579589844
epoch£º583	 i:4 	 global-step:11664	 l-p:0.05614754557609558
epoch£º583	 i:5 	 global-step:11665	 l-p:0.05616409704089165
epoch£º583	 i:6 	 global-step:11666	 l-p:0.05617766082286835
epoch£º583	 i:7 	 global-step:11667	 l-p:0.056073956191539764
epoch£º583	 i:8 	 global-step:11668	 l-p:0.05618470907211304
epoch£º583	 i:9 	 global-step:11669	 l-p:0.05773894116282463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0073, 28.1326, 28.0248],
        [28.0073, 28.0073, 28.0073],
        [28.0073, 28.3840, 28.1118],
        [28.0073, 28.4759, 28.1567]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.056092437356710434 
model_pd.l_d.mean(): 1.2216754839755595e-05 
model_pd.lagr.mean(): 0.05610465258359909 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8458], device='cuda:0')), ('power', tensor([0.0147], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.056092437356710434
epoch£º584	 i:1 	 global-step:11681	 l-p:0.05820322781801224
epoch£º584	 i:2 	 global-step:11682	 l-p:0.056117117404937744
epoch£º584	 i:3 	 global-step:11683	 l-p:0.056643202900886536
epoch£º584	 i:4 	 global-step:11684	 l-p:0.05629873648285866
epoch£º584	 i:5 	 global-step:11685	 l-p:0.05616675317287445
epoch£º584	 i:6 	 global-step:11686	 l-p:0.05697578191757202
epoch£º584	 i:7 	 global-step:11687	 l-p:0.05637054890394211
epoch£º584	 i:8 	 global-step:11688	 l-p:0.056158170104026794
epoch£º584	 i:9 	 global-step:11689	 l-p:0.05634735897183418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8340, 28.3875, 28.0309],
        [27.8340, 27.8752, 27.8369],
        [27.8340, 27.8423, 27.8342],
        [27.8340, 27.8887, 27.8386]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.056240539997816086 
model_pd.l_d.mean(): -0.00012689147843047976 
model_pd.lagr.mean(): 0.056113649159669876 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8300], device='cuda:0')), ('power', tensor([-0.1458], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.056240539997816086
epoch£º585	 i:1 	 global-step:11701	 l-p:0.056292202323675156
epoch£º585	 i:2 	 global-step:11702	 l-p:0.056310806423425674
epoch£º585	 i:3 	 global-step:11703	 l-p:0.05661030486226082
epoch£º585	 i:4 	 global-step:11704	 l-p:0.057809583842754364
epoch£º585	 i:5 	 global-step:11705	 l-p:0.05770984664559364
epoch£º585	 i:6 	 global-step:11706	 l-p:0.056269578635692596
epoch£º585	 i:7 	 global-step:11707	 l-p:0.05621636286377907
epoch£º585	 i:8 	 global-step:11708	 l-p:0.05630204454064369
epoch£º585	 i:9 	 global-step:11709	 l-p:0.0562245212495327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6375, 27.6408, 27.6376],
        [27.6375, 37.1385, 43.5853],
        [27.6375, 28.9373, 28.4358],
        [27.6375, 27.8358, 27.6746]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.056355398148298264 
model_pd.l_d.mean(): -0.00021804329298902303 
model_pd.lagr.mean(): 0.05613735318183899 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7971], device='cuda:0')), ('power', tensor([-0.2674], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.056355398148298264
epoch£º586	 i:1 	 global-step:11721	 l-p:0.057954397052526474
epoch£º586	 i:2 	 global-step:11722	 l-p:0.056348979473114014
epoch£º586	 i:3 	 global-step:11723	 l-p:0.05632995441555977
epoch£º586	 i:4 	 global-step:11724	 l-p:0.05631135776638985
epoch£º586	 i:5 	 global-step:11725	 l-p:0.05663920193910599
epoch£º586	 i:6 	 global-step:11726	 l-p:0.05615503340959549
epoch£º586	 i:7 	 global-step:11727	 l-p:0.05695604160428047
epoch£º586	 i:8 	 global-step:11728	 l-p:0.05633120983839035
epoch£º586	 i:9 	 global-step:11729	 l-p:0.05722559615969658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4527, 30.3216, 30.2703],
        [27.4527, 33.5671, 36.1059],
        [27.4527, 32.9747, 34.9329],
        [27.4527, 27.8137, 27.5514]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.0564018152654171 
model_pd.l_d.mean(): -0.0002645797503646463 
model_pd.lagr.mean(): 0.05613723397254944 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7782], device='cuda:0')), ('power', tensor([-0.3970], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.0564018152654171
epoch£º587	 i:1 	 global-step:11741	 l-p:0.05648069083690643
epoch£º587	 i:2 	 global-step:11742	 l-p:0.057691287249326706
epoch£º587	 i:3 	 global-step:11743	 l-p:0.056319959461688995
epoch£º587	 i:4 	 global-step:11744	 l-p:0.05675128847360611
epoch£º587	 i:5 	 global-step:11745	 l-p:0.05625435709953308
epoch£º587	 i:6 	 global-step:11746	 l-p:0.05621442198753357
epoch£º587	 i:7 	 global-step:11747	 l-p:0.05833321437239647
epoch£º587	 i:8 	 global-step:11748	 l-p:0.05651438608765602
epoch£º587	 i:9 	 global-step:11749	 l-p:0.05622250959277153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3124, 27.3124, 27.3124],
        [27.3124, 27.9333, 27.5532],
        [27.3124, 29.3597, 28.9795],
        [27.3124, 27.8216, 27.4864]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.056337859481573105 
model_pd.l_d.mean(): -0.00027050869539380074 
model_pd.lagr.mean(): 0.05606735125184059 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7901], device='cuda:0')), ('power', tensor([-0.6224], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.056337859481573105
epoch£º588	 i:1 	 global-step:11761	 l-p:0.05620408058166504
epoch£º588	 i:2 	 global-step:11762	 l-p:0.05641976371407509
epoch£º588	 i:3 	 global-step:11763	 l-p:0.05647661164402962
epoch£º588	 i:4 	 global-step:11764	 l-p:0.05627361312508583
epoch£º588	 i:5 	 global-step:11765	 l-p:0.05632472783327103
epoch£º588	 i:6 	 global-step:11766	 l-p:0.0582096241414547
epoch£º588	 i:7 	 global-step:11767	 l-p:0.057942356914281845
epoch£º588	 i:8 	 global-step:11768	 l-p:0.0566609650850296
epoch£º588	 i:9 	 global-step:11769	 l-p:0.05671282485127449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2480, 27.3282, 27.2567],
        [27.2480, 27.2521, 27.2481],
        [27.2480, 27.5147, 27.3086],
        [27.2480, 27.2481, 27.2480]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.05643954128026962 
model_pd.l_d.mean(): -9.11806637304835e-05 
model_pd.lagr.mean(): 0.05634836107492447 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7644], device='cuda:0')), ('power', tensor([-0.6111], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.05643954128026962
epoch£º589	 i:1 	 global-step:11781	 l-p:0.05626926198601723
epoch£º589	 i:2 	 global-step:11782	 l-p:0.05828825756907463
epoch£º589	 i:3 	 global-step:11783	 l-p:0.05630745738744736
epoch£º589	 i:4 	 global-step:11784	 l-p:0.05635818839073181
epoch£º589	 i:5 	 global-step:11785	 l-p:0.05707632005214691
epoch£º589	 i:6 	 global-step:11786	 l-p:0.057302672415971756
epoch£º589	 i:7 	 global-step:11787	 l-p:0.056336186826229095
epoch£º589	 i:8 	 global-step:11788	 l-p:0.056371163576841354
epoch£º589	 i:9 	 global-step:11789	 l-p:0.056861165910959244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.2857, 27.4913, 27.3253],
        [27.2857, 32.3377, 33.8829],
        [27.2857, 27.6554, 27.3888],
        [27.2857, 27.3392, 27.2902]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.05640044063329697 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05640044063329697 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7912], device='cuda:0')), ('power', tensor([-0.5970], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.05640044063329697
epoch£º590	 i:1 	 global-step:11801	 l-p:0.056469693779945374
epoch£º590	 i:2 	 global-step:11802	 l-p:0.05622045323252678
epoch£º590	 i:3 	 global-step:11803	 l-p:0.05723455175757408
epoch£º590	 i:4 	 global-step:11804	 l-p:0.05659214034676552
epoch£º590	 i:5 	 global-step:11805	 l-p:0.05720549076795578
epoch£º590	 i:6 	 global-step:11806	 l-p:0.058145225048065186
epoch£º590	 i:7 	 global-step:11807	 l-p:0.056364450603723526
epoch£º590	 i:8 	 global-step:11808	 l-p:0.05650049075484276
epoch£º590	 i:9 	 global-step:11809	 l-p:0.056241028010845184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3824, 33.6194, 36.2929],
        [27.3824, 30.2003, 30.1271],
        [27.3824, 27.3830, 27.3824],
        [27.3824, 27.3842, 27.3825]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.05845818296074867 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05845818296074867 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7364], device='cuda:0')), ('power', tensor([-0.2114], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.05845818296074867
epoch£º591	 i:1 	 global-step:11821	 l-p:0.05622926726937294
epoch£º591	 i:2 	 global-step:11822	 l-p:0.05617581307888031
epoch£º591	 i:3 	 global-step:11823	 l-p:0.05645884573459625
epoch£º591	 i:4 	 global-step:11824	 l-p:0.05623769015073776
epoch£º591	 i:5 	 global-step:11825	 l-p:0.056383002549409866
epoch£º591	 i:6 	 global-step:11826	 l-p:0.056620750576257706
epoch£º591	 i:7 	 global-step:11827	 l-p:0.05717983469367027
epoch£º591	 i:8 	 global-step:11828	 l-p:0.05640166252851486
epoch£º591	 i:9 	 global-step:11829	 l-p:0.05686500295996666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[27.5051, 29.2688, 28.8142],
        [27.5051, 29.2711, 28.8170],
        [27.5051, 37.5654, 44.7818],
        [27.5051, 29.5649, 29.1812]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.05660122260451317 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05660122260451317 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8120], device='cuda:0')), ('power', tensor([-0.3829], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.05660122260451317
epoch£º592	 i:1 	 global-step:11841	 l-p:0.05626026913523674
epoch£º592	 i:2 	 global-step:11842	 l-p:0.056313998997211456
epoch£º592	 i:3 	 global-step:11843	 l-p:0.05662945285439491
epoch£º592	 i:4 	 global-step:11844	 l-p:0.057293590158224106
epoch£º592	 i:5 	 global-step:11845	 l-p:0.05623412877321243
epoch£º592	 i:6 	 global-step:11846	 l-p:0.056223511695861816
epoch£º592	 i:7 	 global-step:11847	 l-p:0.056338660418987274
epoch£º592	 i:8 	 global-step:11848	 l-p:0.05854135751724243
epoch£º592	 i:9 	 global-step:11849	 l-p:0.056140925735235214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6289, 28.1190, 27.7910],
        [27.6289, 28.5022, 28.0466],
        [27.6289, 27.6290, 27.6289],
        [27.6289, 27.6289, 27.6289]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.056172922253608704 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056172922253608704 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8498], device='cuda:0')), ('power', tensor([-0.3714], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.056172922253608704
epoch£º593	 i:1 	 global-step:11861	 l-p:0.056320589035749435
epoch£º593	 i:2 	 global-step:11862	 l-p:0.05628661811351776
epoch£º593	 i:3 	 global-step:11863	 l-p:0.05626869574189186
epoch£º593	 i:4 	 global-step:11864	 l-p:0.056209027767181396
epoch£º593	 i:5 	 global-step:11865	 l-p:0.057451650500297546
epoch£º593	 i:6 	 global-step:11866	 l-p:0.05691644549369812
epoch£º593	 i:7 	 global-step:11867	 l-p:0.05639667436480522
epoch£º593	 i:8 	 global-step:11868	 l-p:0.05799046531319618
epoch£º593	 i:9 	 global-step:11869	 l-p:0.05614880472421646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7570, 28.5456, 28.1094],
        [27.7570, 27.7572, 27.7570],
        [27.7570, 28.1635, 27.8760],
        [27.7570, 35.5499, 39.8690]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.05625592917203903 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05625592917203903 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8030], device='cuda:0')), ('power', tensor([-0.0946], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.05625592917203903
epoch£º594	 i:1 	 global-step:11881	 l-p:0.05617315322160721
epoch£º594	 i:2 	 global-step:11882	 l-p:0.05618441477417946
epoch£º594	 i:3 	 global-step:11883	 l-p:0.05780382454395294
epoch£º594	 i:4 	 global-step:11884	 l-p:0.05721289664506912
epoch£º594	 i:5 	 global-step:11885	 l-p:0.05636302009224892
epoch£º594	 i:6 	 global-step:11886	 l-p:0.056064967066049576
epoch£º594	 i:7 	 global-step:11887	 l-p:0.05680691450834274
epoch£º594	 i:8 	 global-step:11888	 l-p:0.056295137852430344
epoch£º594	 i:9 	 global-step:11889	 l-p:0.05660002678632736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8907, 35.7568, 40.1371],
        [27.8907, 29.6830, 29.2222],
        [27.8907, 27.8909, 27.8907],
        [27.8907, 27.8990, 27.8910]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.05615655705332756 
model_pd.l_d.mean(): -8.487021432301844e-07 
model_pd.lagr.mean(): 0.05615570768713951 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.6091e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8428], device='cuda:0')), ('power', tensor([-0.0921], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.05615655705332756
epoch£º595	 i:1 	 global-step:11901	 l-p:0.05625363811850548
epoch£º595	 i:2 	 global-step:11902	 l-p:0.05610052868723869
epoch£º595	 i:3 	 global-step:11903	 l-p:0.056152552366256714
epoch£º595	 i:4 	 global-step:11904	 l-p:0.05785438418388367
epoch£º595	 i:5 	 global-step:11905	 l-p:0.05702940374612808
epoch£º595	 i:6 	 global-step:11906	 l-p:0.05607771873474121
epoch£º595	 i:7 	 global-step:11907	 l-p:0.05725666880607605
epoch£º595	 i:8 	 global-step:11908	 l-p:0.05621083080768585
epoch£º595	 i:9 	 global-step:11909	 l-p:0.056248512119054794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0188, 28.2443, 28.0640],
        [28.0188, 28.0228, 28.0189],
        [28.0188, 32.5162, 33.5133],
        [28.0188, 28.0189, 28.0188]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.05616368353366852 
model_pd.l_d.mean(): 2.3935854187584482e-06 
model_pd.lagr.mean(): 0.05616607889533043 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.6309e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8466], device='cuda:0')), ('power', tensor([0.0442], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.05616368353366852
epoch£º596	 i:1 	 global-step:11921	 l-p:0.0578349307179451
epoch£º596	 i:2 	 global-step:11922	 l-p:0.05645345523953438
epoch£º596	 i:3 	 global-step:11923	 l-p:0.05634922906756401
epoch£º596	 i:4 	 global-step:11924	 l-p:0.05621844157576561
epoch£º596	 i:5 	 global-step:11925	 l-p:0.056156110018491745
epoch£º596	 i:6 	 global-step:11926	 l-p:0.05672497674822807
epoch£º596	 i:7 	 global-step:11927	 l-p:0.05609098821878433
epoch£º596	 i:8 	 global-step:11928	 l-p:0.05609701946377754
epoch£º596	 i:9 	 global-step:11929	 l-p:0.05687521770596504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1318, 29.6007, 29.0936],
        [28.1318, 28.2744, 28.1533],
        [28.1318, 28.1320, 28.1318],
        [28.1318, 28.1402, 28.1320]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.05615844577550888 
model_pd.l_d.mean(): 2.61120694631245e-05 
model_pd.lagr.mean(): 0.0561845563352108 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8355], device='cuda:0')), ('power', tensor([0.1631], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.05615844577550888
epoch£º597	 i:1 	 global-step:11941	 l-p:0.05621408671140671
epoch£º597	 i:2 	 global-step:11942	 l-p:0.05604252591729164
epoch£º597	 i:3 	 global-step:11943	 l-p:0.05679444968700409
epoch£º597	 i:4 	 global-step:11944	 l-p:0.056110724806785583
epoch£º597	 i:5 	 global-step:11945	 l-p:0.056391265243291855
epoch£º597	 i:6 	 global-step:11946	 l-p:0.05704180896282196
epoch£º597	 i:7 	 global-step:11947	 l-p:0.05754240229725838
epoch£º597	 i:8 	 global-step:11948	 l-p:0.05621643736958504
epoch£º597	 i:9 	 global-step:11949	 l-p:0.05614456906914711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2062, 28.2102, 28.2063],
        [28.2062, 35.2098, 38.5689],
        [28.2062, 29.3464, 28.8429],
        [28.2062, 37.5482, 43.6671]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.057043228298425674 
model_pd.l_d.mean(): 0.00015684583922848105 
model_pd.lagr.mean(): 0.057200074195861816 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7787], device='cuda:0')), ('power', tensor([0.5012], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.057043228298425674
epoch£º598	 i:1 	 global-step:11961	 l-p:0.056046150624752045
epoch£º598	 i:2 	 global-step:11962	 l-p:0.05819186195731163
epoch£º598	 i:3 	 global-step:11963	 l-p:0.056000564247369766
epoch£º598	 i:4 	 global-step:11964	 l-p:0.05612754821777344
epoch£º598	 i:5 	 global-step:11965	 l-p:0.0560992956161499
epoch£º598	 i:6 	 global-step:11966	 l-p:0.05620110407471657
epoch£º598	 i:7 	 global-step:11967	 l-p:0.056100741028785706
epoch£º598	 i:8 	 global-step:11968	 l-p:0.05614732578396797
epoch£º598	 i:9 	 global-step:11969	 l-p:0.056539516896009445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.2306, 28.2306, 28.2306],
        [28.2306, 29.4619, 28.9517],
        [28.2306, 36.1971, 40.6337],
        [28.2306, 28.2505, 28.2316]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.05627613887190819 
model_pd.l_d.mean(): 0.00022370388614945114 
model_pd.lagr.mean(): 0.056499842554330826 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7657], device='cuda:0')), ('power', tensor([0.4536], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.05627613887190819
epoch£º599	 i:1 	 global-step:11981	 l-p:0.055982764810323715
epoch£º599	 i:2 	 global-step:11982	 l-p:0.0578574538230896
epoch£º599	 i:3 	 global-step:11983	 l-p:0.05652476102113724
epoch£º599	 i:4 	 global-step:11984	 l-p:0.05617423355579376
epoch£º599	 i:5 	 global-step:11985	 l-p:0.0561453141272068
epoch£º599	 i:6 	 global-step:11986	 l-p:0.05599880963563919
epoch£º599	 i:7 	 global-step:11987	 l-p:0.05653436481952667
epoch£º599	 i:8 	 global-step:11988	 l-p:0.056127164512872696
epoch£º599	 i:9 	 global-step:11989	 l-p:0.05690006911754608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1808, 34.5484, 37.2428],
        [28.1808, 28.1808, 28.1807],
        [28.1808, 28.1808, 28.1808],
        [28.1808, 28.1807, 28.1807]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.057537734508514404 
model_pd.l_d.mean(): 0.00026502329274080694 
model_pd.lagr.mean(): 0.05780275911092758 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8301], device='cuda:0')), ('power', tensor([0.3963], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.057537734508514404
epoch£º600	 i:1 	 global-step:12001	 l-p:0.05610232055187225
epoch£º600	 i:2 	 global-step:12002	 l-p:0.05617796629667282
epoch£º600	 i:3 	 global-step:12003	 l-p:0.05623551830649376
epoch£º600	 i:4 	 global-step:12004	 l-p:0.056079939007759094
epoch£º600	 i:5 	 global-step:12005	 l-p:0.05616036430001259
epoch£º600	 i:6 	 global-step:12006	 l-p:0.05703289061784744
epoch£º600	 i:7 	 global-step:12007	 l-p:0.056261345744132996
epoch£º600	 i:8 	 global-step:12008	 l-p:0.056100472807884216
epoch£º600	 i:9 	 global-step:12009	 l-p:0.05699062719941139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1334, 33.6018, 35.4267],
        [28.1334, 31.1913, 31.1992],
        [28.1334, 28.2479, 28.1485],
        [28.1334, 28.1366, 28.1335]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.05611572042107582 
model_pd.l_d.mean(): 9.762429544935003e-05 
model_pd.lagr.mean(): 0.056213345378637314 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8648], device='cuda:0')), ('power', tensor([0.1192], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.05611572042107582
epoch£º601	 i:1 	 global-step:12021	 l-p:0.05628688633441925
epoch£º601	 i:2 	 global-step:12022	 l-p:0.056087084114551544
epoch£º601	 i:3 	 global-step:12023	 l-p:0.05669035762548447
epoch£º601	 i:4 	 global-step:12024	 l-p:0.057587817311286926
epoch£º601	 i:5 	 global-step:12025	 l-p:0.05617358535528183
epoch£º601	 i:6 	 global-step:12026	 l-p:0.05650505796074867
epoch£º601	 i:7 	 global-step:12027	 l-p:0.056270502507686615
epoch£º601	 i:8 	 global-step:12028	 l-p:0.056125592440366745
epoch£º601	 i:9 	 global-step:12029	 l-p:0.057024676352739334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0603, 28.0603, 28.0603],
        [28.0603, 34.2085, 36.6978],
        [28.0603, 28.1977, 28.0806],
        [28.0603, 33.7572, 35.8055]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.05618539825081825 
model_pd.l_d.mean(): 0.00013411149848252535 
model_pd.lagr.mean(): 0.05631950870156288 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8303], device='cuda:0')), ('power', tensor([0.1428], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.05618539825081825
epoch£º602	 i:1 	 global-step:12041	 l-p:0.05770610645413399
epoch£º602	 i:2 	 global-step:12042	 l-p:0.05704352259635925
epoch£º602	 i:3 	 global-step:12043	 l-p:0.056616783142089844
epoch£º602	 i:4 	 global-step:12044	 l-p:0.056140586733818054
epoch£º602	 i:5 	 global-step:12045	 l-p:0.05617396533489227
epoch£º602	 i:6 	 global-step:12046	 l-p:0.05687548965215683
epoch£º602	 i:7 	 global-step:12047	 l-p:0.05613536760210991
epoch£º602	 i:8 	 global-step:12048	 l-p:0.05613849684596062
epoch£º602	 i:9 	 global-step:12049	 l-p:0.05608849227428436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9677, 28.9453, 28.4659],
        [27.9677, 28.0265, 27.9729],
        [27.9677, 27.9681, 27.9677],
        [27.9677, 28.8524, 28.3909]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.05610353499650955 
model_pd.l_d.mean(): -6.821777787990868e-05 
model_pd.lagr.mean(): 0.05603531748056412 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8659], device='cuda:0')), ('power', tensor([-0.0669], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.05610353499650955
epoch£º603	 i:1 	 global-step:12061	 l-p:0.0561426617205143
epoch£º603	 i:2 	 global-step:12062	 l-p:0.05647293105721474
epoch£º603	 i:3 	 global-step:12063	 l-p:0.05691533908247948
epoch£º603	 i:4 	 global-step:12064	 l-p:0.056555185467004776
epoch£º603	 i:5 	 global-step:12065	 l-p:0.056165050715208054
epoch£º603	 i:6 	 global-step:12066	 l-p:0.056158121675252914
epoch£º603	 i:7 	 global-step:12067	 l-p:0.05623095855116844
epoch£º603	 i:8 	 global-step:12068	 l-p:0.05842439830303192
epoch£º603	 i:9 	 global-step:12069	 l-p:0.056283846497535706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8562, 35.9747, 40.6580],
        [27.8562, 27.8562, 27.8562],
        [27.8562, 28.2843, 27.9855],
        [27.8562, 29.8222, 29.4006]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.05618448182940483 
model_pd.l_d.mean(): -0.00012641376815736294 
model_pd.lagr.mean(): 0.056058067828416824 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8306], device='cuda:0')), ('power', tensor([-0.1204], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.05618448182940483
epoch£º604	 i:1 	 global-step:12081	 l-p:0.05667824670672417
epoch£º604	 i:2 	 global-step:12082	 l-p:0.05674676597118378
epoch£º604	 i:3 	 global-step:12083	 l-p:0.056448061019182205
epoch£º604	 i:4 	 global-step:12084	 l-p:0.05704398825764656
epoch£º604	 i:5 	 global-step:12085	 l-p:0.056306298822164536
epoch£º604	 i:6 	 global-step:12086	 l-p:0.057991236448287964
epoch£º604	 i:7 	 global-step:12087	 l-p:0.05610969290137291
epoch£º604	 i:8 	 global-step:12088	 l-p:0.05608443170785904
epoch£º604	 i:9 	 global-step:12089	 l-p:0.05620542913675308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7406, 32.9597, 34.6030],
        [27.7406, 28.5639, 28.1189],
        [27.7406, 28.0862, 27.8319],
        [27.7406, 28.0125, 27.8023]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.05626225471496582 
model_pd.l_d.mean(): -0.0002146819606423378 
model_pd.lagr.mean(): 0.05604757368564606 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8239], device='cuda:0')), ('power', tensor([-0.2096], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.05626225471496582
epoch£º605	 i:1 	 global-step:12101	 l-p:0.05616539716720581
epoch£º605	 i:2 	 global-step:12102	 l-p:0.0561755895614624
epoch£º605	 i:3 	 global-step:12103	 l-p:0.05625879019498825
epoch£º605	 i:4 	 global-step:12104	 l-p:0.0571729950606823
epoch£º605	 i:5 	 global-step:12105	 l-p:0.056806210428476334
epoch£º605	 i:6 	 global-step:12106	 l-p:0.05794095993041992
epoch£º605	 i:7 	 global-step:12107	 l-p:0.05644839257001877
epoch£º605	 i:8 	 global-step:12108	 l-p:0.05662543326616287
epoch£º605	 i:9 	 global-step:12109	 l-p:0.056327104568481445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6255, 27.6255, 27.6255],
        [27.6255, 35.4554, 39.8410],
        [27.6255, 29.5653, 29.1451],
        [27.6255, 27.6272, 27.6255]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.05620042607188225 
model_pd.l_d.mean(): -0.00035299788578413427 
model_pd.lagr.mean(): 0.05584742873907089 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8409], device='cuda:0')), ('power', tensor([-0.3749], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.05620042607188225
epoch£º606	 i:1 	 global-step:12121	 l-p:0.0563674122095108
epoch£º606	 i:2 	 global-step:12122	 l-p:0.05635572224855423
epoch£º606	 i:3 	 global-step:12123	 l-p:0.056268587708473206
epoch£º606	 i:4 	 global-step:12124	 l-p:0.0563025064766407
epoch£º606	 i:5 	 global-step:12125	 l-p:0.05713885650038719
epoch£º606	 i:6 	 global-step:12126	 l-p:0.05624248832464218
epoch£º606	 i:7 	 global-step:12127	 l-p:0.05630127713084221
epoch£º606	 i:8 	 global-step:12128	 l-p:0.058440931141376495
epoch£º606	 i:9 	 global-step:12129	 l-p:0.056937143206596375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5211, 27.5697, 27.5250],
        [27.5211, 29.7156, 29.3730],
        [27.5211, 27.5277, 27.5213],
        [27.5211, 29.2843, 28.8291]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.056291960179805756 
model_pd.l_d.mean(): -0.00033124780748039484 
model_pd.lagr.mean(): 0.05596071109175682 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8101], device='cuda:0')), ('power', tensor([-0.4117], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.056291960179805756
epoch£º607	 i:1 	 global-step:12141	 l-p:0.05717227980494499
epoch£º607	 i:2 	 global-step:12142	 l-p:0.0562492273747921
epoch£º607	 i:3 	 global-step:12143	 l-p:0.05647553503513336
epoch£º607	 i:4 	 global-step:12144	 l-p:0.05633973330259323
epoch£º607	 i:5 	 global-step:12145	 l-p:0.056320931762456894
epoch£º607	 i:6 	 global-step:12146	 l-p:0.05653039366006851
epoch£º607	 i:7 	 global-step:12147	 l-p:0.05718894675374031
epoch£º607	 i:8 	 global-step:12148	 l-p:0.05633174628019333
epoch£º607	 i:9 	 global-step:12149	 l-p:0.05796357989311218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4372, 27.4443, 27.4374],
        [27.4372, 27.4656, 27.4389],
        [27.4372, 27.8351, 27.5530],
        [27.4372, 35.0261, 39.1654]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.05680215731263161 
model_pd.l_d.mean(): -0.0001622128620510921 
model_pd.lagr.mean(): 0.05663994327187538 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7450], device='cuda:0')), ('power', tensor([-0.2614], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.05680215731263161
epoch£º608	 i:1 	 global-step:12161	 l-p:0.05625440552830696
epoch£º608	 i:2 	 global-step:12162	 l-p:0.05636440962553024
epoch£º608	 i:3 	 global-step:12163	 l-p:0.05636592209339142
epoch£º608	 i:4 	 global-step:12164	 l-p:0.05632876604795456
epoch£º608	 i:5 	 global-step:12165	 l-p:0.05800000950694084
epoch£º608	 i:6 	 global-step:12166	 l-p:0.05616725981235504
epoch£º608	 i:7 	 global-step:12167	 l-p:0.05718411132693291
epoch£º608	 i:8 	 global-step:12168	 l-p:0.05660628154873848
epoch£º608	 i:9 	 global-step:12169	 l-p:0.05702292174100876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3816, 27.3816, 27.3816],
        [27.3816, 27.4752, 27.3927],
        [27.3816, 34.9543, 39.0848],
        [27.3816, 28.2151, 27.7710]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.05734740197658539 
model_pd.l_d.mean(): -0.00010431485861772671 
model_pd.lagr.mean(): 0.05724308639764786 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7202], device='cuda:0')), ('power', tensor([-0.2598], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.05734740197658539
epoch£º609	 i:1 	 global-step:12181	 l-p:0.056270670145750046
epoch£º609	 i:2 	 global-step:12182	 l-p:0.05640077590942383
epoch£º609	 i:3 	 global-step:12183	 l-p:0.05654846504330635
epoch£º609	 i:4 	 global-step:12184	 l-p:0.05627642944455147
epoch£º609	 i:5 	 global-step:12185	 l-p:0.05628359317779541
epoch£º609	 i:6 	 global-step:12186	 l-p:0.05812026187777519
epoch£º609	 i:7 	 global-step:12187	 l-p:0.057020556181669235
epoch£º609	 i:8 	 global-step:12188	 l-p:0.05663060024380684
epoch£º609	 i:9 	 global-step:12189	 l-p:0.05633155629038811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3591, 28.1919, 27.7482],
        [27.3591, 27.3608, 27.3592],
        [27.3591, 27.3593, 27.3591],
        [27.3591, 27.5533, 27.3952]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.056377001106739044 
model_pd.l_d.mean(): -9.269034489989281e-05 
model_pd.lagr.mean(): 0.05628431215882301 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7967], device='cuda:0')), ('power', tensor([-0.5717], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.056377001106739044
epoch£º610	 i:1 	 global-step:12201	 l-p:0.05618724599480629
epoch£º610	 i:2 	 global-step:12202	 l-p:0.056988783180713654
epoch£º610	 i:3 	 global-step:12203	 l-p:0.057330917567014694
epoch£º610	 i:4 	 global-step:12204	 l-p:0.05653923749923706
epoch£º610	 i:5 	 global-step:12205	 l-p:0.056245505809783936
epoch£º610	 i:6 	 global-step:12206	 l-p:0.05696120485663414
epoch£º610	 i:7 	 global-step:12207	 l-p:0.056337013840675354
epoch£º610	 i:8 	 global-step:12208	 l-p:0.056199099868535995
epoch£º610	 i:9 	 global-step:12209	 l-p:0.05808454751968384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3716, 29.3633, 28.9654],
        [27.3716, 35.4348, 40.1422],
        [27.3716, 37.0328, 43.7491],
        [27.3716, 27.3768, 27.3718]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.056325480341911316 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056325480341911316 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8164], device='cuda:0')), ('power', tensor([-0.6626], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.056325480341911316
epoch£º611	 i:1 	 global-step:12221	 l-p:0.056324128061532974
epoch£º611	 i:2 	 global-step:12222	 l-p:0.056355152279138565
epoch£º611	 i:3 	 global-step:12223	 l-p:0.05671699345111847
epoch£º611	 i:4 	 global-step:12224	 l-p:0.05678664520382881
epoch£º611	 i:5 	 global-step:12225	 l-p:0.05901280418038368
epoch£º611	 i:6 	 global-step:12226	 l-p:0.05622055009007454
epoch£º611	 i:7 	 global-step:12227	 l-p:0.0568295381963253
epoch£º611	 i:8 	 global-step:12228	 l-p:0.0563402883708477
epoch£º611	 i:9 	 global-step:12229	 l-p:0.05624851584434509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4141, 32.5638, 34.1821],
        [27.4141, 27.4142, 27.4141],
        [27.4141, 29.1175, 28.6549],
        [27.4141, 27.4184, 27.4141]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.05733923241496086 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05733923241496086 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7773], device='cuda:0')), ('power', tensor([-0.3079], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.05733923241496086
epoch£º612	 i:1 	 global-step:12241	 l-p:0.05633218213915825
epoch£º612	 i:2 	 global-step:12242	 l-p:0.05633211135864258
epoch£º612	 i:3 	 global-step:12243	 l-p:0.0561942420899868
epoch£º612	 i:4 	 global-step:12244	 l-p:0.05682860687375069
epoch£º612	 i:5 	 global-step:12245	 l-p:0.05650630220770836
epoch£º612	 i:6 	 global-step:12246	 l-p:0.05822569876909256
epoch£º612	 i:7 	 global-step:12247	 l-p:0.05624379962682724
epoch£º612	 i:8 	 global-step:12248	 l-p:0.05628155544400215
epoch£º612	 i:9 	 global-step:12249	 l-p:0.056715305894613266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4663, 27.4669, 27.4663],
        [27.4663, 27.6007, 27.4862],
        [27.4663, 27.4821, 27.4670],
        [27.4663, 27.8157, 27.5599]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.056280866265296936 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056280866265296936 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8205], device='cuda:0')), ('power', tensor([-0.4879], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.056280866265296936
epoch£º613	 i:1 	 global-step:12261	 l-p:0.05631396174430847
epoch£º613	 i:2 	 global-step:12262	 l-p:0.05657953768968582
epoch£º613	 i:3 	 global-step:12263	 l-p:0.05642594397068024
epoch£º613	 i:4 	 global-step:12264	 l-p:0.05619993060827255
epoch£º613	 i:5 	 global-step:12265	 l-p:0.05890826880931854
epoch£º613	 i:6 	 global-step:12266	 l-p:0.05627225711941719
epoch£º613	 i:7 	 global-step:12267	 l-p:0.05669613555073738
epoch£º613	 i:8 	 global-step:12268	 l-p:0.056227512657642365
epoch£º613	 i:9 	 global-step:12269	 l-p:0.05691288039088249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5216, 32.0354, 33.0946],
        [27.5216, 29.9067, 29.6326],
        [27.5216, 27.5383, 27.5223],
        [27.5216, 27.5298, 27.5219]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.05629542097449303 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05629542097449303 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8030], device='cuda:0')), ('power', tensor([-0.4032], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.05629542097449303
epoch£º614	 i:1 	 global-step:12281	 l-p:0.056344177573919296
epoch£º614	 i:2 	 global-step:12282	 l-p:0.056370820850133896
epoch£º614	 i:3 	 global-step:12283	 l-p:0.05713358893990517
epoch£º614	 i:4 	 global-step:12284	 l-p:0.056442078202962875
epoch£º614	 i:5 	 global-step:12285	 l-p:0.05715693160891533
epoch£º614	 i:6 	 global-step:12286	 l-p:0.05619959533214569
epoch£º614	 i:7 	 global-step:12287	 l-p:0.05623870715498924
epoch£º614	 i:8 	 global-step:12288	 l-p:0.05808936804533005
epoch£º614	 i:9 	 global-step:12289	 l-p:0.05635833740234375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5777, 30.1838, 29.9992],
        [27.5777, 29.8954, 29.5933],
        [27.5777, 33.7782, 36.3870],
        [27.5777, 27.5777, 27.5777]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.058096520602703094 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058096520602703094 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7606], device='cuda:0')), ('power', tensor([-0.0802], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.058096520602703094
epoch£º615	 i:1 	 global-step:12301	 l-p:0.05629090964794159
epoch£º615	 i:2 	 global-step:12302	 l-p:0.0565006323158741
epoch£º615	 i:3 	 global-step:12303	 l-p:0.056212957948446274
epoch£º615	 i:4 	 global-step:12304	 l-p:0.05615612864494324
epoch£º615	 i:5 	 global-step:12305	 l-p:0.056157730519771576
epoch£º615	 i:6 	 global-step:12306	 l-p:0.056182581931352615
epoch£º615	 i:7 	 global-step:12307	 l-p:0.05626745894551277
epoch£º615	 i:8 	 global-step:12308	 l-p:0.05777598172426224
epoch£º615	 i:9 	 global-step:12309	 l-p:0.056807372719049454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6364, 27.6465, 27.6368],
        [27.6364, 27.8227, 27.6699],
        [27.6364, 34.8500, 38.5306],
        [27.6364, 31.5258, 32.0953]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.05746955797076225 
model_pd.l_d.mean(): 2.6157974275520246e-07 
model_pd.lagr.mean(): 0.057469818741083145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.0655e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6994], device='cuda:0')), ('power', tensor([0.1163], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.05746955797076225
epoch£º616	 i:1 	 global-step:12321	 l-p:0.05614756420254707
epoch£º616	 i:2 	 global-step:12322	 l-p:0.05843475088477135
epoch£º616	 i:3 	 global-step:12323	 l-p:0.056177906692028046
epoch£º616	 i:4 	 global-step:12324	 l-p:0.05661474168300629
epoch£º616	 i:5 	 global-step:12325	 l-p:0.05629323422908783
epoch£º616	 i:6 	 global-step:12326	 l-p:0.056407421827316284
epoch£º616	 i:7 	 global-step:12327	 l-p:0.05626235902309418
epoch£º616	 i:8 	 global-step:12328	 l-p:0.056174878031015396
epoch£º616	 i:9 	 global-step:12329	 l-p:0.056280747056007385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6957, 28.0680, 27.7989],
        [27.6957, 28.9202, 28.4193],
        [27.6957, 37.6902, 44.7736],
        [27.6957, 28.0601, 27.7954]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.056378282606601715 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056378282606601715 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8093], device='cuda:0')), ('power', tensor([-0.1998], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.056378282606601715
epoch£º617	 i:1 	 global-step:12341	 l-p:0.05782066285610199
epoch£º617	 i:2 	 global-step:12342	 l-p:0.05676516517996788
epoch£º617	 i:3 	 global-step:12343	 l-p:0.05663830786943436
epoch£º617	 i:4 	 global-step:12344	 l-p:0.05630515143275261
epoch£º617	 i:5 	 global-step:12345	 l-p:0.05615811049938202
epoch£º617	 i:6 	 global-step:12346	 l-p:0.05707630515098572
epoch£º617	 i:7 	 global-step:12347	 l-p:0.056211117655038834
epoch£º617	 i:8 	 global-step:12348	 l-p:0.05643938109278679
epoch£º617	 i:9 	 global-step:12349	 l-p:0.05627806484699249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7534, 30.2908, 30.0675],
        [27.7534, 28.1364, 27.8614],
        [27.7534, 37.6148, 44.5080],
        [27.7534, 29.4958, 29.0301]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.05632612109184265 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05632612109184265 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8114], device='cuda:0')), ('power', tensor([-0.1479], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.05632612109184265
epoch£º618	 i:1 	 global-step:12361	 l-p:0.056113351136446
epoch£º618	 i:2 	 global-step:12362	 l-p:0.056465908885002136
epoch£º618	 i:3 	 global-step:12363	 l-p:0.05666271969676018
epoch£º618	 i:4 	 global-step:12364	 l-p:0.056483492255210876
epoch£º618	 i:5 	 global-step:12365	 l-p:0.05718943476676941
epoch£º618	 i:6 	 global-step:12366	 l-p:0.05614849925041199
epoch£º618	 i:7 	 global-step:12367	 l-p:0.05609573423862457
epoch£º618	 i:8 	 global-step:12368	 l-p:0.058251507580280304
epoch£º618	 i:9 	 global-step:12369	 l-p:0.05614013969898224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8086, 28.1745, 27.9086],
        [27.8086, 28.6728, 28.2174],
        [27.8086, 28.7173, 28.2524],
        [27.8086, 34.5342, 37.6562]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.05625591054558754 
model_pd.l_d.mean(): -2.045591287469506e-07 
model_pd.lagr.mean(): 0.056255705654621124 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.9669e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8111], device='cuda:0')), ('power', tensor([-0.0601], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.05625591054558754
epoch£º619	 i:1 	 global-step:12381	 l-p:0.05776793882250786
epoch£º619	 i:2 	 global-step:12382	 l-p:0.056605804711580276
epoch£º619	 i:3 	 global-step:12383	 l-p:0.05651024356484413
epoch£º619	 i:4 	 global-step:12384	 l-p:0.05609294772148132
epoch£º619	 i:5 	 global-step:12385	 l-p:0.05771751329302788
epoch£º619	 i:6 	 global-step:12386	 l-p:0.056334659457206726
epoch£º619	 i:7 	 global-step:12387	 l-p:0.05612378567457199
epoch£º619	 i:8 	 global-step:12388	 l-p:0.05612754821777344
epoch£º619	 i:9 	 global-step:12389	 l-p:0.056169793009757996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8668, 27.9913, 27.8841],
        [27.8668, 29.0625, 28.5602],
        [27.8668, 27.8667, 27.8668],
        [27.8668, 33.3659, 35.2516]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.05608557537198067 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05608557537198067 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8713], device='cuda:0')), ('power', tensor([-0.1406], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.05608557537198067
epoch£º620	 i:1 	 global-step:12401	 l-p:0.0562032125890255
epoch£º620	 i:2 	 global-step:12402	 l-p:0.05608328804373741
epoch£º620	 i:3 	 global-step:12403	 l-p:0.056387871503829956
epoch£º620	 i:4 	 global-step:12404	 l-p:0.056300170719623566
epoch£º620	 i:5 	 global-step:12405	 l-p:0.057220857590436935
epoch£º620	 i:6 	 global-step:12406	 l-p:0.05835149064660072
epoch£º620	 i:7 	 global-step:12407	 l-p:0.05617256090044975
epoch£º620	 i:8 	 global-step:12408	 l-p:0.05611705780029297
epoch£º620	 i:9 	 global-step:12409	 l-p:0.056596044450998306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9221, 34.2881, 37.0175],
        [27.9221, 27.9264, 27.9222],
        [27.9221, 27.9221, 27.9221],
        [27.9221, 27.9222, 27.9221]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.05608522891998291 
model_pd.l_d.mean(): -5.598162715614308e-06 
model_pd.lagr.mean(): 0.05607962980866432 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.1118e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8728], device='cuda:0')), ('power', tensor([-0.1458], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.05608522891998291
epoch£º621	 i:1 	 global-step:12421	 l-p:0.05701451748609543
epoch£º621	 i:2 	 global-step:12422	 l-p:0.056228283792734146
epoch£º621	 i:3 	 global-step:12423	 l-p:0.05643894895911217
epoch£º621	 i:4 	 global-step:12424	 l-p:0.05619771033525467
epoch£º621	 i:5 	 global-step:12425	 l-p:0.057831961661577225
epoch£º621	 i:6 	 global-step:12426	 l-p:0.056683506816625595
epoch£º621	 i:7 	 global-step:12427	 l-p:0.05612775683403015
epoch£º621	 i:8 	 global-step:12428	 l-p:0.05629533529281616
epoch£º621	 i:9 	 global-step:12429	 l-p:0.056445784866809845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9755, 27.9755, 27.9755],
        [27.9755, 29.9504, 29.5269],
        [27.9755, 32.5676, 33.6455],
        [27.9755, 29.3735, 28.8666]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.05608975142240524 
model_pd.l_d.mean(): -3.597743898353656e-06 
model_pd.lagr.mean(): 0.05608615279197693 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.0347e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8623], device='cuda:0')), ('power', tensor([-0.0436], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.05608975142240524
epoch£º622	 i:1 	 global-step:12441	 l-p:0.058633849024772644
epoch£º622	 i:2 	 global-step:12442	 l-p:0.0561818890273571
epoch£º622	 i:3 	 global-step:12443	 l-p:0.05606704577803612
epoch£º622	 i:4 	 global-step:12444	 l-p:0.056079939007759094
epoch£º622	 i:5 	 global-step:12445	 l-p:0.05678972601890564
epoch£º622	 i:6 	 global-step:12446	 l-p:0.05618257820606232
epoch£º622	 i:7 	 global-step:12447	 l-p:0.05619804188609123
epoch£º622	 i:8 	 global-step:12448	 l-p:0.05619284510612488
epoch£º622	 i:9 	 global-step:12449	 l-p:0.0567786805331707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0224, 28.2162, 28.0578],
        [28.0224, 28.8766, 28.4215],
        [28.0224, 28.0549, 28.0244],
        [28.0224, 28.0231, 28.0224]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.05641781538724899 
model_pd.l_d.mean(): 1.0794635272759479e-05 
model_pd.lagr.mean(): 0.056428611278533936 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8467], device='cuda:0')), ('power', tensor([0.0711], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.05641781538724899
epoch£º623	 i:1 	 global-step:12461	 l-p:0.05613070726394653
epoch£º623	 i:2 	 global-step:12462	 l-p:0.05770963430404663
epoch£º623	 i:3 	 global-step:12463	 l-p:0.05636055767536163
epoch£º623	 i:4 	 global-step:12464	 l-p:0.05606425181031227
epoch£º623	 i:5 	 global-step:12465	 l-p:0.056020352989435196
epoch£º623	 i:6 	 global-step:12466	 l-p:0.05697663500905037
epoch£º623	 i:7 	 global-step:12467	 l-p:0.05705547332763672
epoch£º623	 i:8 	 global-step:12468	 l-p:0.05623970925807953
epoch£º623	 i:9 	 global-step:12469	 l-p:0.056086137890815735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0575, 28.1006, 28.0607],
        [28.0575, 37.0273, 42.7079],
        [28.0575, 28.0659, 28.0578],
        [28.0575, 28.0575, 28.0575]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.056155718863010406 
model_pd.l_d.mean(): 2.5133453164016828e-05 
model_pd.lagr.mean(): 0.05618085339665413 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8354], device='cuda:0')), ('power', tensor([0.1041], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.056155718863010406
epoch£º624	 i:1 	 global-step:12481	 l-p:0.056086216121912
epoch£º624	 i:2 	 global-step:12482	 l-p:0.0576556921005249
epoch£º624	 i:3 	 global-step:12483	 l-p:0.05611998960375786
epoch£º624	 i:4 	 global-step:12484	 l-p:0.056611888110637665
epoch£º624	 i:5 	 global-step:12485	 l-p:0.05623596906661987
epoch£º624	 i:6 	 global-step:12486	 l-p:0.05613771453499794
epoch£º624	 i:7 	 global-step:12487	 l-p:0.05721508339047432
epoch£º624	 i:8 	 global-step:12488	 l-p:0.05654863640666008
epoch£º624	 i:9 	 global-step:12489	 l-p:0.05620420351624489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0778, 28.0778, 28.0778],
        [28.0778, 28.1427, 28.0839],
        [28.0778, 28.0837, 28.0779],
        [28.0778, 31.2146, 31.2694]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.0562097430229187 
model_pd.l_d.mean(): 6.173704605316743e-05 
model_pd.lagr.mean(): 0.05627147853374481 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8247], device='cuda:0')), ('power', tensor([0.1789], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.0562097430229187
epoch£º625	 i:1 	 global-step:12501	 l-p:0.056740377098321915
epoch£º625	 i:2 	 global-step:12502	 l-p:0.0565573014318943
epoch£º625	 i:3 	 global-step:12503	 l-p:0.05600213259458542
epoch£º625	 i:4 	 global-step:12504	 l-p:0.056165263056755066
epoch£º625	 i:5 	 global-step:12505	 l-p:0.05691633000969887
epoch£º625	 i:6 	 global-step:12506	 l-p:0.05769602581858635
epoch£º625	 i:7 	 global-step:12507	 l-p:0.05642685666680336
epoch£º625	 i:8 	 global-step:12508	 l-p:0.056058552116155624
epoch£º625	 i:9 	 global-step:12509	 l-p:0.05616045743227005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0798, 28.3254, 28.1317],
        [28.0798, 30.1010, 29.6857],
        [28.0798, 28.0798, 28.0798],
        [28.0798, 37.1988, 43.0619]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.05604824051260948 
model_pd.l_d.mean(): 1.1162474038428627e-05 
model_pd.lagr.mean(): 0.056059401482343674 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8737], device='cuda:0')), ('power', tensor([0.0245], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.05604824051260948
epoch£º626	 i:1 	 global-step:12521	 l-p:0.05696015805006027
epoch£º626	 i:2 	 global-step:12522	 l-p:0.056114573031663895
epoch£º626	 i:3 	 global-step:12523	 l-p:0.056732699275016785
epoch£º626	 i:4 	 global-step:12524	 l-p:0.056306056678295135
epoch£º626	 i:5 	 global-step:12525	 l-p:0.05620817467570305
epoch£º626	 i:6 	 global-step:12526	 l-p:0.057741355150938034
epoch£º626	 i:7 	 global-step:12527	 l-p:0.056065671145915985
epoch£º626	 i:8 	 global-step:12528	 l-p:0.05629530921578407
epoch£º626	 i:9 	 global-step:12529	 l-p:0.05648138001561165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0624, 28.7377, 28.3337],
        [28.0624, 31.2407, 31.3198],
        [28.0624, 28.6699, 28.2906],
        [28.0624, 28.2428, 28.0939]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.05607949197292328 
model_pd.l_d.mean(): 5.652846084558405e-07 
model_pd.lagr.mean(): 0.056080058217048645 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8622], device='cuda:0')), ('power', tensor([0.0010], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.05607949197292328
epoch£º627	 i:1 	 global-step:12541	 l-p:0.056382160633802414
epoch£º627	 i:2 	 global-step:12542	 l-p:0.05608567222952843
epoch£º627	 i:3 	 global-step:12543	 l-p:0.05605272203683853
epoch£º627	 i:4 	 global-step:12544	 l-p:0.057599496096372604
epoch£º627	 i:5 	 global-step:12545	 l-p:0.05662139877676964
epoch£º627	 i:6 	 global-step:12546	 l-p:0.05630236864089966
epoch£º627	 i:7 	 global-step:12547	 l-p:0.05611632391810417
epoch£º627	 i:8 	 global-step:12548	 l-p:0.05618443712592125
epoch£º627	 i:9 	 global-step:12549	 l-p:0.057617127895355225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0243, 30.4788, 30.2086],
        [28.0243, 28.0259, 28.0243],
        [28.0243, 28.0286, 28.0244],
        [28.0243, 28.4350, 28.1446]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.05607307329773903 
model_pd.l_d.mean(): -1.639268703002017e-05 
model_pd.lagr.mean(): 0.056056682020425797 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8594], device='cuda:0')), ('power', tensor([-0.0250], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.05607307329773903
epoch£º628	 i:1 	 global-step:12561	 l-p:0.05615759640932083
epoch£º628	 i:2 	 global-step:12562	 l-p:0.05616123974323273
epoch£º628	 i:3 	 global-step:12563	 l-p:0.056240420788526535
epoch£º628	 i:4 	 global-step:12564	 l-p:0.05714607983827591
epoch£º628	 i:5 	 global-step:12565	 l-p:0.05605866014957428
epoch£º628	 i:6 	 global-step:12566	 l-p:0.056398432701826096
epoch£º628	 i:7 	 global-step:12567	 l-p:0.0577225387096405
epoch£º628	 i:8 	 global-step:12568	 l-p:0.056701142340898514
epoch£º628	 i:9 	 global-step:12569	 l-p:0.05652548745274544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9705, 27.9747, 27.9706],
        [27.9705, 29.9395, 29.5147],
        [27.9705, 27.9928, 27.9716],
        [27.9705, 35.7139, 39.9381]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.05602969229221344 
model_pd.l_d.mean(): -0.00010090243449667469 
model_pd.lagr.mean(): 0.0559287890791893 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8966], device='cuda:0')), ('power', tensor([-0.1391], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.05602969229221344
epoch£º629	 i:1 	 global-step:12581	 l-p:0.05651208758354187
epoch£º629	 i:2 	 global-step:12582	 l-p:0.05670532211661339
epoch£º629	 i:3 	 global-step:12583	 l-p:0.05707860738039017
epoch£º629	 i:4 	 global-step:12584	 l-p:0.0561484694480896
epoch£º629	 i:5 	 global-step:12585	 l-p:0.05618739873170853
epoch£º629	 i:6 	 global-step:12586	 l-p:0.05625347048044205
epoch£º629	 i:7 	 global-step:12587	 l-p:0.05633804202079773
epoch£º629	 i:8 	 global-step:12588	 l-p:0.05782729387283325
epoch£º629	 i:9 	 global-step:12589	 l-p:0.05629027262330055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9033, 27.9034, 27.9033],
        [27.9033, 29.8646, 29.4402],
        [27.9033, 30.5418, 30.3551],
        [27.9033, 27.9033, 27.9033]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.056509483605623245 
model_pd.l_d.mean(): 5.028646774007939e-05 
model_pd.lagr.mean(): 0.05655977129936218 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8155], device='cuda:0')), ('power', tensor([0.0656], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.056509483605623245
epoch£º630	 i:1 	 global-step:12601	 l-p:0.056370191276073456
epoch£º630	 i:2 	 global-step:12602	 l-p:0.0561685748398304
epoch£º630	 i:3 	 global-step:12603	 l-p:0.056539323180913925
epoch£º630	 i:4 	 global-step:12604	 l-p:0.056264009326696396
epoch£º630	 i:5 	 global-step:12605	 l-p:0.056192561984062195
epoch£º630	 i:6 	 global-step:12606	 l-p:0.05616343393921852
epoch£º630	 i:7 	 global-step:12607	 l-p:0.05668538063764572
epoch£º630	 i:8 	 global-step:12608	 l-p:0.057686470448970795
epoch£º630	 i:9 	 global-step:12609	 l-p:0.057027749717235565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8248, 27.9115, 27.8345],
        [27.8248, 33.0557, 34.6999],
        [27.8248, 27.8248, 27.8248],
        [27.8248, 28.7971, 28.3203]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.05607828125357628 
model_pd.l_d.mean(): -0.00021555763669312 
model_pd.lagr.mean(): 0.05586272478103638 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8799], device='cuda:0')), ('power', tensor([-0.2789], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.05607828125357628
epoch£º631	 i:1 	 global-step:12621	 l-p:0.05712655931711197
epoch£º631	 i:2 	 global-step:12622	 l-p:0.05625837296247482
epoch£º631	 i:3 	 global-step:12623	 l-p:0.056978803128004074
epoch£º631	 i:4 	 global-step:12624	 l-p:0.056358326226472855
epoch£º631	 i:5 	 global-step:12625	 l-p:0.05787459760904312
epoch£º631	 i:6 	 global-step:12626	 l-p:0.05621588975191116
epoch£º631	 i:7 	 global-step:12627	 l-p:0.05609156936407089
epoch£º631	 i:8 	 global-step:12628	 l-p:0.0565667599439621
epoch£º631	 i:9 	 global-step:12629	 l-p:0.0562957264482975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7457, 28.0690, 27.8276],
        [27.7457, 27.8333, 27.7556],
        [27.7457, 27.7711, 27.7470],
        [27.7457, 27.7656, 27.7466]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.05781567096710205 
model_pd.l_d.mean(): 2.181705440307269e-06 
model_pd.lagr.mean(): 0.05781785398721695 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7864], device='cuda:0')), ('power', tensor([0.0029], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.05781567096710205
epoch£º632	 i:1 	 global-step:12641	 l-p:0.05697391927242279
epoch£º632	 i:2 	 global-step:12642	 l-p:0.05618266761302948
epoch£º632	 i:3 	 global-step:12643	 l-p:0.056131761521101
epoch£º632	 i:4 	 global-step:12644	 l-p:0.056402478367090225
epoch£º632	 i:5 	 global-step:12645	 l-p:0.05619049817323685
epoch£º632	 i:6 	 global-step:12646	 l-p:0.05631962791085243
epoch£º632	 i:7 	 global-step:12647	 l-p:0.05659053847193718
epoch£º632	 i:8 	 global-step:12648	 l-p:0.056213367730379105
epoch£º632	 i:9 	 global-step:12649	 l-p:0.05726955831050873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6687, 27.7418, 27.6761],
        [27.6687, 30.5505, 30.4933],
        [27.6687, 28.4546, 28.0199],
        [27.6687, 37.8310, 45.1455]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.05675883591175079 
model_pd.l_d.mean(): -8.341952343471348e-05 
model_pd.lagr.mean(): 0.056675415486097336 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7934], device='cuda:0')), ('power', tensor([-0.1248], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.05675883591175079
epoch£º633	 i:1 	 global-step:12661	 l-p:0.056291885673999786
epoch£º633	 i:2 	 global-step:12662	 l-p:0.056236106902360916
epoch£º633	 i:3 	 global-step:12663	 l-p:0.05788972228765488
epoch£º633	 i:4 	 global-step:12664	 l-p:0.05642041563987732
epoch£º633	 i:5 	 global-step:12665	 l-p:0.056269798427820206
epoch£º633	 i:6 	 global-step:12666	 l-p:0.056224893778562546
epoch£º633	 i:7 	 global-step:12667	 l-p:0.056466978043317795
epoch£º633	 i:8 	 global-step:12668	 l-p:0.05617382004857063
epoch£º633	 i:9 	 global-step:12669	 l-p:0.05760546401143074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6015, 27.6015, 27.6015],
        [27.6015, 27.6200, 27.6024],
        [27.6015, 28.3037, 27.8940],
        [27.6015, 28.2162, 27.8367]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.05643921718001366 
model_pd.l_d.mean(): -0.00014781003119423985 
model_pd.lagr.mean(): 0.05629140883684158 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7801], device='cuda:0')), ('power', tensor([-0.2634], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.05643921718001366
epoch£º634	 i:1 	 global-step:12681	 l-p:0.05621664971113205
epoch£º634	 i:2 	 global-step:12682	 l-p:0.0566149465739727
epoch£º634	 i:3 	 global-step:12683	 l-p:0.05636223033070564
epoch£º634	 i:4 	 global-step:12684	 l-p:0.056456539779901505
epoch£º634	 i:5 	 global-step:12685	 l-p:0.056164223700761795
epoch£º634	 i:6 	 global-step:12686	 l-p:0.056882623583078384
epoch£º634	 i:7 	 global-step:12687	 l-p:0.056302640587091446
epoch£º634	 i:8 	 global-step:12688	 l-p:0.05886858329176903
epoch£º634	 i:9 	 global-step:12689	 l-p:0.05623753368854523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5508, 28.7685, 28.2703],
        [27.5508, 28.6355, 28.1470],
        [27.5508, 27.5550, 27.5509],
        [27.5508, 31.7636, 32.5811]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.05622752010822296 
model_pd.l_d.mean(): -0.0001712881785351783 
model_pd.lagr.mean(): 0.05605623126029968 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8239], device='cuda:0')), ('power', tensor([-0.4041], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.05622752010822296
epoch£º635	 i:1 	 global-step:12701	 l-p:0.059070080518722534
epoch£º635	 i:2 	 global-step:12702	 l-p:0.05635920912027359
epoch£º635	 i:3 	 global-step:12703	 l-p:0.056606534868478775
epoch£º635	 i:4 	 global-step:12704	 l-p:0.05639702081680298
epoch£º635	 i:5 	 global-step:12705	 l-p:0.05685056000947952
epoch£º635	 i:6 	 global-step:12706	 l-p:0.05620839074254036
epoch£º635	 i:7 	 global-step:12707	 l-p:0.05637799948453903
epoch£º635	 i:8 	 global-step:12708	 l-p:0.05624205619096756
epoch£º635	 i:9 	 global-step:12709	 l-p:0.05631629377603531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5275, 27.8895, 27.6265],
        [27.5275, 27.6191, 27.5382],
        [27.5275, 32.7045, 34.3343],
        [27.5275, 27.5280, 27.5275]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.05630676448345184 
model_pd.l_d.mean(): -0.00012672100274357945 
model_pd.lagr.mean(): 0.05618004500865936 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8170], device='cuda:0')), ('power', tensor([-0.4725], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.05630676448345184
epoch£º636	 i:1 	 global-step:12721	 l-p:0.056201834231615067
epoch£º636	 i:2 	 global-step:12722	 l-p:0.05653940141201019
epoch£º636	 i:3 	 global-step:12723	 l-p:0.056389205157756805
epoch£º636	 i:4 	 global-step:12724	 l-p:0.056288622319698334
epoch£º636	 i:5 	 global-step:12725	 l-p:0.05714646354317665
epoch£º636	 i:6 	 global-step:12726	 l-p:0.05717959254980087
epoch£º636	 i:7 	 global-step:12727	 l-p:0.058047570288181305
epoch£º636	 i:8 	 global-step:12728	 l-p:0.05640104040503502
epoch£º636	 i:9 	 global-step:12729	 l-p:0.056210894137620926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5264, 29.2561, 28.7948],
        [27.5264, 37.5949, 44.8173],
        [27.5264, 28.5030, 28.0291],
        [27.5264, 27.6133, 27.5362]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.05713324248790741 
model_pd.l_d.mean(): -2.135070644726511e-05 
model_pd.lagr.mean(): 0.057111892849206924 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.4894e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7783], device='cuda:0')), ('power', tensor([-0.2032], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.05713324248790741
epoch£º637	 i:1 	 global-step:12741	 l-p:0.058176711201667786
epoch£º637	 i:2 	 global-step:12742	 l-p:0.056457098573446274
epoch£º637	 i:3 	 global-step:12743	 l-p:0.05622308701276779
epoch£º637	 i:4 	 global-step:12744	 l-p:0.05655791983008385
epoch£º637	 i:5 	 global-step:12745	 l-p:0.05620275065302849
epoch£º637	 i:6 	 global-step:12746	 l-p:0.05709538236260414
epoch£º637	 i:7 	 global-step:12747	 l-p:0.05625617876648903
epoch£º637	 i:8 	 global-step:12748	 l-p:0.05625665932893753
epoch£º637	 i:9 	 global-step:12749	 l-p:0.05630602315068245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5584, 27.5597, 27.5584],
        [27.5584, 27.5583, 27.5584],
        [27.5584, 35.1811, 39.3384],
        [27.5584, 36.6548, 42.5988]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.05626300349831581 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05626300349831581 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7995], device='cuda:0')), ('power', tensor([-0.2846], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.05626300349831581
epoch£º638	 i:1 	 global-step:12761	 l-p:0.056319646537303925
epoch£º638	 i:2 	 global-step:12762	 l-p:0.0562664158642292
epoch£º638	 i:3 	 global-step:12763	 l-p:0.05796213448047638
epoch£º638	 i:4 	 global-step:12764	 l-p:0.05718192085623741
epoch£º638	 i:5 	 global-step:12765	 l-p:0.05675134062767029
epoch£º638	 i:6 	 global-step:12766	 l-p:0.056200407445430756
epoch£º638	 i:7 	 global-step:12767	 l-p:0.056592922657728195
epoch£º638	 i:8 	 global-step:12768	 l-p:0.0565657876431942
epoch£º638	 i:9 	 global-step:12769	 l-p:0.05642496421933174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6088, 36.9658, 43.2326],
        [27.6088, 27.6273, 27.6097],
        [27.6088, 29.6797, 29.2952],
        [27.6088, 29.4245, 28.9770]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.057451266795396805 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057451266795396805 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.1216e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6633], device='cuda:0')), ('power', tensor([0.1824], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.057451266795396805
epoch£º639	 i:1 	 global-step:12781	 l-p:0.056235987693071365
epoch£º639	 i:2 	 global-step:12782	 l-p:0.05638731271028519
epoch£º639	 i:3 	 global-step:12783	 l-p:0.0561634823679924
epoch£º639	 i:4 	 global-step:12784	 l-p:0.05617237463593483
epoch£º639	 i:5 	 global-step:12785	 l-p:0.05634811148047447
epoch£º639	 i:6 	 global-step:12786	 l-p:0.05624622106552124
epoch£º639	 i:7 	 global-step:12787	 l-p:0.05612035095691681
epoch£º639	 i:8 	 global-step:12788	 l-p:0.05876268073916435
epoch£º639	 i:9 	 global-step:12789	 l-p:0.05645415186882019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6658, 29.6176, 29.1989],
        [27.6658, 30.5354, 30.4722],
        [27.6658, 27.6658, 27.6658],
        [27.6658, 27.7579, 27.6765]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.056362275034189224 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056362275034189224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7658], device='cuda:0')), ('power', tensor([-0.1236], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.056362275034189224
epoch£º640	 i:1 	 global-step:12801	 l-p:0.056341130286455154
epoch£º640	 i:2 	 global-step:12802	 l-p:0.05637882649898529
epoch£º640	 i:3 	 global-step:12803	 l-p:0.05614464730024338
epoch£º640	 i:4 	 global-step:12804	 l-p:0.05622980371117592
epoch£º640	 i:5 	 global-step:12805	 l-p:0.056629031896591187
epoch£º640	 i:6 	 global-step:12806	 l-p:0.05728811025619507
epoch£º640	 i:7 	 global-step:12807	 l-p:0.056188277900218964
epoch£º640	 i:8 	 global-step:12808	 l-p:0.05672239139676094
epoch£º640	 i:9 	 global-step:12809	 l-p:0.05786130949854851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7263, 27.7329, 27.7264],
        [27.7263, 33.1657, 35.0129],
        [27.7263, 27.7263, 27.7263],
        [27.7263, 28.0911, 27.8261]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.056170519441366196 
model_pd.l_d.mean(): -3.2659264093126694e-07 
model_pd.lagr.mean(): 0.05617019161581993 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8343], device='cuda:0')), ('power', tensor([-0.2240], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.056170519441366196
epoch£º641	 i:1 	 global-step:12821	 l-p:0.05702275037765503
epoch£º641	 i:2 	 global-step:12822	 l-p:0.056458767503499985
epoch£º641	 i:3 	 global-step:12823	 l-p:0.05627332627773285
epoch£º641	 i:4 	 global-step:12824	 l-p:0.05751974135637283
epoch£º641	 i:5 	 global-step:12825	 l-p:0.05619708076119423
epoch£º641	 i:6 	 global-step:12826	 l-p:0.05613884702324867
epoch£º641	 i:7 	 global-step:12827	 l-p:0.05787669122219086
epoch£º641	 i:8 	 global-step:12828	 l-p:0.05612054839730263
epoch£º641	 i:9 	 global-step:12829	 l-p:0.056175269186496735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7912, 27.7923, 27.7912],
        [27.7912, 29.6196, 29.1689],
        [27.7912, 27.7912, 27.7911],
        [27.7912, 37.3713, 43.8870]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.05719243735074997 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05719243735074997 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([9.3997e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7374], device='cuda:0')), ('power', tensor([0.1880], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.05719243735074997
epoch£º642	 i:1 	 global-step:12841	 l-p:0.05614681914448738
epoch£º642	 i:2 	 global-step:12842	 l-p:0.056128185242414474
epoch£º642	 i:3 	 global-step:12843	 l-p:0.05610976740717888
epoch£º642	 i:4 	 global-step:12844	 l-p:0.05617240443825722
epoch£º642	 i:5 	 global-step:12845	 l-p:0.056436918675899506
epoch£º642	 i:6 	 global-step:12846	 l-p:0.05678878724575043
epoch£º642	 i:7 	 global-step:12847	 l-p:0.05817534402012825
epoch£º642	 i:8 	 global-step:12848	 l-p:0.05624512583017349
epoch£º642	 i:9 	 global-step:12849	 l-p:0.056350454688072205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8548, 28.2628, 27.9743],
        [27.8548, 27.8548, 27.8548],
        [27.8548, 32.3307, 33.3263],
        [27.8548, 27.8549, 27.8548]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.05789262056350708 
model_pd.l_d.mean(): 3.2285381621477427e-06 
model_pd.lagr.mean(): 0.05789585039019585 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.5434e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7455], device='cuda:0')), ('power', tensor([0.2434], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.05789262056350708
epoch£º643	 i:1 	 global-step:12861	 l-p:0.0565166175365448
epoch£º643	 i:2 	 global-step:12862	 l-p:0.05608409643173218
epoch£º643	 i:3 	 global-step:12863	 l-p:0.056295786052942276
epoch£º643	 i:4 	 global-step:12864	 l-p:0.0561029426753521
epoch£º643	 i:5 	 global-step:12865	 l-p:0.05808253958821297
epoch£º643	 i:6 	 global-step:12866	 l-p:0.05609257519245148
epoch£º643	 i:7 	 global-step:12867	 l-p:0.056088559329509735
epoch£º643	 i:8 	 global-step:12868	 l-p:0.05625244602560997
epoch£º643	 i:9 	 global-step:12869	 l-p:0.05614572390913963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[27.9187, 30.3402, 30.0620],
        [27.9187, 32.4053, 33.4034],
        [27.9187, 29.7962, 29.3518],
        [27.9187, 28.5916, 28.1894]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.05625956878066063 
model_pd.l_d.mean(): 1.4839872619631933e-06 
model_pd.lagr.mean(): 0.05626105144619942 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.9762e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8125], device='cuda:0')), ('power', tensor([0.0549], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.05625956878066063
epoch£º644	 i:1 	 global-step:12881	 l-p:0.056494567543268204
epoch£º644	 i:2 	 global-step:12882	 l-p:0.05785373970866203
epoch£º644	 i:3 	 global-step:12883	 l-p:0.05612614005804062
epoch£º644	 i:4 	 global-step:12884	 l-p:0.05610334128141403
epoch£º644	 i:5 	 global-step:12885	 l-p:0.05676741525530815
epoch£º644	 i:6 	 global-step:12886	 l-p:0.05613365024328232
epoch£º644	 i:7 	 global-step:12887	 l-p:0.05710666999220848
epoch£º644	 i:8 	 global-step:12888	 l-p:0.05628896504640579
epoch£º644	 i:9 	 global-step:12889	 l-p:0.05622228607535362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9769, 28.3256, 28.0690],
        [27.9769, 29.5659, 29.0725],
        [27.9769, 33.6560, 35.6979],
        [27.9769, 27.9926, 27.9775]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.056319769471883774 
model_pd.l_d.mean(): 1.016796250041807e-05 
model_pd.lagr.mean(): 0.056329935789108276 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.7702e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7962], device='cuda:0')), ('power', tensor([0.1442], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.056319769471883774
epoch£º645	 i:1 	 global-step:12901	 l-p:0.05606600642204285
epoch£º645	 i:2 	 global-step:12902	 l-p:0.05697551742196083
epoch£º645	 i:3 	 global-step:12903	 l-p:0.05631176754832268
epoch£º645	 i:4 	 global-step:12904	 l-p:0.05858973413705826
epoch£º645	 i:5 	 global-step:12905	 l-p:0.05628751590847969
epoch£º645	 i:6 	 global-step:12906	 l-p:0.056144069880247116
epoch£º645	 i:7 	 global-step:12907	 l-p:0.05625881254673004
epoch£º645	 i:8 	 global-step:12908	 l-p:0.056176017969846725
epoch£º645	 i:9 	 global-step:12909	 l-p:0.0560527965426445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[28.0286, 33.0278, 34.4435],
        [28.0286, 37.6646, 44.2002],
        [28.0286, 37.9322, 44.8181],
        [28.0286, 29.7893, 29.3188]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.05667748302221298 
model_pd.l_d.mean(): 4.82391806144733e-05 
model_pd.lagr.mean(): 0.056725721806287766 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7561], device='cuda:0')), ('power', tensor([0.3414], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.05667748302221298
epoch£º646	 i:1 	 global-step:12921	 l-p:0.056100230664014816
epoch£º646	 i:2 	 global-step:12922	 l-p:0.05625225976109505
epoch£º646	 i:3 	 global-step:12923	 l-p:0.05617699399590492
epoch£º646	 i:4 	 global-step:12924	 l-p:0.057600367814302444
epoch£º646	 i:5 	 global-step:12925	 l-p:0.05613056942820549
epoch£º646	 i:6 	 global-step:12926	 l-p:0.05608627572655678
epoch£º646	 i:7 	 global-step:12927	 l-p:0.05618620663881302
epoch£º646	 i:8 	 global-step:12928	 l-p:0.05605480074882507
epoch£º646	 i:9 	 global-step:12929	 l-p:0.05776957422494888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0662, 28.0662, 28.0662],
        [28.0662, 28.0676, 28.0662],
        [28.0662, 32.4973, 33.4370],
        [28.0662, 28.0679, 28.0663]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.05625512823462486 
model_pd.l_d.mean(): 5.806604167446494e-05 
model_pd.lagr.mean(): 0.05631319433450699 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7893], device='cuda:0')), ('power', tensor([0.2476], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.05625512823462486
epoch£º647	 i:1 	 global-step:12941	 l-p:0.05606713145971298
epoch£º647	 i:2 	 global-step:12942	 l-p:0.05675794556736946
epoch£º647	 i:3 	 global-step:12943	 l-p:0.056864842772483826
epoch£º647	 i:4 	 global-step:12944	 l-p:0.056050270795822144
epoch£º647	 i:5 	 global-step:12945	 l-p:0.056040942668914795
epoch£º647	 i:6 	 global-step:12946	 l-p:0.056194037199020386
epoch£º647	 i:7 	 global-step:12947	 l-p:0.056168440729379654
epoch£º647	 i:8 	 global-step:12948	 l-p:0.056913893669843674
epoch£º647	 i:9 	 global-step:12949	 l-p:0.05762545391917229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0877, 33.8455, 35.9486],
        [28.0877, 28.0879, 28.0877],
        [28.0877, 28.2998, 28.1286],
        [28.0877, 28.1101, 28.0888]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.05605011805891991 
model_pd.l_d.mean(): 3.0451585189439356e-05 
model_pd.lagr.mean(): 0.056080568581819534 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8686], device='cuda:0')), ('power', tensor([0.0888], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.05605011805891991
epoch£º648	 i:1 	 global-step:12961	 l-p:0.05611405149102211
epoch£º648	 i:2 	 global-step:12962	 l-p:0.056535504758358
epoch£º648	 i:3 	 global-step:12963	 l-p:0.05663697049021721
epoch£º648	 i:4 	 global-step:12964	 l-p:0.05602770671248436
epoch£º648	 i:5 	 global-step:12965	 l-p:0.056367095559835434
epoch£º648	 i:6 	 global-step:12966	 l-p:0.05757971480488777
epoch£º648	 i:7 	 global-step:12967	 l-p:0.05610837787389755
epoch£º648	 i:8 	 global-step:12968	 l-p:0.05615833029150963
epoch£º648	 i:9 	 global-step:12969	 l-p:0.0573207326233387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0918, 28.2992, 28.1313],
        [28.0918, 28.1062, 28.0924],
        [28.0918, 28.1028, 28.0922],
        [28.0918, 29.9817, 29.5344]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.056452199816703796 
model_pd.l_d.mean(): 0.00010117149940924719 
model_pd.lagr.mean(): 0.056553371250629425 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8273], device='cuda:0')), ('power', tensor([0.2209], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.056452199816703796
epoch£º649	 i:1 	 global-step:12981	 l-p:0.05640361085534096
epoch£º649	 i:2 	 global-step:12982	 l-p:0.05699437856674194
epoch£º649	 i:3 	 global-step:12983	 l-p:0.056167811155319214
epoch£º649	 i:4 	 global-step:12984	 l-p:0.056290652602910995
epoch£º649	 i:5 	 global-step:12985	 l-p:0.05602850764989853
epoch£º649	 i:6 	 global-step:12986	 l-p:0.05760845169425011
epoch£º649	 i:7 	 global-step:12987	 l-p:0.0561005063354969
epoch£º649	 i:8 	 global-step:12988	 l-p:0.056652095168828964
epoch£º649	 i:9 	 global-step:12989	 l-p:0.05621641129255295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0745, 28.7004, 28.3140],
        [28.0745, 28.3512, 28.1376],
        [28.0745, 32.0289, 32.6082],
        [28.0745, 29.3089, 28.8011]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.05607515200972557 
model_pd.l_d.mean(): 4.176651418674737e-05 
model_pd.lagr.mean(): 0.056116919964551926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8569], device='cuda:0')), ('power', tensor([0.0732], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.05607515200972557
epoch£º650	 i:1 	 global-step:13001	 l-p:0.05832778289914131
epoch£º650	 i:2 	 global-step:13002	 l-p:0.05616705119609833
epoch£º650	 i:3 	 global-step:13003	 l-p:0.056456439197063446
epoch£º650	 i:4 	 global-step:13004	 l-p:0.056451063603162766
epoch£º650	 i:5 	 global-step:13005	 l-p:0.05607566609978676
epoch£º650	 i:6 	 global-step:13006	 l-p:0.05627371743321419
epoch£º650	 i:7 	 global-step:13007	 l-p:0.05602755397558212
epoch£º650	 i:8 	 global-step:13008	 l-p:0.05602148920297623
epoch£º650	 i:9 	 global-step:13009	 l-p:0.05712149664759636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0328, 28.0328, 28.0328],
        [28.0328, 37.5668, 43.9693],
        [28.0328, 28.0328, 28.0328],
        [28.0328, 29.6252, 29.1308]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.056211479008197784 
model_pd.l_d.mean(): 0.00010249370097881183 
model_pd.lagr.mean(): 0.05631397292017937 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8144], device='cuda:0')), ('power', tensor([0.1530], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.056211479008197784
epoch£º651	 i:1 	 global-step:13021	 l-p:0.05627908185124397
epoch£º651	 i:2 	 global-step:13022	 l-p:0.05621028691530228
epoch£º651	 i:3 	 global-step:13023	 l-p:0.05615905672311783
epoch£º651	 i:4 	 global-step:13024	 l-p:0.05657397210597992
epoch£º651	 i:5 	 global-step:13025	 l-p:0.05627121031284332
epoch£º651	 i:6 	 global-step:13026	 l-p:0.05778833106160164
epoch£º651	 i:7 	 global-step:13027	 l-p:0.05623703449964523
epoch£º651	 i:8 	 global-step:13028	 l-p:0.05611047521233559
epoch£º651	 i:9 	 global-step:13029	 l-p:0.05733402073383331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9675, 28.7537, 28.3164],
        [27.9675, 28.0264, 27.9728],
        [27.9675, 27.9675, 27.9675],
        [27.9675, 27.9736, 27.9677]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.056180037558078766 
model_pd.l_d.mean(): -2.445919017191045e-05 
model_pd.lagr.mean(): 0.056155577301979065 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8413], device='cuda:0')), ('power', tensor([-0.0330], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.056180037558078766
epoch£º652	 i:1 	 global-step:13041	 l-p:0.056131791323423386
epoch£º652	 i:2 	 global-step:13042	 l-p:0.056227147579193115
epoch£º652	 i:3 	 global-step:13043	 l-p:0.0562330037355423
epoch£º652	 i:4 	 global-step:13044	 l-p:0.05642976984381676
epoch£º652	 i:5 	 global-step:13045	 l-p:0.05768589675426483
epoch£º652	 i:6 	 global-step:13046	 l-p:0.05703488364815712
epoch£º652	 i:7 	 global-step:13047	 l-p:0.056724224239587784
epoch£º652	 i:8 	 global-step:13048	 l-p:0.056265123188495636
epoch£º652	 i:9 	 global-step:13049	 l-p:0.05648473650217056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8895, 34.9859, 38.4961],
        [27.8895, 32.7257, 34.0161],
        [27.8895, 35.0505, 38.6317],
        [27.8895, 28.2980, 28.0091]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.0563095286488533 
model_pd.l_d.mean(): -4.690678281349392e-07 
model_pd.lagr.mean(): 0.056309059262275696 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8011], device='cuda:0')), ('power', tensor([-0.0006], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.0563095286488533
epoch£º653	 i:1 	 global-step:13061	 l-p:0.05771305412054062
epoch£º653	 i:2 	 global-step:13062	 l-p:0.056531500071287155
epoch£º653	 i:3 	 global-step:13063	 l-p:0.05608820915222168
epoch£º653	 i:4 	 global-step:13064	 l-p:0.056262776255607605
epoch£º653	 i:5 	 global-step:13065	 l-p:0.05608262121677399
epoch£º653	 i:6 	 global-step:13066	 l-p:0.05690945312380791
epoch£º653	 i:7 	 global-step:13067	 l-p:0.05622337386012077
epoch£º653	 i:8 	 global-step:13068	 l-p:0.056199993938207626
epoch£º653	 i:9 	 global-step:13069	 l-p:0.05732526257634163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8026, 34.7013, 38.0097],
        [27.8026, 29.1749, 28.6709],
        [27.8026, 27.8026, 27.8026],
        [27.8026, 29.5297, 29.0599]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.05705917254090309 
model_pd.l_d.mean(): 4.480295683606528e-05 
model_pd.lagr.mean(): 0.05710397660732269 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7922], device='cuda:0')), ('power', tensor([0.0576], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.05705917254090309
epoch£º654	 i:1 	 global-step:13081	 l-p:0.05662454664707184
epoch£º654	 i:2 	 global-step:13082	 l-p:0.05641872435808182
epoch£º654	 i:3 	 global-step:13083	 l-p:0.056184329092502594
epoch£º654	 i:4 	 global-step:13084	 l-p:0.05628182366490364
epoch£º654	 i:5 	 global-step:13085	 l-p:0.05790167301893234
epoch£º654	 i:6 	 global-step:13086	 l-p:0.056742291897535324
epoch£º654	 i:7 	 global-step:13087	 l-p:0.05630718171596527
epoch£º654	 i:8 	 global-step:13088	 l-p:0.0561051219701767
epoch£º654	 i:9 	 global-step:13089	 l-p:0.05630386248230934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[27.7130, 30.6108, 30.5592],
        [27.7130, 31.4887, 31.9718],
        [27.7130, 29.5678, 29.1251],
        [27.7130, 28.6811, 28.2063]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.056235551834106445 
model_pd.l_d.mean(): -0.00017181334260385484 
model_pd.lagr.mean(): 0.05606373772025108 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8371], device='cuda:0')), ('power', tensor([-0.2350], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.056235551834106445
epoch£º655	 i:1 	 global-step:13101	 l-p:0.05612568184733391
epoch£º655	 i:2 	 global-step:13102	 l-p:0.05631858855485916
epoch£º655	 i:3 	 global-step:13103	 l-p:0.057292964309453964
epoch£º655	 i:4 	 global-step:13104	 l-p:0.05658651888370514
epoch£º655	 i:5 	 global-step:13105	 l-p:0.056304652243852615
epoch£º655	 i:6 	 global-step:13106	 l-p:0.05695389583706856
epoch£º655	 i:7 	 global-step:13107	 l-p:0.056206896901130676
epoch£º655	 i:8 	 global-step:13108	 l-p:0.05622005835175514
epoch£º655	 i:9 	 global-step:13109	 l-p:0.05799003317952156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6265, 27.7327, 27.6400],
        [27.6265, 27.8029, 27.6571],
        [27.6265, 27.7013, 27.6342],
        [27.6265, 32.2790, 33.4420]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.05623006820678711 
model_pd.l_d.mean(): -0.00023436897026840597 
model_pd.lagr.mean(): 0.055995699018239975 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8277], device='cuda:0')), ('power', tensor([-0.3656], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.05623006820678711
epoch£º656	 i:1 	 global-step:13121	 l-p:0.05624200776219368
epoch£º656	 i:2 	 global-step:13122	 l-p:0.056802261620759964
epoch£º656	 i:3 	 global-step:13123	 l-p:0.058565277606248856
epoch£º656	 i:4 	 global-step:13124	 l-p:0.05620875209569931
epoch£º656	 i:5 	 global-step:13125	 l-p:0.05623836815357208
epoch£º656	 i:6 	 global-step:13126	 l-p:0.05630792677402496
epoch£º656	 i:7 	 global-step:13127	 l-p:0.05616484582424164
epoch£º656	 i:8 	 global-step:13128	 l-p:0.056199874728918076
epoch£º656	 i:9 	 global-step:13129	 l-p:0.05751733481884003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[27.5594, 29.2889, 28.8266],
        [27.5594, 30.2565, 30.1146],
        [27.5594, 33.3682, 35.5886],
        [27.5594, 32.4258, 33.7773]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.056299980729818344 
model_pd.l_d.mean(): -0.00016708853945601732 
model_pd.lagr.mean(): 0.05613289400935173 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8140], device='cuda:0')), ('power', tensor([-0.3260], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.056299980729818344
epoch£º657	 i:1 	 global-step:13141	 l-p:0.056267619132995605
epoch£º657	 i:2 	 global-step:13142	 l-p:0.05623336136341095
epoch£º657	 i:3 	 global-step:13143	 l-p:0.05628504604101181
epoch£º657	 i:4 	 global-step:13144	 l-p:0.05723179131746292
epoch£º657	 i:5 	 global-step:13145	 l-p:0.05800098180770874
epoch£º657	 i:6 	 global-step:13146	 l-p:0.05734633281826973
epoch£º657	 i:7 	 global-step:13147	 l-p:0.05656755343079567
epoch£º657	 i:8 	 global-step:13148	 l-p:0.056149642914533615
epoch£º657	 i:9 	 global-step:13149	 l-p:0.056289270520210266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5145, 27.5146, 27.5145],
        [27.5145, 30.4906, 30.4921],
        [27.5145, 29.5172, 29.1171],
        [27.5145, 29.7085, 29.3660]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.05637113377451897 
model_pd.l_d.mean(): -0.0001339256705250591 
model_pd.lagr.mean(): 0.05623720958828926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7781], device='cuda:0')), ('power', tensor([-0.3766], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.05637113377451897
epoch£º658	 i:1 	 global-step:13161	 l-p:0.056193698197603226
epoch£º658	 i:2 	 global-step:13162	 l-p:0.05640394240617752
epoch£º658	 i:3 	 global-step:13163	 l-p:0.05710909888148308
epoch£º658	 i:4 	 global-step:13164	 l-p:0.056653983891010284
epoch£º658	 i:5 	 global-step:13165	 l-p:0.056298572570085526
epoch£º658	 i:6 	 global-step:13166	 l-p:0.056335125118494034
epoch£º658	 i:7 	 global-step:13167	 l-p:0.05793890729546547
epoch£º658	 i:8 	 global-step:13168	 l-p:0.056159086525440216
epoch£º658	 i:9 	 global-step:13169	 l-p:0.05731578171253204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4975, 33.1283, 35.1845],
        [27.4975, 27.4989, 27.4975],
        [27.4975, 30.1056, 29.9260],
        [27.4975, 27.5017, 27.4975]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.05637334659695625 
model_pd.l_d.mean(): -8.851766324369237e-05 
model_pd.lagr.mean(): 0.0562848299741745 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8275], device='cuda:0')), ('power', tensor([-0.4847], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.05637334659695625
epoch£º659	 i:1 	 global-step:13181	 l-p:0.05642092600464821
epoch£º659	 i:2 	 global-step:13182	 l-p:0.056291550397872925
epoch£º659	 i:3 	 global-step:13183	 l-p:0.05620794743299484
epoch£º659	 i:4 	 global-step:13184	 l-p:0.05677061900496483
epoch£º659	 i:5 	 global-step:13185	 l-p:0.05673956871032715
epoch£º659	 i:6 	 global-step:13186	 l-p:0.057312652468681335
epoch£º659	 i:7 	 global-step:13187	 l-p:0.0564621239900589
epoch£º659	 i:8 	 global-step:13188	 l-p:0.05626426637172699
epoch£º659	 i:9 	 global-step:13189	 l-p:0.05793597921729088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5147, 35.1179, 39.2602],
        [27.5147, 28.3451, 27.9004],
        [27.5147, 33.3544, 35.6111],
        [27.5147, 27.5151, 27.5147]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.05671462044119835 
model_pd.l_d.mean(): -1.5144131566557917e-06 
model_pd.lagr.mean(): 0.056713104248046875 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.6268e-07], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7469], device='cuda:0')), ('power', tensor([-0.1714], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.05671462044119835
epoch£º660	 i:1 	 global-step:13201	 l-p:0.05653804540634155
epoch£º660	 i:2 	 global-step:13202	 l-p:0.056412775069475174
epoch£º660	 i:3 	 global-step:13203	 l-p:0.0563269779086113
epoch£º660	 i:4 	 global-step:13204	 l-p:0.056780703365802765
epoch£º660	 i:5 	 global-step:13205	 l-p:0.058215994387865067
epoch£º660	 i:6 	 global-step:13206	 l-p:0.05619431287050247
epoch£º660	 i:7 	 global-step:13207	 l-p:0.05621637403964996
epoch£º660	 i:8 	 global-step:13208	 l-p:0.0570472851395607
epoch£º660	 i:9 	 global-step:13209	 l-p:0.05622312054038048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5658, 27.5701, 27.5659],
        [27.5658, 28.2317, 27.8342],
        [27.5658, 27.7054, 27.5868],
        [27.5658, 28.7073, 28.2134]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.05676739662885666 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05676739662885666 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8030], device='cuda:0')), ('power', tensor([-0.2483], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.05676739662885666
epoch£º661	 i:1 	 global-step:13221	 l-p:0.05629574507474899
epoch£º661	 i:2 	 global-step:13222	 l-p:0.0561809241771698
epoch£º661	 i:3 	 global-step:13223	 l-p:0.05624169111251831
epoch£º661	 i:4 	 global-step:13224	 l-p:0.056785259395837784
epoch£º661	 i:5 	 global-step:13225	 l-p:0.05741799995303154
epoch£º661	 i:6 	 global-step:13226	 l-p:0.05616160109639168
epoch£º661	 i:7 	 global-step:13227	 l-p:0.05633247271180153
epoch£º661	 i:8 	 global-step:13228	 l-p:0.05635877326130867
epoch£º661	 i:9 	 global-step:13229	 l-p:0.057931702584028244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6276, 27.6276, 27.6276],
        [27.6276, 27.6276, 27.6276],
        [27.6276, 27.7399, 27.6424],
        [27.6276, 29.1956, 28.7086]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.05615144222974777 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05615144222974777 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8573], device='cuda:0')), ('power', tensor([-0.4651], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.05615144222974777
epoch£º662	 i:1 	 global-step:13241	 l-p:0.056788522750139236
epoch£º662	 i:2 	 global-step:13242	 l-p:0.05894366279244423
epoch£º662	 i:3 	 global-step:13243	 l-p:0.05619294196367264
epoch£º662	 i:4 	 global-step:13244	 l-p:0.056380707770586014
epoch£º662	 i:5 	 global-step:13245	 l-p:0.05635146051645279
epoch£º662	 i:6 	 global-step:13246	 l-p:0.05620727315545082
epoch£º662	 i:7 	 global-step:13247	 l-p:0.056211844086647034
epoch£º662	 i:8 	 global-step:13248	 l-p:0.056339796632528305
epoch£º662	 i:9 	 global-step:13249	 l-p:0.05670548230409622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6984, 34.8081, 38.3634],
        [27.6984, 28.0509, 27.7928],
        [27.6984, 30.4157, 30.2759],
        [27.6984, 27.7624, 27.7044]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.056240491569042206 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056240491569042206 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8340], device='cuda:0')), ('power', tensor([-0.3147], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.056240491569042206
epoch£º663	 i:1 	 global-step:13261	 l-p:0.05727285146713257
epoch£º663	 i:2 	 global-step:13262	 l-p:0.05618838965892792
epoch£º663	 i:3 	 global-step:13263	 l-p:0.05639078840613365
epoch£º663	 i:4 	 global-step:13264	 l-p:0.05621369183063507
epoch£º663	 i:5 	 global-step:13265	 l-p:0.05642693489789963
epoch£º663	 i:6 	 global-step:13266	 l-p:0.056594155728816986
epoch£º663	 i:7 	 global-step:13267	 l-p:0.05669508874416351
epoch£º663	 i:8 	 global-step:13268	 l-p:0.0578283928334713
epoch£º663	 i:9 	 global-step:13269	 l-p:0.05618462339043617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7664, 27.7665, 27.7663],
        [27.7664, 30.7712, 30.7729],
        [27.7664, 27.7745, 27.7666],
        [27.7664, 29.2098, 28.7093]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.0562606118619442 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0562606118619442 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8097], device='cuda:0')), ('power', tensor([-0.1557], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.0562606118619442
epoch£º664	 i:1 	 global-step:13281	 l-p:0.05778633803129196
epoch£º664	 i:2 	 global-step:13282	 l-p:0.05617736652493477
epoch£º664	 i:3 	 global-step:13283	 l-p:0.0565536767244339
epoch£º664	 i:4 	 global-step:13284	 l-p:0.05732492357492447
epoch£º664	 i:5 	 global-step:13285	 l-p:0.056104134768247604
epoch£º664	 i:6 	 global-step:13286	 l-p:0.05628263205289841
epoch£º664	 i:7 	 global-step:13287	 l-p:0.056453075259923935
epoch£º664	 i:8 	 global-step:13288	 l-p:0.0567132942378521
epoch£º664	 i:9 	 global-step:13289	 l-p:0.05616690218448639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8384, 27.8492, 27.8388],
        [27.8384, 27.8496, 27.8388],
        [27.8384, 30.7499, 30.6981],
        [27.8384, 28.2877, 27.9784]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.056229814887046814 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056229814887046814 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8167], device='cuda:0')), ('power', tensor([-0.0400], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.056229814887046814
epoch£º665	 i:1 	 global-step:13301	 l-p:0.05644473806023598
epoch£º665	 i:2 	 global-step:13302	 l-p:0.05711128190159798
epoch£º665	 i:3 	 global-step:13303	 l-p:0.05625421553850174
epoch£º665	 i:4 	 global-step:13304	 l-p:0.05772187560796738
epoch£º665	 i:5 	 global-step:13305	 l-p:0.056477680802345276
epoch£º665	 i:6 	 global-step:13306	 l-p:0.05666859820485115
epoch£º665	 i:7 	 global-step:13307	 l-p:0.056196410208940506
epoch£º665	 i:8 	 global-step:13308	 l-p:0.05619439855217934
epoch£º665	 i:9 	 global-step:13309	 l-p:0.056295208632946014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9082, 27.9082, 27.9082],
        [27.9082, 33.1364, 34.7684],
        [27.9082, 33.3306, 35.1401],
        [27.9082, 29.2588, 28.7522]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.0574827566742897 
model_pd.l_d.mean(): 2.753244189079851e-06 
model_pd.lagr.mean(): 0.05748550966382027 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.4720e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7551], device='cuda:0')), ('power', tensor([0.3249], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.0574827566742897
epoch£º666	 i:1 	 global-step:13321	 l-p:0.0563117116689682
epoch£º666	 i:2 	 global-step:13322	 l-p:0.05611996352672577
epoch£º666	 i:3 	 global-step:13323	 l-p:0.05772315710783005
epoch£º666	 i:4 	 global-step:13324	 l-p:0.05679980292916298
epoch£º666	 i:5 	 global-step:13325	 l-p:0.05622807890176773
epoch£º666	 i:6 	 global-step:13326	 l-p:0.05623759329319
epoch£º666	 i:7 	 global-step:13327	 l-p:0.05615551024675369
epoch£º666	 i:8 	 global-step:13328	 l-p:0.056212469935417175
epoch£º666	 i:9 	 global-step:13329	 l-p:0.056109268218278885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[27.9762, 37.9204, 44.8718],
        [27.9762, 30.1190, 29.7420],
        [27.9762, 36.2260, 41.0431],
        [27.9762, 31.1011, 31.1557]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.056248124688863754 
model_pd.l_d.mean(): 6.572831352968933e-06 
model_pd.lagr.mean(): 0.05625469610095024 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.5890e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8147], device='cuda:0')), ('power', tensor([0.1336], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.056248124688863754
epoch£º667	 i:1 	 global-step:13341	 l-p:0.05641401186585426
epoch£º667	 i:2 	 global-step:13342	 l-p:0.0570392943918705
epoch£º667	 i:3 	 global-step:13343	 l-p:0.05616915598511696
epoch£º667	 i:4 	 global-step:13344	 l-p:0.057718075811862946
epoch£º667	 i:5 	 global-step:13345	 l-p:0.05652967467904091
epoch£º667	 i:6 	 global-step:13346	 l-p:0.05608533322811127
epoch£º667	 i:7 	 global-step:13347	 l-p:0.05660223960876465
epoch£º667	 i:8 	 global-step:13348	 l-p:0.056175000965595245
epoch£º667	 i:9 	 global-step:13349	 l-p:0.056192509829998016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0354, 28.1920, 28.0604],
        [28.0354, 28.0355, 28.0354],
        [28.0354, 29.8335, 29.3694],
        [28.0354, 33.0358, 34.4519]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.056343622505664825 
model_pd.l_d.mean(): 2.410065098956693e-05 
model_pd.lagr.mean(): 0.056367721408605576 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7850], device='cuda:0')), ('power', tensor([0.1987], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.056343622505664825
epoch£º668	 i:1 	 global-step:13361	 l-p:0.056318774819374084
epoch£º668	 i:2 	 global-step:13362	 l-p:0.05690302699804306
epoch£º668	 i:3 	 global-step:13363	 l-p:0.0566391758620739
epoch£º668	 i:4 	 global-step:13364	 l-p:0.056221723556518555
epoch£º668	 i:5 	 global-step:13365	 l-p:0.05614595115184784
epoch£º668	 i:6 	 global-step:13366	 l-p:0.05607886239886284
epoch£º668	 i:7 	 global-step:13367	 l-p:0.05648626387119293
epoch£º668	 i:8 	 global-step:13368	 l-p:0.05776418372988701
epoch£º668	 i:9 	 global-step:13369	 l-p:0.05610036849975586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0806, 29.3306, 28.8221],
        [28.0806, 28.0810, 28.0806],
        [28.0806, 28.0806, 28.0806],
        [28.0806, 31.3838, 31.5345]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.05686220899224281 
model_pd.l_d.mean(): 3.3210162655450404e-05 
model_pd.lagr.mean(): 0.0568954199552536 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8556], device='cuda:0')), ('power', tensor([0.1512], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.05686220899224281
epoch£º669	 i:1 	 global-step:13381	 l-p:0.0561191625893116
epoch£º669	 i:2 	 global-step:13382	 l-p:0.05631163343787193
epoch£º669	 i:3 	 global-step:13383	 l-p:0.056376200169324875
epoch£º669	 i:4 	 global-step:13384	 l-p:0.0561070516705513
epoch£º669	 i:5 	 global-step:13385	 l-p:0.05637055262923241
epoch£º669	 i:6 	 global-step:13386	 l-p:0.056012142449617386
epoch£º669	 i:7 	 global-step:13387	 l-p:0.05758238211274147
epoch£º669	 i:8 	 global-step:13388	 l-p:0.05672153830528259
epoch£º669	 i:9 	 global-step:13389	 l-p:0.056422214955091476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1091, 28.1091, 28.1091],
        [28.1091, 28.5318, 28.2349],
        [28.1091, 28.1159, 28.1093],
        [28.1091, 28.1091, 28.1091]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.056084293872117996 
model_pd.l_d.mean(): 6.008572017890401e-05 
model_pd.lagr.mean(): 0.05614437907934189 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8453], device='cuda:0')), ('power', tensor([0.1785], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.056084293872117996
epoch£º670	 i:1 	 global-step:13401	 l-p:0.056508976966142654
epoch£º670	 i:2 	 global-step:13402	 l-p:0.056206997483968735
epoch£º670	 i:3 	 global-step:13403	 l-p:0.05665689334273338
epoch£º670	 i:4 	 global-step:13404	 l-p:0.05793727561831474
epoch£º670	 i:5 	 global-step:13405	 l-p:0.056995078921318054
epoch£º670	 i:6 	 global-step:13406	 l-p:0.05612260848283768
epoch£º670	 i:7 	 global-step:13407	 l-p:0.056075047701597214
epoch£º670	 i:8 	 global-step:13408	 l-p:0.05621367320418358
epoch£º670	 i:9 	 global-step:13409	 l-p:0.056027330458164215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1156, 28.1683, 28.1199],
        [28.1156, 28.1327, 28.1163],
        [28.1156, 30.1009, 29.6752],
        [28.1156, 28.1333, 28.1163]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.05609532818198204 
model_pd.l_d.mean(): 4.147485378780402e-05 
model_pd.lagr.mean(): 0.05613680183887482 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8728], device='cuda:0')), ('power', tensor([0.0896], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.05609532818198204
epoch£º671	 i:1 	 global-step:13421	 l-p:0.05615074187517166
epoch£º671	 i:2 	 global-step:13422	 l-p:0.05617065727710724
epoch£º671	 i:3 	 global-step:13423	 l-p:0.05801565945148468
epoch£º671	 i:4 	 global-step:13424	 l-p:0.056258898228406906
epoch£º671	 i:5 	 global-step:13425	 l-p:0.05609409138560295
epoch£º671	 i:6 	 global-step:13426	 l-p:0.056330375373363495
epoch£º671	 i:7 	 global-step:13427	 l-p:0.05615253746509552
epoch£º671	 i:8 	 global-step:13428	 l-p:0.057039450854063034
epoch£º671	 i:9 	 global-step:13429	 l-p:0.05654153600335121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0928, 28.2009, 28.1066],
        [28.0928, 32.7053, 33.7881],
        [28.0928, 28.0928, 28.0928],
        [28.0928, 29.8578, 29.3861]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.05630391836166382 
model_pd.l_d.mean(): 0.00014243870100472122 
model_pd.lagr.mean(): 0.05644635856151581 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8085], device='cuda:0')), ('power', tensor([0.2430], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.05630391836166382
epoch£º672	 i:1 	 global-step:13441	 l-p:0.05793109908699989
epoch£º672	 i:2 	 global-step:13442	 l-p:0.05666251480579376
epoch£º672	 i:3 	 global-step:13443	 l-p:0.05604018643498421
epoch£º672	 i:4 	 global-step:13444	 l-p:0.056056976318359375
epoch£º672	 i:5 	 global-step:13445	 l-p:0.05626906454563141
epoch£º672	 i:6 	 global-step:13446	 l-p:0.05611741542816162
epoch£º672	 i:7 	 global-step:13447	 l-p:0.056190378963947296
epoch£º672	 i:8 	 global-step:13448	 l-p:0.0570952408015728
epoch£º672	 i:9 	 global-step:13449	 l-p:0.05628199130296707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0446, 28.0458, 28.0446],
        [28.0446, 28.0446, 28.0446],
        [28.0446, 34.2156, 36.7298],
        [28.0446, 28.0957, 28.0488]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.05642913654446602 
model_pd.l_d.mean(): 9.01257517398335e-05 
model_pd.lagr.mean(): 0.05651926249265671 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8418], device='cuda:0')), ('power', tensor([0.1301], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.05642913654446602
epoch£º673	 i:1 	 global-step:13461	 l-p:0.05611209571361542
epoch£º673	 i:2 	 global-step:13462	 l-p:0.05674246698617935
epoch£º673	 i:3 	 global-step:13463	 l-p:0.05609605833888054
epoch£º673	 i:4 	 global-step:13464	 l-p:0.05615444853901863
epoch£º673	 i:5 	 global-step:13465	 l-p:0.05622055009007454
epoch£º673	 i:6 	 global-step:13466	 l-p:0.05772014334797859
epoch£º673	 i:7 	 global-step:13467	 l-p:0.05719783529639244
epoch£º673	 i:8 	 global-step:13468	 l-p:0.05626130476593971
epoch£º673	 i:9 	 global-step:13469	 l-p:0.056212957948446274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9701, 28.7188, 28.2921],
        [27.9701, 28.5935, 28.2087],
        [27.9701, 29.1002, 28.6011],
        [27.9701, 30.6438, 30.4694]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.05674079805612564 
model_pd.l_d.mean(): 0.0002108514599967748 
model_pd.lagr.mean(): 0.056951649487018585 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7685], device='cuda:0')), ('power', tensor([0.2741], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.05674079805612564
epoch£º674	 i:1 	 global-step:13481	 l-p:0.05610217899084091
epoch£º674	 i:2 	 global-step:13482	 l-p:0.056237272918224335
epoch£º674	 i:3 	 global-step:13483	 l-p:0.05727383866906166
epoch£º674	 i:4 	 global-step:13484	 l-p:0.05623503401875496
epoch£º674	 i:5 	 global-step:13485	 l-p:0.056192655116319656
epoch£º674	 i:6 	 global-step:13486	 l-p:0.05618492141366005
epoch£º674	 i:7 	 global-step:13487	 l-p:0.056488893926143646
epoch£º674	 i:8 	 global-step:13488	 l-p:0.05617835745215416
epoch£º674	 i:9 	 global-step:13489	 l-p:0.05777828395366669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8764, 29.2829, 28.7784],
        [27.8764, 30.2088, 29.8991],
        [27.8764, 28.4796, 28.1029],
        [27.8764, 28.3076, 28.0071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.056379690766334534 
model_pd.l_d.mean(): -2.3793707441654988e-05 
model_pd.lagr.mean(): 0.05635589733719826 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8123], device='cuda:0')), ('power', tensor([-0.0296], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.056379690766334534
epoch£º675	 i:1 	 global-step:13501	 l-p:0.05634951591491699
epoch£º675	 i:2 	 global-step:13502	 l-p:0.05614858493208885
epoch£º675	 i:3 	 global-step:13503	 l-p:0.05657433345913887
epoch£º675	 i:4 	 global-step:13504	 l-p:0.056349705904722214
epoch£º675	 i:5 	 global-step:13505	 l-p:0.05662177875638008
epoch£º675	 i:6 	 global-step:13506	 l-p:0.05619446560740471
epoch£º675	 i:7 	 global-step:13507	 l-p:0.05787299573421478
epoch£º675	 i:8 	 global-step:13508	 l-p:0.05704531446099281
epoch£º675	 i:9 	 global-step:13509	 l-p:0.05619136616587639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7737, 27.7738, 27.7736],
        [27.7737, 31.1366, 31.3447],
        [27.7737, 27.8321, 27.7788],
        [27.7737, 27.7922, 27.7745]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.05624381825327873 
model_pd.l_d.mean(): -0.0001644836738705635 
model_pd.lagr.mean(): 0.05607933551073074 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8178], device='cuda:0')), ('power', tensor([-0.2076], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.05624381825327873
epoch£º676	 i:1 	 global-step:13521	 l-p:0.05610014870762825
epoch£º676	 i:2 	 global-step:13522	 l-p:0.056221187114715576
epoch£º676	 i:3 	 global-step:13523	 l-p:0.05696997418999672
epoch£º676	 i:4 	 global-step:13524	 l-p:0.05617961287498474
epoch£º676	 i:5 	 global-step:13525	 l-p:0.05677303299307823
epoch£º676	 i:6 	 global-step:13526	 l-p:0.056408002972602844
epoch£º676	 i:7 	 global-step:13527	 l-p:0.05710791051387787
epoch£º676	 i:8 	 global-step:13528	 l-p:0.057813338935375214
epoch£º676	 i:9 	 global-step:13529	 l-p:0.05624710023403168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6707, 27.6707, 27.6707],
        [27.6707, 28.3004, 27.9150],
        [27.6707, 29.5156, 29.0721],
        [27.6707, 27.7511, 27.6794]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.05623211711645126 
model_pd.l_d.mean(): -0.00021351677423808724 
model_pd.lagr.mean(): 0.05601860210299492 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8298], device='cuda:0')), ('power', tensor([-0.2931], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.05623211711645126
epoch£º677	 i:1 	 global-step:13541	 l-p:0.05644316226243973
epoch£º677	 i:2 	 global-step:13542	 l-p:0.05617447569966316
epoch£º677	 i:3 	 global-step:13543	 l-p:0.057908933609724045
epoch£º677	 i:4 	 global-step:13544	 l-p:0.0561612993478775
epoch£º677	 i:5 	 global-step:13545	 l-p:0.056615199893713
epoch£º677	 i:6 	 global-step:13546	 l-p:0.057086143642663956
epoch£º677	 i:7 	 global-step:13547	 l-p:0.05628877505660057
epoch£º677	 i:8 	 global-step:13548	 l-p:0.056617021560668945
epoch£º677	 i:9 	 global-step:13549	 l-p:0.056838732212781906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5814, 27.7422, 27.6079],
        [27.5814, 27.5885, 27.5816],
        [27.5814, 27.7211, 27.6025],
        [27.5814, 27.7894, 27.6216]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): 0.05646032094955444 
model_pd.l_d.mean(): -0.00011608246859395877 
model_pd.lagr.mean(): 0.05634423717856407 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7490], device='cuda:0')), ('power', tensor([-0.1882], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:0.05646032094955444
epoch£º678	 i:1 	 global-step:13561	 l-p:0.05833851173520088
epoch£º678	 i:2 	 global-step:13562	 l-p:0.05649872124195099
epoch£º678	 i:3 	 global-step:13563	 l-p:0.05644545331597328
epoch£º678	 i:4 	 global-step:13564	 l-p:0.05626320093870163
epoch£º678	 i:5 	 global-step:13565	 l-p:0.056240957230329514
epoch£º678	 i:6 	 global-step:13566	 l-p:0.05715186893939972
epoch£º678	 i:7 	 global-step:13567	 l-p:0.05617900565266609
epoch£º678	 i:8 	 global-step:13568	 l-p:0.0561397485435009
epoch£º678	 i:9 	 global-step:13569	 l-p:0.056901901960372925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5121, 27.5124, 27.5121],
        [27.5121, 33.3513, 35.6078],
        [27.5121, 27.6460, 27.5317],
        [27.5121, 27.7815, 27.5733]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.05632958188652992 
model_pd.l_d.mean(): -0.000194584034034051 
model_pd.lagr.mean(): 0.05613499879837036 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8045], device='cuda:0')), ('power', tensor([-0.4176], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.05632958188652992
epoch£º679	 i:1 	 global-step:13581	 l-p:0.056356534361839294
epoch£º679	 i:2 	 global-step:13582	 l-p:0.056299008429050446
epoch£º679	 i:3 	 global-step:13583	 l-p:0.057948581874370575
epoch£º679	 i:4 	 global-step:13584	 l-p:0.05633822828531265
epoch£º679	 i:5 	 global-step:13585	 l-p:0.05646441504359245
epoch£º679	 i:6 	 global-step:13586	 l-p:0.057532213628292084
epoch£º679	 i:7 	 global-step:13587	 l-p:0.056243542581796646
epoch£º679	 i:8 	 global-step:13588	 l-p:0.056363895535469055
epoch£º679	 i:9 	 global-step:13589	 l-p:0.05694764852523804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4700, 27.5015, 27.4720],
        [27.4700, 27.4811, 27.4704],
        [27.4700, 27.6651, 27.5063],
        [27.4700, 35.2539, 39.6136]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.05617501214146614 
model_pd.l_d.mean(): -0.00016476384189445525 
model_pd.lagr.mean(): 0.05601025000214577 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8585], device='cuda:0')), ('power', tensor([-0.5757], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.05617501214146614
epoch£º680	 i:1 	 global-step:13601	 l-p:0.056179676204919815
epoch£º680	 i:2 	 global-step:13602	 l-p:0.05800314620137215
epoch£º680	 i:3 	 global-step:13603	 l-p:0.05639518052339554
epoch£º680	 i:4 	 global-step:13604	 l-p:0.05639101564884186
epoch£º680	 i:5 	 global-step:13605	 l-p:0.05638647824525833
epoch£º680	 i:6 	 global-step:13606	 l-p:0.058196719735860825
epoch£º680	 i:7 	 global-step:13607	 l-p:0.056549061089754105
epoch£º680	 i:8 	 global-step:13608	 l-p:0.0563284195959568
epoch£º680	 i:9 	 global-step:13609	 l-p:0.05630059912800789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4674, 29.0335, 28.5503],
        [27.4674, 36.5539, 42.5045],
        [27.4674, 27.4713, 27.4675],
        [27.4674, 27.4717, 27.4675]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.05635633319616318 
model_pd.l_d.mean(): -4.502720548771322e-05 
model_pd.lagr.mean(): 0.05631130561232567 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.0470e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7952], device='cuda:0')), ('power', tensor([-0.4773], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.05635633319616318
epoch£º681	 i:1 	 global-step:13621	 l-p:0.05634979158639908
epoch£º681	 i:2 	 global-step:13622	 l-p:0.05745375528931618
epoch£º681	 i:3 	 global-step:13623	 l-p:0.05660158395767212
epoch£º681	 i:4 	 global-step:13624	 l-p:0.05637967959046364
epoch£º681	 i:5 	 global-step:13625	 l-p:0.056319475173950195
epoch£º681	 i:6 	 global-step:13626	 l-p:0.058795928955078125
epoch£º681	 i:7 	 global-step:13627	 l-p:0.056160494685173035
epoch£º681	 i:8 	 global-step:13628	 l-p:0.05620713531970978
epoch£º681	 i:9 	 global-step:13629	 l-p:0.05623176321387291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5036, 28.0983, 27.7269],
        [27.5036, 28.3431, 27.8964],
        [27.5036, 27.5146, 27.5040],
        [27.5036, 27.5763, 27.5110]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.05627783015370369 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05627783015370369 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8128], device='cuda:0')), ('power', tensor([-0.4471], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.05627783015370369
epoch£º682	 i:1 	 global-step:13641	 l-p:0.05619867146015167
epoch£º682	 i:2 	 global-step:13642	 l-p:0.056420113891363144
epoch£º682	 i:3 	 global-step:13643	 l-p:0.05742105469107628
epoch£º682	 i:4 	 global-step:13644	 l-p:0.05638762190937996
epoch£º682	 i:5 	 global-step:13645	 l-p:0.05625592917203903
epoch£º682	 i:6 	 global-step:13646	 l-p:0.05619044974446297
epoch£º682	 i:7 	 global-step:13647	 l-p:0.056405629962682724
epoch£º682	 i:8 	 global-step:13648	 l-p:0.058859147131443024
epoch£º682	 i:9 	 global-step:13649	 l-p:0.05626753717660904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5635, 28.3463, 27.9133],
        [27.5635, 27.7493, 27.5969],
        [27.5635, 27.5636, 27.5635],
        [27.5635, 28.6891, 28.1966]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.056177206337451935 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056177206337451935 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8529], device='cuda:0')), ('power', tensor([-0.4344], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.056177206337451935
epoch£º683	 i:1 	 global-step:13661	 l-p:0.056374792009592056
epoch£º683	 i:2 	 global-step:13662	 l-p:0.05660542473196983
epoch£º683	 i:3 	 global-step:13663	 l-p:0.056902751326560974
epoch£º683	 i:4 	 global-step:13664	 l-p:0.057960089296102524
epoch£º683	 i:5 	 global-step:13665	 l-p:0.057231828570365906
epoch£º683	 i:6 	 global-step:13666	 l-p:0.056199830025434494
epoch£º683	 i:7 	 global-step:13667	 l-p:0.05640876665711403
epoch£º683	 i:8 	 global-step:13668	 l-p:0.05626760795712471
epoch£º683	 i:9 	 global-step:13669	 l-p:0.05634625256061554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6377, 32.0768, 33.0642],
        [27.6377, 27.6377, 27.6377],
        [27.6377, 35.8670, 40.7233],
        [27.6377, 27.6477, 27.6380]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.056330010294914246 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056330010294914246 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8023], device='cuda:0')), ('power', tensor([-0.2784], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.056330010294914246
epoch£º684	 i:1 	 global-step:13681	 l-p:0.0563858225941658
epoch£º684	 i:2 	 global-step:13682	 l-p:0.056195516139268875
epoch£º684	 i:3 	 global-step:13683	 l-p:0.05628884211182594
epoch£º684	 i:4 	 global-step:13684	 l-p:0.05828333646059036
epoch£º684	 i:5 	 global-step:13685	 l-p:0.05677017569541931
epoch£º684	 i:6 	 global-step:13686	 l-p:0.05612320825457573
epoch£º684	 i:7 	 global-step:13687	 l-p:0.05619705095887184
epoch£º684	 i:8 	 global-step:13688	 l-p:0.05714999884366989
epoch£º684	 i:9 	 global-step:13689	 l-p:0.056501906365156174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7127, 28.1761, 27.8604],
        [27.7127, 36.8541, 42.8225],
        [27.7127, 30.4316, 30.2916],
        [27.7127, 27.7131, 27.7127]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.056197185069322586 
model_pd.l_d.mean(): -2.422014055980526e-08 
model_pd.lagr.mean(): 0.0561971589922905 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8287], device='cuda:0')), ('power', tensor([-0.2377], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.056197185069322586
epoch£º685	 i:1 	 global-step:13701	 l-p:0.056139253079891205
epoch£º685	 i:2 	 global-step:13702	 l-p:0.056349143385887146
epoch£º685	 i:3 	 global-step:13703	 l-p:0.05610356107354164
epoch£º685	 i:4 	 global-step:13704	 l-p:0.05801663175225258
epoch£º685	 i:5 	 global-step:13705	 l-p:0.057556554675102234
epoch£º685	 i:6 	 global-step:13706	 l-p:0.05627884715795517
epoch£º685	 i:7 	 global-step:13707	 l-p:0.056320007890462875
epoch£º685	 i:8 	 global-step:13708	 l-p:0.056272752583026886
epoch£º685	 i:9 	 global-step:13709	 l-p:0.05674802139401436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7891, 27.7906, 27.7891],
        [27.7891, 27.7916, 27.7891],
        [27.7891, 27.9481, 27.8149],
        [27.7891, 35.8873, 40.5587]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.056984152644872665 
model_pd.l_d.mean(): 1.090611135623476e-06 
model_pd.lagr.mean(): 0.056985244154930115 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.5536e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7121], device='cuda:0')), ('power', tensor([0.2036], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.056984152644872665
epoch£º686	 i:1 	 global-step:13721	 l-p:0.05618647113442421
epoch£º686	 i:2 	 global-step:13722	 l-p:0.056610822677612305
epoch£º686	 i:3 	 global-step:13723	 l-p:0.05696995556354523
epoch£º686	 i:4 	 global-step:13724	 l-p:0.0561956949532032
epoch£º686	 i:5 	 global-step:13725	 l-p:0.05632658302783966
epoch£º686	 i:6 	 global-step:13726	 l-p:0.057971589267253876
epoch£º686	 i:7 	 global-step:13727	 l-p:0.05617668107151985
epoch£º686	 i:8 	 global-step:13728	 l-p:0.056155942380428314
epoch£º686	 i:9 	 global-step:13729	 l-p:0.05616169795393944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[27.8656, 35.7011, 40.0504],
        [27.8656, 31.6802, 32.1779],
        [27.8656, 29.7312, 29.2859],
        [27.8656, 34.7807, 38.0971]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.05643962323665619 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05643962323665619 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.6739e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7466], device='cuda:0')), ('power', tensor([0.1335], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.05643962323665619
epoch£º687	 i:1 	 global-step:13741	 l-p:0.057702139019966125
epoch£º687	 i:2 	 global-step:13742	 l-p:0.05635404586791992
epoch£º687	 i:3 	 global-step:13743	 l-p:0.056638360023498535
epoch£º687	 i:4 	 global-step:13744	 l-p:0.056056056171655655
epoch£º687	 i:5 	 global-step:13745	 l-p:0.05639779940247536
epoch£º687	 i:6 	 global-step:13746	 l-p:0.056661639362573624
epoch£º687	 i:7 	 global-step:13747	 l-p:0.0561019591987133
epoch£º687	 i:8 	 global-step:13748	 l-p:0.056117184460163116
epoch£º687	 i:9 	 global-step:13749	 l-p:0.05703078210353851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9412, 27.9412, 27.9412],
        [27.9412, 31.2271, 31.3770],
        [27.9412, 29.1776, 28.6721],
        [27.9412, 27.9415, 27.9412]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.05705640837550163 
model_pd.l_d.mean(): 5.349955245037563e-06 
model_pd.lagr.mean(): 0.057061757892370224 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.3938e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7717], device='cuda:0')), ('power', tensor([0.2489], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.05705640837550163
epoch£º688	 i:1 	 global-step:13761	 l-p:0.0562046617269516
epoch£º688	 i:2 	 global-step:13762	 l-p:0.05787515640258789
epoch£º688	 i:3 	 global-step:13763	 l-p:0.05606495589017868
epoch£º688	 i:4 	 global-step:13764	 l-p:0.0566449835896492
epoch£º688	 i:5 	 global-step:13765	 l-p:0.05607554316520691
epoch£º688	 i:6 	 global-step:13766	 l-p:0.05619640648365021
epoch£º688	 i:7 	 global-step:13767	 l-p:0.05620001628994942
epoch£º688	 i:8 	 global-step:13768	 l-p:0.05655756965279579
epoch£º688	 i:9 	 global-step:13769	 l-p:0.05639367923140526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0133, 28.3901, 28.1178],
        [28.0133, 28.0470, 28.0154],
        [28.0133, 28.0134, 28.0133],
        [28.0133, 28.0133, 28.0132]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.05609948933124542 
model_pd.l_d.mean(): 1.6417391179857077e-06 
model_pd.lagr.mean(): 0.056101132184267044 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([8.0485e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8470], device='cuda:0')), ('power', tensor([0.0207], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.05609948933124542
epoch£º689	 i:1 	 global-step:13781	 l-p:0.05633065104484558
epoch£º689	 i:2 	 global-step:13782	 l-p:0.05647488310933113
epoch£º689	 i:3 	 global-step:13783	 l-p:0.056194622069597244
epoch£º689	 i:4 	 global-step:13784	 l-p:0.05615437030792236
epoch£º689	 i:5 	 global-step:13785	 l-p:0.056213948875665665
epoch£º689	 i:6 	 global-step:13786	 l-p:0.05662451684474945
epoch£º689	 i:7 	 global-step:13787	 l-p:0.05718965083360672
epoch£º689	 i:8 	 global-step:13788	 l-p:0.056062325835227966
epoch£º689	 i:9 	 global-step:13789	 l-p:0.05770338699221611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0705, 28.0708, 28.0705],
        [28.0705, 37.7217, 44.2676],
        [28.0705, 28.0730, 28.0706],
        [28.0705, 28.0705, 28.0705]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.05611797794699669 
model_pd.l_d.mean(): 2.0370467609609477e-05 
model_pd.lagr.mean(): 0.05613834783434868 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8498], device='cuda:0')), ('power', tensor([0.1203], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.05611797794699669
epoch£º690	 i:1 	 global-step:13801	 l-p:0.056922852993011475
epoch£º690	 i:2 	 global-step:13802	 l-p:0.058433808386325836
epoch£º690	 i:3 	 global-step:13803	 l-p:0.05617116019129753
epoch£º690	 i:4 	 global-step:13804	 l-p:0.05648108199238777
epoch£º690	 i:5 	 global-step:13805	 l-p:0.056487780064344406
epoch£º690	 i:6 	 global-step:13806	 l-p:0.056125156581401825
epoch£º690	 i:7 	 global-step:13807	 l-p:0.056041400879621506
epoch£º690	 i:8 	 global-step:13808	 l-p:0.05603804439306259
epoch£º690	 i:9 	 global-step:13809	 l-p:0.05607888102531433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1161, 28.1163, 28.1161],
        [28.1161, 28.7927, 28.3880],
        [28.1161, 33.3853, 35.0303],
        [28.1161, 28.1264, 28.1165]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.056995924562215805 
model_pd.l_d.mean(): 0.0001098630455089733 
model_pd.lagr.mean(): 0.057105787098407745 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8034], device='cuda:0')), ('power', tensor([0.3844], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.056995924562215805
epoch£º691	 i:1 	 global-step:13821	 l-p:0.057675838470458984
epoch£º691	 i:2 	 global-step:13822	 l-p:0.05678142234683037
epoch£º691	 i:3 	 global-step:13823	 l-p:0.056191299110651016
epoch£º691	 i:4 	 global-step:13824	 l-p:0.05606357753276825
epoch£º691	 i:5 	 global-step:13825	 l-p:0.05605605989694595
epoch£º691	 i:6 	 global-step:13826	 l-p:0.05616343393921852
epoch£º691	 i:7 	 global-step:13827	 l-p:0.05627467855811119
epoch£º691	 i:8 	 global-step:13828	 l-p:0.05615614727139473
epoch£º691	 i:9 	 global-step:13829	 l-p:0.05643489584326744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1324, 28.1337, 28.1324],
        [28.1324, 28.1349, 28.1324],
        [28.1324, 28.4937, 28.2297],
        [28.1324, 28.1324, 28.1324]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.05603841692209244 
model_pd.l_d.mean(): 2.5027284209500067e-05 
model_pd.lagr.mean(): 0.056063443422317505 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8768], device='cuda:0')), ('power', tensor([0.0598], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.05603841692209244
epoch£º692	 i:1 	 global-step:13841	 l-p:0.056152213364839554
epoch£º692	 i:2 	 global-step:13842	 l-p:0.05622921884059906
epoch£º692	 i:3 	 global-step:13843	 l-p:0.05669257417321205
epoch£º692	 i:4 	 global-step:13844	 l-p:0.056423310190439224
epoch£º692	 i:5 	 global-step:13845	 l-p:0.05626272037625313
epoch£º692	 i:6 	 global-step:13846	 l-p:0.05604039132595062
epoch£º692	 i:7 	 global-step:13847	 l-p:0.05636369436979294
epoch£º692	 i:8 	 global-step:13848	 l-p:0.057685486972332
epoch£º692	 i:9 	 global-step:13849	 l-p:0.056905172765254974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1148, 28.1319, 28.1155],
        [28.1148, 35.4594, 39.2073],
        [28.1148, 37.3948, 43.4542],
        [28.1148, 37.6780, 44.1002]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.05656878650188446 
model_pd.l_d.mean(): 0.00025208358420059085 
model_pd.lagr.mean(): 0.05682086944580078 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7620], device='cuda:0')), ('power', tensor([0.4577], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.05656878650188446
epoch£º693	 i:1 	 global-step:13861	 l-p:0.05626828223466873
epoch£º693	 i:2 	 global-step:13862	 l-p:0.05605097487568855
epoch£º693	 i:3 	 global-step:13863	 l-p:0.05761197954416275
epoch£º693	 i:4 	 global-step:13864	 l-p:0.05759969726204872
epoch£º693	 i:5 	 global-step:13865	 l-p:0.05603589862585068
epoch£º693	 i:6 	 global-step:13866	 l-p:0.05627278983592987
epoch£º693	 i:7 	 global-step:13867	 l-p:0.05607285350561142
epoch£º693	 i:8 	 global-step:13868	 l-p:0.05627334490418434
epoch£º693	 i:9 	 global-step:13869	 l-p:0.05611806735396385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0726, 28.4348, 28.1704],
        [28.0726, 31.2866, 31.3856],
        [28.0726, 28.0728, 28.0726],
        [28.0726, 28.0726, 28.0726]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.05690351501107216 
model_pd.l_d.mean(): 0.00027517086709849536 
model_pd.lagr.mean(): 0.05717868730425835 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7564], device='cuda:0')), ('power', tensor([0.4110], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.05690351501107216
epoch£º694	 i:1 	 global-step:13881	 l-p:0.056067392230033875
epoch£º694	 i:2 	 global-step:13882	 l-p:0.05610301345586777
epoch£º694	 i:3 	 global-step:13883	 l-p:0.05769974738359451
epoch£º694	 i:4 	 global-step:13884	 l-p:0.0561409667134285
epoch£º694	 i:5 	 global-step:13885	 l-p:0.05651908367872238
epoch£º694	 i:6 	 global-step:13886	 l-p:0.05616242438554764
epoch£º694	 i:7 	 global-step:13887	 l-p:0.0570472814142704
epoch£º694	 i:8 	 global-step:13888	 l-p:0.056238945573568344
epoch£º694	 i:9 	 global-step:13889	 l-p:0.056167297065258026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9982, 30.3413, 30.0302],
        [27.9982, 32.4923, 33.4886],
        [27.9982, 32.6276, 33.7338],
        [27.9982, 27.9995, 27.9982]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.05766335874795914 
model_pd.l_d.mean(): 0.00020253416732884943 
model_pd.lagr.mean(): 0.057865891605615616 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8090], device='cuda:0')), ('power', tensor([0.2664], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.05766335874795914
epoch£º695	 i:1 	 global-step:13901	 l-p:0.05606963112950325
epoch£º695	 i:2 	 global-step:13902	 l-p:0.05663694813847542
epoch£º695	 i:3 	 global-step:13903	 l-p:0.056399568915367126
epoch£º695	 i:4 	 global-step:13904	 l-p:0.05615558475255966
epoch£º695	 i:5 	 global-step:13905	 l-p:0.056308820843696594
epoch£º695	 i:6 	 global-step:13906	 l-p:0.05668294429779053
epoch£º695	 i:7 	 global-step:13907	 l-p:0.057232074439525604
epoch£º695	 i:8 	 global-step:13908	 l-p:0.0560329407453537
epoch£º695	 i:9 	 global-step:13909	 l-p:0.05612429603934288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9009, 27.9422, 27.9039],
        [27.9009, 32.3786, 33.3712],
        [27.9009, 28.1368, 27.9497],
        [27.9009, 36.9592, 42.7832]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.05609825626015663 
model_pd.l_d.mean(): -9.680671792011708e-05 
model_pd.lagr.mean(): 0.0560014508664608 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8622], device='cuda:0')), ('power', tensor([-0.1196], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.05609825626015663
epoch£º696	 i:1 	 global-step:13921	 l-p:0.05620334669947624
epoch£º696	 i:2 	 global-step:13922	 l-p:0.05611805245280266
epoch£º696	 i:3 	 global-step:13923	 l-p:0.05730731412768364
epoch£º696	 i:4 	 global-step:13924	 l-p:0.05767257511615753
epoch£º696	 i:5 	 global-step:13925	 l-p:0.05677000805735588
epoch£º696	 i:6 	 global-step:13926	 l-p:0.05673802271485329
epoch£º696	 i:7 	 global-step:13927	 l-p:0.0562313012778759
epoch£º696	 i:8 	 global-step:13928	 l-p:0.05615847930312157
epoch£º696	 i:9 	 global-step:13929	 l-p:0.05635344237089157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7882, 27.7882, 27.7882],
        [27.7882, 27.7884, 27.7882],
        [27.7882, 34.9221, 38.4896],
        [27.7882, 27.7921, 27.7883]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.05666297674179077 
model_pd.l_d.mean(): -9.13953481358476e-05 
model_pd.lagr.mean(): 0.05657158046960831 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8235], device='cuda:0')), ('power', tensor([-0.1134], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.05666297674179077
epoch£º697	 i:1 	 global-step:13941	 l-p:0.05741145834326744
epoch£º697	 i:2 	 global-step:13942	 l-p:0.05622033402323723
epoch£º697	 i:3 	 global-step:13943	 l-p:0.056234732270240784
epoch£º697	 i:4 	 global-step:13944	 l-p:0.05621907487511635
epoch£º697	 i:5 	 global-step:13945	 l-p:0.056250810623168945
epoch£º697	 i:6 	 global-step:13946	 l-p:0.056187644600868225
epoch£º697	 i:7 	 global-step:13947	 l-p:0.05805065110325813
epoch£º697	 i:8 	 global-step:13948	 l-p:0.05642233416438103
epoch£º697	 i:9 	 global-step:13949	 l-p:0.056359656155109406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6728, 27.8586, 27.7062],
        [27.6728, 27.6840, 27.6732],
        [27.6728, 29.2436, 28.7558],
        [27.6728, 30.3154, 30.1423]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.05624644085764885 
model_pd.l_d.mean(): -0.00022389437071979046 
model_pd.lagr.mean(): 0.05602254718542099 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8296], device='cuda:0')), ('power', tensor([-0.2997], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.05624644085764885
epoch£º698	 i:1 	 global-step:13961	 l-p:0.056164491921663284
epoch£º698	 i:2 	 global-step:13962	 l-p:0.056151408702135086
epoch£º698	 i:3 	 global-step:13963	 l-p:0.05783497542142868
epoch£º698	 i:4 	 global-step:13964	 l-p:0.05725570023059845
epoch£º698	 i:5 	 global-step:13965	 l-p:0.05620549991726875
epoch£º698	 i:6 	 global-step:13966	 l-p:0.0563339926302433
epoch£º698	 i:7 	 global-step:13967	 l-p:0.056673068553209305
epoch£º698	 i:8 	 global-step:13968	 l-p:0.05643853917717934
epoch£º698	 i:9 	 global-step:13969	 l-p:0.05707550048828125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5701, 27.6021, 27.5721],
        [27.5701, 27.9209, 27.6640],
        [27.5701, 30.4293, 30.3663],
        [27.5701, 27.5701, 27.5701]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.056301526725292206 
model_pd.l_d.mean(): -0.00022059014008846134 
model_pd.lagr.mean(): 0.05608093738555908 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8068], device='cuda:0')), ('power', tensor([-0.3482], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.056301526725292206
epoch£º699	 i:1 	 global-step:13981	 l-p:0.05698573961853981
epoch£º699	 i:2 	 global-step:13982	 l-p:0.05729415640234947
epoch£º699	 i:3 	 global-step:13983	 l-p:0.05621872842311859
epoch£º699	 i:4 	 global-step:13984	 l-p:0.05833747610449791
epoch£º699	 i:5 	 global-step:13985	 l-p:0.056248947978019714
epoch£º699	 i:6 	 global-step:13986	 l-p:0.056278046220541
epoch£º699	 i:7 	 global-step:13987	 l-p:0.05639288201928139
epoch£º699	 i:8 	 global-step:13988	 l-p:0.05626437067985535
epoch£º699	 i:9 	 global-step:13989	 l-p:0.056353941559791565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4911, 27.5136, 27.4922],
        [27.4911, 29.6845, 29.3427],
        [27.4911, 29.2185, 28.7578],
        [27.4911, 27.8526, 27.5900]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.056390780955553055 
model_pd.l_d.mean(): -0.00022641186660621315 
model_pd.lagr.mean(): 0.056164368987083435 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8209], device='cuda:0')), ('power', tensor([-0.4772], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.056390780955553055
epoch£º700	 i:1 	 global-step:14001	 l-p:0.057106420397758484
epoch£º700	 i:2 	 global-step:14002	 l-p:0.05641062557697296
epoch£º700	 i:3 	 global-step:14003	 l-p:0.05627558007836342
epoch£º700	 i:4 	 global-step:14004	 l-p:0.05678209289908409
epoch£º700	 i:5 	 global-step:14005	 l-p:0.05622895061969757
epoch£º700	 i:6 	 global-step:14006	 l-p:0.056316275149583817
epoch£º700	 i:7 	 global-step:14007	 l-p:0.05619650334119797
epoch£º700	 i:8 	 global-step:14008	 l-p:0.056309204548597336
epoch£º700	 i:9 	 global-step:14009	 l-p:0.05890015885233879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4390, 27.4720, 27.4411],
        [27.4390, 27.8975, 27.5851],
        [27.4390, 27.4390, 27.4390],
        [27.4390, 28.7289, 28.2312]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.056452468037605286 
model_pd.l_d.mean(): -0.00012025923933833838 
model_pd.lagr.mean(): 0.05633220821619034 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7725], device='cuda:0')), ('power', tensor([-0.4261], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.056452468037605286
epoch£º701	 i:1 	 global-step:14021	 l-p:0.056326091289520264
epoch£º701	 i:2 	 global-step:14022	 l-p:0.05626712366938591
epoch£º701	 i:3 	 global-step:14023	 l-p:0.05648665875196457
epoch£º701	 i:4 	 global-step:14024	 l-p:0.05631931498646736
epoch£º701	 i:5 	 global-step:14025	 l-p:0.05684131383895874
epoch£º701	 i:6 	 global-step:14026	 l-p:0.05719238519668579
epoch£º701	 i:7 	 global-step:14027	 l-p:0.05670132488012314
epoch£º701	 i:8 	 global-step:14028	 l-p:0.05625259876251221
epoch£º701	 i:9 	 global-step:14029	 l-p:0.0581754669547081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4339, 27.7828, 27.5273],
        [27.4339, 35.0219, 39.1608],
        [27.4339, 27.7703, 27.5219],
        [27.4339, 27.4339, 27.4338]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.05638576298952103 
model_pd.l_d.mean(): -3.41577033395879e-05 
model_pd.lagr.mean(): 0.05635160580277443 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.1363e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7861], device='cuda:0')), ('power', tensor([-0.4595], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.05638576298952103
epoch£º702	 i:1 	 global-step:14041	 l-p:0.05640910193324089
epoch£º702	 i:2 	 global-step:14042	 l-p:0.056508444249629974
epoch£º702	 i:3 	 global-step:14043	 l-p:0.05634336918592453
epoch£º702	 i:4 	 global-step:14044	 l-p:0.05673418939113617
epoch£º702	 i:5 	 global-step:14045	 l-p:0.05641265586018562
epoch£º702	 i:6 	 global-step:14046	 l-p:0.05627541244029999
epoch£º702	 i:7 	 global-step:14047	 l-p:0.05618397518992424
epoch£º702	 i:8 	 global-step:14048	 l-p:0.057965993881225586
epoch£º702	 i:9 	 global-step:14049	 l-p:0.057729728519916534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4761, 27.4888, 27.4766],
        [27.4761, 35.6551, 40.4814],
        [27.4761, 27.5338, 27.4812],
        [27.4761, 35.8341, 40.8768]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.056290123611688614 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056290123611688614 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7954], device='cuda:0')), ('power', tensor([-0.4235], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.056290123611688614
epoch£º703	 i:1 	 global-step:14061	 l-p:0.05671146139502525
epoch£º703	 i:2 	 global-step:14062	 l-p:0.05642227828502655
epoch£º703	 i:3 	 global-step:14063	 l-p:0.056189436465501785
epoch£º703	 i:4 	 global-step:14064	 l-p:0.05626947432756424
epoch£º703	 i:5 	 global-step:14065	 l-p:0.05628862604498863
epoch£º703	 i:6 	 global-step:14066	 l-p:0.05724075064063072
epoch£º703	 i:7 	 global-step:14067	 l-p:0.05657915025949478
epoch£º703	 i:8 	 global-step:14068	 l-p:0.05687595158815384
epoch£º703	 i:9 	 global-step:14069	 l-p:0.057887572795152664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5453, 27.5453, 27.5453],
        [27.5453, 27.7780, 27.5934],
        [27.5453, 28.0118, 27.6951],
        [27.5453, 31.6408, 32.3695]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.0561295785009861 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.0561295785009861 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8793], device='cuda:0')), ('power', tensor([-0.5799], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.0561295785009861
epoch£º704	 i:1 	 global-step:14081	 l-p:0.05635887384414673
epoch£º704	 i:2 	 global-step:14082	 l-p:0.056814953684806824
epoch£º704	 i:3 	 global-step:14083	 l-p:0.056268252432346344
epoch£º704	 i:4 	 global-step:14084	 l-p:0.05800658091902733
epoch£º704	 i:5 	 global-step:14085	 l-p:0.056364670395851135
epoch£º704	 i:6 	 global-step:14086	 l-p:0.05703676864504814
epoch£º704	 i:7 	 global-step:14087	 l-p:0.05617763474583626
epoch£º704	 i:8 	 global-step:14088	 l-p:0.057059887796640396
epoch£º704	 i:9 	 global-step:14089	 l-p:0.05630047619342804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6275, 27.6275, 27.6274],
        [27.6275, 27.6474, 27.6284],
        [27.6275, 31.9059, 32.7672],
        [27.6275, 34.6542, 38.1296]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 0.05636204779148102 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05636204779148102 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7800], device='cuda:0')), ('power', tensor([-0.1833], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:0.05636204779148102
epoch£º705	 i:1 	 global-step:14101	 l-p:0.05785862356424332
epoch£º705	 i:2 	 global-step:14102	 l-p:0.056433457881212234
epoch£º705	 i:3 	 global-step:14103	 l-p:0.056333303451538086
epoch£º705	 i:4 	 global-step:14104	 l-p:0.05612824484705925
epoch£º705	 i:5 	 global-step:14105	 l-p:0.05689079314470291
epoch£º705	 i:6 	 global-step:14106	 l-p:0.05622159689664841
epoch£º705	 i:7 	 global-step:14107	 l-p:0.05749456584453583
epoch£º705	 i:8 	 global-step:14108	 l-p:0.056256864219903946
epoch£º705	 i:9 	 global-step:14109	 l-p:0.05627213045954704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7122, 28.1407, 27.8421],
        [27.7122, 35.4921, 39.8039],
        [27.7122, 27.8983, 27.7456],
        [27.7122, 30.4591, 30.3326]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.05621705576777458 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05621705576777458 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8160], device='cuda:0')), ('power', tensor([-0.2188], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.05621705576777458
epoch£º706	 i:1 	 global-step:14121	 l-p:0.0561983585357666
epoch£º706	 i:2 	 global-step:14122	 l-p:0.05788888782262802
epoch£º706	 i:3 	 global-step:14123	 l-p:0.056215960532426834
epoch£º706	 i:4 	 global-step:14124	 l-p:0.056579284369945526
epoch£º706	 i:5 	 global-step:14125	 l-p:0.05629272386431694
epoch£º706	 i:6 	 global-step:14126	 l-p:0.056927639991045
epoch£º706	 i:7 	 global-step:14127	 l-p:0.05730699375271797
epoch£º706	 i:8 	 global-step:14128	 l-p:0.05621663108468056
epoch£º706	 i:9 	 global-step:14129	 l-p:0.05613365024328232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7958, 27.7971, 27.7958],
        [27.7958, 27.7959, 27.7958],
        [27.7958, 27.7959, 27.7958],
        [27.7958, 28.4655, 28.0652]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.05610118806362152 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05610118806362152 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8680], device='cuda:0')), ('power', tensor([-0.2414], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.05610118806362152
epoch£º707	 i:1 	 global-step:14141	 l-p:0.05621403455734253
epoch£º707	 i:2 	 global-step:14142	 l-p:0.056253571063280106
epoch£º707	 i:3 	 global-step:14143	 l-p:0.05654388293623924
epoch£º707	 i:4 	 global-step:14144	 l-p:0.05665068328380585
epoch£º707	 i:5 	 global-step:14145	 l-p:0.0564190149307251
epoch£º707	 i:6 	 global-step:14146	 l-p:0.056298911571502686
epoch£º707	 i:7 	 global-step:14147	 l-p:0.05718986317515373
epoch£º707	 i:8 	 global-step:14148	 l-p:0.0561014860868454
epoch£º707	 i:9 	 global-step:14149	 l-p:0.05792571231722832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8752, 28.2914, 27.9986],
        [27.8752, 29.1869, 28.6808],
        [27.8752, 33.8124, 36.1172],
        [27.8752, 28.2970, 28.0013]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.056248147040605545 
model_pd.l_d.mean(): -2.880957765682979e-07 
model_pd.lagr.mean(): 0.056247860193252563 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4548e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8181], device='cuda:0')), ('power', tensor([-0.0186], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.056248147040605545
epoch£º708	 i:1 	 global-step:14161	 l-p:0.05620831251144409
epoch£º708	 i:2 	 global-step:14162	 l-p:0.05616907775402069
epoch£º708	 i:3 	 global-step:14163	 l-p:0.05633692443370819
epoch£º708	 i:4 	 global-step:14164	 l-p:0.0576992891728878
epoch£º708	 i:5 	 global-step:14165	 l-p:0.05699152871966362
epoch£º708	 i:6 	 global-step:14166	 l-p:0.056278686970472336
epoch£º708	 i:7 	 global-step:14167	 l-p:0.056130751967430115
epoch£º708	 i:8 	 global-step:14168	 l-p:0.05713251605629921
epoch£º708	 i:9 	 global-step:14169	 l-p:0.056257762014865875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9584, 30.0192, 29.6190],
        [27.9584, 28.1517, 27.9937],
        [27.9584, 37.8898, 44.8284],
        [27.9584, 32.2085, 33.0173]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.0563521645963192 
model_pd.l_d.mean(): 3.1912595659377985e-06 
model_pd.lagr.mean(): 0.05635535717010498 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([4.6687e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8195], device='cuda:0')), ('power', tensor([0.0743], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.0563521645963192
epoch£º709	 i:1 	 global-step:14181	 l-p:0.05614346265792847
epoch£º709	 i:2 	 global-step:14182	 l-p:0.05625257268548012
epoch£º709	 i:3 	 global-step:14183	 l-p:0.056264132261276245
epoch£º709	 i:4 	 global-step:14184	 l-p:0.05617232248187065
epoch£º709	 i:5 	 global-step:14185	 l-p:0.05703200027346611
epoch£º709	 i:6 	 global-step:14186	 l-p:0.056084081530570984
epoch£º709	 i:7 	 global-step:14187	 l-p:0.05819770693778992
epoch£º709	 i:8 	 global-step:14188	 l-p:0.05665634945034981
epoch£º709	 i:9 	 global-step:14189	 l-p:0.05604345723986626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0327, 28.1154, 28.0417],
        [28.0327, 30.0504, 29.6358],
        [28.0327, 29.4171, 28.9087],
        [28.0327, 28.1693, 28.0528]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.05617399141192436 
model_pd.l_d.mean(): 3.1300721730076475e-06 
model_pd.lagr.mean(): 0.05617712065577507 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8382], device='cuda:0')), ('power', tensor([0.0286], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.05617399141192436
epoch£º710	 i:1 	 global-step:14201	 l-p:0.05625179037451744
epoch£º710	 i:2 	 global-step:14202	 l-p:0.05694970861077309
epoch£º710	 i:3 	 global-step:14203	 l-p:0.05628814920783043
epoch£º710	 i:4 	 global-step:14204	 l-p:0.056414488703012466
epoch£º710	 i:5 	 global-step:14205	 l-p:0.05637678503990173
epoch£º710	 i:6 	 global-step:14206	 l-p:0.05602918565273285
epoch£º710	 i:7 	 global-step:14207	 l-p:0.05770605802536011
epoch£º710	 i:8 	 global-step:14208	 l-p:0.05611326918005943
epoch£º710	 i:9 	 global-step:14209	 l-p:0.056682784110307693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0936, 28.1448, 28.0978],
        [28.0936, 35.1144, 38.5096],
        [28.0936, 34.4407, 37.1265],
        [28.0936, 28.0936, 28.0936]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.05639973655343056 
model_pd.l_d.mean(): 6.665068212896585e-05 
model_pd.lagr.mean(): 0.05646638572216034 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7649], device='cuda:0')), ('power', tensor([0.3173], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.05639973655343056
epoch£º711	 i:1 	 global-step:14221	 l-p:0.05609652400016785
epoch£º711	 i:2 	 global-step:14222	 l-p:0.05620579421520233
epoch£º711	 i:3 	 global-step:14223	 l-p:0.05761849135160446
epoch£º711	 i:4 	 global-step:14224	 l-p:0.05656461417675018
epoch£º711	 i:5 	 global-step:14225	 l-p:0.05658692121505737
epoch£º711	 i:6 	 global-step:14226	 l-p:0.05696107819676399
epoch£º711	 i:7 	 global-step:14227	 l-p:0.05607249587774277
epoch£º711	 i:8 	 global-step:14228	 l-p:0.05603325366973877
epoch£º711	 i:9 	 global-step:14229	 l-p:0.05629240721464157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1327, 33.9002, 36.0070],
        [28.1327, 35.2941, 38.8366],
        [28.1327, 28.5530, 28.2572],
        [28.1327, 36.3357, 41.0680]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 0.05618048459291458 
model_pd.l_d.mean(): 0.00011087788880104199 
model_pd.lagr.mean(): 0.056291364133358 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8064], device='cuda:0')), ('power', tensor([0.3298], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:0.05618048459291458
epoch£º712	 i:1 	 global-step:14241	 l-p:0.05810881406068802
epoch£º712	 i:2 	 global-step:14242	 l-p:0.05612247437238693
epoch£º712	 i:3 	 global-step:14243	 l-p:0.05604804307222366
epoch£º712	 i:4 	 global-step:14244	 l-p:0.0564502514898777
epoch£º712	 i:5 	 global-step:14245	 l-p:0.05609234794974327
epoch£º712	 i:6 	 global-step:14246	 l-p:0.05642733350396156
epoch£º712	 i:7 	 global-step:14247	 l-p:0.05614001303911209
epoch£º712	 i:8 	 global-step:14248	 l-p:0.05609273165464401
epoch£º712	 i:9 	 global-step:14249	 l-p:0.05709076300263405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1390, 28.1713, 28.1410],
        [28.1390, 31.0838, 31.0316],
        [28.1390, 28.1390, 28.1390],
        [28.1390, 32.2175, 32.8814]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.056019075214862823 
model_pd.l_d.mean(): 2.433022018522024e-05 
model_pd.lagr.mean(): 0.05604340508580208 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8831], device='cuda:0')), ('power', tensor([0.0513], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.056019075214862823
epoch£º713	 i:1 	 global-step:14261	 l-p:0.0582427978515625
epoch£º713	 i:2 	 global-step:14262	 l-p:0.056234702467918396
epoch£º713	 i:3 	 global-step:14263	 l-p:0.05617019906640053
epoch£º713	 i:4 	 global-step:14264	 l-p:0.05610283091664314
epoch£º713	 i:5 	 global-step:14265	 l-p:0.05607583001255989
epoch£º713	 i:6 	 global-step:14266	 l-p:0.056675251573324203
epoch£º713	 i:7 	 global-step:14267	 l-p:0.05614979565143585
epoch£º713	 i:8 	 global-step:14268	 l-p:0.056159477680921555
epoch£º713	 i:9 	 global-step:14269	 l-p:0.05694935843348503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1106, 28.1106, 28.1106],
        [28.1106, 33.8734, 35.9784],
        [28.1106, 32.4678, 33.3453],
        [28.1106, 28.1294, 28.1115]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.05611397698521614 
model_pd.l_d.mean(): 9.098393638851121e-05 
model_pd.lagr.mean(): 0.056204959750175476 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8546], device='cuda:0')), ('power', tensor([0.1495], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.05611397698521614
epoch£º714	 i:1 	 global-step:14281	 l-p:0.05612500011920929
epoch£º714	 i:2 	 global-step:14282	 l-p:0.056325383484363556
epoch£º714	 i:3 	 global-step:14283	 l-p:0.05696043372154236
epoch£º714	 i:4 	 global-step:14284	 l-p:0.057672373950481415
epoch£º714	 i:5 	 global-step:14285	 l-p:0.05623983219265938
epoch£º714	 i:6 	 global-step:14286	 l-p:0.056169427931308746
epoch£º714	 i:7 	 global-step:14287	 l-p:0.056213196367025375
epoch£º714	 i:8 	 global-step:14288	 l-p:0.05705159157514572
epoch£º714	 i:9 	 global-step:14289	 l-p:0.056049153208732605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0467, 28.1027, 28.0515],
        [28.0467, 37.4468, 43.6741],
        [28.0467, 30.9369, 30.8622],
        [28.0467, 28.0467, 28.0467]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.05620504170656204 
model_pd.l_d.mean(): 8.629317017039284e-05 
model_pd.lagr.mean(): 0.056291334331035614 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8241], device='cuda:0')), ('power', tensor([0.1198], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.05620504170656204
epoch£º715	 i:1 	 global-step:14301	 l-p:0.05708242580294609
epoch£º715	 i:2 	 global-step:14302	 l-p:0.05615568161010742
epoch£º715	 i:3 	 global-step:14303	 l-p:0.05606069415807724
epoch£º715	 i:4 	 global-step:14304	 l-p:0.05613819882273674
epoch£º715	 i:5 	 global-step:14305	 l-p:0.05641677603125572
epoch£º715	 i:6 	 global-step:14306	 l-p:0.0561625175178051
epoch£º715	 i:7 	 global-step:14307	 l-p:0.056642796844244
epoch£º715	 i:8 	 global-step:14308	 l-p:0.05768561735749245
epoch£º715	 i:9 	 global-step:14309	 l-p:0.05662410706281662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9510, 33.1843, 34.8159],
        [27.9510, 27.9622, 27.9514],
        [27.9510, 27.9510, 27.9510],
        [27.9510, 37.1832, 43.2165]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.0562206506729126 
model_pd.l_d.mean(): 4.646516390494071e-05 
model_pd.lagr.mean(): 0.05626711621880531 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8114], device='cuda:0')), ('power', tensor([0.0585], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.0562206506729126
epoch£º716	 i:1 	 global-step:14321	 l-p:0.05699148401618004
epoch£º716	 i:2 	 global-step:14322	 l-p:0.057663701474666595
epoch£º716	 i:3 	 global-step:14323	 l-p:0.056195370852947235
epoch£º716	 i:4 	 global-step:14324	 l-p:0.05659566819667816
epoch£º716	 i:5 	 global-step:14325	 l-p:0.05631913244724274
epoch£º716	 i:6 	 global-step:14326	 l-p:0.05641768500208855
epoch£º716	 i:7 	 global-step:14327	 l-p:0.05612044036388397
epoch£º716	 i:8 	 global-step:14328	 l-p:0.056215398013591766
epoch£º716	 i:9 	 global-step:14329	 l-p:0.056737225502729416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8392, 28.4473, 28.0690],
        [27.8392, 28.6800, 28.2298],
        [27.8392, 27.8627, 27.8404],
        [27.8392, 35.2679, 39.1559]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.056102972477674484 
model_pd.l_d.mean(): -0.00017424646648578346 
model_pd.lagr.mean(): 0.05592872574925423 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8638], device='cuda:0')), ('power', tensor([-0.2135], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.056102972477674484
epoch£º717	 i:1 	 global-step:14341	 l-p:0.05724088475108147
epoch£º717	 i:2 	 global-step:14342	 l-p:0.05779397487640381
epoch£º717	 i:3 	 global-step:14343	 l-p:0.05614463984966278
epoch£º717	 i:4 	 global-step:14344	 l-p:0.056145355105400085
epoch£º717	 i:5 	 global-step:14345	 l-p:0.05618063360452652
epoch£º717	 i:6 	 global-step:14346	 l-p:0.0565950870513916
epoch£º717	 i:7 	 global-step:14347	 l-p:0.05636496841907501
epoch£º717	 i:8 	 global-step:14348	 l-p:0.05696902796626091
epoch£º717	 i:9 	 global-step:14349	 l-p:0.05631498619914055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[27.7161, 29.4925, 29.0340],
        [27.7161, 28.9415, 28.4403],
        [27.7161, 37.2455, 43.7119],
        [27.7161, 28.6409, 28.1739]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.05614684894680977 
model_pd.l_d.mean(): -0.0002657464356161654 
model_pd.lagr.mean(): 0.05588110163807869 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8659], device='cuda:0')), ('power', tensor([-0.3403], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.05614684894680977
epoch£º718	 i:1 	 global-step:14361	 l-p:0.056253816932439804
epoch£º718	 i:2 	 global-step:14362	 l-p:0.057250723242759705
epoch£º718	 i:3 	 global-step:14363	 l-p:0.05620212107896805
epoch£º718	 i:4 	 global-step:14364	 l-p:0.056246403604745865
epoch£º718	 i:5 	 global-step:14365	 l-p:0.05647175759077072
epoch£º718	 i:6 	 global-step:14366	 l-p:0.05638350546360016
epoch£º718	 i:7 	 global-step:14367	 l-p:0.05841471627354622
epoch£º718	 i:8 	 global-step:14368	 l-p:0.056286588311195374
epoch£º718	 i:9 	 global-step:14369	 l-p:0.05661876127123833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5955, 28.6202, 28.1383],
        [27.5955, 33.0386, 34.9050],
        [27.5955, 34.6777, 38.2191],
        [27.5955, 27.7016, 27.6091]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.0577613040804863 
model_pd.l_d.mean(): 8.200732554541901e-05 
model_pd.lagr.mean(): 0.05784331262111664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7067], device='cuda:0')), ('power', tensor([0.1198], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.0577613040804863
epoch£º719	 i:1 	 global-step:14381	 l-p:0.05639297515153885
epoch£º719	 i:2 	 global-step:14382	 l-p:0.05631482973694801
epoch£º719	 i:3 	 global-step:14383	 l-p:0.05621349811553955
epoch£º719	 i:4 	 global-step:14384	 l-p:0.05792476609349251
epoch£º719	 i:5 	 global-step:14385	 l-p:0.0563625693321228
epoch£º719	 i:6 	 global-step:14386	 l-p:0.056174714118242264
epoch£º719	 i:7 	 global-step:14387	 l-p:0.056351933628320694
epoch£º719	 i:8 	 global-step:14388	 l-p:0.05675847828388214
epoch£º719	 i:9 	 global-step:14389	 l-p:0.05635834112763405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4988, 27.8780, 27.6057],
        [27.4988, 29.2645, 28.8105],
        [27.4988, 31.6754, 32.4699],
        [27.4988, 29.9047, 29.6397]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.05640057474374771 
model_pd.l_d.mean(): -0.0002173388347728178 
model_pd.lagr.mean(): 0.056183237582445145 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8097], device='cuda:0')), ('power', tensor([-0.4068], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.05640057474374771
epoch£º720	 i:1 	 global-step:14401	 l-p:0.056586507707834244
epoch£º720	 i:2 	 global-step:14402	 l-p:0.05721334367990494
epoch£º720	 i:3 	 global-step:14403	 l-p:0.05804629996418953
epoch£º720	 i:4 	 global-step:14404	 l-p:0.056677378714084625
epoch£º720	 i:5 	 global-step:14405	 l-p:0.05635010451078415
epoch£º720	 i:6 	 global-step:14406	 l-p:0.0561639741063118
epoch£º720	 i:7 	 global-step:14407	 l-p:0.05631513521075249
epoch£º720	 i:8 	 global-step:14408	 l-p:0.056814081966876984
epoch£º720	 i:9 	 global-step:14409	 l-p:0.05632955580949783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[27.4333, 32.2128, 33.5032],
        [27.4333, 36.8557, 43.2456],
        [27.4333, 30.0250, 29.8414],
        [27.4333, 30.5702, 30.6665]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.05723797529935837 
model_pd.l_d.mean(): -9.693627362139523e-05 
model_pd.lagr.mean(): 0.05714103952050209 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7690], device='cuda:0')), ('power', tensor([-0.2828], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.05723797529935837
epoch£º721	 i:1 	 global-step:14421	 l-p:0.056364621967077255
epoch£º721	 i:2 	 global-step:14422	 l-p:0.05625079944729805
epoch£º721	 i:3 	 global-step:14423	 l-p:0.056598637253046036
epoch£º721	 i:4 	 global-step:14424	 l-p:0.05624301731586456
epoch£º721	 i:5 	 global-step:14425	 l-p:0.056697919964790344
epoch£º721	 i:6 	 global-step:14426	 l-p:0.056815240532159805
epoch£º721	 i:7 	 global-step:14427	 l-p:0.05620195344090462
epoch£º721	 i:8 	 global-step:14428	 l-p:0.05801647901535034
epoch£º721	 i:9 	 global-step:14429	 l-p:0.05663704872131348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4096, 27.4096, 27.4095],
        [27.4096, 27.4203, 27.4099],
        [27.4096, 27.4096, 27.4095],
        [27.4096, 27.6474, 27.4596]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.056411802768707275 
model_pd.l_d.mean(): -5.875201168237254e-05 
model_pd.lagr.mean(): 0.05635305121541023 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7743], device='cuda:0')), ('power', tensor([-0.4595], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.056411802768707275
epoch£º722	 i:1 	 global-step:14441	 l-p:0.05816473811864853
epoch£º722	 i:2 	 global-step:14442	 l-p:0.056212782859802246
epoch£º722	 i:3 	 global-step:14443	 l-p:0.05670333281159401
epoch£º722	 i:4 	 global-step:14444	 l-p:0.05725898966193199
epoch£º722	 i:5 	 global-step:14445	 l-p:0.05664393678307533
epoch£º722	 i:6 	 global-step:14446	 l-p:0.05681827291846275
epoch£º722	 i:7 	 global-step:14447	 l-p:0.05622252821922302
epoch£º722	 i:8 	 global-step:14448	 l-p:0.05632480978965759
epoch£º722	 i:9 	 global-step:14449	 l-p:0.056295257061719894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4443, 27.7863, 27.5347],
        [27.4443, 32.9313, 34.8574],
        [27.4443, 27.4450, 27.4443],
        [27.4443, 27.4443, 27.4442]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.05630360543727875 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05630360543727875 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8072], device='cuda:0')), ('power', tensor([-0.4885], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.05630360543727875
epoch£º723	 i:1 	 global-step:14461	 l-p:0.05703483521938324
epoch£º723	 i:2 	 global-step:14462	 l-p:0.05722660571336746
epoch£º723	 i:3 	 global-step:14463	 l-p:0.05668430030345917
epoch£º723	 i:4 	 global-step:14464	 l-p:0.05638046935200691
epoch£º723	 i:5 	 global-step:14465	 l-p:0.056296929717063904
epoch£º723	 i:6 	 global-step:14466	 l-p:0.05650864541530609
epoch£º723	 i:7 	 global-step:14467	 l-p:0.057917460799217224
epoch£º723	 i:8 	 global-step:14468	 l-p:0.056229036301374435
epoch£º723	 i:9 	 global-step:14469	 l-p:0.056298017501831055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5119, 27.5237, 27.5123],
        [27.5119, 33.3679, 35.6409],
        [27.5119, 27.5653, 27.5164],
        [27.5119, 29.5380, 29.1443]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.05682843178510666 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05682843178510666 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7748], device='cuda:0')), ('power', tensor([-0.2779], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.05682843178510666
epoch£º724	 i:1 	 global-step:14481	 l-p:0.05631064251065254
epoch£º724	 i:2 	 global-step:14482	 l-p:0.05641556531190872
epoch£º724	 i:3 	 global-step:14483	 l-p:0.05645390972495079
epoch£º724	 i:4 	 global-step:14484	 l-p:0.056684236973524094
epoch£º724	 i:5 	 global-step:14485	 l-p:0.056264087557792664
epoch£º724	 i:6 	 global-step:14486	 l-p:0.056161992251873016
epoch£º724	 i:7 	 global-step:14487	 l-p:0.05630260705947876
epoch£º724	 i:8 	 global-step:14488	 l-p:0.05810743197798729
epoch£º724	 i:9 	 global-step:14489	 l-p:0.057092078030109406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5920, 27.6104, 27.5929],
        [27.5920, 32.4006, 33.6990],
        [27.5920, 31.8116, 32.6305],
        [27.5920, 28.4274, 27.9808]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.05628512427210808 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05628512427210808 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8272], device='cuda:0')), ('power', tensor([-0.4208], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.05628512427210808
epoch£º725	 i:1 	 global-step:14501	 l-p:0.05732693523168564
epoch£º725	 i:2 	 global-step:14502	 l-p:0.05630822852253914
epoch£º725	 i:3 	 global-step:14503	 l-p:0.05619606748223305
epoch£º725	 i:4 	 global-step:14504	 l-p:0.056584034115076065
epoch£º725	 i:5 	 global-step:14505	 l-p:0.05623149871826172
epoch£º725	 i:6 	 global-step:14506	 l-p:0.05619422718882561
epoch£º725	 i:7 	 global-step:14507	 l-p:0.05786236748099327
epoch£º725	 i:8 	 global-step:14508	 l-p:0.05674436315894127
epoch£º725	 i:9 	 global-step:14509	 l-p:0.056616850197315216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[27.6808, 28.4584, 28.0258],
        [27.6808, 30.4245, 30.2980],
        [27.6808, 33.7609, 36.2337],
        [27.6808, 35.8395, 40.6032]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.05725756660103798 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05725756660103798 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.0601e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7320], device='cuda:0')), ('power', tensor([0.0412], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.05725756660103798
epoch£º726	 i:1 	 global-step:14521	 l-p:0.056327760219573975
epoch£º726	 i:2 	 global-step:14522	 l-p:0.0563405305147171
epoch£º726	 i:3 	 global-step:14523	 l-p:0.056148629635572433
epoch£º726	 i:4 	 global-step:14524	 l-p:0.056214943528175354
epoch£º726	 i:5 	 global-step:14525	 l-p:0.05660311505198479
epoch£º726	 i:6 	 global-step:14526	 l-p:0.05789349600672722
epoch£º726	 i:7 	 global-step:14527	 l-p:0.05646183714270592
epoch£º726	 i:8 	 global-step:14528	 l-p:0.05616075545549393
epoch£º726	 i:9 	 global-step:14529	 l-p:0.05665408819913864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7718, 27.7718, 27.7718],
        [27.7718, 35.4567, 39.6480],
        [27.7718, 37.3452, 43.8563],
        [27.7718, 27.7718, 27.7718]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.056211646646261215 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056211646646261215 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8259], device='cuda:0')), ('power', tensor([-0.1529], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.056211646646261215
epoch£º727	 i:1 	 global-step:14541	 l-p:0.05614614486694336
epoch£º727	 i:2 	 global-step:14542	 l-p:0.05614609643816948
epoch£º727	 i:3 	 global-step:14543	 l-p:0.056296076625585556
epoch£º727	 i:4 	 global-step:14544	 l-p:0.05626043304800987
epoch£º727	 i:5 	 global-step:14545	 l-p:0.0573960579931736
epoch£º727	 i:6 	 global-step:14546	 l-p:0.05680156499147415
epoch£º727	 i:7 	 global-step:14547	 l-p:0.05633212625980377
epoch£º727	 i:8 	 global-step:14548	 l-p:0.05774034187197685
epoch£º727	 i:9 	 global-step:14549	 l-p:0.05643381178379059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8599, 27.9217, 27.8656],
        [27.8599, 27.8632, 27.8599],
        [27.8599, 32.0943, 32.9001],
        [27.8599, 37.6691, 44.4689]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.05621038377285004 
model_pd.l_d.mean(): -1.5667145589759457e-06 
model_pd.lagr.mean(): 0.056208815425634384 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4587e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8207], device='cuda:0')), ('power', tensor([-0.0835], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.05621038377285004
epoch£º728	 i:1 	 global-step:14561	 l-p:0.05710579827427864
epoch£º728	 i:2 	 global-step:14562	 l-p:0.056430041790008545
epoch£º728	 i:3 	 global-step:14563	 l-p:0.056238822638988495
epoch£º728	 i:4 	 global-step:14564	 l-p:0.058166734874248505
epoch£º728	 i:5 	 global-step:14565	 l-p:0.056237418204545975
epoch£º728	 i:6 	 global-step:14566	 l-p:0.05670055374503136
epoch£º728	 i:7 	 global-step:14567	 l-p:0.056201815605163574
epoch£º728	 i:8 	 global-step:14568	 l-p:0.05613113194704056
epoch£º728	 i:9 	 global-step:14569	 l-p:0.056076355278491974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9495, 28.8674, 28.3992],
        [27.9495, 28.3590, 28.0694],
        [27.9495, 29.1780, 28.6726],
        [27.9495, 27.9495, 27.9495]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.05679967626929283 
model_pd.l_d.mean(): 1.3915322597313207e-05 
model_pd.lagr.mean(): 0.05681359022855759 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.7669e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7356], device='cuda:0')), ('power', tensor([0.3437], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.05679967626929283
epoch£º729	 i:1 	 global-step:14581	 l-p:0.05630328878760338
epoch£º729	 i:2 	 global-step:14582	 l-p:0.058900922536849976
epoch£º729	 i:3 	 global-step:14583	 l-p:0.05606883764266968
epoch£º729	 i:4 	 global-step:14584	 l-p:0.05627363920211792
epoch£º729	 i:5 	 global-step:14585	 l-p:0.056106775999069214
epoch£º729	 i:6 	 global-step:14586	 l-p:0.056151654571294785
epoch£º729	 i:7 	 global-step:14587	 l-p:0.05619208142161369
epoch£º729	 i:8 	 global-step:14588	 l-p:0.05627387389540672
epoch£º729	 i:9 	 global-step:14589	 l-p:0.056164149194955826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[28.0294, 33.7034, 35.7338],
        [28.0294, 36.2009, 40.9149],
        [28.0294, 32.3734, 33.2481],
        [28.0294, 32.5288, 33.5263]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.05709977075457573 
model_pd.l_d.mean(): 4.293841266189702e-05 
model_pd.lagr.mean(): 0.0571427084505558 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7674], device='cuda:0')), ('power', tensor([0.4108], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.05709977075457573
epoch£º730	 i:1 	 global-step:14601	 l-p:0.056103114038705826
epoch£º730	 i:2 	 global-step:14602	 l-p:0.05658935755491257
epoch£º730	 i:3 	 global-step:14603	 l-p:0.05609666556119919
epoch£º730	 i:4 	 global-step:14604	 l-p:0.057712361216545105
epoch£º730	 i:5 	 global-step:14605	 l-p:0.05629347637295723
epoch£º730	 i:6 	 global-step:14606	 l-p:0.05664611607789993
epoch£º730	 i:7 	 global-step:14607	 l-p:0.056098274886608124
epoch£º730	 i:8 	 global-step:14608	 l-p:0.05621739849448204
epoch£º730	 i:9 	 global-step:14609	 l-p:0.05614618957042694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0919, 37.1892, 43.0224],
        [28.0919, 29.4961, 28.9870],
        [28.0919, 30.9871, 30.9122],
        [28.0919, 28.1416, 28.0959]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.05620677024126053 
model_pd.l_d.mean(): 4.3949643441010267e-05 
model_pd.lagr.mean(): 0.05625072121620178 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8075], device='cuda:0')), ('power', tensor([0.2155], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.05620677024126053
epoch£º731	 i:1 	 global-step:14621	 l-p:0.056019868701696396
epoch£º731	 i:2 	 global-step:14622	 l-p:0.056163251399993896
epoch£º731	 i:3 	 global-step:14623	 l-p:0.05660831928253174
epoch£º731	 i:4 	 global-step:14624	 l-p:0.05757704749703407
epoch£º731	 i:5 	 global-step:14625	 l-p:0.056444477289915085
epoch£º731	 i:6 	 global-step:14626	 l-p:0.05635971575975418
epoch£º731	 i:7 	 global-step:14627	 l-p:0.05634445697069168
epoch£º731	 i:8 	 global-step:14628	 l-p:0.05708213523030281
epoch£º731	 i:9 	 global-step:14629	 l-p:0.05603070557117462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1306, 34.1143, 36.4310],
        [28.1306, 28.1408, 28.1309],
        [28.1306, 28.1305, 28.1305],
        [28.1306, 28.1467, 28.1312]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.056982677429914474 
model_pd.l_d.mean(): 0.00013710468192584813 
model_pd.lagr.mean(): 0.05711978301405907 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7850], device='cuda:0')), ('power', tensor([0.4171], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.056982677429914474
epoch£º732	 i:1 	 global-step:14641	 l-p:0.05638647824525833
epoch£º732	 i:2 	 global-step:14642	 l-p:0.05623077601194382
epoch£º732	 i:3 	 global-step:14643	 l-p:0.0563213974237442
epoch£º732	 i:4 	 global-step:14644	 l-p:0.057710837572813034
epoch£º732	 i:5 	 global-step:14645	 l-p:0.056196488440036774
epoch£º732	 i:6 	 global-step:14646	 l-p:0.05610813945531845
epoch£º732	 i:7 	 global-step:14647	 l-p:0.0561031810939312
epoch£º732	 i:8 	 global-step:14648	 l-p:0.05611714348196983
epoch£º732	 i:9 	 global-step:14649	 l-p:0.05659876763820648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1397, 28.1397, 28.1397],
        [28.1397, 28.1945, 28.1443],
        [28.1397, 28.5752, 28.2717],
        [28.1397, 28.3008, 28.1659]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.056611333042383194 
model_pd.l_d.mean(): 0.00016948036500252783 
model_pd.lagr.mean(): 0.05678081512451172 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8114], device='cuda:0')), ('power', tensor([0.3631], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.056611333042383194
epoch£º733	 i:1 	 global-step:14661	 l-p:0.05621416121721268
epoch£º733	 i:2 	 global-step:14662	 l-p:0.05619768053293228
epoch£º733	 i:3 	 global-step:14663	 l-p:0.056112345308065414
epoch£º733	 i:4 	 global-step:14664	 l-p:0.05627373605966568
epoch£º733	 i:5 	 global-step:14665	 l-p:0.056066062301397324
epoch£º733	 i:6 	 global-step:14666	 l-p:0.0560922846198082
epoch£º733	 i:7 	 global-step:14667	 l-p:0.05775251239538193
epoch£º733	 i:8 	 global-step:14668	 l-p:0.05704590305685997
epoch£º733	 i:9 	 global-step:14669	 l-p:0.056421875953674316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1086, 28.1138, 28.1087],
        [28.1086, 35.6126, 39.5402],
        [28.1086, 37.2375, 43.1071],
        [28.1086, 31.9582, 32.4607]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.056167952716350555 
model_pd.l_d.mean(): 9.881802543532103e-05 
model_pd.lagr.mean(): 0.056266769766807556 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8341], device='cuda:0')), ('power', tensor([0.1646], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.056167952716350555
epoch£º734	 i:1 	 global-step:14681	 l-p:0.05619541183114052
epoch£º734	 i:2 	 global-step:14682	 l-p:0.056065600365400314
epoch£º734	 i:3 	 global-step:14683	 l-p:0.05620630457997322
epoch£º734	 i:4 	 global-step:14684	 l-p:0.0561029426753521
epoch£º734	 i:5 	 global-step:14685	 l-p:0.05695632845163345
epoch£º734	 i:6 	 global-step:14686	 l-p:0.0563436821103096
epoch£º734	 i:7 	 global-step:14687	 l-p:0.05825662612915039
epoch£º734	 i:8 	 global-step:14688	 l-p:0.056000955402851105
epoch£º734	 i:9 	 global-step:14689	 l-p:0.05664059519767761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0434, 30.4997, 30.2294],
        [28.0434, 28.5127, 28.1930],
        [28.0434, 33.6563, 35.6272],
        [28.0434, 33.2949, 34.9322]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.056594330817461014 
model_pd.l_d.mean(): 0.00014365710376296192 
model_pd.lagr.mean(): 0.0567379891872406 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8251], device='cuda:0')), ('power', tensor([0.2022], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.056594330817461014
epoch£º735	 i:1 	 global-step:14701	 l-p:0.05610939860343933
epoch£º735	 i:2 	 global-step:14702	 l-p:0.056240204721689224
epoch£º735	 i:3 	 global-step:14703	 l-p:0.05634307861328125
epoch£º735	 i:4 	 global-step:14704	 l-p:0.05763712897896767
epoch£º735	 i:5 	 global-step:14705	 l-p:0.057130079716444016
epoch£º735	 i:6 	 global-step:14706	 l-p:0.05617860332131386
epoch£º735	 i:7 	 global-step:14707	 l-p:0.05648173391819
epoch£º735	 i:8 	 global-step:14708	 l-p:0.05609992891550064
epoch£º735	 i:9 	 global-step:14709	 l-p:0.05635422095656395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9493, 37.0957, 43.0207],
        [27.9493, 28.3052, 28.0446],
        [27.9493, 30.9214, 30.8946],
        [27.9493, 30.8289, 30.7544]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.056207120418548584 
model_pd.l_d.mean(): 5.1878469093935564e-05 
model_pd.lagr.mean(): 0.056258998811244965 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8320], device='cuda:0')), ('power', tensor([0.0663], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.056207120418548584
epoch£º736	 i:1 	 global-step:14721	 l-p:0.05609447509050369
epoch£º736	 i:2 	 global-step:14722	 l-p:0.05617063492536545
epoch£º736	 i:3 	 global-step:14723	 l-p:0.0574742890894413
epoch£º736	 i:4 	 global-step:14724	 l-p:0.05797765776515007
epoch£º736	 i:5 	 global-step:14725	 l-p:0.0561847910284996
epoch£º736	 i:6 	 global-step:14726	 l-p:0.05614732205867767
epoch£º736	 i:7 	 global-step:14727	 l-p:0.056203849613666534
epoch£º736	 i:8 	 global-step:14728	 l-p:0.05679093673825264
epoch£º736	 i:9 	 global-step:14729	 l-p:0.05625210702419281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8300, 27.9923, 27.8567],
        [27.8300, 37.5915, 44.3353],
        [27.8300, 33.2196, 35.0079],
        [27.8300, 37.1343, 43.2854]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.05617877468466759 
model_pd.l_d.mean(): -0.00016040279297158122 
model_pd.lagr.mean(): 0.056018371134996414 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8527], device='cuda:0')), ('power', tensor([-0.1999], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.05617877468466759
epoch£º737	 i:1 	 global-step:14741	 l-p:0.05655362084507942
epoch£º737	 i:2 	 global-step:14742	 l-p:0.056101273745298386
epoch£º737	 i:3 	 global-step:14743	 l-p:0.057115402072668076
epoch£º737	 i:4 	 global-step:14744	 l-p:0.056202009320259094
epoch£º737	 i:5 	 global-step:14745	 l-p:0.056983135640621185
epoch£º737	 i:6 	 global-step:14746	 l-p:0.05628519505262375
epoch£º737	 i:7 	 global-step:14747	 l-p:0.05803503468632698
epoch£º737	 i:8 	 global-step:14748	 l-p:0.056218311190605164
epoch£º737	 i:9 	 global-step:14749	 l-p:0.05624131113290787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6998, 27.7550, 27.7045],
        [27.6998, 27.7106, 27.7001],
        [27.6998, 27.7029, 27.6998],
        [27.6998, 27.6998, 27.6998]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.056321535259485245 
model_pd.l_d.mean(): -0.00014140781422611326 
model_pd.lagr.mean(): 0.05618012696504593 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7949], device='cuda:0')), ('power', tensor([-0.1860], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.056321535259485245
epoch£º738	 i:1 	 global-step:14761	 l-p:0.05638686195015907
epoch£º738	 i:2 	 global-step:14762	 l-p:0.05883682891726494
epoch£º738	 i:3 	 global-step:14763	 l-p:0.05670997127890587
epoch£º738	 i:4 	 global-step:14764	 l-p:0.056608591228723526
epoch£º738	 i:5 	 global-step:14765	 l-p:0.05641455948352814
epoch£º738	 i:6 	 global-step:14766	 l-p:0.056322164833545685
epoch£º738	 i:7 	 global-step:14767	 l-p:0.05613727122545242
epoch£º738	 i:8 	 global-step:14768	 l-p:0.0562397800385952
epoch£º738	 i:9 	 global-step:14769	 l-p:0.056318528950214386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5791, 27.5791, 27.5791],
        [27.5791, 27.5791, 27.5791],
        [27.5791, 33.3965, 35.6227],
        [27.5791, 28.4211, 27.9731]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.056820448487997055 
model_pd.l_d.mean(): -9.404077718500048e-05 
model_pd.lagr.mean(): 0.05672640725970268 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7669], device='cuda:0')), ('power', tensor([-0.1432], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.056820448487997055
epoch£º739	 i:1 	 global-step:14781	 l-p:0.05712559074163437
epoch£º739	 i:2 	 global-step:14782	 l-p:0.0562504343688488
epoch£º739	 i:3 	 global-step:14783	 l-p:0.05658748373389244
epoch£º739	 i:4 	 global-step:14784	 l-p:0.05663579702377319
epoch£º739	 i:5 	 global-step:14785	 l-p:0.056359853595495224
epoch£º739	 i:6 	 global-step:14786	 l-p:0.056434374302625656
epoch£º739	 i:7 	 global-step:14787	 l-p:0.05622915178537369
epoch£º739	 i:8 	 global-step:14788	 l-p:0.05796017497777939
epoch£º739	 i:9 	 global-step:14789	 l-p:0.05629173293709755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4753, 27.4752, 27.4753],
        [27.4753, 27.5252, 27.4793],
        [27.4753, 29.4129, 28.9973],
        [27.4753, 31.2170, 31.6956]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.05808412283658981 
model_pd.l_d.mean(): -7.148253644118086e-05 
model_pd.lagr.mean(): 0.058012641966342926 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7493], device='cuda:0')), ('power', tensor([-0.1440], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.05808412283658981
epoch£º740	 i:1 	 global-step:14801	 l-p:0.05625343695282936
epoch£º740	 i:2 	 global-step:14802	 l-p:0.05661098286509514
epoch£º740	 i:3 	 global-step:14803	 l-p:0.05644870549440384
epoch£º740	 i:4 	 global-step:14804	 l-p:0.05697070062160492
epoch£º740	 i:5 	 global-step:14805	 l-p:0.05621469020843506
epoch£º740	 i:6 	 global-step:14806	 l-p:0.05753972753882408
epoch£º740	 i:7 	 global-step:14807	 l-p:0.05626959726214409
epoch£º740	 i:8 	 global-step:14808	 l-p:0.05633269250392914
epoch£º740	 i:9 	 global-step:14809	 l-p:0.05623567849397659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4155, 31.9111, 32.9660],
        [27.4155, 36.8370, 43.2297],
        [27.4155, 32.0308, 33.1843],
        [27.4155, 27.4155, 27.4155]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.056358907371759415 
model_pd.l_d.mean(): -0.00013633367780130357 
model_pd.lagr.mean(): 0.056222572922706604 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7871], device='cuda:0')), ('power', tensor([-0.4627], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.056358907371759415
epoch£º741	 i:1 	 global-step:14821	 l-p:0.05643351376056671
epoch£º741	 i:2 	 global-step:14822	 l-p:0.0581328421831131
epoch£º741	 i:3 	 global-step:14823	 l-p:0.05622370168566704
epoch£º741	 i:4 	 global-step:14824	 l-p:0.05725523829460144
epoch£º741	 i:5 	 global-step:14825	 l-p:0.056333914399147034
epoch£º741	 i:6 	 global-step:14826	 l-p:0.056469280272722244
epoch£º741	 i:7 	 global-step:14827	 l-p:0.056261830031871796
epoch£º741	 i:8 	 global-step:14828	 l-p:0.05643957853317261
epoch£º741	 i:9 	 global-step:14829	 l-p:0.0571945495903492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4048, 36.8460, 43.2668],
        [27.4048, 28.1828, 27.7524],
        [27.4048, 27.4220, 27.4055],
        [27.4048, 29.7072, 29.4071]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.05653635784983635 
model_pd.l_d.mean(): -2.5598395950510167e-05 
model_pd.lagr.mean(): 0.05651075765490532 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.6266e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7350], device='cuda:0')), ('power', tensor([-0.3476], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.05653635784983635
epoch£º742	 i:1 	 global-step:14841	 l-p:0.05626358836889267
epoch£º742	 i:2 	 global-step:14842	 l-p:0.0564359612762928
epoch£º742	 i:3 	 global-step:14843	 l-p:0.05841434374451637
epoch£º742	 i:4 	 global-step:14844	 l-p:0.05624251812696457
epoch£º742	 i:5 	 global-step:14845	 l-p:0.05758962035179138
epoch£º742	 i:6 	 global-step:14846	 l-p:0.05641377717256546
epoch£º742	 i:7 	 global-step:14847	 l-p:0.056232236325740814
epoch£º742	 i:8 	 global-step:14848	 l-p:0.05620748549699783
epoch£º742	 i:9 	 global-step:14849	 l-p:0.05671801418066025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4522, 27.4523, 27.4521],
        [27.4522, 27.7888, 27.5403],
        [27.4522, 33.4796, 35.9308],
        [27.4522, 31.8542, 32.8298]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.056562066078186035 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056562066078186035 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7533], device='cuda:0')), ('power', tensor([-0.3329], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.056562066078186035
epoch£º743	 i:1 	 global-step:14861	 l-p:0.05656508356332779
epoch£º743	 i:2 	 global-step:14862	 l-p:0.05631319805979729
epoch£º743	 i:3 	 global-step:14863	 l-p:0.05626356974244118
epoch£º743	 i:4 	 global-step:14864	 l-p:0.058109767735004425
epoch£º743	 i:5 	 global-step:14865	 l-p:0.056767139583826065
epoch£º743	 i:6 	 global-step:14866	 l-p:0.056174058467149734
epoch£º743	 i:7 	 global-step:14867	 l-p:0.05635968968272209
epoch£º743	 i:8 	 global-step:14868	 l-p:0.057292331010103226
epoch£º743	 i:9 	 global-step:14869	 l-p:0.05642843618988991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5313, 27.9976, 27.6811],
        [27.5313, 28.3865, 27.9359],
        [27.5313, 27.9880, 27.6761],
        [27.5313, 27.7168, 27.5647]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.05617624148726463 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05617624148726463 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8591], device='cuda:0')), ('power', tensor([-0.5613], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.05617624148726463
epoch£º744	 i:1 	 global-step:14881	 l-p:0.056656427681446075
epoch£º744	 i:2 	 global-step:14882	 l-p:0.056315407156944275
epoch£º744	 i:3 	 global-step:14883	 l-p:0.05690033733844757
epoch£º744	 i:4 	 global-step:14884	 l-p:0.056273944675922394
epoch£º744	 i:5 	 global-step:14885	 l-p:0.05649794638156891
epoch£º744	 i:6 	 global-step:14886	 l-p:0.05617065355181694
epoch£º744	 i:7 	 global-step:14887	 l-p:0.05628950148820877
epoch£º744	 i:8 	 global-step:14888	 l-p:0.056322887539863586
epoch£º744	 i:9 	 global-step:14889	 l-p:0.05893617495894432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6186, 27.7765, 27.6442],
        [27.6186, 27.6188, 27.6186],
        [27.6186, 37.3026, 43.9926],
        [27.6186, 32.5411, 33.9348]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.05671761557459831 
model_pd.l_d.mean(): -1.26165184610727e-06 
model_pd.lagr.mean(): 0.056716352701187134 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.1640e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7567], device='cuda:0')), ('power', tensor([-0.1154], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.05671761557459831
epoch£º745	 i:1 	 global-step:14901	 l-p:0.05633733049035072
epoch£º745	 i:2 	 global-step:14902	 l-p:0.05619261786341667
epoch£º745	 i:3 	 global-step:14903	 l-p:0.05625244975090027
epoch£º745	 i:4 	 global-step:14904	 l-p:0.05629674345254898
epoch£º745	 i:5 	 global-step:14905	 l-p:0.058531925082206726
epoch£º745	 i:6 	 global-step:14906	 l-p:0.05718604847788811
epoch£º745	 i:7 	 global-step:14907	 l-p:0.056431327015161514
epoch£º745	 i:8 	 global-step:14908	 l-p:0.05612851679325104
epoch£º745	 i:9 	 global-step:14909	 l-p:0.056180886924266815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7167, 27.7453, 27.7183],
        [27.7167, 27.7592, 27.7198],
        [27.7167, 34.6388, 37.9860],
        [27.7167, 27.7167, 27.7167]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.05790110304951668 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05790110304951668 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.1549e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7568], device='cuda:0')), ('power', tensor([0.1031], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.05790110304951668
epoch£º746	 i:1 	 global-step:14921	 l-p:0.05614429712295532
epoch£º746	 i:2 	 global-step:14922	 l-p:0.05714498087763786
epoch£º746	 i:3 	 global-step:14923	 l-p:0.05617256090044975
epoch£º746	 i:4 	 global-step:14924	 l-p:0.056194331496953964
epoch£º746	 i:5 	 global-step:14925	 l-p:0.05616244301199913
epoch£º746	 i:6 	 global-step:14926	 l-p:0.056441083550453186
epoch£º746	 i:7 	 global-step:14927	 l-p:0.056415729224681854
epoch£º746	 i:8 	 global-step:14928	 l-p:0.05673582851886749
epoch£º746	 i:9 	 global-step:14929	 l-p:0.056633882224559784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8161, 28.0888, 27.8780],
        [27.8161, 28.0037, 27.8498],
        [27.8161, 34.3264, 37.2203],
        [27.8161, 34.5113, 37.5996]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.056567076593637466 
model_pd.l_d.mean(): 7.607471275150601e-08 
model_pd.lagr.mean(): 0.056567151099443436 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.9015e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7965], device='cuda:0')), ('power', tensor([0.0399], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.056567076593637466
epoch£º747	 i:1 	 global-step:14941	 l-p:0.05622727423906326
epoch£º747	 i:2 	 global-step:14942	 l-p:0.05622488260269165
epoch£º747	 i:3 	 global-step:14943	 l-p:0.057026833295822144
epoch£º747	 i:4 	 global-step:14944	 l-p:0.056098926812410355
epoch£º747	 i:5 	 global-step:14945	 l-p:0.05780509486794472
epoch£º747	 i:6 	 global-step:14946	 l-p:0.0562576949596405
epoch£º747	 i:7 	 global-step:14947	 l-p:0.056818075478076935
epoch£º747	 i:8 	 global-step:14948	 l-p:0.056447628885507584
epoch£º747	 i:9 	 global-step:14949	 l-p:0.05615173280239105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9119, 29.4273, 28.9284],
        [27.9119, 37.5358, 44.0814],
        [27.9119, 28.5635, 28.1688],
        [27.9119, 29.6736, 29.2066]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.05612723156809807 
model_pd.l_d.mean(): -1.806489876798878e-06 
model_pd.lagr.mean(): 0.056125424802303314 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.4619e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8565], device='cuda:0')), ('power', tensor([-0.0936], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.05612723156809807
epoch£º748	 i:1 	 global-step:14961	 l-p:0.05631469935178757
epoch£º748	 i:2 	 global-step:14962	 l-p:0.05664296820759773
epoch£º748	 i:3 	 global-step:14963	 l-p:0.05607136711478233
epoch£º748	 i:4 	 global-step:14964	 l-p:0.057958200573921204
epoch£º748	 i:5 	 global-step:14965	 l-p:0.056517019867897034
epoch£º748	 i:6 	 global-step:14966	 l-p:0.056284502148628235
epoch£º748	 i:7 	 global-step:14967	 l-p:0.05626387521624565
epoch£º748	 i:8 	 global-step:14968	 l-p:0.056188326328992844
epoch£º748	 i:9 	 global-step:14969	 l-p:0.05695632845163345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0037, 37.3888, 43.6061],
        [28.0037, 28.1114, 28.0174],
        [28.0037, 28.1326, 28.0220],
        [28.0037, 31.5477, 31.8532]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.056289251893758774 
model_pd.l_d.mean(): 1.15296070362092e-05 
model_pd.lagr.mean(): 0.05630078166723251 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([7.5447e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7847], device='cuda:0')), ('power', tensor([0.1725], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.056289251893758774
epoch£º749	 i:1 	 global-step:14981	 l-p:0.05608557537198067
epoch£º749	 i:2 	 global-step:14982	 l-p:0.05622594803571701
epoch£º749	 i:3 	 global-step:14983	 l-p:0.05618928372859955
epoch£º749	 i:4 	 global-step:14984	 l-p:0.05605366453528404
epoch£º749	 i:5 	 global-step:14985	 l-p:0.058380596339702606
epoch£º749	 i:6 	 global-step:14986	 l-p:0.056175459176301956
epoch£º749	 i:7 	 global-step:14987	 l-p:0.056043121963739395
epoch£º749	 i:8 	 global-step:14988	 l-p:0.05654125660657883
epoch£º749	 i:9 	 global-step:14989	 l-p:0.05706443265080452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0833, 33.7700, 35.8057],
        [28.0833, 37.2037, 43.0678],
        [28.0833, 28.1024, 28.0842],
        [28.0833, 28.1070, 28.0845]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.056173209100961685 
model_pd.l_d.mean(): 2.262434827571269e-05 
model_pd.lagr.mean(): 0.056195832788944244 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8240], device='cuda:0')), ('power', tensor([0.1441], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.056173209100961685
epoch£º750	 i:1 	 global-step:15001	 l-p:0.056644123047590256
epoch£º750	 i:2 	 global-step:15002	 l-p:0.05612393841147423
epoch£º750	 i:3 	 global-step:15003	 l-p:0.05641280114650726
epoch£º750	 i:4 	 global-step:15004	 l-p:0.05619274452328682
epoch£º750	 i:5 	 global-step:15005	 l-p:0.057002708315849304
epoch£º750	 i:6 	 global-step:15006	 l-p:0.05645813047885895
epoch£º750	 i:7 	 global-step:15007	 l-p:0.05758102610707283
epoch£º750	 i:8 	 global-step:15008	 l-p:0.056125298142433167
epoch£º750	 i:9 	 global-step:15009	 l-p:0.05611836165189743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1412, 28.8219, 28.4156],
        [28.1412, 34.0831, 36.3575],
        [28.1412, 28.1412, 28.1412],
        [28.1412, 29.3867, 28.8773]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.05605936795473099 
model_pd.l_d.mean(): 5.2391475037438795e-05 
model_pd.lagr.mean(): 0.056111760437488556 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8541], device='cuda:0')), ('power', tensor([0.1857], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.05605936795473099
epoch£º751	 i:1 	 global-step:15021	 l-p:0.0560164637863636
epoch£º751	 i:2 	 global-step:15022	 l-p:0.05618305131793022
epoch£º751	 i:3 	 global-step:15023	 l-p:0.05615472048521042
epoch£º751	 i:4 	 global-step:15024	 l-p:0.056638021022081375
epoch£º751	 i:5 	 global-step:15025	 l-p:0.05727958679199219
epoch£º751	 i:6 	 global-step:15026	 l-p:0.05751081928610802
epoch£º751	 i:7 	 global-step:15027	 l-p:0.0561506487429142
epoch£º751	 i:8 	 global-step:15028	 l-p:0.05638286843895912
epoch£º751	 i:9 	 global-step:15029	 l-p:0.05632685124874115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1653, 28.1653, 28.1653],
        [28.1653, 28.3677, 28.2031],
        [28.1653, 28.9662, 28.5232],
        [28.1653, 28.1841, 28.1661]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.05672452971339226 
model_pd.l_d.mean(): 0.00019855181744787842 
model_pd.lagr.mean(): 0.05692308023571968 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7685], device='cuda:0')), ('power', tensor([0.4636], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.05672452971339226
epoch£º752	 i:1 	 global-step:15041	 l-p:0.05623587965965271
epoch£º752	 i:2 	 global-step:15042	 l-p:0.05695502087473869
epoch£º752	 i:3 	 global-step:15043	 l-p:0.0561499297618866
epoch£º752	 i:4 	 global-step:15044	 l-p:0.05610346794128418
epoch£º752	 i:5 	 global-step:15045	 l-p:0.05603973567485809
epoch£º752	 i:6 	 global-step:15046	 l-p:0.057589080184698105
epoch£º752	 i:7 	 global-step:15047	 l-p:0.056368548423051834
epoch£º752	 i:8 	 global-step:15048	 l-p:0.05597744882106781
epoch£º752	 i:9 	 global-step:15049	 l-p:0.05653967708349228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1479, 28.1562, 28.1481],
        [28.1479, 28.1481, 28.1479],
        [28.1479, 28.9986, 28.5432],
        [28.1479, 30.1724, 29.7555]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.05609150230884552 
model_pd.l_d.mean(): 0.00012418549158610404 
model_pd.lagr.mean(): 0.056215688586235046 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8516], device='cuda:0')), ('power', tensor([0.2149], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.05609150230884552
epoch£º753	 i:1 	 global-step:15061	 l-p:0.05599667876958847
epoch£º753	 i:2 	 global-step:15062	 l-p:0.05638211965560913
epoch£º753	 i:3 	 global-step:15063	 l-p:0.05614288151264191
epoch£º753	 i:4 	 global-step:15064	 l-p:0.056047745048999786
epoch£º753	 i:5 	 global-step:15065	 l-p:0.0575089305639267
epoch£º753	 i:6 	 global-step:15066	 l-p:0.05773157998919487
epoch£º753	 i:7 	 global-step:15067	 l-p:0.05613714084029198
epoch£º753	 i:8 	 global-step:15068	 l-p:0.05608321726322174
epoch£º753	 i:9 	 global-step:15069	 l-p:0.05668570473790169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0852, 28.1089, 28.0864],
        [28.0852, 28.1029, 28.0860],
        [28.0852, 35.5476, 39.4323],
        [28.0852, 28.0858, 28.0852]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.05615582317113876 
model_pd.l_d.mean(): 8.403506217291579e-05 
model_pd.lagr.mean(): 0.05623985826969147 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8408], device='cuda:0')), ('power', tensor([0.1186], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.05615582317113876
epoch£º754	 i:1 	 global-step:15081	 l-p:0.05620621517300606
epoch£º754	 i:2 	 global-step:15082	 l-p:0.057620175182819366
epoch£º754	 i:3 	 global-step:15083	 l-p:0.05611424148082733
epoch£º754	 i:4 	 global-step:15084	 l-p:0.05604429915547371
epoch£º754	 i:5 	 global-step:15085	 l-p:0.056636255234479904
epoch£º754	 i:6 	 global-step:15086	 l-p:0.056706368923187256
epoch£º754	 i:7 	 global-step:15087	 l-p:0.05630185827612877
epoch£º754	 i:8 	 global-step:15088	 l-p:0.05620403587818146
epoch£º754	 i:9 	 global-step:15089	 l-p:0.057058464735746384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9844, 29.1151, 28.6158],
        [27.9844, 28.1909, 28.0236],
        [27.9844, 33.5850, 35.5514],
        [27.9844, 30.2181, 29.8696]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.05631525069475174 
model_pd.l_d.mean(): 8.692021219758317e-05 
model_pd.lagr.mean(): 0.05640216916799545 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7947], device='cuda:0')), ('power', tensor([0.1087], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.05631525069475174
epoch£º755	 i:1 	 global-step:15101	 l-p:0.05700794979929924
epoch£º755	 i:2 	 global-step:15102	 l-p:0.05610613524913788
epoch£º755	 i:3 	 global-step:15103	 l-p:0.056295089423656464
epoch£º755	 i:4 	 global-step:15104	 l-p:0.05697318911552429
epoch£º755	 i:5 	 global-step:15105	 l-p:0.05639788508415222
epoch£º755	 i:6 	 global-step:15106	 l-p:0.0560881681740284
epoch£º755	 i:7 	 global-step:15107	 l-p:0.05627378448843956
epoch£º755	 i:8 	 global-step:15108	 l-p:0.057896897196769714
epoch£º755	 i:9 	 global-step:15109	 l-p:0.05606692284345627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8508, 33.1131, 34.7827],
        [27.8508, 27.8508, 27.8508],
        [27.8508, 31.1256, 31.2749],
        [27.8508, 30.1810, 29.8716]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.05619267746806145 
model_pd.l_d.mean(): -0.00011193251702934504 
model_pd.lagr.mean(): 0.05608074367046356 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8469], device='cuda:0')), ('power', tensor([-0.1342], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.05619267746806145
epoch£º756	 i:1 	 global-step:15121	 l-p:0.05626100301742554
epoch£º756	 i:2 	 global-step:15122	 l-p:0.056754834949970245
epoch£º756	 i:3 	 global-step:15123	 l-p:0.056443747133016586
epoch£º756	 i:4 	 global-step:15124	 l-p:0.05619562044739723
epoch£º756	 i:5 	 global-step:15125	 l-p:0.0561087466776371
epoch£º756	 i:6 	 global-step:15126	 l-p:0.0583055354654789
epoch£º756	 i:7 	 global-step:15127	 l-p:0.05716000869870186
epoch£º756	 i:8 	 global-step:15128	 l-p:0.05629336088895798
epoch£º756	 i:9 	 global-step:15129	 l-p:0.056157760322093964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7039, 31.9134, 32.7144],
        [27.7039, 33.2455, 35.1911],
        [27.7039, 28.5404, 28.0925],
        [27.7039, 27.7039, 27.7039]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.058282189071178436 
model_pd.l_d.mean(): 0.00012708624126389623 
model_pd.lagr.mean(): 0.058409273624420166 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7297], device='cuda:0')), ('power', tensor([0.1591], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.058282189071178436
epoch£º757	 i:1 	 global-step:15141	 l-p:0.056146230548620224
epoch£º757	 i:2 	 global-step:15142	 l-p:0.05781986936926842
epoch£º757	 i:3 	 global-step:15143	 l-p:0.05629778653383255
epoch£º757	 i:4 	 global-step:15144	 l-p:0.056314315646886826
epoch£º757	 i:5 	 global-step:15145	 l-p:0.056306470185518265
epoch£º757	 i:6 	 global-step:15146	 l-p:0.05627872422337532
epoch£º757	 i:7 	 global-step:15147	 l-p:0.056276820600032806
epoch£º757	 i:8 	 global-step:15148	 l-p:0.05616532638669014
epoch£º757	 i:9 	 global-step:15149	 l-p:0.056402601301670074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5653, 29.0030, 28.5065],
        [27.5653, 28.6781, 28.1866],
        [27.5653, 27.9390, 27.6695],
        [27.5653, 29.9774, 29.7118]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.056198619306087494 
model_pd.l_d.mean(): -0.0003191875293850899 
model_pd.lagr.mean(): 0.05587943270802498 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8439], device='cuda:0')), ('power', tensor([-0.4603], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.056198619306087494
epoch£º758	 i:1 	 global-step:15161	 l-p:0.057740628719329834
epoch£º758	 i:2 	 global-step:15162	 l-p:0.056301698088645935
epoch£º758	 i:3 	 global-step:15163	 l-p:0.0564061775803566
epoch£º758	 i:4 	 global-step:15164	 l-p:0.056261222809553146
epoch£º758	 i:5 	 global-step:15165	 l-p:0.0564083531498909
epoch£º758	 i:6 	 global-step:15166	 l-p:0.058003731071949005
epoch£º758	 i:7 	 global-step:15167	 l-p:0.05635407939553261
epoch£º758	 i:8 	 global-step:15168	 l-p:0.05615878105163574
epoch£º758	 i:9 	 global-step:15169	 l-p:0.05693185329437256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4448, 27.4450, 27.4448],
        [27.4448, 27.4448, 27.4448],
        [27.4448, 32.7561, 34.5182],
        [27.4448, 27.4448, 27.4448]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.05685954913496971 
model_pd.l_d.mean(): -0.00020460449741221964 
model_pd.lagr.mean(): 0.05665494501590729 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7823], device='cuda:0')), ('power', tensor([-0.3917], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.05685954913496971
epoch£º759	 i:1 	 global-step:15181	 l-p:0.05672885850071907
epoch£º759	 i:2 	 global-step:15182	 l-p:0.05637289956212044
epoch£º759	 i:3 	 global-step:15183	 l-p:0.057224687188863754
epoch£º759	 i:4 	 global-step:15184	 l-p:0.05632006376981735
epoch£º759	 i:5 	 global-step:15185	 l-p:0.05623241141438484
epoch£º759	 i:6 	 global-step:15186	 l-p:0.05639379471540451
epoch£º759	 i:7 	 global-step:15187	 l-p:0.056222908198833466
epoch£º759	 i:8 	 global-step:15188	 l-p:0.05666082352399826
epoch£º759	 i:9 	 global-step:15189	 l-p:0.058094386011362076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3681, 27.5208, 27.3925],
        [27.3681, 28.7897, 28.2967],
        [27.3681, 30.5200, 30.6292],
        [27.3681, 27.7279, 27.4665]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.05737030878663063 
model_pd.l_d.mean(): -7.03456680639647e-05 
model_pd.lagr.mean(): 0.057299964129924774 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7277], device='cuda:0')), ('power', tensor([-0.2333], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.05737030878663063
epoch£º760	 i:1 	 global-step:15201	 l-p:0.058592796325683594
epoch£º760	 i:2 	 global-step:15202	 l-p:0.05689223110675812
epoch£º760	 i:3 	 global-step:15203	 l-p:0.056484173983335495
epoch£º760	 i:4 	 global-step:15204	 l-p:0.056271955370903015
epoch£º760	 i:5 	 global-step:15205	 l-p:0.056329838931560516
epoch£º760	 i:6 	 global-step:15206	 l-p:0.056224048137664795
epoch£º760	 i:7 	 global-step:15207	 l-p:0.0564027838408947
epoch£º760	 i:8 	 global-step:15208	 l-p:0.05638357996940613
epoch£º760	 i:9 	 global-step:15209	 l-p:0.0563017837703228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3587, 27.4323, 27.3663],
        [27.3587, 31.5409, 32.3524],
        [27.3587, 29.1127, 28.6606],
        [27.3587, 33.3716, 35.8209]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.058019500225782394 
model_pd.l_d.mean(): -1.7223640043084743e-06 
model_pd.lagr.mean(): 0.058017779141664505 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.6724e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6891], device='cuda:0')), ('power', tensor([-0.0296], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.058019500225782394
epoch£º761	 i:1 	 global-step:15221	 l-p:0.05627173185348511
epoch£º761	 i:2 	 global-step:15222	 l-p:0.0563364177942276
epoch£º761	 i:3 	 global-step:15223	 l-p:0.056232716888189316
epoch£º761	 i:4 	 global-step:15224	 l-p:0.05626247450709343
epoch£º761	 i:5 	 global-step:15225	 l-p:0.05817627161741257
epoch£º761	 i:6 	 global-step:15226	 l-p:0.05643147975206375
epoch£º761	 i:7 	 global-step:15227	 l-p:0.056690700352191925
epoch£º761	 i:8 	 global-step:15228	 l-p:0.056376054883003235
epoch£º761	 i:9 	 global-step:15229	 l-p:0.05641113221645355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4069, 28.9404, 28.4552],
        [27.4069, 29.1349, 28.6768],
        [27.4069, 31.0879, 31.5301],
        [27.4069, 27.4069, 27.4069]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.05626821890473366 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05626821890473366 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8312], device='cuda:0')), ('power', tensor([-0.5659], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.05626821890473366
epoch£º762	 i:1 	 global-step:15241	 l-p:0.05630752071738243
epoch£º762	 i:2 	 global-step:15242	 l-p:0.056276459246873856
epoch£º762	 i:3 	 global-step:15243	 l-p:0.056386299431324005
epoch£º762	 i:4 	 global-step:15244	 l-p:0.056295160204172134
epoch£º762	 i:5 	 global-step:15245	 l-p:0.05815364792943001
epoch£º762	 i:6 	 global-step:15246	 l-p:0.05646631121635437
epoch£º762	 i:7 	 global-step:15247	 l-p:0.05664772912859917
epoch£º762	 i:8 	 global-step:15248	 l-p:0.056437939405441284
epoch£º762	 i:9 	 global-step:15249	 l-p:0.05773557350039482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4883, 30.2119, 30.0863],
        [27.4883, 27.5107, 27.4894],
        [27.4883, 27.6855, 27.5252],
        [27.4883, 35.6573, 40.4693]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.057289429008960724 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.057289429008960724 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7544], device='cuda:0')), ('power', tensor([-0.2156], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.057289429008960724
epoch£º763	 i:1 	 global-step:15261	 l-p:0.05671147257089615
epoch£º763	 i:2 	 global-step:15262	 l-p:0.056469619274139404
epoch£º763	 i:3 	 global-step:15263	 l-p:0.05619286373257637
epoch£º763	 i:4 	 global-step:15264	 l-p:0.056231964379549026
epoch£º763	 i:5 	 global-step:15265	 l-p:0.05696440488100052
epoch£º763	 i:6 	 global-step:15266	 l-p:0.05623413994908333
epoch£º763	 i:7 	 global-step:15267	 l-p:0.05793064460158348
epoch£º763	 i:8 	 global-step:15268	 l-p:0.056222256273031235
epoch£º763	 i:9 	 global-step:15269	 l-p:0.05643394961953163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5875, 27.9877, 27.7040],
        [27.5875, 27.6030, 27.5881],
        [27.5875, 31.3453, 31.8261],
        [27.5875, 29.3553, 28.8990]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.05617495998740196 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05617495998740196 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8435], device='cuda:0')), ('power', tensor([-0.4128], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.05617495998740196
epoch£º764	 i:1 	 global-step:15281	 l-p:0.05814696103334427
epoch£º764	 i:2 	 global-step:15282	 l-p:0.056162141263484955
epoch£º764	 i:3 	 global-step:15283	 l-p:0.05627036094665527
epoch£º764	 i:4 	 global-step:15284	 l-p:0.056244246661663055
epoch£º764	 i:5 	 global-step:15285	 l-p:0.057148370891809464
epoch£º764	 i:6 	 global-step:15286	 l-p:0.056787848472595215
epoch£º764	 i:7 	 global-step:15287	 l-p:0.05616721138358116
epoch£º764	 i:8 	 global-step:15288	 l-p:0.05676761642098427
epoch£º764	 i:9 	 global-step:15289	 l-p:0.05648448318243027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.6922, 28.2912, 27.9171],
        [27.6922, 27.7176, 27.6936],
        [27.6922, 27.6927, 27.6922],
        [27.6922, 28.3968, 27.9857]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.058336351066827774 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058336351066827774 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([1.1258e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7174], device='cuda:0')), ('power', tensor([0.2252], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.058336351066827774
epoch£º765	 i:1 	 global-step:15301	 l-p:0.0564146488904953
epoch£º765	 i:2 	 global-step:15302	 l-p:0.0561942383646965
epoch£º765	 i:3 	 global-step:15303	 l-p:0.05625978484749794
epoch£º765	 i:4 	 global-step:15304	 l-p:0.05637360364198685
epoch£º765	 i:5 	 global-step:15305	 l-p:0.056185606867074966
epoch£º765	 i:6 	 global-step:15306	 l-p:0.05610199272632599
epoch£º765	 i:7 	 global-step:15307	 l-p:0.05635117366909981
epoch£º765	 i:8 	 global-step:15308	 l-p:0.05698574706912041
epoch£º765	 i:9 	 global-step:15309	 l-p:0.05681239813566208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7965, 36.9669, 42.9544],
        [27.7965, 30.4528, 30.2794],
        [27.7965, 28.0315, 27.8451],
        [27.7965, 35.2134, 39.0951]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.05619663745164871 
model_pd.l_d.mean(): -6.676615953438159e-07 
model_pd.lagr.mean(): 0.05619597062468529 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8271], device='cuda:0')), ('power', tensor([-0.1574], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.05619663745164871
epoch£º766	 i:1 	 global-step:15321	 l-p:0.05643853545188904
epoch£º766	 i:2 	 global-step:15322	 l-p:0.05616883561015129
epoch£º766	 i:3 	 global-step:15323	 l-p:0.057237815111875534
epoch£º766	 i:4 	 global-step:15324	 l-p:0.057760752737522125
epoch£º766	 i:5 	 global-step:15325	 l-p:0.0560854896903038
epoch£º766	 i:6 	 global-step:15326	 l-p:0.05650528892874718
epoch£º766	 i:7 	 global-step:15327	 l-p:0.05627484992146492
epoch£º766	 i:8 	 global-step:15328	 l-p:0.056150149554014206
epoch£º766	 i:9 	 global-step:15329	 l-p:0.05686011165380478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8989, 28.2242, 27.9813],
        [27.8989, 27.8993, 27.8989],
        [27.8989, 27.8990, 27.8989],
        [27.8989, 36.3918, 41.5163]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.056095872074365616 
model_pd.l_d.mean(): -1.5630213283657213e-06 
model_pd.lagr.mean(): 0.05609430745244026 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([3.5431e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8627], device='cuda:0')), ('power', tensor([-0.1449], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.056095872074365616
epoch£º767	 i:1 	 global-step:15341	 l-p:0.05636847764253616
epoch£º767	 i:2 	 global-step:15342	 l-p:0.05679018795490265
epoch£º767	 i:3 	 global-step:15343	 l-p:0.056070517748594284
epoch£º767	 i:4 	 global-step:15344	 l-p:0.05762747675180435
epoch£º767	 i:5 	 global-step:15345	 l-p:0.05608035996556282
epoch£º767	 i:6 	 global-step:15346	 l-p:0.05695858970284462
epoch£º767	 i:7 	 global-step:15347	 l-p:0.05686377361416817
epoch£º767	 i:8 	 global-step:15348	 l-p:0.056313712149858475
epoch£º767	 i:9 	 global-step:15349	 l-p:0.05618652328848839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9997, 30.7484, 30.6070],
        [27.9997, 38.2494, 45.6026],
        [27.9997, 28.2366, 28.0487],
        [27.9997, 33.9103, 36.1726]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.056958477944135666 
model_pd.l_d.mean(): 1.6029965991037898e-05 
model_pd.lagr.mean(): 0.05697450786828995 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([6.8673e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8034], device='cuda:0')), ('power', tensor([0.2981], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.056958477944135666
epoch£º768	 i:1 	 global-step:15361	 l-p:0.05619734525680542
epoch£º768	 i:2 	 global-step:15362	 l-p:0.05708447843790054
epoch£º768	 i:3 	 global-step:15363	 l-p:0.05611047521233559
epoch£º768	 i:4 	 global-step:15364	 l-p:0.0560639463365078
epoch£º768	 i:5 	 global-step:15365	 l-p:0.05777226388454437
epoch£º768	 i:6 	 global-step:15366	 l-p:0.056088343262672424
epoch£º768	 i:7 	 global-step:15367	 l-p:0.05621008574962616
epoch£º768	 i:8 	 global-step:15368	 l-p:0.05626098811626434
epoch£º768	 i:9 	 global-step:15369	 l-p:0.05630985274910927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0900, 31.4807, 31.6836],
        [28.0900, 34.4964, 37.2434],
        [28.0900, 34.4162, 37.0810],
        [28.0900, 28.1524, 28.0957]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.05623948946595192 
model_pd.l_d.mean(): 2.9369852200034074e-05 
model_pd.lagr.mean(): 0.05626885965466499 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8218], device='cuda:0')), ('power', tensor([0.2030], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.05623948946595192
epoch£º769	 i:1 	 global-step:15381	 l-p:0.05607791617512703
epoch£º769	 i:2 	 global-step:15382	 l-p:0.05616091564297676
epoch£º769	 i:3 	 global-step:15383	 l-p:0.05631355196237564
epoch£º769	 i:4 	 global-step:15384	 l-p:0.05623865872621536
epoch£º769	 i:5 	 global-step:15385	 l-p:0.056148920208215714
epoch£º769	 i:6 	 global-step:15386	 l-p:0.05798466503620148
epoch£º769	 i:7 	 global-step:15387	 l-p:0.056054648011922836
epoch£º769	 i:8 	 global-step:15388	 l-p:0.05691935494542122
epoch£º769	 i:9 	 global-step:15389	 l-p:0.05666476488113403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1526, 28.1528, 28.1526],
        [28.1526, 34.3416, 36.8591],
        [28.1526, 31.8252, 32.2037],
        [28.1526, 28.8302, 28.4249]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.05766715854406357 
model_pd.l_d.mean(): 0.0001980077795451507 
model_pd.lagr.mean(): 0.057865165174007416 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7110], device='cuda:0')), ('power', tensor([0.7229], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.05766715854406357
epoch£º770	 i:1 	 global-step:15401	 l-p:0.056065987795591354
epoch£º770	 i:2 	 global-step:15402	 l-p:0.05614977702498436
epoch£º770	 i:3 	 global-step:15403	 l-p:0.05625854432582855
epoch£º770	 i:4 	 global-step:15404	 l-p:0.05603724345564842
epoch£º770	 i:5 	 global-step:15405	 l-p:0.056039921939373016
epoch£º770	 i:6 	 global-step:15406	 l-p:0.05640619993209839
epoch£º770	 i:7 	 global-step:15407	 l-p:0.05610734969377518
epoch£º770	 i:8 	 global-step:15408	 l-p:0.057866085320711136
epoch£º770	 i:9 	 global-step:15409	 l-p:0.056060902774333954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1806, 28.1810, 28.1806],
        [28.1806, 28.3887, 28.2202],
        [28.1806, 28.2004, 28.1816],
        [28.1806, 31.4070, 31.5061]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.05605363845825195 
model_pd.l_d.mean(): 4.823171184398234e-05 
model_pd.lagr.mean(): 0.05610186979174614 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0004], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8726], device='cuda:0')), ('power', tensor([0.1128], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.05605363845825195
epoch£º771	 i:1 	 global-step:15421	 l-p:0.0561823770403862
epoch£º771	 i:2 	 global-step:15422	 l-p:0.056298211216926575
epoch£º771	 i:3 	 global-step:15423	 l-p:0.0560024231672287
epoch£º771	 i:4 	 global-step:15424	 l-p:0.05603014677762985
epoch£º771	 i:5 	 global-step:15425	 l-p:0.05614641681313515
epoch£º771	 i:6 	 global-step:15426	 l-p:0.0572245791554451
epoch£º771	 i:7 	 global-step:15427	 l-p:0.057087626308202744
epoch£º771	 i:8 	 global-step:15428	 l-p:0.05754486471414566
epoch£º771	 i:9 	 global-step:15429	 l-p:0.05607334151864052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1616, 28.3618, 28.1988],
        [28.1616, 28.5439, 28.2682],
        [28.1616, 37.2825, 43.1310],
        [28.1616, 28.3695, 28.2011]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.05603867769241333 
model_pd.l_d.mean(): 7.564613770227879e-05 
model_pd.lagr.mean(): 0.0561143234372139 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8696], device='cuda:0')), ('power', tensor([0.1296], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.05603867769241333
epoch£º772	 i:1 	 global-step:15441	 l-p:0.05630503222346306
epoch£º772	 i:2 	 global-step:15442	 l-p:0.05681998282670975
epoch£º772	 i:3 	 global-step:15443	 l-p:0.05604444816708565
epoch£º772	 i:4 	 global-step:15444	 l-p:0.056067727506160736
epoch£º772	 i:5 	 global-step:15445	 l-p:0.05727747827768326
epoch£º772	 i:6 	 global-step:15446	 l-p:0.05765122175216675
epoch£º772	 i:7 	 global-step:15447	 l-p:0.05608547106385231
epoch£º772	 i:8 	 global-step:15448	 l-p:0.056221045553684235
epoch£º772	 i:9 	 global-step:15449	 l-p:0.0562506802380085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0973, 28.0973, 28.0973],
        [28.0973, 28.2769, 28.1286],
        [28.0973, 30.3420, 29.9924],
        [28.0973, 30.2501, 29.8713]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.05600079521536827 
model_pd.l_d.mean(): 1.6290589428535895e-06 
model_pd.lagr.mean(): 0.0560024231672287 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.9014], device='cuda:0')), ('power', tensor([0.0023], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.05600079521536827
epoch£º773	 i:1 	 global-step:15461	 l-p:0.05819571390748024
epoch£º773	 i:2 	 global-step:15462	 l-p:0.05721496045589447
epoch£º773	 i:3 	 global-step:15463	 l-p:0.05633210018277168
epoch£º773	 i:4 	 global-step:15464	 l-p:0.05602508783340454
epoch£º773	 i:5 	 global-step:15465	 l-p:0.056320808827877045
epoch£º773	 i:6 	 global-step:15466	 l-p:0.056055448949337006
epoch£º773	 i:7 	 global-step:15467	 l-p:0.05611001327633858
epoch£º773	 i:8 	 global-step:15468	 l-p:0.05661079287528992
epoch£º773	 i:9 	 global-step:15469	 l-p:0.056137941777706146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9871, 28.5439, 28.1853],
        [27.9871, 28.1807, 28.0225],
        [27.9871, 30.6615, 30.4865],
        [27.9871, 28.0744, 27.9969]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.05608849227428436 
model_pd.l_d.mean(): -9.838448931986932e-06 
model_pd.lagr.mean(): 0.056078653782606125 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8615], device='cuda:0')), ('power', tensor([-0.0120], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.05608849227428436
epoch£º774	 i:1 	 global-step:15481	 l-p:0.05776092782616615
epoch£º774	 i:2 	 global-step:15482	 l-p:0.056280430406332016
epoch£º774	 i:3 	 global-step:15483	 l-p:0.05614958703517914
epoch£º774	 i:4 	 global-step:15484	 l-p:0.05623708665370941
epoch£º774	 i:5 	 global-step:15485	 l-p:0.05714644119143486
epoch£º774	 i:6 	 global-step:15486	 l-p:0.056985896080732346
epoch£º774	 i:7 	 global-step:15487	 l-p:0.056245654821395874
epoch£º774	 i:8 	 global-step:15488	 l-p:0.05620526149868965
epoch£º774	 i:9 	 global-step:15489	 l-p:0.05631224438548088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8390, 30.2084, 29.9139],
        [27.8390, 29.7027, 29.2579],
        [27.8390, 27.8390, 27.8390],
        [27.8390, 35.2678, 39.1558]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.05693100765347481 
model_pd.l_d.mean(): 0.0002868357114493847 
model_pd.lagr.mean(): 0.05721784383058548 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0009], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6745], device='cuda:0')), ('power', tensor([0.3376], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.05693100765347481
epoch£º775	 i:1 	 global-step:15501	 l-p:0.05662162974476814
epoch£º775	 i:2 	 global-step:15502	 l-p:0.056080181151628494
epoch£º775	 i:3 	 global-step:15503	 l-p:0.05800867825746536
epoch£º775	 i:4 	 global-step:15504	 l-p:0.056127604097127914
epoch£º775	 i:5 	 global-step:15505	 l-p:0.05700810253620148
epoch£º775	 i:6 	 global-step:15506	 l-p:0.05637030303478241
epoch£º775	 i:7 	 global-step:15507	 l-p:0.05617058649659157
epoch£º775	 i:8 	 global-step:15508	 l-p:0.05617254972457886
epoch£º775	 i:9 	 global-step:15509	 l-p:0.05641612038016319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[27.6756, 28.8914, 28.3912],
        [27.6756, 29.6642, 29.2546],
        [27.6756, 29.5278, 29.0857],
        [27.6756, 36.8128, 42.7838]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.05619415268301964 
model_pd.l_d.mean(): -0.00022064730001147836 
model_pd.lagr.mean(): 0.05597350373864174 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8338], device='cuda:0')), ('power', tensor([-0.2740], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.05619415268301964
epoch£º776	 i:1 	 global-step:15521	 l-p:0.056741971522569656
epoch£º776	 i:2 	 global-step:15522	 l-p:0.057965584099292755
epoch£º776	 i:3 	 global-step:15523	 l-p:0.056522492319345474
epoch£º776	 i:4 	 global-step:15524	 l-p:0.056377142667770386
epoch£º776	 i:5 	 global-step:15525	 l-p:0.0562240406870842
epoch£º776	 i:6 	 global-step:15526	 l-p:0.056515034288167953
epoch£º776	 i:7 	 global-step:15527	 l-p:0.05625680834054947
epoch£º776	 i:8 	 global-step:15528	 l-p:0.05635496601462364
epoch£º776	 i:9 	 global-step:15529	 l-p:0.05728840082883835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5196, 29.2282, 28.7634],
        [27.5196, 27.9635, 27.6578],
        [27.5196, 29.4517, 29.0332],
        [27.5196, 30.3852, 30.3283]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.05625388026237488 
model_pd.l_d.mean(): -0.0003090719983447343 
model_pd.lagr.mean(): 0.05594480782747269 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8237], device='cuda:0')), ('power', tensor([-0.4537], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.05625388026237488
epoch£º777	 i:1 	 global-step:15541	 l-p:0.05619952082633972
epoch£º777	 i:2 	 global-step:15542	 l-p:0.05896683409810066
epoch£º777	 i:3 	 global-step:15543	 l-p:0.05715964734554291
epoch£º777	 i:4 	 global-step:15544	 l-p:0.05617609992623329
epoch£º777	 i:5 	 global-step:15545	 l-p:0.0562610998749733
epoch£º777	 i:6 	 global-step:15546	 l-p:0.05640865117311478
epoch£º777	 i:7 	 global-step:15547	 l-p:0.0567859448492527
epoch£º777	 i:8 	 global-step:15548	 l-p:0.05636196956038475
epoch£º777	 i:9 	 global-step:15549	 l-p:0.05632719770073891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3994, 27.4032, 27.3995],
        [27.3994, 27.8023, 27.5177],
        [27.3994, 27.4731, 27.4070],
        [27.3994, 27.4277, 27.4011]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.057103078812360764 
model_pd.l_d.mean(): -7.542564708273858e-05 
model_pd.lagr.mean(): 0.057027652859687805 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.6819], device='cuda:0')), ('power', tensor([-0.1546], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.057103078812360764
epoch£º778	 i:1 	 global-step:15561	 l-p:0.056288547813892365
epoch£º778	 i:2 	 global-step:15562	 l-p:0.058468617498874664
epoch£º778	 i:3 	 global-step:15563	 l-p:0.056237366050481796
epoch£º778	 i:4 	 global-step:15564	 l-p:0.057184379547834396
epoch£º778	 i:5 	 global-step:15565	 l-p:0.056482963263988495
epoch£º778	 i:6 	 global-step:15566	 l-p:0.05629919096827507
epoch£º778	 i:7 	 global-step:15567	 l-p:0.05632810294628143
epoch£º778	 i:8 	 global-step:15568	 l-p:0.05659445747733116
epoch£º778	 i:9 	 global-step:15569	 l-p:0.05624846741557121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3306, 28.3449, 27.8678],
        [27.3306, 30.5159, 30.6471],
        [27.3306, 27.3310, 27.3306],
        [27.3306, 27.7324, 27.4486]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.05634554848074913 
model_pd.l_d.mean(): -0.00015250928117893636 
model_pd.lagr.mean(): 0.0561930388212204 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8004], device='cuda:0')), ('power', tensor([-0.6195], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.05634554848074913
epoch£º779	 i:1 	 global-step:15581	 l-p:0.056838586926460266
epoch£º779	 i:2 	 global-step:15582	 l-p:0.057288140058517456
epoch£º779	 i:3 	 global-step:15583	 l-p:0.05643155798316002
epoch£º779	 i:4 	 global-step:15584	 l-p:0.05626785755157471
epoch£º779	 i:5 	 global-step:15585	 l-p:0.05645134672522545
epoch£º779	 i:6 	 global-step:15586	 l-p:0.05626627802848816
epoch£º779	 i:7 	 global-step:15587	 l-p:0.056467454880476
epoch£º779	 i:8 	 global-step:15588	 l-p:0.05815018340945244
epoch£º779	 i:9 	 global-step:15589	 l-p:0.0568811260163784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3263, 27.3481, 27.3274],
        [27.3263, 27.4068, 27.3351],
        [27.3263, 35.0433, 39.3506],
        [27.3263, 34.9931, 39.2418]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.058682240545749664 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.058682240545749664 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7132], device='cuda:0')), ('power', tensor([-0.1407], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.058682240545749664
epoch£º780	 i:1 	 global-step:15601	 l-p:0.056265562772750854
epoch£º780	 i:2 	 global-step:15602	 l-p:0.05619404837489128
epoch£º780	 i:3 	 global-step:15603	 l-p:0.05720345303416252
epoch£º780	 i:4 	 global-step:15604	 l-p:0.05630151927471161
epoch£º780	 i:5 	 global-step:15605	 l-p:0.05623790994286537
epoch£º780	 i:6 	 global-step:15606	 l-p:0.05647164210677147
epoch£º780	 i:7 	 global-step:15607	 l-p:0.057217616587877274
epoch£º780	 i:8 	 global-step:15608	 l-p:0.05653641000390053
epoch£º780	 i:9 	 global-step:15609	 l-p:0.05617125704884529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3989, 35.1202, 39.4195],
        [27.3989, 27.3989, 27.3989],
        [27.3989, 27.4240, 27.4003],
        [27.3989, 27.4099, 27.3993]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.05630544200539589 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05630544200539589 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8062], device='cuda:0')), ('power', tensor([-0.5199], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.05630544200539589
epoch£º781	 i:1 	 global-step:15621	 l-p:0.05619841441512108
epoch£º781	 i:2 	 global-step:15622	 l-p:0.05876453220844269
epoch£º781	 i:3 	 global-step:15623	 l-p:0.056284427642822266
epoch£º781	 i:4 	 global-step:15624	 l-p:0.05622544512152672
epoch£º781	 i:5 	 global-step:15625	 l-p:0.057418715208768845
epoch£º781	 i:6 	 global-step:15626	 l-p:0.056815020740032196
epoch£º781	 i:7 	 global-step:15627	 l-p:0.05622943863272667
epoch£º781	 i:8 	 global-step:15628	 l-p:0.05653560161590576
epoch£º781	 i:9 	 global-step:15629	 l-p:0.056217316538095474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4964, 27.5762, 27.5050],
        [27.4964, 34.4526, 37.8715],
        [27.4964, 27.8167, 27.5776],
        [27.4964, 27.8391, 27.5870]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.05674276128411293 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05674276128411293 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8111], device='cuda:0')), ('power', tensor([-0.4089], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.05674276128411293
epoch£º782	 i:1 	 global-step:15641	 l-p:0.05618774890899658
epoch£º782	 i:2 	 global-step:15642	 l-p:0.056339673697948456
epoch£º782	 i:3 	 global-step:15643	 l-p:0.05637750402092934
epoch£º782	 i:4 	 global-step:15644	 l-p:0.05671314895153046
epoch£º782	 i:5 	 global-step:15645	 l-p:0.0564868338406086
epoch£º782	 i:6 	 global-step:15646	 l-p:0.05721481144428253
epoch£º782	 i:7 	 global-step:15647	 l-p:0.056309010833501816
epoch£º782	 i:8 	 global-step:15648	 l-p:0.05642203614115715
epoch£º782	 i:9 	 global-step:15649	 l-p:0.057842884212732315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5977, 33.7221, 36.2509],
        [27.5977, 29.6069, 29.2056],
        [27.5977, 33.3844, 35.5783],
        [27.5977, 27.8079, 27.6385]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.05624820291996002 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.05624820291996002 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8428], device='cuda:0')), ('power', tensor([-0.4111], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.05624820291996002
epoch£º783	 i:1 	 global-step:15661	 l-p:0.05642576888203621
epoch£º783	 i:2 	 global-step:15662	 l-p:0.05840727686882019
epoch£º783	 i:3 	 global-step:15663	 l-p:0.056170012801885605
epoch£º783	 i:4 	 global-step:15664	 l-p:0.056945618242025375
epoch£º783	 i:5 	 global-step:15665	 l-p:0.05628552287817001
epoch£º783	 i:6 	 global-step:15666	 l-p:0.05623052641749382
epoch£º783	 i:7 	 global-step:15667	 l-p:0.05618865042924881
epoch£º783	 i:8 	 global-step:15668	 l-p:0.05722527205944061
epoch£º783	 i:9 	 global-step:15669	 l-p:0.05618104711174965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7106, 33.8380, 36.3542],
        [27.7106, 27.7105, 27.7105],
        [27.7106, 28.2717, 27.9126],
        [27.7106, 28.3278, 27.9467]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.056304119527339935 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056304119527339935 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7807], device='cuda:0')), ('power', tensor([-0.1344], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.056304119527339935
epoch£º784	 i:1 	 global-step:15681	 l-p:0.059313420206308365
epoch£º784	 i:2 	 global-step:15682	 l-p:0.056203220039606094
epoch£º784	 i:3 	 global-step:15683	 l-p:0.056148186326026917
epoch£º784	 i:4 	 global-step:15684	 l-p:0.056407250463962555
epoch£º784	 i:5 	 global-step:15685	 l-p:0.05608177185058594
epoch£º784	 i:6 	 global-step:15686	 l-p:0.05633735656738281
epoch£º784	 i:7 	 global-step:15687	 l-p:0.056154459714889526
epoch£º784	 i:8 	 global-step:15688	 l-p:0.056342847645282745
epoch£º784	 i:9 	 global-step:15689	 l-p:0.05665949732065201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.8237, 27.8237, 27.8237],
        [27.8237, 27.8423, 27.8246],
        [27.8237, 31.9096, 32.6064],
        [27.8237, 28.9606, 28.4632]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.056991059333086014 
model_pd.l_d.mean(): -4.48913404227369e-08 
model_pd.lagr.mean(): 0.05699101462960243 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.3079e-06], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8234], device='cuda:0')), ('power', tensor([-0.0147], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.056991059333086014
epoch£º785	 i:1 	 global-step:15701	 l-p:0.056095872074365616
epoch£º785	 i:2 	 global-step:15702	 l-p:0.056190431118011475
epoch£º785	 i:3 	 global-step:15703	 l-p:0.05655025690793991
epoch£º785	 i:4 	 global-step:15704	 l-p:0.05626092851161957
epoch£º785	 i:5 	 global-step:15705	 l-p:0.05611526221036911
epoch£º785	 i:6 	 global-step:15706	 l-p:0.05819106101989746
epoch£º785	 i:7 	 global-step:15707	 l-p:0.05621582269668579
epoch£º785	 i:8 	 global-step:15708	 l-p:0.05632217228412628
epoch£º785	 i:9 	 global-step:15709	 l-p:0.056648578494787216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9308, 27.9308, 27.9308],
        [27.9308, 37.5376, 44.0568],
        [27.9308, 34.4591, 37.3549],
        [27.9308, 28.2565, 28.0133]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.05612281709909439 
model_pd.l_d.mean(): -2.669386276465957e-06 
model_pd.lagr.mean(): 0.056120146065950394 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([2.3953e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8544], device='cuda:0')), ('power', tensor([-0.0933], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.05612281709909439
epoch£º786	 i:1 	 global-step:15721	 l-p:0.05610624700784683
epoch£º786	 i:2 	 global-step:15722	 l-p:0.056227248162031174
epoch£º786	 i:3 	 global-step:15723	 l-p:0.056586138904094696
epoch£º786	 i:4 	 global-step:15724	 l-p:0.05775192752480507
epoch£º786	 i:5 	 global-step:15725	 l-p:0.056499700993299484
epoch£º786	 i:6 	 global-step:15726	 l-p:0.05626167356967926
epoch£º786	 i:7 	 global-step:15727	 l-p:0.056116875261068344
epoch£º786	 i:8 	 global-step:15728	 l-p:0.05605681613087654
epoch£º786	 i:9 	 global-step:15729	 l-p:0.057516295462846756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0330, 34.1752, 36.6621],
        [28.0330, 28.2468, 28.0745],
        [28.0330, 28.1144, 28.0417],
        [28.0330, 28.4517, 28.1571]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.05666540935635567 
model_pd.l_d.mean(): 3.245807965868153e-05 
model_pd.lagr.mean(): 0.056697867810726166 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0001], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7449], device='cuda:0')), ('power', tensor([0.3695], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.05666540935635567
epoch£º787	 i:1 	 global-step:15741	 l-p:0.05633354187011719
epoch£º787	 i:2 	 global-step:15742	 l-p:0.056135520339012146
epoch£º787	 i:3 	 global-step:15743	 l-p:0.056119877845048904
epoch£º787	 i:4 	 global-step:15744	 l-p:0.056625574827194214
epoch£º787	 i:5 	 global-step:15745	 l-p:0.0577162429690361
epoch£º787	 i:6 	 global-step:15746	 l-p:0.056194204837083817
epoch£º787	 i:7 	 global-step:15747	 l-p:0.05606287717819214
epoch£º787	 i:8 	 global-step:15748	 l-p:0.05607282370328903
epoch£º787	 i:9 	 global-step:15749	 l-p:0.05702410638332367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1186, 30.4725, 30.1600],
        [28.1186, 28.1186, 28.1186],
        [28.1186, 30.7788, 30.5906],
        [28.1186, 35.5761, 39.4496]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.05608493834733963 
model_pd.l_d.mean(): 1.548452019051183e-05 
model_pd.lagr.mean(): 0.056100424379110336 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0002], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8616], device='cuda:0')), ('power', tensor([0.0797], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.05608493834733963
epoch£º788	 i:1 	 global-step:15761	 l-p:0.056153472512960434
epoch£º788	 i:2 	 global-step:15762	 l-p:0.05745193362236023
epoch£º788	 i:3 	 global-step:15763	 l-p:0.056289415806531906
epoch£º788	 i:4 	 global-step:15764	 l-p:0.05608193203806877
epoch£º788	 i:5 	 global-step:15765	 l-p:0.05682869255542755
epoch£º788	 i:6 	 global-step:15766	 l-p:0.05602617561817169
epoch£º788	 i:7 	 global-step:15767	 l-p:0.056096624583005905
epoch£º788	 i:8 	 global-step:15768	 l-p:0.05603816360235214
epoch£º788	 i:9 	 global-step:15769	 l-p:0.05767582729458809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1710, 28.3927, 28.2148],
        [28.1710, 30.2865, 29.8939],
        [28.1710, 28.1709, 28.1710],
        [28.1710, 28.1710, 28.1709]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.056159790605306625 
model_pd.l_d.mean(): 7.344863115577027e-05 
model_pd.lagr.mean(): 0.0562332384288311 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8326], device='cuda:0')), ('power', tensor([0.2185], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.056159790605306625
epoch£º789	 i:1 	 global-step:15781	 l-p:0.0566154420375824
epoch£º789	 i:2 	 global-step:15782	 l-p:0.056045111268758774
epoch£º789	 i:3 	 global-step:15783	 l-p:0.05698958411812782
epoch£º789	 i:4 	 global-step:15784	 l-p:0.05607720836997032
epoch£º789	 i:5 	 global-step:15785	 l-p:0.05752262473106384
epoch£º789	 i:6 	 global-step:15786	 l-p:0.05672476813197136
epoch£º789	 i:7 	 global-step:15787	 l-p:0.05613671988248825
epoch£º789	 i:8 	 global-step:15788	 l-p:0.05615876615047455
epoch£º789	 i:9 	 global-step:15789	 l-p:0.05619552358984947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1825, 28.5618, 28.2877],
        [28.1825, 28.1825, 28.1825],
        [28.1825, 34.5306, 37.2048],
        [28.1825, 30.2990, 29.9062]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.05599913373589516 
model_pd.l_d.mean(): 5.981492722639814e-05 
model_pd.lagr.mean(): 0.05605894699692726 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0005], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8914], device='cuda:0')), ('power', tensor([0.1209], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.05599913373589516
epoch£º790	 i:1 	 global-step:15801	 l-p:0.05608247593045235
epoch£º790	 i:2 	 global-step:15802	 l-p:0.05605136230587959
epoch£º790	 i:3 	 global-step:15803	 l-p:0.05612868815660477
epoch£º790	 i:4 	 global-step:15804	 l-p:0.05608438327908516
epoch£º790	 i:5 	 global-step:15805	 l-p:0.05621316656470299
epoch£º790	 i:6 	 global-step:15806	 l-p:0.058293651789426804
epoch£º790	 i:7 	 global-step:15807	 l-p:0.05693088099360466
epoch£º790	 i:8 	 global-step:15808	 l-p:0.05643131211400032
epoch£º790	 i:9 	 global-step:15809	 l-p:0.056457698345184326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.1401, 30.0252, 29.5754],
        [28.1401, 38.1454, 45.1398],
        [28.1401, 28.7494, 28.3689],
        [28.1401, 29.5576, 29.0479]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.05611613020300865 
model_pd.l_d.mean(): 6.628489791182801e-05 
model_pd.lagr.mean(): 0.056182414293289185 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8628], device='cuda:0')), ('power', tensor([0.1025], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.05611613020300865
epoch£º791	 i:1 	 global-step:15821	 l-p:0.05624876916408539
epoch£º791	 i:2 	 global-step:15822	 l-p:0.05634636804461479
epoch£º791	 i:3 	 global-step:15823	 l-p:0.05697007104754448
epoch£º791	 i:4 	 global-step:15824	 l-p:0.056769609451293945
epoch£º791	 i:5 	 global-step:15825	 l-p:0.056093424558639526
epoch£º791	 i:6 	 global-step:15826	 l-p:0.05615825578570366
epoch£º791	 i:7 	 global-step:15827	 l-p:0.056070804595947266
epoch£º791	 i:8 	 global-step:15828	 l-p:0.05804150551557541
epoch£º791	 i:9 	 global-step:15829	 l-p:0.0560552254319191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[28.0484, 31.4338, 31.6364],
        [28.0484, 33.4824, 35.2857],
        [28.0484, 28.3984, 28.1409],
        [28.0484, 28.8959, 28.4421]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.05606980249285698 
model_pd.l_d.mean(): 6.651823241554666e-06 
model_pd.lagr.mean(): 0.05607645586133003 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8698], device='cuda:0')), ('power', tensor([0.0087], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.05606980249285698
epoch£º792	 i:1 	 global-step:15841	 l-p:0.0565386563539505
epoch£º792	 i:2 	 global-step:15842	 l-p:0.056116361171007156
epoch£º792	 i:3 	 global-step:15843	 l-p:0.05655466765165329
epoch£º792	 i:4 	 global-step:15844	 l-p:0.05625595897436142
epoch£º792	 i:5 	 global-step:15845	 l-p:0.05608779937028885
epoch£º792	 i:6 	 global-step:15846	 l-p:0.057343825697898865
epoch£º792	 i:7 	 global-step:15847	 l-p:0.05789555236697197
epoch£º792	 i:8 	 global-step:15848	 l-p:0.056240376085042953
epoch£º792	 i:9 	 global-step:15849	 l-p:0.05612225458025932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.9130, 27.9131, 27.9130],
        [27.9130, 27.9242, 27.9134],
        [27.9130, 27.9131, 27.9130],
        [27.9130, 28.1362, 27.9576]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.056139469146728516 
model_pd.l_d.mean(): -0.000126743339933455 
model_pd.lagr.mean(): 0.056012727320194244 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8666], device='cuda:0')), ('power', tensor([-0.1521], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.056139469146728516
epoch£º793	 i:1 	 global-step:15861	 l-p:0.05875851586461067
epoch£º793	 i:2 	 global-step:15862	 l-p:0.05617406219244003
epoch£º793	 i:3 	 global-step:15863	 l-p:0.056105468422174454
epoch£º793	 i:4 	 global-step:15864	 l-p:0.05684628337621689
epoch£º793	 i:5 	 global-step:15865	 l-p:0.05611748620867729
epoch£º793	 i:6 	 global-step:15866	 l-p:0.05625549331307411
epoch£º793	 i:7 	 global-step:15867	 l-p:0.0562407486140728
epoch£º793	 i:8 	 global-step:15868	 l-p:0.05637304112315178
epoch£º793	 i:9 	 global-step:15869	 l-p:0.05664008483290672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.7544, 30.6130, 30.5389],
        [27.7544, 27.8494, 27.7657],
        [27.7544, 30.3887, 30.2076],
        [27.7544, 27.7544, 27.7544]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.05631440505385399 
model_pd.l_d.mean(): -9.834984666667879e-05 
model_pd.lagr.mean(): 0.056216053664684296 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0008], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7924], device='cuda:0')), ('power', tensor([-0.1189], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.05631440505385399
epoch£º794	 i:1 	 global-step:15881	 l-p:0.05665047839283943
epoch£º794	 i:2 	 global-step:15882	 l-p:0.05615318566560745
epoch£º794	 i:3 	 global-step:15883	 l-p:0.05623384937644005
epoch£º794	 i:4 	 global-step:15884	 l-p:0.058012716472148895
epoch£º794	 i:5 	 global-step:15885	 l-p:0.05617733672261238
epoch£º794	 i:6 	 global-step:15886	 l-p:0.05630878359079361
epoch£º794	 i:7 	 global-step:15887	 l-p:0.05679451301693916
epoch£º794	 i:8 	 global-step:15888	 l-p:0.056328315287828445
epoch£º794	 i:9 	 global-step:15889	 l-p:0.05724328011274338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.5856, 27.5856, 27.5856],
        [27.5856, 31.9372, 32.8597],
        [27.5856, 27.7816, 27.6220],
        [27.5856, 27.7715, 27.6191]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.05632635951042175 
model_pd.l_d.mean(): -0.00026325395447202027 
model_pd.lagr.mean(): 0.056063104420900345 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0007], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8013], device='cuda:0')), ('power', tensor([-0.3561], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.05632635951042175
epoch£º795	 i:1 	 global-step:15901	 l-p:0.056386277079582214
epoch£º795	 i:2 	 global-step:15902	 l-p:0.056237176060676575
epoch£º795	 i:3 	 global-step:15903	 l-p:0.0566890612244606
epoch£º795	 i:4 	 global-step:15904	 l-p:0.05655452609062195
epoch£º795	 i:5 	 global-step:15905	 l-p:0.056875426322221756
epoch£º795	 i:6 	 global-step:15906	 l-p:0.057179324328899384
epoch£º795	 i:7 	 global-step:15907	 l-p:0.056331388652324677
epoch£º795	 i:8 	 global-step:15908	 l-p:0.05796877667307854
epoch£º795	 i:9 	 global-step:15909	 l-p:0.056208278983831406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.4393, 27.6789, 27.4899],
        [27.4393, 33.6075, 36.2026],
        [27.4393, 28.1978, 27.7725],
        [27.4393, 27.4394, 27.4393]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.056681156158447266 
model_pd.l_d.mean(): -0.0002115183451678604 
model_pd.lagr.mean(): 0.0564696379005909 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0006], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7794], device='cuda:0')), ('power', tensor([-0.3695], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.056681156158447266
epoch£º796	 i:1 	 global-step:15921	 l-p:0.05797373503446579
epoch£º796	 i:2 	 global-step:15922	 l-p:0.05707840621471405
epoch£º796	 i:3 	 global-step:15923	 l-p:0.05637867748737335
epoch£º796	 i:4 	 global-step:15924	 l-p:0.05721116438508034
epoch£º796	 i:5 	 global-step:15925	 l-p:0.05625951290130615
epoch£º796	 i:6 	 global-step:15926	 l-p:0.05646834149956703
epoch£º796	 i:7 	 global-step:15927	 l-p:0.05635060369968414
epoch£º796	 i:8 	 global-step:15928	 l-p:0.056433744728565216
epoch£º796	 i:9 	 global-step:15929	 l-p:0.0562908835709095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3468, 27.8077, 27.4944],
        [27.3468, 27.4163, 27.3537],
        [27.3468, 27.3492, 27.3468],
        [27.3468, 27.3468, 27.3468]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.056328561156988144 
model_pd.l_d.mean(): -0.0002057673700619489 
model_pd.lagr.mean(): 0.0561227947473526 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.0003], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8043], device='cuda:0')), ('power', tensor([-0.5955], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.056328561156988144
epoch£º797	 i:1 	 global-step:15941	 l-p:0.05623789504170418
epoch£º797	 i:2 	 global-step:15942	 l-p:0.05619298666715622
epoch£º797	 i:3 	 global-step:15943	 l-p:0.05649149790406227
epoch£º797	 i:4 	 global-step:15944	 l-p:0.057391636073589325
epoch£º797	 i:5 	 global-step:15945	 l-p:0.056546252220869064
epoch£º797	 i:6 	 global-step:15946	 l-p:0.056764550507068634
epoch£º797	 i:7 	 global-step:15947	 l-p:0.05635152757167816
epoch£º797	 i:8 	 global-step:15948	 l-p:0.05681552365422249
epoch£º797	 i:9 	 global-step:15949	 l-p:0.05826719105243683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3121, 34.2551, 37.6888],
        [27.3121, 27.3306, 27.3130],
        [27.3121, 28.0788, 27.6523],
        [27.3121, 36.0333, 41.5556]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.057208068668842316 
model_pd.l_d.mean(): -4.1645034798420966e-05 
model_pd.lagr.mean(): 0.05716642364859581 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([5.9707e-05], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.7927], device='cuda:0')), ('power', tensor([-0.4935], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.057208068668842316
epoch£º798	 i:1 	 global-step:15961	 l-p:0.05696458742022514
epoch£º798	 i:2 	 global-step:15962	 l-p:0.05629023537039757
epoch£º798	 i:3 	 global-step:15963	 l-p:0.056281059980392456
epoch£º798	 i:4 	 global-step:15964	 l-p:0.05846351385116577
epoch£º798	 i:5 	 global-step:15965	 l-p:0.05623655766248703
epoch£º798	 i:6 	 global-step:15966	 l-p:0.05668399855494499
epoch£º798	 i:7 	 global-step:15967	 l-p:0.056277140974998474
epoch£º798	 i:8 	 global-step:15968	 l-p:0.05655958876013756
epoch£º798	 i:9 	 global-step:15969	 l-p:0.0563940592110157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[27.3681, 27.5624, 27.4042],
        [27.3681, 27.3733, 27.3683],
        [27.3681, 27.3681, 27.3681],
        [27.3681, 27.3788, 27.3685]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.056190695613622665 
model_pd.l_d.mean(): 0.0 
model_pd.lagr.mean(): 0.056190695613622665 
model_pd.lambdas: dict_items([('pout', tensor([0.], device='cuda:0')), ('power', tensor([0.], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([-1.8597], device='cuda:0')), ('power', tensor([-0.7320], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.056190695613622665
epoch£º799	 i:1 	 global-step:15981	 l-p:0.05639070272445679
epoch£º799	 i:2 	 global-step:15982	 l-p:0.056441910564899445
epoch£º799	 i:3 	 global-step:15983	 l-p:0.05666578188538551
epoch£º799	 i:4 	 global-step:15984	 l-p:0.056288521736860275
epoch£º799	 i:5 	 global-step:15985	 l-p:0.058134544640779495
epoch£º799	 i:6 	 global-step:15986	 l-p:0.056228119879961014
epoch£º799	 i:7 	 global-step:15987	 l-p:0.05648687109351158
epoch£º799	 i:8 	 global-step:15988	 l-p:0.05624912306666374
epoch£º799	 i:9 	 global-step:15989	 l-p:0.058012571185827255
