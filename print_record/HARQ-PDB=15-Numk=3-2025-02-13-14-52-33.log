
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[-0.8147, -0.8147, -0.8147],
        [-0.8147, -0.8147, -0.8147],
        [-0.8147, -0.8147, -0.8147],
        [-0.8147, -0.8147, -0.8147]], device='cuda:0',
       grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): 0.009970693849027157 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([0.9978], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([-43.2683], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:0.009970693849027157
epoch£º0	 i:1 	 global-step:1	 l-p:0.008578438311815262
epoch£º0	 i:2 	 global-step:2	 l-p:nan
epoch£º0	 i:3 	 global-step:3	 l-p:nan
epoch£º0	 i:4 	 global-step:4	 l-p:nan
epoch£º0	 i:5 	 global-step:5	 l-p:nan
epoch£º0	 i:6 	 global-step:6	 l-p:nan
epoch£º0	 i:7 	 global-step:7	 l-p:nan
epoch£º0	 i:8 	 global-step:8	 l-p:nan
epoch£º0	 i:9 	 global-step:9	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:nan
epoch£º1	 i:1 	 global-step:21	 l-p:nan
epoch£º1	 i:2 	 global-step:22	 l-p:nan
epoch£º1	 i:3 	 global-step:23	 l-p:nan
epoch£º1	 i:4 	 global-step:24	 l-p:nan
epoch£º1	 i:5 	 global-step:25	 l-p:nan
epoch£º1	 i:6 	 global-step:26	 l-p:nan
epoch£º1	 i:7 	 global-step:27	 l-p:nan
epoch£º1	 i:8 	 global-step:28	 l-p:nan
epoch£º1	 i:9 	 global-step:29	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:nan
epoch£º2	 i:1 	 global-step:41	 l-p:nan
epoch£º2	 i:2 	 global-step:42	 l-p:nan
epoch£º2	 i:3 	 global-step:43	 l-p:nan
epoch£º2	 i:4 	 global-step:44	 l-p:nan
epoch£º2	 i:5 	 global-step:45	 l-p:nan
epoch£º2	 i:6 	 global-step:46	 l-p:nan
epoch£º2	 i:7 	 global-step:47	 l-p:nan
epoch£º2	 i:8 	 global-step:48	 l-p:nan
epoch£º2	 i:9 	 global-step:49	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:nan
epoch£º3	 i:1 	 global-step:61	 l-p:nan
epoch£º3	 i:2 	 global-step:62	 l-p:nan
epoch£º3	 i:3 	 global-step:63	 l-p:nan
epoch£º3	 i:4 	 global-step:64	 l-p:nan
epoch£º3	 i:5 	 global-step:65	 l-p:nan
epoch£º3	 i:6 	 global-step:66	 l-p:nan
epoch£º3	 i:7 	 global-step:67	 l-p:nan
epoch£º3	 i:8 	 global-step:68	 l-p:nan
epoch£º3	 i:9 	 global-step:69	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:nan
epoch£º4	 i:1 	 global-step:81	 l-p:nan
epoch£º4	 i:2 	 global-step:82	 l-p:nan
epoch£º4	 i:3 	 global-step:83	 l-p:nan
epoch£º4	 i:4 	 global-step:84	 l-p:nan
epoch£º4	 i:5 	 global-step:85	 l-p:nan
epoch£º4	 i:6 	 global-step:86	 l-p:nan
epoch£º4	 i:7 	 global-step:87	 l-p:nan
epoch£º4	 i:8 	 global-step:88	 l-p:nan
epoch£º4	 i:9 	 global-step:89	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:nan
epoch£º5	 i:1 	 global-step:101	 l-p:nan
epoch£º5	 i:2 	 global-step:102	 l-p:nan
epoch£º5	 i:3 	 global-step:103	 l-p:nan
epoch£º5	 i:4 	 global-step:104	 l-p:nan
epoch£º5	 i:5 	 global-step:105	 l-p:nan
epoch£º5	 i:6 	 global-step:106	 l-p:nan
epoch£º5	 i:7 	 global-step:107	 l-p:nan
epoch£º5	 i:8 	 global-step:108	 l-p:nan
epoch£º5	 i:9 	 global-step:109	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:nan
epoch£º6	 i:1 	 global-step:121	 l-p:nan
epoch£º6	 i:2 	 global-step:122	 l-p:nan
epoch£º6	 i:3 	 global-step:123	 l-p:nan
epoch£º6	 i:4 	 global-step:124	 l-p:nan
epoch£º6	 i:5 	 global-step:125	 l-p:nan
epoch£º6	 i:6 	 global-step:126	 l-p:nan
epoch£º6	 i:7 	 global-step:127	 l-p:nan
epoch£º6	 i:8 	 global-step:128	 l-p:nan
epoch£º6	 i:9 	 global-step:129	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:nan
epoch£º7	 i:1 	 global-step:141	 l-p:nan
epoch£º7	 i:2 	 global-step:142	 l-p:nan
epoch£º7	 i:3 	 global-step:143	 l-p:nan
epoch£º7	 i:4 	 global-step:144	 l-p:nan
epoch£º7	 i:5 	 global-step:145	 l-p:nan
epoch£º7	 i:6 	 global-step:146	 l-p:nan
epoch£º7	 i:7 	 global-step:147	 l-p:nan
epoch£º7	 i:8 	 global-step:148	 l-p:nan
epoch£º7	 i:9 	 global-step:149	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:nan
epoch£º8	 i:1 	 global-step:161	 l-p:nan
epoch£º8	 i:2 	 global-step:162	 l-p:nan
epoch£º8	 i:3 	 global-step:163	 l-p:nan
epoch£º8	 i:4 	 global-step:164	 l-p:nan
epoch£º8	 i:5 	 global-step:165	 l-p:nan
epoch£º8	 i:6 	 global-step:166	 l-p:nan
epoch£º8	 i:7 	 global-step:167	 l-p:nan
epoch£º8	 i:8 	 global-step:168	 l-p:nan
epoch£º8	 i:9 	 global-step:169	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:nan
epoch£º9	 i:1 	 global-step:181	 l-p:nan
epoch£º9	 i:2 	 global-step:182	 l-p:nan
epoch£º9	 i:3 	 global-step:183	 l-p:nan
epoch£º9	 i:4 	 global-step:184	 l-p:nan
epoch£º9	 i:5 	 global-step:185	 l-p:nan
epoch£º9	 i:6 	 global-step:186	 l-p:nan
epoch£º9	 i:7 	 global-step:187	 l-p:nan
epoch£º9	 i:8 	 global-step:188	 l-p:nan
epoch£º9	 i:9 	 global-step:189	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:nan
epoch£º10	 i:1 	 global-step:201	 l-p:nan
epoch£º10	 i:2 	 global-step:202	 l-p:nan
epoch£º10	 i:3 	 global-step:203	 l-p:nan
epoch£º10	 i:4 	 global-step:204	 l-p:nan
epoch£º10	 i:5 	 global-step:205	 l-p:nan
epoch£º10	 i:6 	 global-step:206	 l-p:nan
epoch£º10	 i:7 	 global-step:207	 l-p:nan
epoch£º10	 i:8 	 global-step:208	 l-p:nan
epoch£º10	 i:9 	 global-step:209	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:nan
epoch£º11	 i:1 	 global-step:221	 l-p:nan
epoch£º11	 i:2 	 global-step:222	 l-p:nan
epoch£º11	 i:3 	 global-step:223	 l-p:nan
epoch£º11	 i:4 	 global-step:224	 l-p:nan
epoch£º11	 i:5 	 global-step:225	 l-p:nan
epoch£º11	 i:6 	 global-step:226	 l-p:nan
epoch£º11	 i:7 	 global-step:227	 l-p:nan
epoch£º11	 i:8 	 global-step:228	 l-p:nan
epoch£º11	 i:9 	 global-step:229	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:nan
epoch£º12	 i:1 	 global-step:241	 l-p:nan
epoch£º12	 i:2 	 global-step:242	 l-p:nan
epoch£º12	 i:3 	 global-step:243	 l-p:nan
epoch£º12	 i:4 	 global-step:244	 l-p:nan
epoch£º12	 i:5 	 global-step:245	 l-p:nan
epoch£º12	 i:6 	 global-step:246	 l-p:nan
epoch£º12	 i:7 	 global-step:247	 l-p:nan
epoch£º12	 i:8 	 global-step:248	 l-p:nan
epoch£º12	 i:9 	 global-step:249	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:nan
epoch£º13	 i:1 	 global-step:261	 l-p:nan
epoch£º13	 i:2 	 global-step:262	 l-p:nan
epoch£º13	 i:3 	 global-step:263	 l-p:nan
epoch£º13	 i:4 	 global-step:264	 l-p:nan
epoch£º13	 i:5 	 global-step:265	 l-p:nan
epoch£º13	 i:6 	 global-step:266	 l-p:nan
epoch£º13	 i:7 	 global-step:267	 l-p:nan
epoch£º13	 i:8 	 global-step:268	 l-p:nan
epoch£º13	 i:9 	 global-step:269	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:nan
epoch£º14	 i:1 	 global-step:281	 l-p:nan
epoch£º14	 i:2 	 global-step:282	 l-p:nan
epoch£º14	 i:3 	 global-step:283	 l-p:nan
epoch£º14	 i:4 	 global-step:284	 l-p:nan
epoch£º14	 i:5 	 global-step:285	 l-p:nan
epoch£º14	 i:6 	 global-step:286	 l-p:nan
epoch£º14	 i:7 	 global-step:287	 l-p:nan
epoch£º14	 i:8 	 global-step:288	 l-p:nan
epoch£º14	 i:9 	 global-step:289	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:nan
epoch£º15	 i:1 	 global-step:301	 l-p:nan
epoch£º15	 i:2 	 global-step:302	 l-p:nan
epoch£º15	 i:3 	 global-step:303	 l-p:nan
epoch£º15	 i:4 	 global-step:304	 l-p:nan
epoch£º15	 i:5 	 global-step:305	 l-p:nan
epoch£º15	 i:6 	 global-step:306	 l-p:nan
epoch£º15	 i:7 	 global-step:307	 l-p:nan
epoch£º15	 i:8 	 global-step:308	 l-p:nan
epoch£º15	 i:9 	 global-step:309	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3192,  0.2181,  1.0000,  0.1491,
          1.0000,  0.6834, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:nan
epoch£º16	 i:1 	 global-step:321	 l-p:nan
epoch£º16	 i:2 	 global-step:322	 l-p:nan
epoch£º16	 i:3 	 global-step:323	 l-p:nan
epoch£º16	 i:4 	 global-step:324	 l-p:nan
epoch£º16	 i:5 	 global-step:325	 l-p:nan
epoch£º16	 i:6 	 global-step:326	 l-p:nan
epoch£º16	 i:7 	 global-step:327	 l-p:nan
epoch£º16	 i:8 	 global-step:328	 l-p:nan
epoch£º16	 i:9 	 global-step:329	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:nan
epoch£º17	 i:1 	 global-step:341	 l-p:nan
epoch£º17	 i:2 	 global-step:342	 l-p:nan
epoch£º17	 i:3 	 global-step:343	 l-p:nan
epoch£º17	 i:4 	 global-step:344	 l-p:nan
epoch£º17	 i:5 	 global-step:345	 l-p:nan
epoch£º17	 i:6 	 global-step:346	 l-p:nan
epoch£º17	 i:7 	 global-step:347	 l-p:nan
epoch£º17	 i:8 	 global-step:348	 l-p:nan
epoch£º17	 i:9 	 global-step:349	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:nan
epoch£º18	 i:1 	 global-step:361	 l-p:nan
epoch£º18	 i:2 	 global-step:362	 l-p:nan
epoch£º18	 i:3 	 global-step:363	 l-p:nan
epoch£º18	 i:4 	 global-step:364	 l-p:nan
epoch£º18	 i:5 	 global-step:365	 l-p:nan
epoch£º18	 i:6 	 global-step:366	 l-p:nan
epoch£º18	 i:7 	 global-step:367	 l-p:nan
epoch£º18	 i:8 	 global-step:368	 l-p:nan
epoch£º18	 i:9 	 global-step:369	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:nan
epoch£º19	 i:1 	 global-step:381	 l-p:nan
epoch£º19	 i:2 	 global-step:382	 l-p:nan
epoch£º19	 i:3 	 global-step:383	 l-p:nan
epoch£º19	 i:4 	 global-step:384	 l-p:nan
epoch£º19	 i:5 	 global-step:385	 l-p:nan
epoch£º19	 i:6 	 global-step:386	 l-p:nan
epoch£º19	 i:7 	 global-step:387	 l-p:nan
epoch£º19	 i:8 	 global-step:388	 l-p:nan
epoch£º19	 i:9 	 global-step:389	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:nan
epoch£º20	 i:1 	 global-step:401	 l-p:nan
epoch£º20	 i:2 	 global-step:402	 l-p:nan
epoch£º20	 i:3 	 global-step:403	 l-p:nan
epoch£º20	 i:4 	 global-step:404	 l-p:nan
epoch£º20	 i:5 	 global-step:405	 l-p:nan
epoch£º20	 i:6 	 global-step:406	 l-p:nan
epoch£º20	 i:7 	 global-step:407	 l-p:nan
epoch£º20	 i:8 	 global-step:408	 l-p:nan
epoch£º20	 i:9 	 global-step:409	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:nan
epoch£º21	 i:1 	 global-step:421	 l-p:nan
epoch£º21	 i:2 	 global-step:422	 l-p:nan
epoch£º21	 i:3 	 global-step:423	 l-p:nan
epoch£º21	 i:4 	 global-step:424	 l-p:nan
epoch£º21	 i:5 	 global-step:425	 l-p:nan
epoch£º21	 i:6 	 global-step:426	 l-p:nan
epoch£º21	 i:7 	 global-step:427	 l-p:nan
epoch£º21	 i:8 	 global-step:428	 l-p:nan
epoch£º21	 i:9 	 global-step:429	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:nan
epoch£º22	 i:1 	 global-step:441	 l-p:nan
epoch£º22	 i:2 	 global-step:442	 l-p:nan
epoch£º22	 i:3 	 global-step:443	 l-p:nan
epoch£º22	 i:4 	 global-step:444	 l-p:nan
epoch£º22	 i:5 	 global-step:445	 l-p:nan
epoch£º22	 i:6 	 global-step:446	 l-p:nan
epoch£º22	 i:7 	 global-step:447	 l-p:nan
epoch£º22	 i:8 	 global-step:448	 l-p:nan
epoch£º22	 i:9 	 global-step:449	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:nan
epoch£º23	 i:1 	 global-step:461	 l-p:nan
epoch£º23	 i:2 	 global-step:462	 l-p:nan
epoch£º23	 i:3 	 global-step:463	 l-p:nan
epoch£º23	 i:4 	 global-step:464	 l-p:nan
epoch£º23	 i:5 	 global-step:465	 l-p:nan
epoch£º23	 i:6 	 global-step:466	 l-p:nan
epoch£º23	 i:7 	 global-step:467	 l-p:nan
epoch£º23	 i:8 	 global-step:468	 l-p:nan
epoch£º23	 i:9 	 global-step:469	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1381,  0.0714,  1.0000,  0.0369,
          1.0000,  0.5169, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2731,  0.1772,  1.0000,  0.1150,
          1.0000,  0.6488, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:nan
epoch£º24	 i:1 	 global-step:481	 l-p:nan
epoch£º24	 i:2 	 global-step:482	 l-p:nan
epoch£º24	 i:3 	 global-step:483	 l-p:nan
epoch£º24	 i:4 	 global-step:484	 l-p:nan
epoch£º24	 i:5 	 global-step:485	 l-p:nan
epoch£º24	 i:6 	 global-step:486	 l-p:nan
epoch£º24	 i:7 	 global-step:487	 l-p:nan
epoch£º24	 i:8 	 global-step:488	 l-p:nan
epoch£º24	 i:9 	 global-step:489	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:nan
epoch£º25	 i:1 	 global-step:501	 l-p:nan
epoch£º25	 i:2 	 global-step:502	 l-p:nan
epoch£º25	 i:3 	 global-step:503	 l-p:nan
epoch£º25	 i:4 	 global-step:504	 l-p:nan
epoch£º25	 i:5 	 global-step:505	 l-p:nan
epoch£º25	 i:6 	 global-step:506	 l-p:nan
epoch£º25	 i:7 	 global-step:507	 l-p:nan
epoch£º25	 i:8 	 global-step:508	 l-p:nan
epoch£º25	 i:9 	 global-step:509	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:nan
epoch£º26	 i:1 	 global-step:521	 l-p:nan
epoch£º26	 i:2 	 global-step:522	 l-p:nan
epoch£º26	 i:3 	 global-step:523	 l-p:nan
epoch£º26	 i:4 	 global-step:524	 l-p:nan
epoch£º26	 i:5 	 global-step:525	 l-p:nan
epoch£º26	 i:6 	 global-step:526	 l-p:nan
epoch£º26	 i:7 	 global-step:527	 l-p:nan
epoch£º26	 i:8 	 global-step:528	 l-p:nan
epoch£º26	 i:9 	 global-step:529	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:nan
epoch£º27	 i:1 	 global-step:541	 l-p:nan
epoch£º27	 i:2 	 global-step:542	 l-p:nan
epoch£º27	 i:3 	 global-step:543	 l-p:nan
epoch£º27	 i:4 	 global-step:544	 l-p:nan
epoch£º27	 i:5 	 global-step:545	 l-p:nan
epoch£º27	 i:6 	 global-step:546	 l-p:nan
epoch£º27	 i:7 	 global-step:547	 l-p:nan
epoch£º27	 i:8 	 global-step:548	 l-p:nan
epoch£º27	 i:9 	 global-step:549	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:nan
epoch£º28	 i:1 	 global-step:561	 l-p:nan
epoch£º28	 i:2 	 global-step:562	 l-p:nan
epoch£º28	 i:3 	 global-step:563	 l-p:nan
epoch£º28	 i:4 	 global-step:564	 l-p:nan
epoch£º28	 i:5 	 global-step:565	 l-p:nan
epoch£º28	 i:6 	 global-step:566	 l-p:nan
epoch£º28	 i:7 	 global-step:567	 l-p:nan
epoch£º28	 i:8 	 global-step:568	 l-p:nan
epoch£º28	 i:9 	 global-step:569	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:nan
epoch£º29	 i:1 	 global-step:581	 l-p:nan
epoch£º29	 i:2 	 global-step:582	 l-p:nan
epoch£º29	 i:3 	 global-step:583	 l-p:nan
epoch£º29	 i:4 	 global-step:584	 l-p:nan
epoch£º29	 i:5 	 global-step:585	 l-p:nan
epoch£º29	 i:6 	 global-step:586	 l-p:nan
epoch£º29	 i:7 	 global-step:587	 l-p:nan
epoch£º29	 i:8 	 global-step:588	 l-p:nan
epoch£º29	 i:9 	 global-step:589	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:nan
epoch£º30	 i:1 	 global-step:601	 l-p:nan
epoch£º30	 i:2 	 global-step:602	 l-p:nan
epoch£º30	 i:3 	 global-step:603	 l-p:nan
epoch£º30	 i:4 	 global-step:604	 l-p:nan
epoch£º30	 i:5 	 global-step:605	 l-p:nan
epoch£º30	 i:6 	 global-step:606	 l-p:nan
epoch£º30	 i:7 	 global-step:607	 l-p:nan
epoch£º30	 i:8 	 global-step:608	 l-p:nan
epoch£º30	 i:9 	 global-step:609	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:nan
epoch£º31	 i:1 	 global-step:621	 l-p:nan
epoch£º31	 i:2 	 global-step:622	 l-p:nan
epoch£º31	 i:3 	 global-step:623	 l-p:nan
epoch£º31	 i:4 	 global-step:624	 l-p:nan
epoch£º31	 i:5 	 global-step:625	 l-p:nan
epoch£º31	 i:6 	 global-step:626	 l-p:nan
epoch£º31	 i:7 	 global-step:627	 l-p:nan
epoch£º31	 i:8 	 global-step:628	 l-p:nan
epoch£º31	 i:9 	 global-step:629	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:nan
epoch£º32	 i:1 	 global-step:641	 l-p:nan
epoch£º32	 i:2 	 global-step:642	 l-p:nan
epoch£º32	 i:3 	 global-step:643	 l-p:nan
epoch£º32	 i:4 	 global-step:644	 l-p:nan
epoch£º32	 i:5 	 global-step:645	 l-p:nan
epoch£º32	 i:6 	 global-step:646	 l-p:nan
epoch£º32	 i:7 	 global-step:647	 l-p:nan
epoch£º32	 i:8 	 global-step:648	 l-p:nan
epoch£º32	 i:9 	 global-step:649	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:nan
epoch£º33	 i:1 	 global-step:661	 l-p:nan
epoch£º33	 i:2 	 global-step:662	 l-p:nan
epoch£º33	 i:3 	 global-step:663	 l-p:nan
epoch£º33	 i:4 	 global-step:664	 l-p:nan
epoch£º33	 i:5 	 global-step:665	 l-p:nan
epoch£º33	 i:6 	 global-step:666	 l-p:nan
epoch£º33	 i:7 	 global-step:667	 l-p:nan
epoch£º33	 i:8 	 global-step:668	 l-p:nan
epoch£º33	 i:9 	 global-step:669	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:nan
epoch£º34	 i:1 	 global-step:681	 l-p:nan
epoch£º34	 i:2 	 global-step:682	 l-p:nan
epoch£º34	 i:3 	 global-step:683	 l-p:nan
epoch£º34	 i:4 	 global-step:684	 l-p:nan
epoch£º34	 i:5 	 global-step:685	 l-p:nan
epoch£º34	 i:6 	 global-step:686	 l-p:nan
epoch£º34	 i:7 	 global-step:687	 l-p:nan
epoch£º34	 i:8 	 global-step:688	 l-p:nan
epoch£º34	 i:9 	 global-step:689	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:nan
epoch£º35	 i:1 	 global-step:701	 l-p:nan
epoch£º35	 i:2 	 global-step:702	 l-p:nan
epoch£º35	 i:3 	 global-step:703	 l-p:nan
epoch£º35	 i:4 	 global-step:704	 l-p:nan
epoch£º35	 i:5 	 global-step:705	 l-p:nan
epoch£º35	 i:6 	 global-step:706	 l-p:nan
epoch£º35	 i:7 	 global-step:707	 l-p:nan
epoch£º35	 i:8 	 global-step:708	 l-p:nan
epoch£º35	 i:9 	 global-step:709	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:nan
epoch£º36	 i:1 	 global-step:721	 l-p:nan
epoch£º36	 i:2 	 global-step:722	 l-p:nan
epoch£º36	 i:3 	 global-step:723	 l-p:nan
epoch£º36	 i:4 	 global-step:724	 l-p:nan
epoch£º36	 i:5 	 global-step:725	 l-p:nan
epoch£º36	 i:6 	 global-step:726	 l-p:nan
epoch£º36	 i:7 	 global-step:727	 l-p:nan
epoch£º36	 i:8 	 global-step:728	 l-p:nan
epoch£º36	 i:9 	 global-step:729	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:nan
epoch£º37	 i:1 	 global-step:741	 l-p:nan
epoch£º37	 i:2 	 global-step:742	 l-p:nan
epoch£º37	 i:3 	 global-step:743	 l-p:nan
epoch£º37	 i:4 	 global-step:744	 l-p:nan
epoch£º37	 i:5 	 global-step:745	 l-p:nan
epoch£º37	 i:6 	 global-step:746	 l-p:nan
epoch£º37	 i:7 	 global-step:747	 l-p:nan
epoch£º37	 i:8 	 global-step:748	 l-p:nan
epoch£º37	 i:9 	 global-step:749	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:nan
epoch£º38	 i:1 	 global-step:761	 l-p:nan
epoch£º38	 i:2 	 global-step:762	 l-p:nan
epoch£º38	 i:3 	 global-step:763	 l-p:nan
epoch£º38	 i:4 	 global-step:764	 l-p:nan
epoch£º38	 i:5 	 global-step:765	 l-p:nan
epoch£º38	 i:6 	 global-step:766	 l-p:nan
epoch£º38	 i:7 	 global-step:767	 l-p:nan
epoch£º38	 i:8 	 global-step:768	 l-p:nan
epoch£º38	 i:9 	 global-step:769	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:nan
epoch£º39	 i:1 	 global-step:781	 l-p:nan
epoch£º39	 i:2 	 global-step:782	 l-p:nan
epoch£º39	 i:3 	 global-step:783	 l-p:nan
epoch£º39	 i:4 	 global-step:784	 l-p:nan
epoch£º39	 i:5 	 global-step:785	 l-p:nan
epoch£º39	 i:6 	 global-step:786	 l-p:nan
epoch£º39	 i:7 	 global-step:787	 l-p:nan
epoch£º39	 i:8 	 global-step:788	 l-p:nan
epoch£º39	 i:9 	 global-step:789	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4287,  0.3233,  1.0000,  0.2437,
          1.0000,  0.7540, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5229,  0.4213,  1.0000,  0.3394,
          1.0000,  0.8056, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:nan
epoch£º40	 i:1 	 global-step:801	 l-p:nan
epoch£º40	 i:2 	 global-step:802	 l-p:nan
epoch£º40	 i:3 	 global-step:803	 l-p:nan
epoch£º40	 i:4 	 global-step:804	 l-p:nan
epoch£º40	 i:5 	 global-step:805	 l-p:nan
epoch£º40	 i:6 	 global-step:806	 l-p:nan
epoch£º40	 i:7 	 global-step:807	 l-p:nan
epoch£º40	 i:8 	 global-step:808	 l-p:nan
epoch£º40	 i:9 	 global-step:809	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:nan
epoch£º41	 i:1 	 global-step:821	 l-p:nan
epoch£º41	 i:2 	 global-step:822	 l-p:nan
epoch£º41	 i:3 	 global-step:823	 l-p:nan
epoch£º41	 i:4 	 global-step:824	 l-p:nan
epoch£º41	 i:5 	 global-step:825	 l-p:nan
epoch£º41	 i:6 	 global-step:826	 l-p:nan
epoch£º41	 i:7 	 global-step:827	 l-p:nan
epoch£º41	 i:8 	 global-step:828	 l-p:nan
epoch£º41	 i:9 	 global-step:829	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:nan
epoch£º42	 i:1 	 global-step:841	 l-p:nan
epoch£º42	 i:2 	 global-step:842	 l-p:nan
epoch£º42	 i:3 	 global-step:843	 l-p:nan
epoch£º42	 i:4 	 global-step:844	 l-p:nan
epoch£º42	 i:5 	 global-step:845	 l-p:nan
epoch£º42	 i:6 	 global-step:846	 l-p:nan
epoch£º42	 i:7 	 global-step:847	 l-p:nan
epoch£º42	 i:8 	 global-step:848	 l-p:nan
epoch£º42	 i:9 	 global-step:849	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:nan
epoch£º43	 i:1 	 global-step:861	 l-p:nan
epoch£º43	 i:2 	 global-step:862	 l-p:nan
epoch£º43	 i:3 	 global-step:863	 l-p:nan
epoch£º43	 i:4 	 global-step:864	 l-p:nan
epoch£º43	 i:5 	 global-step:865	 l-p:nan
epoch£º43	 i:6 	 global-step:866	 l-p:nan
epoch£º43	 i:7 	 global-step:867	 l-p:nan
epoch£º43	 i:8 	 global-step:868	 l-p:nan
epoch£º43	 i:9 	 global-step:869	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:nan
epoch£º44	 i:1 	 global-step:881	 l-p:nan
epoch£º44	 i:2 	 global-step:882	 l-p:nan
epoch£º44	 i:3 	 global-step:883	 l-p:nan
epoch£º44	 i:4 	 global-step:884	 l-p:nan
epoch£º44	 i:5 	 global-step:885	 l-p:nan
epoch£º44	 i:6 	 global-step:886	 l-p:nan
epoch£º44	 i:7 	 global-step:887	 l-p:nan
epoch£º44	 i:8 	 global-step:888	 l-p:nan
epoch£º44	 i:9 	 global-step:889	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:nan
epoch£º45	 i:1 	 global-step:901	 l-p:nan
epoch£º45	 i:2 	 global-step:902	 l-p:nan
epoch£º45	 i:3 	 global-step:903	 l-p:nan
epoch£º45	 i:4 	 global-step:904	 l-p:nan
epoch£º45	 i:5 	 global-step:905	 l-p:nan
epoch£º45	 i:6 	 global-step:906	 l-p:nan
epoch£º45	 i:7 	 global-step:907	 l-p:nan
epoch£º45	 i:8 	 global-step:908	 l-p:nan
epoch£º45	 i:9 	 global-step:909	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:nan
epoch£º46	 i:1 	 global-step:921	 l-p:nan
epoch£º46	 i:2 	 global-step:922	 l-p:nan
epoch£º46	 i:3 	 global-step:923	 l-p:nan
epoch£º46	 i:4 	 global-step:924	 l-p:nan
epoch£º46	 i:5 	 global-step:925	 l-p:nan
epoch£º46	 i:6 	 global-step:926	 l-p:nan
epoch£º46	 i:7 	 global-step:927	 l-p:nan
epoch£º46	 i:8 	 global-step:928	 l-p:nan
epoch£º46	 i:9 	 global-step:929	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:nan
epoch£º47	 i:1 	 global-step:941	 l-p:nan
epoch£º47	 i:2 	 global-step:942	 l-p:nan
epoch£º47	 i:3 	 global-step:943	 l-p:nan
epoch£º47	 i:4 	 global-step:944	 l-p:nan
epoch£º47	 i:5 	 global-step:945	 l-p:nan
epoch£º47	 i:6 	 global-step:946	 l-p:nan
epoch£º47	 i:7 	 global-step:947	 l-p:nan
epoch£º47	 i:8 	 global-step:948	 l-p:nan
epoch£º47	 i:9 	 global-step:949	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:nan
epoch£º48	 i:1 	 global-step:961	 l-p:nan
epoch£º48	 i:2 	 global-step:962	 l-p:nan
epoch£º48	 i:3 	 global-step:963	 l-p:nan
epoch£º48	 i:4 	 global-step:964	 l-p:nan
epoch£º48	 i:5 	 global-step:965	 l-p:nan
epoch£º48	 i:6 	 global-step:966	 l-p:nan
epoch£º48	 i:7 	 global-step:967	 l-p:nan
epoch£º48	 i:8 	 global-step:968	 l-p:nan
epoch£º48	 i:9 	 global-step:969	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:nan
epoch£º49	 i:1 	 global-step:981	 l-p:nan
epoch£º49	 i:2 	 global-step:982	 l-p:nan
epoch£º49	 i:3 	 global-step:983	 l-p:nan
epoch£º49	 i:4 	 global-step:984	 l-p:nan
epoch£º49	 i:5 	 global-step:985	 l-p:nan
epoch£º49	 i:6 	 global-step:986	 l-p:nan
epoch£º49	 i:7 	 global-step:987	 l-p:nan
epoch£º49	 i:8 	 global-step:988	 l-p:nan
epoch£º49	 i:9 	 global-step:989	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:nan
epoch£º50	 i:1 	 global-step:1001	 l-p:nan
epoch£º50	 i:2 	 global-step:1002	 l-p:nan
epoch£º50	 i:3 	 global-step:1003	 l-p:nan
epoch£º50	 i:4 	 global-step:1004	 l-p:nan
epoch£º50	 i:5 	 global-step:1005	 l-p:nan
epoch£º50	 i:6 	 global-step:1006	 l-p:nan
epoch£º50	 i:7 	 global-step:1007	 l-p:nan
epoch£º50	 i:8 	 global-step:1008	 l-p:nan
epoch£º50	 i:9 	 global-step:1009	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:nan
epoch£º51	 i:1 	 global-step:1021	 l-p:nan
epoch£º51	 i:2 	 global-step:1022	 l-p:nan
epoch£º51	 i:3 	 global-step:1023	 l-p:nan
epoch£º51	 i:4 	 global-step:1024	 l-p:nan
epoch£º51	 i:5 	 global-step:1025	 l-p:nan
epoch£º51	 i:6 	 global-step:1026	 l-p:nan
epoch£º51	 i:7 	 global-step:1027	 l-p:nan
epoch£º51	 i:8 	 global-step:1028	 l-p:nan
epoch£º51	 i:9 	 global-step:1029	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:nan
epoch£º52	 i:1 	 global-step:1041	 l-p:nan
epoch£º52	 i:2 	 global-step:1042	 l-p:nan
epoch£º52	 i:3 	 global-step:1043	 l-p:nan
epoch£º52	 i:4 	 global-step:1044	 l-p:nan
epoch£º52	 i:5 	 global-step:1045	 l-p:nan
epoch£º52	 i:6 	 global-step:1046	 l-p:nan
epoch£º52	 i:7 	 global-step:1047	 l-p:nan
epoch£º52	 i:8 	 global-step:1048	 l-p:nan
epoch£º52	 i:9 	 global-step:1049	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:nan
epoch£º53	 i:1 	 global-step:1061	 l-p:nan
epoch£º53	 i:2 	 global-step:1062	 l-p:nan
epoch£º53	 i:3 	 global-step:1063	 l-p:nan
epoch£º53	 i:4 	 global-step:1064	 l-p:nan
epoch£º53	 i:5 	 global-step:1065	 l-p:nan
epoch£º53	 i:6 	 global-step:1066	 l-p:nan
epoch£º53	 i:7 	 global-step:1067	 l-p:nan
epoch£º53	 i:8 	 global-step:1068	 l-p:nan
epoch£º53	 i:9 	 global-step:1069	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:nan
epoch£º54	 i:1 	 global-step:1081	 l-p:nan
epoch£º54	 i:2 	 global-step:1082	 l-p:nan
epoch£º54	 i:3 	 global-step:1083	 l-p:nan
epoch£º54	 i:4 	 global-step:1084	 l-p:nan
epoch£º54	 i:5 	 global-step:1085	 l-p:nan
epoch£º54	 i:6 	 global-step:1086	 l-p:nan
epoch£º54	 i:7 	 global-step:1087	 l-p:nan
epoch£º54	 i:8 	 global-step:1088	 l-p:nan
epoch£º54	 i:9 	 global-step:1089	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:nan
epoch£º55	 i:1 	 global-step:1101	 l-p:nan
epoch£º55	 i:2 	 global-step:1102	 l-p:nan
epoch£º55	 i:3 	 global-step:1103	 l-p:nan
epoch£º55	 i:4 	 global-step:1104	 l-p:nan
epoch£º55	 i:5 	 global-step:1105	 l-p:nan
epoch£º55	 i:6 	 global-step:1106	 l-p:nan
epoch£º55	 i:7 	 global-step:1107	 l-p:nan
epoch£º55	 i:8 	 global-step:1108	 l-p:nan
epoch£º55	 i:9 	 global-step:1109	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:nan
epoch£º56	 i:1 	 global-step:1121	 l-p:nan
epoch£º56	 i:2 	 global-step:1122	 l-p:nan
epoch£º56	 i:3 	 global-step:1123	 l-p:nan
epoch£º56	 i:4 	 global-step:1124	 l-p:nan
epoch£º56	 i:5 	 global-step:1125	 l-p:nan
epoch£º56	 i:6 	 global-step:1126	 l-p:nan
epoch£º56	 i:7 	 global-step:1127	 l-p:nan
epoch£º56	 i:8 	 global-step:1128	 l-p:nan
epoch£º56	 i:9 	 global-step:1129	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:nan
epoch£º57	 i:1 	 global-step:1141	 l-p:nan
epoch£º57	 i:2 	 global-step:1142	 l-p:nan
epoch£º57	 i:3 	 global-step:1143	 l-p:nan
epoch£º57	 i:4 	 global-step:1144	 l-p:nan
epoch£º57	 i:5 	 global-step:1145	 l-p:nan
epoch£º57	 i:6 	 global-step:1146	 l-p:nan
epoch£º57	 i:7 	 global-step:1147	 l-p:nan
epoch£º57	 i:8 	 global-step:1148	 l-p:nan
epoch£º57	 i:9 	 global-step:1149	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:nan
epoch£º58	 i:1 	 global-step:1161	 l-p:nan
epoch£º58	 i:2 	 global-step:1162	 l-p:nan
epoch£º58	 i:3 	 global-step:1163	 l-p:nan
epoch£º58	 i:4 	 global-step:1164	 l-p:nan
epoch£º58	 i:5 	 global-step:1165	 l-p:nan
epoch£º58	 i:6 	 global-step:1166	 l-p:nan
epoch£º58	 i:7 	 global-step:1167	 l-p:nan
epoch£º58	 i:8 	 global-step:1168	 l-p:nan
epoch£º58	 i:9 	 global-step:1169	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:nan
epoch£º59	 i:1 	 global-step:1181	 l-p:nan
epoch£º59	 i:2 	 global-step:1182	 l-p:nan
epoch£º59	 i:3 	 global-step:1183	 l-p:nan
epoch£º59	 i:4 	 global-step:1184	 l-p:nan
epoch£º59	 i:5 	 global-step:1185	 l-p:nan
epoch£º59	 i:6 	 global-step:1186	 l-p:nan
epoch£º59	 i:7 	 global-step:1187	 l-p:nan
epoch£º59	 i:8 	 global-step:1188	 l-p:nan
epoch£º59	 i:9 	 global-step:1189	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3351,  0.2328,  1.0000,  0.1617,
          1.0000,  0.6946, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:nan
epoch£º60	 i:1 	 global-step:1201	 l-p:nan
epoch£º60	 i:2 	 global-step:1202	 l-p:nan
epoch£º60	 i:3 	 global-step:1203	 l-p:nan
epoch£º60	 i:4 	 global-step:1204	 l-p:nan
epoch£º60	 i:5 	 global-step:1205	 l-p:nan
epoch£º60	 i:6 	 global-step:1206	 l-p:nan
epoch£º60	 i:7 	 global-step:1207	 l-p:nan
epoch£º60	 i:8 	 global-step:1208	 l-p:nan
epoch£º60	 i:9 	 global-step:1209	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:nan
epoch£º61	 i:1 	 global-step:1221	 l-p:nan
epoch£º61	 i:2 	 global-step:1222	 l-p:nan
epoch£º61	 i:3 	 global-step:1223	 l-p:nan
epoch£º61	 i:4 	 global-step:1224	 l-p:nan
epoch£º61	 i:5 	 global-step:1225	 l-p:nan
epoch£º61	 i:6 	 global-step:1226	 l-p:nan
epoch£º61	 i:7 	 global-step:1227	 l-p:nan
epoch£º61	 i:8 	 global-step:1228	 l-p:nan
epoch£º61	 i:9 	 global-step:1229	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3086,  0.2086,  1.0000,  0.1410,
          1.0000,  0.6758, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5226,  0.4209,  1.0000,  0.3391,
          1.0000,  0.8055, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:nan
epoch£º62	 i:1 	 global-step:1241	 l-p:nan
epoch£º62	 i:2 	 global-step:1242	 l-p:nan
epoch£º62	 i:3 	 global-step:1243	 l-p:nan
epoch£º62	 i:4 	 global-step:1244	 l-p:nan
epoch£º62	 i:5 	 global-step:1245	 l-p:nan
epoch£º62	 i:6 	 global-step:1246	 l-p:nan
epoch£º62	 i:7 	 global-step:1247	 l-p:nan
epoch£º62	 i:8 	 global-step:1248	 l-p:nan
epoch£º62	 i:9 	 global-step:1249	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:nan
epoch£º63	 i:1 	 global-step:1261	 l-p:nan
epoch£º63	 i:2 	 global-step:1262	 l-p:nan
epoch£º63	 i:3 	 global-step:1263	 l-p:nan
epoch£º63	 i:4 	 global-step:1264	 l-p:nan
epoch£º63	 i:5 	 global-step:1265	 l-p:nan
epoch£º63	 i:6 	 global-step:1266	 l-p:nan
epoch£º63	 i:7 	 global-step:1267	 l-p:nan
epoch£º63	 i:8 	 global-step:1268	 l-p:nan
epoch£º63	 i:9 	 global-step:1269	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:nan
epoch£º64	 i:1 	 global-step:1281	 l-p:nan
epoch£º64	 i:2 	 global-step:1282	 l-p:nan
epoch£º64	 i:3 	 global-step:1283	 l-p:nan
epoch£º64	 i:4 	 global-step:1284	 l-p:nan
epoch£º64	 i:5 	 global-step:1285	 l-p:nan
epoch£º64	 i:6 	 global-step:1286	 l-p:nan
epoch£º64	 i:7 	 global-step:1287	 l-p:nan
epoch£º64	 i:8 	 global-step:1288	 l-p:nan
epoch£º64	 i:9 	 global-step:1289	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:nan
epoch£º65	 i:1 	 global-step:1301	 l-p:nan
epoch£º65	 i:2 	 global-step:1302	 l-p:nan
epoch£º65	 i:3 	 global-step:1303	 l-p:nan
epoch£º65	 i:4 	 global-step:1304	 l-p:nan
epoch£º65	 i:5 	 global-step:1305	 l-p:nan
epoch£º65	 i:6 	 global-step:1306	 l-p:nan
epoch£º65	 i:7 	 global-step:1307	 l-p:nan
epoch£º65	 i:8 	 global-step:1308	 l-p:nan
epoch£º65	 i:9 	 global-step:1309	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:nan
epoch£º66	 i:1 	 global-step:1321	 l-p:nan
epoch£º66	 i:2 	 global-step:1322	 l-p:nan
epoch£º66	 i:3 	 global-step:1323	 l-p:nan
epoch£º66	 i:4 	 global-step:1324	 l-p:nan
epoch£º66	 i:5 	 global-step:1325	 l-p:nan
epoch£º66	 i:6 	 global-step:1326	 l-p:nan
epoch£º66	 i:7 	 global-step:1327	 l-p:nan
epoch£º66	 i:8 	 global-step:1328	 l-p:nan
epoch£º66	 i:9 	 global-step:1329	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:nan
epoch£º67	 i:1 	 global-step:1341	 l-p:nan
epoch£º67	 i:2 	 global-step:1342	 l-p:nan
epoch£º67	 i:3 	 global-step:1343	 l-p:nan
epoch£º67	 i:4 	 global-step:1344	 l-p:nan
epoch£º67	 i:5 	 global-step:1345	 l-p:nan
epoch£º67	 i:6 	 global-step:1346	 l-p:nan
epoch£º67	 i:7 	 global-step:1347	 l-p:nan
epoch£º67	 i:8 	 global-step:1348	 l-p:nan
epoch£º67	 i:9 	 global-step:1349	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:nan
epoch£º68	 i:1 	 global-step:1361	 l-p:nan
epoch£º68	 i:2 	 global-step:1362	 l-p:nan
epoch£º68	 i:3 	 global-step:1363	 l-p:nan
epoch£º68	 i:4 	 global-step:1364	 l-p:nan
epoch£º68	 i:5 	 global-step:1365	 l-p:nan
epoch£º68	 i:6 	 global-step:1366	 l-p:nan
epoch£º68	 i:7 	 global-step:1367	 l-p:nan
epoch£º68	 i:8 	 global-step:1368	 l-p:nan
epoch£º68	 i:9 	 global-step:1369	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:nan
epoch£º69	 i:1 	 global-step:1381	 l-p:nan
epoch£º69	 i:2 	 global-step:1382	 l-p:nan
epoch£º69	 i:3 	 global-step:1383	 l-p:nan
epoch£º69	 i:4 	 global-step:1384	 l-p:nan
epoch£º69	 i:5 	 global-step:1385	 l-p:nan
epoch£º69	 i:6 	 global-step:1386	 l-p:nan
epoch£º69	 i:7 	 global-step:1387	 l-p:nan
epoch£º69	 i:8 	 global-step:1388	 l-p:nan
epoch£º69	 i:9 	 global-step:1389	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7943,  0.7356,  1.0000,  0.6812,
          1.0000,  0.9261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3591,  0.2552,  1.0000,  0.1814,
          1.0000,  0.7108, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3302,  0.2282,  1.0000,  0.1578,
          1.0000,  0.6912, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:nan
epoch£º70	 i:1 	 global-step:1401	 l-p:nan
epoch£º70	 i:2 	 global-step:1402	 l-p:nan
epoch£º70	 i:3 	 global-step:1403	 l-p:nan
epoch£º70	 i:4 	 global-step:1404	 l-p:nan
epoch£º70	 i:5 	 global-step:1405	 l-p:nan
epoch£º70	 i:6 	 global-step:1406	 l-p:nan
epoch£º70	 i:7 	 global-step:1407	 l-p:nan
epoch£º70	 i:8 	 global-step:1408	 l-p:nan
epoch£º70	 i:9 	 global-step:1409	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:nan
epoch£º71	 i:1 	 global-step:1421	 l-p:nan
epoch£º71	 i:2 	 global-step:1422	 l-p:nan
epoch£º71	 i:3 	 global-step:1423	 l-p:nan
epoch£º71	 i:4 	 global-step:1424	 l-p:nan
epoch£º71	 i:5 	 global-step:1425	 l-p:nan
epoch£º71	 i:6 	 global-step:1426	 l-p:nan
epoch£º71	 i:7 	 global-step:1427	 l-p:nan
epoch£º71	 i:8 	 global-step:1428	 l-p:nan
epoch£º71	 i:9 	 global-step:1429	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:nan
epoch£º72	 i:1 	 global-step:1441	 l-p:nan
epoch£º72	 i:2 	 global-step:1442	 l-p:nan
epoch£º72	 i:3 	 global-step:1443	 l-p:nan
epoch£º72	 i:4 	 global-step:1444	 l-p:nan
epoch£º72	 i:5 	 global-step:1445	 l-p:nan
epoch£º72	 i:6 	 global-step:1446	 l-p:nan
epoch£º72	 i:7 	 global-step:1447	 l-p:nan
epoch£º72	 i:8 	 global-step:1448	 l-p:nan
epoch£º72	 i:9 	 global-step:1449	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:nan
epoch£º73	 i:1 	 global-step:1461	 l-p:nan
epoch£º73	 i:2 	 global-step:1462	 l-p:nan
epoch£º73	 i:3 	 global-step:1463	 l-p:nan
epoch£º73	 i:4 	 global-step:1464	 l-p:nan
epoch£º73	 i:5 	 global-step:1465	 l-p:nan
epoch£º73	 i:6 	 global-step:1466	 l-p:nan
epoch£º73	 i:7 	 global-step:1467	 l-p:nan
epoch£º73	 i:8 	 global-step:1468	 l-p:nan
epoch£º73	 i:9 	 global-step:1469	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:nan
epoch£º74	 i:1 	 global-step:1481	 l-p:nan
epoch£º74	 i:2 	 global-step:1482	 l-p:nan
epoch£º74	 i:3 	 global-step:1483	 l-p:nan
epoch£º74	 i:4 	 global-step:1484	 l-p:nan
epoch£º74	 i:5 	 global-step:1485	 l-p:nan
epoch£º74	 i:6 	 global-step:1486	 l-p:nan
epoch£º74	 i:7 	 global-step:1487	 l-p:nan
epoch£º74	 i:8 	 global-step:1488	 l-p:nan
epoch£º74	 i:9 	 global-step:1489	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:nan
epoch£º75	 i:1 	 global-step:1501	 l-p:nan
epoch£º75	 i:2 	 global-step:1502	 l-p:nan
epoch£º75	 i:3 	 global-step:1503	 l-p:nan
epoch£º75	 i:4 	 global-step:1504	 l-p:nan
epoch£º75	 i:5 	 global-step:1505	 l-p:nan
epoch£º75	 i:6 	 global-step:1506	 l-p:nan
epoch£º75	 i:7 	 global-step:1507	 l-p:nan
epoch£º75	 i:8 	 global-step:1508	 l-p:nan
epoch£º75	 i:9 	 global-step:1509	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:nan
epoch£º76	 i:1 	 global-step:1521	 l-p:nan
epoch£º76	 i:2 	 global-step:1522	 l-p:nan
epoch£º76	 i:3 	 global-step:1523	 l-p:nan
epoch£º76	 i:4 	 global-step:1524	 l-p:nan
epoch£º76	 i:5 	 global-step:1525	 l-p:nan
epoch£º76	 i:6 	 global-step:1526	 l-p:nan
epoch£º76	 i:7 	 global-step:1527	 l-p:nan
epoch£º76	 i:8 	 global-step:1528	 l-p:nan
epoch£º76	 i:9 	 global-step:1529	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:nan
epoch£º77	 i:1 	 global-step:1541	 l-p:nan
epoch£º77	 i:2 	 global-step:1542	 l-p:nan
epoch£º77	 i:3 	 global-step:1543	 l-p:nan
epoch£º77	 i:4 	 global-step:1544	 l-p:nan
epoch£º77	 i:5 	 global-step:1545	 l-p:nan
epoch£º77	 i:6 	 global-step:1546	 l-p:nan
epoch£º77	 i:7 	 global-step:1547	 l-p:nan
epoch£º77	 i:8 	 global-step:1548	 l-p:nan
epoch£º77	 i:9 	 global-step:1549	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:nan
epoch£º78	 i:1 	 global-step:1561	 l-p:nan
epoch£º78	 i:2 	 global-step:1562	 l-p:nan
epoch£º78	 i:3 	 global-step:1563	 l-p:nan
epoch£º78	 i:4 	 global-step:1564	 l-p:nan
epoch£º78	 i:5 	 global-step:1565	 l-p:nan
epoch£º78	 i:6 	 global-step:1566	 l-p:nan
epoch£º78	 i:7 	 global-step:1567	 l-p:nan
epoch£º78	 i:8 	 global-step:1568	 l-p:nan
epoch£º78	 i:9 	 global-step:1569	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:nan
epoch£º79	 i:1 	 global-step:1581	 l-p:nan
epoch£º79	 i:2 	 global-step:1582	 l-p:nan
epoch£º79	 i:3 	 global-step:1583	 l-p:nan
epoch£º79	 i:4 	 global-step:1584	 l-p:nan
epoch£º79	 i:5 	 global-step:1585	 l-p:nan
epoch£º79	 i:6 	 global-step:1586	 l-p:nan
epoch£º79	 i:7 	 global-step:1587	 l-p:nan
epoch£º79	 i:8 	 global-step:1588	 l-p:nan
epoch£º79	 i:9 	 global-step:1589	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:nan
epoch£º80	 i:1 	 global-step:1601	 l-p:nan
epoch£º80	 i:2 	 global-step:1602	 l-p:nan
epoch£º80	 i:3 	 global-step:1603	 l-p:nan
epoch£º80	 i:4 	 global-step:1604	 l-p:nan
epoch£º80	 i:5 	 global-step:1605	 l-p:nan
epoch£º80	 i:6 	 global-step:1606	 l-p:nan
epoch£º80	 i:7 	 global-step:1607	 l-p:nan
epoch£º80	 i:8 	 global-step:1608	 l-p:nan
epoch£º80	 i:9 	 global-step:1609	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:nan
epoch£º81	 i:1 	 global-step:1621	 l-p:nan
epoch£º81	 i:2 	 global-step:1622	 l-p:nan
epoch£º81	 i:3 	 global-step:1623	 l-p:nan
epoch£º81	 i:4 	 global-step:1624	 l-p:nan
epoch£º81	 i:5 	 global-step:1625	 l-p:nan
epoch£º81	 i:6 	 global-step:1626	 l-p:nan
epoch£º81	 i:7 	 global-step:1627	 l-p:nan
epoch£º81	 i:8 	 global-step:1628	 l-p:nan
epoch£º81	 i:9 	 global-step:1629	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:nan
epoch£º82	 i:1 	 global-step:1641	 l-p:nan
epoch£º82	 i:2 	 global-step:1642	 l-p:nan
epoch£º82	 i:3 	 global-step:1643	 l-p:nan
epoch£º82	 i:4 	 global-step:1644	 l-p:nan
epoch£º82	 i:5 	 global-step:1645	 l-p:nan
epoch£º82	 i:6 	 global-step:1646	 l-p:nan
epoch£º82	 i:7 	 global-step:1647	 l-p:nan
epoch£º82	 i:8 	 global-step:1648	 l-p:nan
epoch£º82	 i:9 	 global-step:1649	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:nan
epoch£º83	 i:1 	 global-step:1661	 l-p:nan
epoch£º83	 i:2 	 global-step:1662	 l-p:nan
epoch£º83	 i:3 	 global-step:1663	 l-p:nan
epoch£º83	 i:4 	 global-step:1664	 l-p:nan
epoch£º83	 i:5 	 global-step:1665	 l-p:nan
epoch£º83	 i:6 	 global-step:1666	 l-p:nan
epoch£º83	 i:7 	 global-step:1667	 l-p:nan
epoch£º83	 i:8 	 global-step:1668	 l-p:nan
epoch£º83	 i:9 	 global-step:1669	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:nan
epoch£º84	 i:1 	 global-step:1681	 l-p:nan
epoch£º84	 i:2 	 global-step:1682	 l-p:nan
epoch£º84	 i:3 	 global-step:1683	 l-p:nan
epoch£º84	 i:4 	 global-step:1684	 l-p:nan
epoch£º84	 i:5 	 global-step:1685	 l-p:nan
epoch£º84	 i:6 	 global-step:1686	 l-p:nan
epoch£º84	 i:7 	 global-step:1687	 l-p:nan
epoch£º84	 i:8 	 global-step:1688	 l-p:nan
epoch£º84	 i:9 	 global-step:1689	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:nan
epoch£º85	 i:1 	 global-step:1701	 l-p:nan
epoch£º85	 i:2 	 global-step:1702	 l-p:nan
epoch£º85	 i:3 	 global-step:1703	 l-p:nan
epoch£º85	 i:4 	 global-step:1704	 l-p:nan
epoch£º85	 i:5 	 global-step:1705	 l-p:nan
epoch£º85	 i:6 	 global-step:1706	 l-p:nan
epoch£º85	 i:7 	 global-step:1707	 l-p:nan
epoch£º85	 i:8 	 global-step:1708	 l-p:nan
epoch£º85	 i:9 	 global-step:1709	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:nan
epoch£º86	 i:1 	 global-step:1721	 l-p:nan
epoch£º86	 i:2 	 global-step:1722	 l-p:nan
epoch£º86	 i:3 	 global-step:1723	 l-p:nan
epoch£º86	 i:4 	 global-step:1724	 l-p:nan
epoch£º86	 i:5 	 global-step:1725	 l-p:nan
epoch£º86	 i:6 	 global-step:1726	 l-p:nan
epoch£º86	 i:7 	 global-step:1727	 l-p:nan
epoch£º86	 i:8 	 global-step:1728	 l-p:nan
epoch£º86	 i:9 	 global-step:1729	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:nan
epoch£º87	 i:1 	 global-step:1741	 l-p:nan
epoch£º87	 i:2 	 global-step:1742	 l-p:nan
epoch£º87	 i:3 	 global-step:1743	 l-p:nan
epoch£º87	 i:4 	 global-step:1744	 l-p:nan
epoch£º87	 i:5 	 global-step:1745	 l-p:nan
epoch£º87	 i:6 	 global-step:1746	 l-p:nan
epoch£º87	 i:7 	 global-step:1747	 l-p:nan
epoch£º87	 i:8 	 global-step:1748	 l-p:nan
epoch£º87	 i:9 	 global-step:1749	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:nan
epoch£º88	 i:1 	 global-step:1761	 l-p:nan
epoch£º88	 i:2 	 global-step:1762	 l-p:nan
epoch£º88	 i:3 	 global-step:1763	 l-p:nan
epoch£º88	 i:4 	 global-step:1764	 l-p:nan
epoch£º88	 i:5 	 global-step:1765	 l-p:nan
epoch£º88	 i:6 	 global-step:1766	 l-p:nan
epoch£º88	 i:7 	 global-step:1767	 l-p:nan
epoch£º88	 i:8 	 global-step:1768	 l-p:nan
epoch£º88	 i:9 	 global-step:1769	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:nan
epoch£º89	 i:1 	 global-step:1781	 l-p:nan
epoch£º89	 i:2 	 global-step:1782	 l-p:nan
epoch£º89	 i:3 	 global-step:1783	 l-p:nan
epoch£º89	 i:4 	 global-step:1784	 l-p:nan
epoch£º89	 i:5 	 global-step:1785	 l-p:nan
epoch£º89	 i:6 	 global-step:1786	 l-p:nan
epoch£º89	 i:7 	 global-step:1787	 l-p:nan
epoch£º89	 i:8 	 global-step:1788	 l-p:nan
epoch£º89	 i:9 	 global-step:1789	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:nan
epoch£º90	 i:1 	 global-step:1801	 l-p:nan
epoch£º90	 i:2 	 global-step:1802	 l-p:nan
epoch£º90	 i:3 	 global-step:1803	 l-p:nan
epoch£º90	 i:4 	 global-step:1804	 l-p:nan
epoch£º90	 i:5 	 global-step:1805	 l-p:nan
epoch£º90	 i:6 	 global-step:1806	 l-p:nan
epoch£º90	 i:7 	 global-step:1807	 l-p:nan
epoch£º90	 i:8 	 global-step:1808	 l-p:nan
epoch£º90	 i:9 	 global-step:1809	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:nan
epoch£º91	 i:1 	 global-step:1821	 l-p:nan
epoch£º91	 i:2 	 global-step:1822	 l-p:nan
epoch£º91	 i:3 	 global-step:1823	 l-p:nan
epoch£º91	 i:4 	 global-step:1824	 l-p:nan
epoch£º91	 i:5 	 global-step:1825	 l-p:nan
epoch£º91	 i:6 	 global-step:1826	 l-p:nan
epoch£º91	 i:7 	 global-step:1827	 l-p:nan
epoch£º91	 i:8 	 global-step:1828	 l-p:nan
epoch£º91	 i:9 	 global-step:1829	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:nan
epoch£º92	 i:1 	 global-step:1841	 l-p:nan
epoch£º92	 i:2 	 global-step:1842	 l-p:nan
epoch£º92	 i:3 	 global-step:1843	 l-p:nan
epoch£º92	 i:4 	 global-step:1844	 l-p:nan
epoch£º92	 i:5 	 global-step:1845	 l-p:nan
epoch£º92	 i:6 	 global-step:1846	 l-p:nan
epoch£º92	 i:7 	 global-step:1847	 l-p:nan
epoch£º92	 i:8 	 global-step:1848	 l-p:nan
epoch£º92	 i:9 	 global-step:1849	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7393,  0.6685,  1.0000,  0.6044,
          1.0000,  0.9042, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:nan
epoch£º93	 i:1 	 global-step:1861	 l-p:nan
epoch£º93	 i:2 	 global-step:1862	 l-p:nan
epoch£º93	 i:3 	 global-step:1863	 l-p:nan
epoch£º93	 i:4 	 global-step:1864	 l-p:nan
epoch£º93	 i:5 	 global-step:1865	 l-p:nan
epoch£º93	 i:6 	 global-step:1866	 l-p:nan
epoch£º93	 i:7 	 global-step:1867	 l-p:nan
epoch£º93	 i:8 	 global-step:1868	 l-p:nan
epoch£º93	 i:9 	 global-step:1869	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:nan
epoch£º94	 i:1 	 global-step:1881	 l-p:nan
epoch£º94	 i:2 	 global-step:1882	 l-p:nan
epoch£º94	 i:3 	 global-step:1883	 l-p:nan
epoch£º94	 i:4 	 global-step:1884	 l-p:nan
epoch£º94	 i:5 	 global-step:1885	 l-p:nan
epoch£º94	 i:6 	 global-step:1886	 l-p:nan
epoch£º94	 i:7 	 global-step:1887	 l-p:nan
epoch£º94	 i:8 	 global-step:1888	 l-p:nan
epoch£º94	 i:9 	 global-step:1889	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:nan
epoch£º95	 i:1 	 global-step:1901	 l-p:nan
epoch£º95	 i:2 	 global-step:1902	 l-p:nan
epoch£º95	 i:3 	 global-step:1903	 l-p:nan
epoch£º95	 i:4 	 global-step:1904	 l-p:nan
epoch£º95	 i:5 	 global-step:1905	 l-p:nan
epoch£º95	 i:6 	 global-step:1906	 l-p:nan
epoch£º95	 i:7 	 global-step:1907	 l-p:nan
epoch£º95	 i:8 	 global-step:1908	 l-p:nan
epoch£º95	 i:9 	 global-step:1909	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:nan
epoch£º96	 i:1 	 global-step:1921	 l-p:nan
epoch£º96	 i:2 	 global-step:1922	 l-p:nan
epoch£º96	 i:3 	 global-step:1923	 l-p:nan
epoch£º96	 i:4 	 global-step:1924	 l-p:nan
epoch£º96	 i:5 	 global-step:1925	 l-p:nan
epoch£º96	 i:6 	 global-step:1926	 l-p:nan
epoch£º96	 i:7 	 global-step:1927	 l-p:nan
epoch£º96	 i:8 	 global-step:1928	 l-p:nan
epoch£º96	 i:9 	 global-step:1929	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:nan
epoch£º97	 i:1 	 global-step:1941	 l-p:nan
epoch£º97	 i:2 	 global-step:1942	 l-p:nan
epoch£º97	 i:3 	 global-step:1943	 l-p:nan
epoch£º97	 i:4 	 global-step:1944	 l-p:nan
epoch£º97	 i:5 	 global-step:1945	 l-p:nan
epoch£º97	 i:6 	 global-step:1946	 l-p:nan
epoch£º97	 i:7 	 global-step:1947	 l-p:nan
epoch£º97	 i:8 	 global-step:1948	 l-p:nan
epoch£º97	 i:9 	 global-step:1949	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:nan
epoch£º98	 i:1 	 global-step:1961	 l-p:nan
epoch£º98	 i:2 	 global-step:1962	 l-p:nan
epoch£º98	 i:3 	 global-step:1963	 l-p:nan
epoch£º98	 i:4 	 global-step:1964	 l-p:nan
epoch£º98	 i:5 	 global-step:1965	 l-p:nan
epoch£º98	 i:6 	 global-step:1966	 l-p:nan
epoch£º98	 i:7 	 global-step:1967	 l-p:nan
epoch£º98	 i:8 	 global-step:1968	 l-p:nan
epoch£º98	 i:9 	 global-step:1969	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:nan
epoch£º99	 i:1 	 global-step:1981	 l-p:nan
epoch£º99	 i:2 	 global-step:1982	 l-p:nan
epoch£º99	 i:3 	 global-step:1983	 l-p:nan
epoch£º99	 i:4 	 global-step:1984	 l-p:nan
epoch£º99	 i:5 	 global-step:1985	 l-p:nan
epoch£º99	 i:6 	 global-step:1986	 l-p:nan
epoch£º99	 i:7 	 global-step:1987	 l-p:nan
epoch£º99	 i:8 	 global-step:1988	 l-p:nan
epoch£º99	 i:9 	 global-step:1989	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:nan
epoch£º100	 i:1 	 global-step:2001	 l-p:nan
epoch£º100	 i:2 	 global-step:2002	 l-p:nan
epoch£º100	 i:3 	 global-step:2003	 l-p:nan
epoch£º100	 i:4 	 global-step:2004	 l-p:nan
epoch£º100	 i:5 	 global-step:2005	 l-p:nan
epoch£º100	 i:6 	 global-step:2006	 l-p:nan
epoch£º100	 i:7 	 global-step:2007	 l-p:nan
epoch£º100	 i:8 	 global-step:2008	 l-p:nan
epoch£º100	 i:9 	 global-step:2009	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:nan
epoch£º101	 i:1 	 global-step:2021	 l-p:nan
epoch£º101	 i:2 	 global-step:2022	 l-p:nan
epoch£º101	 i:3 	 global-step:2023	 l-p:nan
epoch£º101	 i:4 	 global-step:2024	 l-p:nan
epoch£º101	 i:5 	 global-step:2025	 l-p:nan
epoch£º101	 i:6 	 global-step:2026	 l-p:nan
epoch£º101	 i:7 	 global-step:2027	 l-p:nan
epoch£º101	 i:8 	 global-step:2028	 l-p:nan
epoch£º101	 i:9 	 global-step:2029	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:nan
epoch£º102	 i:1 	 global-step:2041	 l-p:nan
epoch£º102	 i:2 	 global-step:2042	 l-p:nan
epoch£º102	 i:3 	 global-step:2043	 l-p:nan
epoch£º102	 i:4 	 global-step:2044	 l-p:nan
epoch£º102	 i:5 	 global-step:2045	 l-p:nan
epoch£º102	 i:6 	 global-step:2046	 l-p:nan
epoch£º102	 i:7 	 global-step:2047	 l-p:nan
epoch£º102	 i:8 	 global-step:2048	 l-p:nan
epoch£º102	 i:9 	 global-step:2049	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:nan
epoch£º103	 i:1 	 global-step:2061	 l-p:nan
epoch£º103	 i:2 	 global-step:2062	 l-p:nan
epoch£º103	 i:3 	 global-step:2063	 l-p:nan
epoch£º103	 i:4 	 global-step:2064	 l-p:nan
epoch£º103	 i:5 	 global-step:2065	 l-p:nan
epoch£º103	 i:6 	 global-step:2066	 l-p:nan
epoch£º103	 i:7 	 global-step:2067	 l-p:nan
epoch£º103	 i:8 	 global-step:2068	 l-p:nan
epoch£º103	 i:9 	 global-step:2069	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:nan
epoch£º104	 i:1 	 global-step:2081	 l-p:nan
epoch£º104	 i:2 	 global-step:2082	 l-p:nan
epoch£º104	 i:3 	 global-step:2083	 l-p:nan
epoch£º104	 i:4 	 global-step:2084	 l-p:nan
epoch£º104	 i:5 	 global-step:2085	 l-p:nan
epoch£º104	 i:6 	 global-step:2086	 l-p:nan
epoch£º104	 i:7 	 global-step:2087	 l-p:nan
epoch£º104	 i:8 	 global-step:2088	 l-p:nan
epoch£º104	 i:9 	 global-step:2089	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:nan
epoch£º105	 i:1 	 global-step:2101	 l-p:nan
epoch£º105	 i:2 	 global-step:2102	 l-p:nan
epoch£º105	 i:3 	 global-step:2103	 l-p:nan
epoch£º105	 i:4 	 global-step:2104	 l-p:nan
epoch£º105	 i:5 	 global-step:2105	 l-p:nan
epoch£º105	 i:6 	 global-step:2106	 l-p:nan
epoch£º105	 i:7 	 global-step:2107	 l-p:nan
epoch£º105	 i:8 	 global-step:2108	 l-p:nan
epoch£º105	 i:9 	 global-step:2109	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:nan
epoch£º106	 i:1 	 global-step:2121	 l-p:nan
epoch£º106	 i:2 	 global-step:2122	 l-p:nan
epoch£º106	 i:3 	 global-step:2123	 l-p:nan
epoch£º106	 i:4 	 global-step:2124	 l-p:nan
epoch£º106	 i:5 	 global-step:2125	 l-p:nan
epoch£º106	 i:6 	 global-step:2126	 l-p:nan
epoch£º106	 i:7 	 global-step:2127	 l-p:nan
epoch£º106	 i:8 	 global-step:2128	 l-p:nan
epoch£º106	 i:9 	 global-step:2129	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:nan
epoch£º107	 i:1 	 global-step:2141	 l-p:nan
epoch£º107	 i:2 	 global-step:2142	 l-p:nan
epoch£º107	 i:3 	 global-step:2143	 l-p:nan
epoch£º107	 i:4 	 global-step:2144	 l-p:nan
epoch£º107	 i:5 	 global-step:2145	 l-p:nan
epoch£º107	 i:6 	 global-step:2146	 l-p:nan
epoch£º107	 i:7 	 global-step:2147	 l-p:nan
epoch£º107	 i:8 	 global-step:2148	 l-p:nan
epoch£º107	 i:9 	 global-step:2149	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:nan
epoch£º108	 i:1 	 global-step:2161	 l-p:nan
epoch£º108	 i:2 	 global-step:2162	 l-p:nan
epoch£º108	 i:3 	 global-step:2163	 l-p:nan
epoch£º108	 i:4 	 global-step:2164	 l-p:nan
epoch£º108	 i:5 	 global-step:2165	 l-p:nan
epoch£º108	 i:6 	 global-step:2166	 l-p:nan
epoch£º108	 i:7 	 global-step:2167	 l-p:nan
epoch£º108	 i:8 	 global-step:2168	 l-p:nan
epoch£º108	 i:9 	 global-step:2169	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:nan
epoch£º109	 i:1 	 global-step:2181	 l-p:nan
epoch£º109	 i:2 	 global-step:2182	 l-p:nan
epoch£º109	 i:3 	 global-step:2183	 l-p:nan
epoch£º109	 i:4 	 global-step:2184	 l-p:nan
epoch£º109	 i:5 	 global-step:2185	 l-p:nan
epoch£º109	 i:6 	 global-step:2186	 l-p:nan
epoch£º109	 i:7 	 global-step:2187	 l-p:nan
epoch£º109	 i:8 	 global-step:2188	 l-p:nan
epoch£º109	 i:9 	 global-step:2189	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:nan
epoch£º110	 i:1 	 global-step:2201	 l-p:nan
epoch£º110	 i:2 	 global-step:2202	 l-p:nan
epoch£º110	 i:3 	 global-step:2203	 l-p:nan
epoch£º110	 i:4 	 global-step:2204	 l-p:nan
epoch£º110	 i:5 	 global-step:2205	 l-p:nan
epoch£º110	 i:6 	 global-step:2206	 l-p:nan
epoch£º110	 i:7 	 global-step:2207	 l-p:nan
epoch£º110	 i:8 	 global-step:2208	 l-p:nan
epoch£º110	 i:9 	 global-step:2209	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:nan
epoch£º111	 i:1 	 global-step:2221	 l-p:nan
epoch£º111	 i:2 	 global-step:2222	 l-p:nan
epoch£º111	 i:3 	 global-step:2223	 l-p:nan
epoch£º111	 i:4 	 global-step:2224	 l-p:nan
epoch£º111	 i:5 	 global-step:2225	 l-p:nan
epoch£º111	 i:6 	 global-step:2226	 l-p:nan
epoch£º111	 i:7 	 global-step:2227	 l-p:nan
epoch£º111	 i:8 	 global-step:2228	 l-p:nan
epoch£º111	 i:9 	 global-step:2229	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:nan
epoch£º112	 i:1 	 global-step:2241	 l-p:nan
epoch£º112	 i:2 	 global-step:2242	 l-p:nan
epoch£º112	 i:3 	 global-step:2243	 l-p:nan
epoch£º112	 i:4 	 global-step:2244	 l-p:nan
epoch£º112	 i:5 	 global-step:2245	 l-p:nan
epoch£º112	 i:6 	 global-step:2246	 l-p:nan
epoch£º112	 i:7 	 global-step:2247	 l-p:nan
epoch£º112	 i:8 	 global-step:2248	 l-p:nan
epoch£º112	 i:9 	 global-step:2249	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:nan
epoch£º113	 i:1 	 global-step:2261	 l-p:nan
epoch£º113	 i:2 	 global-step:2262	 l-p:nan
epoch£º113	 i:3 	 global-step:2263	 l-p:nan
epoch£º113	 i:4 	 global-step:2264	 l-p:nan
epoch£º113	 i:5 	 global-step:2265	 l-p:nan
epoch£º113	 i:6 	 global-step:2266	 l-p:nan
epoch£º113	 i:7 	 global-step:2267	 l-p:nan
epoch£º113	 i:8 	 global-step:2268	 l-p:nan
epoch£º113	 i:9 	 global-step:2269	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:nan
epoch£º114	 i:1 	 global-step:2281	 l-p:nan
epoch£º114	 i:2 	 global-step:2282	 l-p:nan
epoch£º114	 i:3 	 global-step:2283	 l-p:nan
epoch£º114	 i:4 	 global-step:2284	 l-p:nan
epoch£º114	 i:5 	 global-step:2285	 l-p:nan
epoch£º114	 i:6 	 global-step:2286	 l-p:nan
epoch£º114	 i:7 	 global-step:2287	 l-p:nan
epoch£º114	 i:8 	 global-step:2288	 l-p:nan
epoch£º114	 i:9 	 global-step:2289	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:nan
epoch£º115	 i:1 	 global-step:2301	 l-p:nan
epoch£º115	 i:2 	 global-step:2302	 l-p:nan
epoch£º115	 i:3 	 global-step:2303	 l-p:nan
epoch£º115	 i:4 	 global-step:2304	 l-p:nan
epoch£º115	 i:5 	 global-step:2305	 l-p:nan
epoch£º115	 i:6 	 global-step:2306	 l-p:nan
epoch£º115	 i:7 	 global-step:2307	 l-p:nan
epoch£º115	 i:8 	 global-step:2308	 l-p:nan
epoch£º115	 i:9 	 global-step:2309	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:nan
epoch£º116	 i:1 	 global-step:2321	 l-p:nan
epoch£º116	 i:2 	 global-step:2322	 l-p:nan
epoch£º116	 i:3 	 global-step:2323	 l-p:nan
epoch£º116	 i:4 	 global-step:2324	 l-p:nan
epoch£º116	 i:5 	 global-step:2325	 l-p:nan
epoch£º116	 i:6 	 global-step:2326	 l-p:nan
epoch£º116	 i:7 	 global-step:2327	 l-p:nan
epoch£º116	 i:8 	 global-step:2328	 l-p:nan
epoch£º116	 i:9 	 global-step:2329	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:nan
epoch£º117	 i:1 	 global-step:2341	 l-p:nan
epoch£º117	 i:2 	 global-step:2342	 l-p:nan
epoch£º117	 i:3 	 global-step:2343	 l-p:nan
epoch£º117	 i:4 	 global-step:2344	 l-p:nan
epoch£º117	 i:5 	 global-step:2345	 l-p:nan
epoch£º117	 i:6 	 global-step:2346	 l-p:nan
epoch£º117	 i:7 	 global-step:2347	 l-p:nan
epoch£º117	 i:8 	 global-step:2348	 l-p:nan
epoch£º117	 i:9 	 global-step:2349	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:nan
epoch£º118	 i:1 	 global-step:2361	 l-p:nan
epoch£º118	 i:2 	 global-step:2362	 l-p:nan
epoch£º118	 i:3 	 global-step:2363	 l-p:nan
epoch£º118	 i:4 	 global-step:2364	 l-p:nan
epoch£º118	 i:5 	 global-step:2365	 l-p:nan
epoch£º118	 i:6 	 global-step:2366	 l-p:nan
epoch£º118	 i:7 	 global-step:2367	 l-p:nan
epoch£º118	 i:8 	 global-step:2368	 l-p:nan
epoch£º118	 i:9 	 global-step:2369	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4183,  0.3128,  1.0000,  0.2339,
          1.0000,  0.7479, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5183,  0.4163,  1.0000,  0.3344,
          1.0000,  0.8033, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:nan
epoch£º119	 i:1 	 global-step:2381	 l-p:nan
epoch£º119	 i:2 	 global-step:2382	 l-p:nan
epoch£º119	 i:3 	 global-step:2383	 l-p:nan
epoch£º119	 i:4 	 global-step:2384	 l-p:nan
epoch£º119	 i:5 	 global-step:2385	 l-p:nan
epoch£º119	 i:6 	 global-step:2386	 l-p:nan
epoch£º119	 i:7 	 global-step:2387	 l-p:nan
epoch£º119	 i:8 	 global-step:2388	 l-p:nan
epoch£º119	 i:9 	 global-step:2389	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:nan
epoch£º120	 i:1 	 global-step:2401	 l-p:nan
epoch£º120	 i:2 	 global-step:2402	 l-p:nan
epoch£º120	 i:3 	 global-step:2403	 l-p:nan
epoch£º120	 i:4 	 global-step:2404	 l-p:nan
epoch£º120	 i:5 	 global-step:2405	 l-p:nan
epoch£º120	 i:6 	 global-step:2406	 l-p:nan
epoch£º120	 i:7 	 global-step:2407	 l-p:nan
epoch£º120	 i:8 	 global-step:2408	 l-p:nan
epoch£º120	 i:9 	 global-step:2409	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:nan
epoch£º121	 i:1 	 global-step:2421	 l-p:nan
epoch£º121	 i:2 	 global-step:2422	 l-p:nan
epoch£º121	 i:3 	 global-step:2423	 l-p:nan
epoch£º121	 i:4 	 global-step:2424	 l-p:nan
epoch£º121	 i:5 	 global-step:2425	 l-p:nan
epoch£º121	 i:6 	 global-step:2426	 l-p:nan
epoch£º121	 i:7 	 global-step:2427	 l-p:nan
epoch£º121	 i:8 	 global-step:2428	 l-p:nan
epoch£º121	 i:9 	 global-step:2429	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:nan
epoch£º122	 i:1 	 global-step:2441	 l-p:nan
epoch£º122	 i:2 	 global-step:2442	 l-p:nan
epoch£º122	 i:3 	 global-step:2443	 l-p:nan
epoch£º122	 i:4 	 global-step:2444	 l-p:nan
epoch£º122	 i:5 	 global-step:2445	 l-p:nan
epoch£º122	 i:6 	 global-step:2446	 l-p:nan
epoch£º122	 i:7 	 global-step:2447	 l-p:nan
epoch£º122	 i:8 	 global-step:2448	 l-p:nan
epoch£º122	 i:9 	 global-step:2449	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:nan
epoch£º123	 i:1 	 global-step:2461	 l-p:nan
epoch£º123	 i:2 	 global-step:2462	 l-p:nan
epoch£º123	 i:3 	 global-step:2463	 l-p:nan
epoch£º123	 i:4 	 global-step:2464	 l-p:nan
epoch£º123	 i:5 	 global-step:2465	 l-p:nan
epoch£º123	 i:6 	 global-step:2466	 l-p:nan
epoch£º123	 i:7 	 global-step:2467	 l-p:nan
epoch£º123	 i:8 	 global-step:2468	 l-p:nan
epoch£º123	 i:9 	 global-step:2469	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:nan
epoch£º124	 i:1 	 global-step:2481	 l-p:nan
epoch£º124	 i:2 	 global-step:2482	 l-p:nan
epoch£º124	 i:3 	 global-step:2483	 l-p:nan
epoch£º124	 i:4 	 global-step:2484	 l-p:nan
epoch£º124	 i:5 	 global-step:2485	 l-p:nan
epoch£º124	 i:6 	 global-step:2486	 l-p:nan
epoch£º124	 i:7 	 global-step:2487	 l-p:nan
epoch£º124	 i:8 	 global-step:2488	 l-p:nan
epoch£º124	 i:9 	 global-step:2489	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:nan
epoch£º125	 i:1 	 global-step:2501	 l-p:nan
epoch£º125	 i:2 	 global-step:2502	 l-p:nan
epoch£º125	 i:3 	 global-step:2503	 l-p:nan
epoch£º125	 i:4 	 global-step:2504	 l-p:nan
epoch£º125	 i:5 	 global-step:2505	 l-p:nan
epoch£º125	 i:6 	 global-step:2506	 l-p:nan
epoch£º125	 i:7 	 global-step:2507	 l-p:nan
epoch£º125	 i:8 	 global-step:2508	 l-p:nan
epoch£º125	 i:9 	 global-step:2509	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:nan
epoch£º126	 i:1 	 global-step:2521	 l-p:nan
epoch£º126	 i:2 	 global-step:2522	 l-p:nan
epoch£º126	 i:3 	 global-step:2523	 l-p:nan
epoch£º126	 i:4 	 global-step:2524	 l-p:nan
epoch£º126	 i:5 	 global-step:2525	 l-p:nan
epoch£º126	 i:6 	 global-step:2526	 l-p:nan
epoch£º126	 i:7 	 global-step:2527	 l-p:nan
epoch£º126	 i:8 	 global-step:2528	 l-p:nan
epoch£º126	 i:9 	 global-step:2529	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:nan
epoch£º127	 i:1 	 global-step:2541	 l-p:nan
epoch£º127	 i:2 	 global-step:2542	 l-p:nan
epoch£º127	 i:3 	 global-step:2543	 l-p:nan
epoch£º127	 i:4 	 global-step:2544	 l-p:nan
epoch£º127	 i:5 	 global-step:2545	 l-p:nan
epoch£º127	 i:6 	 global-step:2546	 l-p:nan
epoch£º127	 i:7 	 global-step:2547	 l-p:nan
epoch£º127	 i:8 	 global-step:2548	 l-p:nan
epoch£º127	 i:9 	 global-step:2549	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:nan
epoch£º128	 i:1 	 global-step:2561	 l-p:nan
epoch£º128	 i:2 	 global-step:2562	 l-p:nan
epoch£º128	 i:3 	 global-step:2563	 l-p:nan
epoch£º128	 i:4 	 global-step:2564	 l-p:nan
epoch£º128	 i:5 	 global-step:2565	 l-p:nan
epoch£º128	 i:6 	 global-step:2566	 l-p:nan
epoch£º128	 i:7 	 global-step:2567	 l-p:nan
epoch£º128	 i:8 	 global-step:2568	 l-p:nan
epoch£º128	 i:9 	 global-step:2569	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:nan
epoch£º129	 i:1 	 global-step:2581	 l-p:nan
epoch£º129	 i:2 	 global-step:2582	 l-p:nan
epoch£º129	 i:3 	 global-step:2583	 l-p:nan
epoch£º129	 i:4 	 global-step:2584	 l-p:nan
epoch£º129	 i:5 	 global-step:2585	 l-p:nan
epoch£º129	 i:6 	 global-step:2586	 l-p:nan
epoch£º129	 i:7 	 global-step:2587	 l-p:nan
epoch£º129	 i:8 	 global-step:2588	 l-p:nan
epoch£º129	 i:9 	 global-step:2589	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:nan
epoch£º130	 i:1 	 global-step:2601	 l-p:nan
epoch£º130	 i:2 	 global-step:2602	 l-p:nan
epoch£º130	 i:3 	 global-step:2603	 l-p:nan
epoch£º130	 i:4 	 global-step:2604	 l-p:nan
epoch£º130	 i:5 	 global-step:2605	 l-p:nan
epoch£º130	 i:6 	 global-step:2606	 l-p:nan
epoch£º130	 i:7 	 global-step:2607	 l-p:nan
epoch£º130	 i:8 	 global-step:2608	 l-p:nan
epoch£º130	 i:9 	 global-step:2609	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1654,  0.0908,  1.0000,  0.0498,
          1.0000,  0.5489, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1441,  0.0755,  1.0000,  0.0396,
          1.0000,  0.5243, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2414,  0.1503,  1.0000,  0.0936,
          1.0000,  0.6227, 31.6228]], device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:nan
epoch£º131	 i:1 	 global-step:2621	 l-p:nan
epoch£º131	 i:2 	 global-step:2622	 l-p:nan
epoch£º131	 i:3 	 global-step:2623	 l-p:nan
epoch£º131	 i:4 	 global-step:2624	 l-p:nan
epoch£º131	 i:5 	 global-step:2625	 l-p:nan
epoch£º131	 i:6 	 global-step:2626	 l-p:nan
epoch£º131	 i:7 	 global-step:2627	 l-p:nan
epoch£º131	 i:8 	 global-step:2628	 l-p:nan
epoch£º131	 i:9 	 global-step:2629	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:nan
epoch£º132	 i:1 	 global-step:2641	 l-p:nan
epoch£º132	 i:2 	 global-step:2642	 l-p:nan
epoch£º132	 i:3 	 global-step:2643	 l-p:nan
epoch£º132	 i:4 	 global-step:2644	 l-p:nan
epoch£º132	 i:5 	 global-step:2645	 l-p:nan
epoch£º132	 i:6 	 global-step:2646	 l-p:nan
epoch£º132	 i:7 	 global-step:2647	 l-p:nan
epoch£º132	 i:8 	 global-step:2648	 l-p:nan
epoch£º132	 i:9 	 global-step:2649	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:nan
epoch£º133	 i:1 	 global-step:2661	 l-p:nan
epoch£º133	 i:2 	 global-step:2662	 l-p:nan
epoch£º133	 i:3 	 global-step:2663	 l-p:nan
epoch£º133	 i:4 	 global-step:2664	 l-p:nan
epoch£º133	 i:5 	 global-step:2665	 l-p:nan
epoch£º133	 i:6 	 global-step:2666	 l-p:nan
epoch£º133	 i:7 	 global-step:2667	 l-p:nan
epoch£º133	 i:8 	 global-step:2668	 l-p:nan
epoch£º133	 i:9 	 global-step:2669	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:nan
epoch£º134	 i:1 	 global-step:2681	 l-p:nan
epoch£º134	 i:2 	 global-step:2682	 l-p:nan
epoch£º134	 i:3 	 global-step:2683	 l-p:nan
epoch£º134	 i:4 	 global-step:2684	 l-p:nan
epoch£º134	 i:5 	 global-step:2685	 l-p:nan
epoch£º134	 i:6 	 global-step:2686	 l-p:nan
epoch£º134	 i:7 	 global-step:2687	 l-p:nan
epoch£º134	 i:8 	 global-step:2688	 l-p:nan
epoch£º134	 i:9 	 global-step:2689	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:nan
epoch£º135	 i:1 	 global-step:2701	 l-p:nan
epoch£º135	 i:2 	 global-step:2702	 l-p:nan
epoch£º135	 i:3 	 global-step:2703	 l-p:nan
epoch£º135	 i:4 	 global-step:2704	 l-p:nan
epoch£º135	 i:5 	 global-step:2705	 l-p:nan
epoch£º135	 i:6 	 global-step:2706	 l-p:nan
epoch£º135	 i:7 	 global-step:2707	 l-p:nan
epoch£º135	 i:8 	 global-step:2708	 l-p:nan
epoch£º135	 i:9 	 global-step:2709	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:nan
epoch£º136	 i:1 	 global-step:2721	 l-p:nan
epoch£º136	 i:2 	 global-step:2722	 l-p:nan
epoch£º136	 i:3 	 global-step:2723	 l-p:nan
epoch£º136	 i:4 	 global-step:2724	 l-p:nan
epoch£º136	 i:5 	 global-step:2725	 l-p:nan
epoch£º136	 i:6 	 global-step:2726	 l-p:nan
epoch£º136	 i:7 	 global-step:2727	 l-p:nan
epoch£º136	 i:8 	 global-step:2728	 l-p:nan
epoch£º136	 i:9 	 global-step:2729	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:nan
epoch£º137	 i:1 	 global-step:2741	 l-p:nan
epoch£º137	 i:2 	 global-step:2742	 l-p:nan
epoch£º137	 i:3 	 global-step:2743	 l-p:nan
epoch£º137	 i:4 	 global-step:2744	 l-p:nan
epoch£º137	 i:5 	 global-step:2745	 l-p:nan
epoch£º137	 i:6 	 global-step:2746	 l-p:nan
epoch£º137	 i:7 	 global-step:2747	 l-p:nan
epoch£º137	 i:8 	 global-step:2748	 l-p:nan
epoch£º137	 i:9 	 global-step:2749	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:nan
epoch£º138	 i:1 	 global-step:2761	 l-p:nan
epoch£º138	 i:2 	 global-step:2762	 l-p:nan
epoch£º138	 i:3 	 global-step:2763	 l-p:nan
epoch£º138	 i:4 	 global-step:2764	 l-p:nan
epoch£º138	 i:5 	 global-step:2765	 l-p:nan
epoch£º138	 i:6 	 global-step:2766	 l-p:nan
epoch£º138	 i:7 	 global-step:2767	 l-p:nan
epoch£º138	 i:8 	 global-step:2768	 l-p:nan
epoch£º138	 i:9 	 global-step:2769	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan],
        [nan, nan, nan]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): nan 
model_pd.l_d.mean(): nan 
model_pd.lagr.mean(): nan 
model_pd.lambdas: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([nan], device='cuda:0')), ('power', tensor([nan], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:nan
epoch£º139	 i:1 	 global-step:2781	 l-p:nan
epoch£º139	 i:2 	 global-step:2782	 l-p:nan
epoch£º139	 i:3 	 global-step:2783	 l-p:nan
epoch£º139	 i:4 	 global-step:2784	 l-p:nan
epoch£º139	 i:5 	 global-step:2785	 l-p:nan
epoch£º139	 i:6 	 global-step:2786	 l-p:nan
epoch£º139	 i:7 	 global-step:2787	 l-p:nan
epoch£º139	 i:8 	 global-step:2788	 l-p:nan
epoch£º139	 i:9 	 global-step:2789	 l-p:nan
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
